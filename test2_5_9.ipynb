{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test2_5_9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjisuccess/mycolab2/blob/master/test2_5_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTIT8ryQyxk6",
        "colab_type": "code",
        "outputId": "2e647783-d4d4-4ea9-c80f-8e5661748304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130812 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBdV-99uzy-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCGw6l_66tPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEgl2_E6vP6",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第一次\n",
        "我在训练文件所在的同级目录下建立交互文件试运行相关文件，但是不行，仍提示找不到模型，所以在云端跑无论ipynb文件在哪里，模型的读取路径都必须是绝对路径，在这里就是drive/unsupervisedfmnet/Shapes/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "114BR6og0AMP",
        "colab_type": "code",
        "outputId": "32e0f4d6-1692-4c7b-dbaf-94a5a920d99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1112
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=./Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 13:40:03.415047: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 13:40:03.415415: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x29cf180 executing computations on platform Host. Devices:\n",
            "2019-05-09 13:40:03.415456: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 13:40:03.577930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 13:40:03.578561: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x29cf440 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 13:40:03.578601: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 13:40:03.578978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 13:40:03.579014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 13:40:05.149256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 13:40:05.149318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 13:40:05.149338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 13:40:05.149659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "loading data to ram...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\", line 31, in _open_file\n",
            "    return open(file_like, 'rb'), True\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './Shapes/tr_reg_000.mat'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 235, in run_training\n",
            "    load_targets_to_ram()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 144, in load_targets_to_ram\n",
            "    input_data = sio.loadmat(target_file)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\", line 207, in loadmat\n",
            "    MR, file_opened = mat_reader_factory(file_name, appendmat, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\", line 62, in mat_reader_factory\n",
            "    byte_stream, file_opened = _open_file(file_name, appendmat)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\", line 37, in _open_file\n",
            "    return open(file_like, 'rb'), True\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './Shapes/tr_reg_000.mat'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pTleRj6RX4",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第二次\n",
        "模型顶点数为3000，batch_size为16，溢出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBq1dfYN1BRW",
        "colab_type": "code",
        "outputId": "0159b0e6-6bea-40c9-dac6-58202fe3191b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14518
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 13:42:09.719663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 13:42:09.719908: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e31180 executing computations on platform Host. Devices:\n",
            "2019-05-09 13:42:09.719947: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 13:42:09.811118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 13:42:09.811704: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e31440 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 13:42:09.811748: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 13:42:09.812151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 13:42:09.812192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 13:42:10.201818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 13:42:10.201920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 13:42:10.201940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 13:42:10.202334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 13:52:05.976061: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 13:52:07.357206: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x82038e0\n",
            "2019-05-09 13:52:19.083072: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.50GiB.  Current allocation summary follows.\n",
            "2019-05-09 13:52:19.083189: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 115, Chunks in use: 114. 28.8KiB allocated for chunks. 28.5KiB in use in bin. 464B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083240: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 3, Chunks in use: 0. 1.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083267: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 378, Chunks in use: 378. 566.8KiB allocated for chunks. 566.8KiB in use in bin. 519.4KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083288: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 2, Chunks in use: 1. 5.5KiB allocated for chunks. 2.2KiB in use in bin. 1.4KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083308: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 2, Chunks in use: 2. 15.0KiB allocated for chunks. 15.0KiB in use in bin. 15.0KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083328: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 3, Chunks in use: 2. 31.5KiB allocated for chunks. 17.5KiB in use in bin. 17.5KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083378: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083395: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083414: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083432: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083453: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 40, Chunks in use: 40. 18.91MiB allocated for chunks. 18.91MiB in use in bin. 18.91MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083474: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 22, Chunks in use: 22. 18.88MiB allocated for chunks. 18.88MiB in use in bin. 18.12MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083494: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 3. 4.88MiB allocated for chunks. 4.88MiB in use in bin. 2.64MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083513: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 5.16MiB allocated for chunks. 5.16MiB in use in bin. 5.16MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083531: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083549: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083570: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 5, Chunks in use: 4. 110.54MiB allocated for chunks. 87.89MiB in use in bin. 87.89MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083591: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 1. 123.05MiB allocated for chunks. 61.52MiB in use in bin. 61.52MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083611: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 59, Chunks in use: 57. 3.94GiB allocated for chunks. 3.81GiB in use in bin. 3.59GiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083632: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 64.45MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083668: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 3. 6.16GiB allocated for chunks. 4.51GiB in use in bin. 4.51GiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083703: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 1.50GiB was 256.00MiB, Chunk State: \n",
            "2019-05-09 13:52:19.083737: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 334.28MiB | Requested Size: 12B | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083774: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 369.14MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 61.52MiB | Requested Size: 61.52MiB | in_use: 1, next:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083797: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 993.45MiB | Requested Size: 12B | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0000 of size 256\n",
            "2019-05-09 13:52:19.083835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0100 of size 256\n",
            "2019-05-09 13:52:19.083860: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0200 of size 495616\n",
            "2019-05-09 13:52:19.083888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039200 of size 1536\n",
            "2019-05-09 13:52:19.083906: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039800 of size 256\n",
            "2019-05-09 13:52:19.083921: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039900 of size 1536\n",
            "2019-05-09 13:52:19.083936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039f00 of size 1536\n",
            "2019-05-09 13:52:19.083952: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403a500 of size 1536\n",
            "2019-05-09 13:52:19.083967: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403ab00 of size 1536\n",
            "2019-05-09 13:52:19.083983: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403b100 of size 1536\n",
            "2019-05-09 13:52:19.084000: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403b700 of size 1536\n",
            "2019-05-09 13:52:19.084022: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403bd00 of size 1536\n",
            "2019-05-09 13:52:19.084078: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403c300 of size 1536\n",
            "2019-05-09 13:52:19.084096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403c900 of size 1536\n",
            "2019-05-09 13:52:19.084112: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403cf00 of size 1536\n",
            "2019-05-09 13:52:19.084127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403d500 of size 1536\n",
            "2019-05-09 13:52:19.084173: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403db00 of size 256\n",
            "2019-05-09 13:52:19.084203: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403dc00 of size 1536\n",
            "2019-05-09 13:52:19.084234: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e200 of size 256\n",
            "2019-05-09 13:52:19.084250: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e300 of size 1536\n",
            "2019-05-09 13:52:19.084267: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e900 of size 1536\n",
            "2019-05-09 13:52:19.084298: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403ef00 of size 1536\n",
            "2019-05-09 13:52:19.084313: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403f500 of size 1536\n",
            "2019-05-09 13:52:19.084329: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403fb00 of size 1536\n",
            "2019-05-09 13:52:19.084356: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040100 of size 1536\n",
            "2019-05-09 13:52:19.084372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040700 of size 1536\n",
            "2019-05-09 13:52:19.084385: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040d00 of size 1536\n",
            "2019-05-09 13:52:19.084398: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041300 of size 1536\n",
            "2019-05-09 13:52:19.084413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041900 of size 1536\n",
            "2019-05-09 13:52:19.084428: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041f00 of size 1536\n",
            "2019-05-09 13:52:19.084443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042500 of size 256\n",
            "2019-05-09 13:52:19.084459: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042600 of size 1536\n",
            "2019-05-09 13:52:19.084471: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042c00 of size 256\n",
            "2019-05-09 13:52:19.084489: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042d00 of size 1536\n",
            "2019-05-09 13:52:19.084506: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043300 of size 1536\n",
            "2019-05-09 13:52:19.084521: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043900 of size 1536\n",
            "2019-05-09 13:52:19.084537: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043f00 of size 1536\n",
            "2019-05-09 13:52:19.084552: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704044500 of size 1536\n",
            "2019-05-09 13:52:19.084570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704044b00 of size 1536\n",
            "2019-05-09 13:52:19.084587: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045100 of size 1536\n",
            "2019-05-09 13:52:19.084603: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045700 of size 1536\n",
            "2019-05-09 13:52:19.084618: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045d00 of size 1536\n",
            "2019-05-09 13:52:19.084634: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046300 of size 1536\n",
            "2019-05-09 13:52:19.084650: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046900 of size 1536\n",
            "2019-05-09 13:52:19.084665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046f00 of size 256\n",
            "2019-05-09 13:52:19.084680: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047000 of size 1536\n",
            "2019-05-09 13:52:19.084697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047600 of size 256\n",
            "2019-05-09 13:52:19.084712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047700 of size 1536\n",
            "2019-05-09 13:52:19.084742: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047d00 of size 1536\n",
            "2019-05-09 13:52:19.084757: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048300 of size 1536\n",
            "2019-05-09 13:52:19.084772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048900 of size 1536\n",
            "2019-05-09 13:52:19.084788: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048f00 of size 1536\n",
            "2019-05-09 13:52:19.084803: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704049500 of size 1536\n",
            "2019-05-09 13:52:19.084818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704049b00 of size 1536\n",
            "2019-05-09 13:52:19.084833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404a100 of size 1536\n",
            "2019-05-09 13:52:19.084848: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404a700 of size 1536\n",
            "2019-05-09 13:52:19.084863: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404ad00 of size 1536\n",
            "2019-05-09 13:52:19.084889: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404b300 of size 1536\n",
            "2019-05-09 13:52:19.084905: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404b900 of size 256\n",
            "2019-05-09 13:52:19.084921: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404ba00 of size 1536\n",
            "2019-05-09 13:52:19.084936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c000 of size 256\n",
            "2019-05-09 13:52:19.084951: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c100 of size 1536\n",
            "2019-05-09 13:52:19.084966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c700 of size 1536\n",
            "2019-05-09 13:52:19.084981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404cd00 of size 1536\n",
            "2019-05-09 13:52:19.084997: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404d300 of size 1536\n",
            "2019-05-09 13:52:19.085012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404d900 of size 1536\n",
            "2019-05-09 13:52:19.085039: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404df00 of size 1536\n",
            "2019-05-09 13:52:19.085057: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404e500 of size 1536\n",
            "2019-05-09 13:52:19.085073: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404eb00 of size 1536\n",
            "2019-05-09 13:52:19.085088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404f100 of size 1536\n",
            "2019-05-09 13:52:19.085102: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404f700 of size 1536\n",
            "2019-05-09 13:52:19.085117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404fd00 of size 1536\n",
            "2019-05-09 13:52:19.085132: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050300 of size 256\n",
            "2019-05-09 13:52:19.085147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050400 of size 1536\n",
            "2019-05-09 13:52:19.085161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050a00 of size 256\n",
            "2019-05-09 13:52:19.085176: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050b00 of size 1536\n",
            "2019-05-09 13:52:19.085191: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051100 of size 1536\n",
            "2019-05-09 13:52:19.085207: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051700 of size 1536\n",
            "2019-05-09 13:52:19.085221: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051d00 of size 1536\n",
            "2019-05-09 13:52:19.085236: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052300 of size 1536\n",
            "2019-05-09 13:52:19.085251: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052900 of size 1536\n",
            "2019-05-09 13:52:19.085266: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052f00 of size 1536\n",
            "2019-05-09 13:52:19.085280: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704053500 of size 1536\n",
            "2019-05-09 13:52:19.085295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704053b00 of size 1536\n",
            "2019-05-09 13:52:19.085311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054100 of size 1536\n",
            "2019-05-09 13:52:19.085325: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054700 of size 1536\n",
            "2019-05-09 13:52:19.085350: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054d00 of size 256\n",
            "2019-05-09 13:52:19.085367: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054e00 of size 1536\n",
            "2019-05-09 13:52:19.085401: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055400 of size 256\n",
            "2019-05-09 13:52:19.085417: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055500 of size 1536\n",
            "2019-05-09 13:52:19.085449: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055b00 of size 1536\n",
            "2019-05-09 13:52:19.085464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056100 of size 1536\n",
            "2019-05-09 13:52:19.085479: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056700 of size 1536\n",
            "2019-05-09 13:52:19.085508: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056d00 of size 1536\n",
            "2019-05-09 13:52:19.085524: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057300 of size 1536\n",
            "2019-05-09 13:52:19.085540: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057900 of size 1536\n",
            "2019-05-09 13:52:19.085556: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057f00 of size 1536\n",
            "2019-05-09 13:52:19.085571: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704058500 of size 1536\n",
            "2019-05-09 13:52:19.085586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704058b00 of size 1536\n",
            "2019-05-09 13:52:19.085602: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059100 of size 1536\n",
            "2019-05-09 13:52:19.085617: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059700 of size 256\n",
            "2019-05-09 13:52:19.085632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059800 of size 256\n",
            "2019-05-09 13:52:19.085648: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059900 of size 1280\n",
            "2019-05-09 13:52:19.085663: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059e00 of size 1536\n",
            "2019-05-09 13:52:19.085679: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405a400 of size 1536\n",
            "2019-05-09 13:52:19.085694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405aa00 of size 1536\n",
            "2019-05-09 13:52:19.085709: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405b000 of size 1536\n",
            "2019-05-09 13:52:19.085725: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405b600 of size 1536\n",
            "2019-05-09 13:52:19.168645: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405bc00 of size 1536\n",
            "2019-05-09 13:52:19.168718: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405c200 of size 1536\n",
            "2019-05-09 13:52:19.168739: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405c800 of size 1536\n",
            "2019-05-09 13:52:19.168756: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405ce00 of size 1536\n",
            "2019-05-09 13:52:19.168772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405d400 of size 1536\n",
            "2019-05-09 13:52:19.168788: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405da00 of size 1536\n",
            "2019-05-09 13:52:19.168805: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405e000 of size 1536\n",
            "2019-05-09 13:52:19.168824: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405e600 of size 1536\n",
            "2019-05-09 13:52:19.168841: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405ec00 of size 1536\n",
            "2019-05-09 13:52:19.168905: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405f200 of size 1536\n",
            "2019-05-09 13:52:19.168941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405f800 of size 1536\n",
            "2019-05-09 13:52:19.168961: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405fe00 of size 1536\n",
            "2019-05-09 13:52:19.168995: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704060400 of size 1536\n",
            "2019-05-09 13:52:19.169089: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704060a00 of size 1536\n",
            "2019-05-09 13:52:19.169135: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061000 of size 1536\n",
            "2019-05-09 13:52:19.169154: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061600 of size 1536\n",
            "2019-05-09 13:52:19.169173: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061c00 of size 1536\n",
            "2019-05-09 13:52:19.169190: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062200 of size 1536\n",
            "2019-05-09 13:52:19.169206: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062800 of size 1536\n",
            "2019-05-09 13:52:19.169224: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062e00 of size 1536\n",
            "2019-05-09 13:52:19.169243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704063400 of size 1536\n",
            "2019-05-09 13:52:19.169262: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704063a00 of size 1536\n",
            "2019-05-09 13:52:19.169281: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064000 of size 1536\n",
            "2019-05-09 13:52:19.169298: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064600 of size 1536\n",
            "2019-05-09 13:52:19.169315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064c00 of size 1536\n",
            "2019-05-09 13:52:19.169332: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065200 of size 1536\n",
            "2019-05-09 13:52:19.169369: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065800 of size 1536\n",
            "2019-05-09 13:52:19.169389: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065e00 of size 1536\n",
            "2019-05-09 13:52:19.169408: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704066400 of size 1536\n",
            "2019-05-09 13:52:19.169426: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704066a00 of size 1536\n",
            "2019-05-09 13:52:19.169443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067000 of size 1536\n",
            "2019-05-09 13:52:19.169460: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067600 of size 1536\n",
            "2019-05-09 13:52:19.169477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067c00 of size 1536\n",
            "2019-05-09 13:52:19.169493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068200 of size 1536\n",
            "2019-05-09 13:52:19.169510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068800 of size 1536\n",
            "2019-05-09 13:52:19.169528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068e00 of size 1536\n",
            "2019-05-09 13:52:19.169712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704069400 of size 1536\n",
            "2019-05-09 13:52:19.169741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704069a00 of size 1536\n",
            "2019-05-09 13:52:19.169763: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406a000 of size 1536\n",
            "2019-05-09 13:52:19.169797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406a600 of size 1536\n",
            "2019-05-09 13:52:19.169816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ac00 of size 1536\n",
            "2019-05-09 13:52:19.169834: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406b200 of size 1536\n",
            "2019-05-09 13:52:19.169853: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406b800 of size 1536\n",
            "2019-05-09 13:52:19.169881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406be00 of size 1536\n",
            "2019-05-09 13:52:19.169903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406c400 of size 1536\n",
            "2019-05-09 13:52:19.169922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ca00 of size 1536\n",
            "2019-05-09 13:52:19.169941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406d000 of size 1536\n",
            "2019-05-09 13:52:19.169959: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406d600 of size 1536\n",
            "2019-05-09 13:52:19.169992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406dc00 of size 1536\n",
            "2019-05-09 13:52:19.170011: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406e200 of size 1536\n",
            "2019-05-09 13:52:19.170054: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406e800 of size 1536\n",
            "2019-05-09 13:52:19.170080: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ee00 of size 1536\n",
            "2019-05-09 13:52:19.170100: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406f400 of size 1536\n",
            "2019-05-09 13:52:19.170120: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406fa00 of size 1536\n",
            "2019-05-09 13:52:19.170139: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070000 of size 1536\n",
            "2019-05-09 13:52:19.170170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070600 of size 1536\n",
            "2019-05-09 13:52:19.170192: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070c00 of size 1536\n",
            "2019-05-09 13:52:19.170210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071200 of size 1536\n",
            "2019-05-09 13:52:19.170228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071800 of size 1536\n",
            "2019-05-09 13:52:19.170248: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071e00 of size 1536\n",
            "2019-05-09 13:52:19.170267: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704072400 of size 1536\n",
            "2019-05-09 13:52:19.170287: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704072a00 of size 1536\n",
            "2019-05-09 13:52:19.170306: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073000 of size 1536\n",
            "2019-05-09 13:52:19.170324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073600 of size 1536\n",
            "2019-05-09 13:52:19.170360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073c00 of size 1536\n",
            "2019-05-09 13:52:19.170383: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074200 of size 1536\n",
            "2019-05-09 13:52:19.170403: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074800 of size 1536\n",
            "2019-05-09 13:52:19.170422: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074e00 of size 1536\n",
            "2019-05-09 13:52:19.170441: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704075400 of size 1536\n",
            "2019-05-09 13:52:19.170459: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704075a00 of size 1536\n",
            "2019-05-09 13:52:19.170477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076000 of size 1536\n",
            "2019-05-09 13:52:19.170496: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076600 of size 1536\n",
            "2019-05-09 13:52:19.170514: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076c00 of size 1536\n",
            "2019-05-09 13:52:19.170533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077200 of size 1536\n",
            "2019-05-09 13:52:19.170553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077800 of size 1536\n",
            "2019-05-09 13:52:19.170573: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077e00 of size 1536\n",
            "2019-05-09 13:52:19.170591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704078400 of size 1536\n",
            "2019-05-09 13:52:19.170619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704078a00 of size 1536\n",
            "2019-05-09 13:52:19.170640: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079000 of size 1536\n",
            "2019-05-09 13:52:19.170659: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079600 of size 1536\n",
            "2019-05-09 13:52:19.170678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079c00 of size 1536\n",
            "2019-05-09 13:52:19.170697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407a200 of size 1536\n",
            "2019-05-09 13:52:19.170715: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407a800 of size 1536\n",
            "2019-05-09 13:52:19.170734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ae00 of size 1536\n",
            "2019-05-09 13:52:19.170753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407b400 of size 1536\n",
            "2019-05-09 13:52:19.170773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ba00 of size 1536\n",
            "2019-05-09 13:52:19.170792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407c000 of size 1536\n",
            "2019-05-09 13:52:19.170812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407c600 of size 1536\n",
            "2019-05-09 13:52:19.170832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407cc00 of size 1536\n",
            "2019-05-09 13:52:19.170851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407d200 of size 1536\n",
            "2019-05-09 13:52:19.170882: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407d800 of size 1536\n",
            "2019-05-09 13:52:19.170904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407de00 of size 1536\n",
            "2019-05-09 13:52:19.170922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407e400 of size 1536\n",
            "2019-05-09 13:52:19.170941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ea00 of size 1536\n",
            "2019-05-09 13:52:19.170962: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407f000 of size 1536\n",
            "2019-05-09 13:52:19.170981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407f600 of size 1536\n",
            "2019-05-09 13:52:19.171000: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407fc00 of size 1536\n",
            "2019-05-09 13:52:19.171018: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080200 of size 1536\n",
            "2019-05-09 13:52:19.171063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080800 of size 1536\n",
            "2019-05-09 13:52:19.171085: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080e00 of size 1536\n",
            "2019-05-09 13:52:19.171103: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704081400 of size 1536\n",
            "2019-05-09 13:52:19.171122: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704081a00 of size 1536\n",
            "2019-05-09 13:52:19.171141: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082000 of size 1536\n",
            "2019-05-09 13:52:19.171160: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082600 of size 1536\n",
            "2019-05-09 13:52:19.171180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082c00 of size 1536\n",
            "2019-05-09 13:52:19.171199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083200 of size 1536\n",
            "2019-05-09 13:52:19.171218: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083800 of size 1536\n",
            "2019-05-09 13:52:19.171237: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083e00 of size 1536\n",
            "2019-05-09 13:52:19.171255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704084400 of size 1536\n",
            "2019-05-09 13:52:19.171289: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704084a00 of size 1536\n",
            "2019-05-09 13:52:19.171308: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085000 of size 1536\n",
            "2019-05-09 13:52:19.171326: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085600 of size 1536\n",
            "2019-05-09 13:52:19.171360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085c00 of size 1536\n",
            "2019-05-09 13:52:19.171380: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086200 of size 1536\n",
            "2019-05-09 13:52:19.171415: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086800 of size 1536\n",
            "2019-05-09 13:52:19.171433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086e00 of size 1536\n",
            "2019-05-09 13:52:19.171452: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704087400 of size 1536\n",
            "2019-05-09 13:52:19.171472: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704087a00 of size 1536\n",
            "2019-05-09 13:52:19.171492: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088000 of size 1536\n",
            "2019-05-09 13:52:19.171510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088600 of size 1536\n",
            "2019-05-09 13:52:19.171533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088c00 of size 1536\n",
            "2019-05-09 13:52:19.171553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089200 of size 1536\n",
            "2019-05-09 13:52:19.171570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089800 of size 1536\n",
            "2019-05-09 13:52:19.171588: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089e00 of size 1536\n",
            "2019-05-09 13:52:19.171607: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408a400 of size 1536\n",
            "2019-05-09 13:52:19.171625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408aa00 of size 1536\n",
            "2019-05-09 13:52:19.171644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b000 of size 1536\n",
            "2019-05-09 13:52:19.171664: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b600 of size 256\n",
            "2019-05-09 13:52:19.171683: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b700 of size 256\n",
            "2019-05-09 13:52:19.171703: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b800 of size 1536\n",
            "2019-05-09 13:52:19.171722: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408be00 of size 1536\n",
            "2019-05-09 13:52:19.171741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408c400 of size 1536\n",
            "2019-05-09 13:52:19.171760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408ca00 of size 1536\n",
            "2019-05-09 13:52:19.171778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408d000 of size 1536\n",
            "2019-05-09 13:52:19.171796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408d600 of size 1536\n",
            "2019-05-09 13:52:19.171814: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408dc00 of size 1536\n",
            "2019-05-09 13:52:19.171832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408e200 of size 1536\n",
            "2019-05-09 13:52:19.171851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408e800 of size 1536\n",
            "2019-05-09 13:52:19.171880: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408ee00 of size 1536\n",
            "2019-05-09 13:52:19.171903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408f400 of size 1536\n",
            "2019-05-09 13:52:19.171922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408fa00 of size 1536\n",
            "2019-05-09 13:52:19.171950: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090000 of size 1536\n",
            "2019-05-09 13:52:19.171971: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090600 of size 1536\n",
            "2019-05-09 13:52:19.171989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090c00 of size 1536\n",
            "2019-05-09 13:52:19.172008: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091200 of size 1536\n",
            "2019-05-09 13:52:19.172027: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091800 of size 1536\n",
            "2019-05-09 13:52:19.172074: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091e00 of size 1536\n",
            "2019-05-09 13:52:19.172095: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704092400 of size 1536\n",
            "2019-05-09 13:52:19.172115: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704092a00 of size 1536\n",
            "2019-05-09 13:52:19.172133: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093000 of size 1536\n",
            "2019-05-09 13:52:19.172152: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093600 of size 1536\n",
            "2019-05-09 13:52:19.172171: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093c00 of size 256\n",
            "2019-05-09 13:52:19.172190: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093d00 of size 256\n",
            "2019-05-09 13:52:19.172210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093e00 of size 256\n",
            "2019-05-09 13:52:19.172228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093f00 of size 256\n",
            "2019-05-09 13:52:19.172247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094000 of size 256\n",
            "2019-05-09 13:52:19.172265: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094100 of size 256\n",
            "2019-05-09 13:52:19.172284: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094200 of size 256\n",
            "2019-05-09 13:52:19.172302: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094300 of size 256\n",
            "2019-05-09 13:52:19.172322: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094400 of size 256\n",
            "2019-05-09 13:52:19.172359: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094500 of size 256\n",
            "2019-05-09 13:52:19.172382: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094600 of size 256\n",
            "2019-05-09 13:52:19.172402: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094700 of size 1536\n",
            "2019-05-09 13:52:19.172431: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094d00 of size 1536\n",
            "2019-05-09 13:52:19.172450: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095300 of size 1536\n",
            "2019-05-09 13:52:19.172469: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095900 of size 1536\n",
            "2019-05-09 13:52:19.172488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095f00 of size 1536\n",
            "2019-05-09 13:52:19.172507: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704096500 of size 1536\n",
            "2019-05-09 13:52:19.172526: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704096b00 of size 1536\n",
            "2019-05-09 13:52:19.172544: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097100 of size 1536\n",
            "2019-05-09 13:52:19.172563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097700 of size 1536\n",
            "2019-05-09 13:52:19.172581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097d00 of size 1536\n",
            "2019-05-09 13:52:19.172601: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098300 of size 1536\n",
            "2019-05-09 13:52:19.172619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098900 of size 1536\n",
            "2019-05-09 13:52:19.172639: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098f00 of size 1536\n",
            "2019-05-09 13:52:19.172658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704099500 of size 1536\n",
            "2019-05-09 13:52:19.172676: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704099b00 of size 1536\n",
            "2019-05-09 13:52:19.172694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409a100 of size 1536\n",
            "2019-05-09 13:52:19.172712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409a700 of size 1536\n",
            "2019-05-09 13:52:19.172731: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409ad00 of size 1536\n",
            "2019-05-09 13:52:19.172750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409b300 of size 1536\n",
            "2019-05-09 13:52:19.172769: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409b900 of size 1536\n",
            "2019-05-09 13:52:19.172804: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409bf00 of size 1536\n",
            "2019-05-09 13:52:19.172828: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409c500 of size 1536\n",
            "2019-05-09 13:52:19.172851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409cb00 of size 1536\n",
            "2019-05-09 13:52:19.172881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409d100 of size 1536\n",
            "2019-05-09 13:52:19.172903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409d700 of size 1536\n",
            "2019-05-09 13:52:19.172922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409dd00 of size 1536\n",
            "2019-05-09 13:52:19.172942: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409e300 of size 1536\n",
            "2019-05-09 13:52:19.172962: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409e900 of size 1536\n",
            "2019-05-09 13:52:19.172980: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409ef00 of size 1536\n",
            "2019-05-09 13:52:19.172999: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409f500 of size 1536\n",
            "2019-05-09 13:52:19.173018: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409fb00 of size 1536\n",
            "2019-05-09 13:52:19.173072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0100 of size 1536\n",
            "2019-05-09 13:52:19.173095: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0700 of size 1536\n",
            "2019-05-09 13:52:19.173115: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0d00 of size 1536\n",
            "2019-05-09 13:52:19.173134: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1300 of size 1536\n",
            "2019-05-09 13:52:19.173152: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1900 of size 1536\n",
            "2019-05-09 13:52:19.173170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1f00 of size 1536\n",
            "2019-05-09 13:52:19.173189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a2500 of size 1536\n",
            "2019-05-09 13:52:19.173208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a2b00 of size 1536\n",
            "2019-05-09 13:52:19.173228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3100 of size 1536\n",
            "2019-05-09 13:52:19.173247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3700 of size 1536\n",
            "2019-05-09 13:52:19.173266: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3d00 of size 1536\n",
            "2019-05-09 13:52:19.173297: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4300 of size 1536\n",
            "2019-05-09 13:52:19.173331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4900 of size 1536\n",
            "2019-05-09 13:52:19.173398: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4f00 of size 1536\n",
            "2019-05-09 13:52:19.173419: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a5500 of size 1536\n",
            "2019-05-09 13:52:19.173438: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a5b00 of size 1536\n",
            "2019-05-09 13:52:19.173457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6100 of size 1536\n",
            "2019-05-09 13:52:19.173493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6700 of size 1536\n",
            "2019-05-09 13:52:19.173513: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6d00 of size 1536\n",
            "2019-05-09 13:52:19.173532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7300 of size 1536\n",
            "2019-05-09 13:52:19.173551: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7900 of size 1536\n",
            "2019-05-09 13:52:19.173570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7f00 of size 1536\n",
            "2019-05-09 13:52:19.173590: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a8500 of size 1536\n",
            "2019-05-09 13:52:19.173609: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a8b00 of size 1536\n",
            "2019-05-09 13:52:19.173628: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9100 of size 1536\n",
            "2019-05-09 13:52:19.173647: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9700 of size 256\n",
            "2019-05-09 13:52:19.173666: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9800 of size 256\n",
            "2019-05-09 13:52:19.173685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9900 of size 256\n",
            "2019-05-09 13:52:19.173703: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9a00 of size 256\n",
            "2019-05-09 13:52:19.173724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9b00 of size 256\n",
            "2019-05-09 13:52:19.173744: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9c00 of size 256\n",
            "2019-05-09 13:52:19.173763: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9d00 of size 256\n",
            "2019-05-09 13:52:19.173782: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9e00 of size 256\n",
            "2019-05-09 13:52:19.173801: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9f00 of size 256\n",
            "2019-05-09 13:52:19.173819: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa000 of size 256\n",
            "2019-05-09 13:52:19.173839: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa100 of size 256\n",
            "2019-05-09 13:52:19.173858: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa200 of size 256\n",
            "2019-05-09 13:52:19.173891: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa300 of size 256\n",
            "2019-05-09 13:52:19.173914: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa400 of size 256\n",
            "2019-05-09 13:52:19.173933: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa500 of size 256\n",
            "2019-05-09 13:52:19.173953: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa600 of size 256\n",
            "2019-05-09 13:52:19.173973: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa700 of size 256\n",
            "2019-05-09 13:52:19.173993: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa800 of size 256\n",
            "2019-05-09 13:52:19.174012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa900 of size 256\n",
            "2019-05-09 13:52:19.174055: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aaa00 of size 256\n",
            "2019-05-09 13:52:19.174078: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aab00 of size 256\n",
            "2019-05-09 13:52:19.174098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aac00 of size 256\n",
            "2019-05-09 13:52:19.174116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aad00 of size 256\n",
            "2019-05-09 13:52:19.174136: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aae00 of size 256\n",
            "2019-05-09 13:52:19.174156: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aaf00 of size 256\n",
            "2019-05-09 13:52:19.174176: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab000 of size 256\n",
            "2019-05-09 13:52:19.174196: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab100 of size 256\n",
            "2019-05-09 13:52:19.174216: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab200 of size 256\n",
            "2019-05-09 13:52:19.174235: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab300 of size 256\n",
            "2019-05-09 13:52:19.174254: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7040ab400 of size 512\n",
            "2019-05-09 13:52:19.174276: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab600 of size 7680\n",
            "2019-05-09 13:52:19.174295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ad400 of size 256\n",
            "2019-05-09 13:52:19.174315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ad500 of size 7680\n",
            "2019-05-09 13:52:19.174349: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040af300 of size 1536\n",
            "2019-05-09 13:52:19.174373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040af900 of size 1536\n",
            "2019-05-09 13:52:19.174393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aff00 of size 1536\n",
            "2019-05-09 13:52:19.174413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b0500 of size 1536\n",
            "2019-05-09 13:52:19.174433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b0b00 of size 1536\n",
            "2019-05-09 13:52:19.174452: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1100 of size 256\n",
            "2019-05-09 13:52:19.174472: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1200 of size 256\n",
            "2019-05-09 13:52:19.174491: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1300 of size 256\n",
            "2019-05-09 13:52:19.174510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1400 of size 1536\n",
            "2019-05-09 13:52:19.174528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1a00 of size 256\n",
            "2019-05-09 13:52:19.174547: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1b00 of size 1536\n",
            "2019-05-09 13:52:19.174566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2100 of size 1536\n",
            "2019-05-09 13:52:19.174586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2700 of size 1536\n",
            "2019-05-09 13:52:19.174606: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2d00 of size 1536\n",
            "2019-05-09 13:52:19.174625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3300 of size 256\n",
            "2019-05-09 13:52:19.174647: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3400 of size 1536\n",
            "2019-05-09 13:52:19.174666: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3a00 of size 256\n",
            "2019-05-09 13:52:19.174685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3b00 of size 256\n",
            "2019-05-09 13:52:19.174705: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3c00 of size 256\n",
            "2019-05-09 13:52:19.174724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3d00 of size 256\n",
            "2019-05-09 13:52:19.174745: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3e00 of size 256\n",
            "2019-05-09 13:52:19.174765: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3f00 of size 256\n",
            "2019-05-09 13:52:19.174785: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4000 of size 256\n",
            "2019-05-09 13:52:19.174819: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4100 of size 256\n",
            "2019-05-09 13:52:19.174838: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4200 of size 256\n",
            "2019-05-09 13:52:19.174857: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4300 of size 256\n",
            "2019-05-09 13:52:19.174888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4400 of size 256\n",
            "2019-05-09 13:52:19.174908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4500 of size 1536\n",
            "2019-05-09 13:52:19.174928: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4b00 of size 1536\n",
            "2019-05-09 13:52:19.174946: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5100 of size 1536\n",
            "2019-05-09 13:52:19.174965: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5700 of size 256\n",
            "2019-05-09 13:52:19.174985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5800 of size 1536\n",
            "2019-05-09 13:52:19.175005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5e00 of size 1536\n",
            "2019-05-09 13:52:19.175024: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6400 of size 1536\n",
            "2019-05-09 13:52:19.175067: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6a00 of size 256\n",
            "2019-05-09 13:52:19.175088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6b00 of size 256\n",
            "2019-05-09 13:52:19.175123: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6c00 of size 1536\n",
            "2019-05-09 13:52:19.175142: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7200 of size 1536\n",
            "2019-05-09 13:52:19.175161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7800 of size 1536\n",
            "2019-05-09 13:52:19.175180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7e00 of size 1536\n",
            "2019-05-09 13:52:19.175200: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b8400 of size 1536\n",
            "2019-05-09 13:52:19.175219: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b8a00 of size 1536\n",
            "2019-05-09 13:52:19.175239: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9000 of size 1536\n",
            "2019-05-09 13:52:19.175259: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9600 of size 1536\n",
            "2019-05-09 13:52:19.175279: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9c00 of size 1536\n",
            "2019-05-09 13:52:19.175299: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba200 of size 1536\n",
            "2019-05-09 13:52:19.175318: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba800 of size 256\n",
            "2019-05-09 13:52:19.175352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba900 of size 1536\n",
            "2019-05-09 13:52:19.175374: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040baf00 of size 256\n",
            "2019-05-09 13:52:19.175394: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb000 of size 256\n",
            "2019-05-09 13:52:19.175413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb100 of size 256\n",
            "2019-05-09 13:52:19.175433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb200 of size 256\n",
            "2019-05-09 13:52:19.175453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb300 of size 1536\n",
            "2019-05-09 13:52:19.175473: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb900 of size 256\n",
            "2019-05-09 13:52:19.175493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bba00 of size 1536\n",
            "2019-05-09 13:52:19.175512: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bc000 of size 1536\n",
            "2019-05-09 13:52:19.175531: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bc600 of size 1536\n",
            "2019-05-09 13:52:19.175549: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bcc00 of size 1536\n",
            "2019-05-09 13:52:19.175567: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bd200 of size 1536\n",
            "2019-05-09 13:52:19.175586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bd800 of size 1536\n",
            "2019-05-09 13:52:19.175605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bde00 of size 1536\n",
            "2019-05-09 13:52:19.175625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040be400 of size 1536\n",
            "2019-05-09 13:52:19.175644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bea00 of size 1536\n",
            "2019-05-09 13:52:19.175662: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf000 of size 1536\n",
            "2019-05-09 13:52:19.175682: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf600 of size 256\n",
            "2019-05-09 13:52:19.175702: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf700 of size 2304\n",
            "2019-05-09 13:52:19.175724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040c0000 of size 495616\n",
            "2019-05-09 13:52:19.175744: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139000 of size 495616\n",
            "2019-05-09 13:52:19.175777: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b2000 of size 495616\n",
            "2019-05-09 13:52:19.175795: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70422b000 of size 610304\n",
            "2019-05-09 13:52:19.175814: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0000 of size 495616\n",
            "2019-05-09 13:52:19.175833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704339000 of size 495616\n",
            "2019-05-09 13:52:19.175852: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b2000 of size 495616\n",
            "2019-05-09 13:52:19.175883: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442b000 of size 495616\n",
            "2019-05-09 13:52:19.175904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7044a4000 of size 495616\n",
            "2019-05-09 13:52:19.175929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70451d000 of size 495616\n",
            "2019-05-09 13:52:19.175947: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704596000 of size 495616\n",
            "2019-05-09 13:52:19.175966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70460f000 of size 724992\n",
            "2019-05-09 13:52:19.175986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7046c0000 of size 495616\n",
            "2019-05-09 13:52:19.176021: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739000 of size 495616\n",
            "2019-05-09 13:52:19.176066: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b2000 of size 495616\n",
            "2019-05-09 13:52:19.176087: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482b000 of size 495616\n",
            "2019-05-09 13:52:19.176107: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a4000 of size 495616\n",
            "2019-05-09 13:52:19.176127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70491d000 of size 495616\n",
            "2019-05-09 13:52:19.176147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704996000 of size 495616\n",
            "2019-05-09 13:52:19.176166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a0f000 of size 495616\n",
            "2019-05-09 13:52:19.176185: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a88000 of size 495616\n",
            "2019-05-09 13:52:19.176205: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b01000 of size 495616\n",
            "2019-05-09 13:52:19.176224: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b7a000 of size 495616\n",
            "2019-05-09 13:52:19.176243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704bf3000 of size 495616\n",
            "2019-05-09 13:52:19.176263: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c6c000 of size 495616\n",
            "2019-05-09 13:52:19.176282: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ce5000 of size 495616\n",
            "2019-05-09 13:52:19.176318: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704d5e000 of size 495616\n",
            "2019-05-09 13:52:19.176368: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704dd7000 of size 954368\n",
            "2019-05-09 13:52:19.176408: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0000 of size 495616\n",
            "2019-05-09 13:52:19.176444: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704f39000 of size 495616\n",
            "2019-05-09 13:52:19.176464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704fb2000 of size 495616\n",
            "2019-05-09 13:52:19.176482: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502b000 of size 495616\n",
            "2019-05-09 13:52:19.176501: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7050a4000 of size 495616\n",
            "2019-05-09 13:52:19.176521: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70511d000 of size 495616\n",
            "2019-05-09 13:52:19.176540: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705196000 of size 495616\n",
            "2019-05-09 13:52:19.176561: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70520f000 of size 495616\n",
            "2019-05-09 13:52:19.176580: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705288000 of size 495616\n",
            "2019-05-09 13:52:19.176599: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705301000 of size 495616\n",
            "2019-05-09 13:52:19.176619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70537a000 of size 495616\n",
            "2019-05-09 13:52:19.176638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7053f3000 of size 495616\n",
            "2019-05-09 13:52:19.176658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70546c000 of size 495616\n",
            "2019-05-09 13:52:19.176678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054e5000 of size 495616\n",
            "2019-05-09 13:52:19.176697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555e000 of size 1536\n",
            "2019-05-09 13:52:19.176716: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555e600 of size 1536\n",
            "2019-05-09 13:52:19.176735: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555ec00 of size 1536\n",
            "2019-05-09 13:52:19.176754: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f200 of size 256\n",
            "2019-05-09 13:52:19.176773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f300 of size 1536\n",
            "2019-05-09 13:52:19.176793: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f900 of size 256\n",
            "2019-05-09 13:52:19.176828: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555fa00 of size 256\n",
            "2019-05-09 13:52:19.176848: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555fb00 of size 1536\n",
            "2019-05-09 13:52:19.176876: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560100 of size 256\n",
            "2019-05-09 13:52:19.176899: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560200 of size 256\n",
            "2019-05-09 13:52:19.176918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560300 of size 256\n",
            "2019-05-09 13:52:19.176937: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560400 of size 256\n",
            "2019-05-09 13:52:19.176956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560500 of size 256\n",
            "2019-05-09 13:52:19.176976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560600 of size 256\n",
            "2019-05-09 13:52:19.176995: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560700 of size 256\n",
            "2019-05-09 13:52:19.177013: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560800 of size 256\n",
            "2019-05-09 13:52:19.177051: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560900 of size 256\n",
            "2019-05-09 13:52:19.177073: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560a00 of size 1536\n",
            "2019-05-09 13:52:19.177108: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561000 of size 1536\n",
            "2019-05-09 13:52:19.177128: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561600 of size 1536\n",
            "2019-05-09 13:52:19.177147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561c00 of size 1536\n",
            "2019-05-09 13:52:19.177166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562200 of size 1536\n",
            "2019-05-09 13:52:19.177185: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562800 of size 1536\n",
            "2019-05-09 13:52:19.177203: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562e00 of size 256\n",
            "2019-05-09 13:52:19.177223: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562f00 of size 256\n",
            "2019-05-09 13:52:19.177243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563000 of size 256\n",
            "2019-05-09 13:52:19.177274: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563100 of size 256\n",
            "2019-05-09 13:52:19.177294: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563200 of size 256\n",
            "2019-05-09 13:52:19.177314: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563300 of size 1536\n",
            "2019-05-09 13:52:19.177349: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563900 of size 1536\n",
            "2019-05-09 13:52:19.177373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563f00 of size 1536\n",
            "2019-05-09 13:52:19.177393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705564500 of size 1536\n",
            "2019-05-09 13:52:19.177413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705564b00 of size 1536\n",
            "2019-05-09 13:52:19.177432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565100 of size 1536\n",
            "2019-05-09 13:52:19.177453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565700 of size 256\n",
            "2019-05-09 13:52:19.177477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565800 of size 256\n",
            "2019-05-09 13:52:19.177503: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565900 of size 256\n",
            "2019-05-09 13:52:19.177523: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565a00 of size 256\n",
            "2019-05-09 13:52:19.177546: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565b00 of size 1536\n",
            "2019-05-09 13:52:19.177566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566100 of size 1536\n",
            "2019-05-09 13:52:19.177586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566700 of size 1536\n",
            "2019-05-09 13:52:19.177605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566d00 of size 1536\n",
            "2019-05-09 13:52:19.177624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567300 of size 1536\n",
            "2019-05-09 13:52:19.177643: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567900 of size 1536\n",
            "2019-05-09 13:52:19.177662: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567f00 of size 256\n",
            "2019-05-09 13:52:19.177682: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568000 of size 256\n",
            "2019-05-09 13:52:19.177702: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568100 of size 256\n",
            "2019-05-09 13:52:19.177721: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568200 of size 1536\n",
            "2019-05-09 13:52:19.177741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568800 of size 1536\n",
            "2019-05-09 13:52:19.177760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568e00 of size 1536\n",
            "2019-05-09 13:52:19.177778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705569400 of size 1536\n",
            "2019-05-09 13:52:19.177796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705569a00 of size 1536\n",
            "2019-05-09 13:52:19.177815: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556a000 of size 1536\n",
            "2019-05-09 13:52:19.177834: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556a600 of size 1536\n",
            "2019-05-09 13:52:19.177854: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556ac00 of size 1536\n",
            "2019-05-09 13:52:19.177886: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556b200 of size 1536\n",
            "2019-05-09 13:52:19.177909: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556b800 of size 1536\n",
            "2019-05-09 13:52:19.177929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556be00 of size 256\n",
            "2019-05-09 13:52:19.177949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556bf00 of size 1536\n",
            "2019-05-09 13:52:19.177969: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556c500 of size 512\n",
            "2019-05-09 13:52:19.177989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556c700 of size 1536\n",
            "2019-05-09 13:52:19.178007: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556cd00 of size 768\n",
            "2019-05-09 13:52:19.178026: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d000 of size 256\n",
            "2019-05-09 13:52:19.178068: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d100 of size 256\n",
            "2019-05-09 13:52:19.178090: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d200 of size 256\n",
            "2019-05-09 13:52:19.178110: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d300 of size 1536\n",
            "2019-05-09 13:52:19.178131: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d900 of size 1536\n",
            "2019-05-09 13:52:19.178150: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556df00 of size 1536\n",
            "2019-05-09 13:52:19.178170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556e500 of size 1536\n",
            "2019-05-09 13:52:19.178189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556eb00 of size 1536\n",
            "2019-05-09 13:52:19.178208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556f100 of size 1536\n",
            "2019-05-09 13:52:19.178227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556f700 of size 8960\n",
            "2019-05-09 13:52:19.178247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705571a00 of size 1536\n",
            "2019-05-09 13:52:19.178268: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705572000 of size 1536\n",
            "2019-05-09 13:52:19.178288: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705572600 of size 3328\n",
            "2019-05-09 13:52:19.178307: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705573300 of size 8960\n",
            "2019-05-09 13:52:19.178327: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705575600 of size 256\n",
            "2019-05-09 13:52:19.178364: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705575700 of size 14336\n",
            "2019-05-09 13:52:19.178388: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705578f00 of size 1536\n",
            "2019-05-09 13:52:19.178409: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705579500 of size 921600\n",
            "2019-05-09 13:52:19.178430: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70565a500 of size 1781760\n",
            "2019-05-09 13:52:19.178449: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70580d500 of size 2703360\n",
            "2019-05-09 13:52:19.178468: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705aa1500 of size 921600\n",
            "2019-05-09 13:52:19.178487: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705b82500 of size 921600\n",
            "2019-05-09 13:52:19.178507: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705c63500 of size 921600\n",
            "2019-05-09 13:52:19.178528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705d44500 of size 1555200\n",
            "2019-05-09 13:52:19.178550: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7060c0000 of size 134217728\n",
            "2019-05-09 13:52:19.178577: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70e0c0000 of size 23040000\n",
            "2019-05-09 13:52:19.178597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70f6b9000 of size 23040000\n",
            "2019-05-09 13:52:19.178616: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x710cb2000 of size 23040000\n",
            "2019-05-09 13:52:19.178635: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7122ab000 of size 23040000\n",
            "2019-05-09 13:52:19.178655: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7138a4000 of size 2703360\n",
            "2019-05-09 13:52:19.178675: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713b38000 of size 921600\n",
            "2019-05-09 13:52:19.178695: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713c19000 of size 921600\n",
            "2019-05-09 13:52:19.178714: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713cfa000 of size 1781760\n",
            "2019-05-09 13:52:19.178734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713ead000 of size 921600\n",
            "2019-05-09 13:52:19.178753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713f8e000 of size 921600\n",
            "2019-05-09 13:52:19.178773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71406f000 of size 921600\n",
            "2019-05-09 13:52:19.178793: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714150000 of size 921600\n",
            "2019-05-09 13:52:19.178812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714231000 of size 921600\n",
            "2019-05-09 13:52:19.178831: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714312000 of size 921600\n",
            "2019-05-09 13:52:19.178904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7143f3000 of size 921600\n",
            "2019-05-09 13:52:19.178936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7144d4000 of size 921600\n",
            "2019-05-09 13:52:19.178970: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7145b5000 of size 921600\n",
            "2019-05-09 13:52:19.179019: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714696000 of size 921600\n",
            "2019-05-09 13:52:19.179094: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714777000 of size 921600\n",
            "2019-05-09 13:52:19.179116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714858000 of size 921600\n",
            "2019-05-09 13:52:19.179137: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714939000 of size 921600\n",
            "2019-05-09 13:52:19.179156: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x714a1a000 of size 23748608\n",
            "2019-05-09 13:52:19.179177: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7160c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179197: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179217: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71e1a8000 of size 133267456\n",
            "2019-05-09 13:52:19.179238: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7260c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179258: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179277: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179296: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x736290000 of size 67584000\n",
            "2019-05-09 13:52:19.179348: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73e378000 of size 131366912\n",
            "2019-05-09 13:52:19.179396: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7460c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179415: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179434: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179454: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179474: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x756290000 of size 67584000\n",
            "2019-05-09 13:52:19.179494: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179513: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75e378000 of size 67584000\n",
            "2019-05-09 13:52:19.179532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7623ec000 of size 67584000\n",
            "2019-05-09 13:52:19.179561: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x766460000 of size 67584000\n",
            "2019-05-09 13:52:19.179581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76a4d4000 of size 67584000\n",
            "2019-05-09 13:52:19.179600: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76e548000 of size 67584000\n",
            "2019-05-09 13:52:19.179619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7725bc000 of size 67584000\n",
            "2019-05-09 13:52:19.179638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x776630000 of size 67584000\n",
            "2019-05-09 13:52:19.179657: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77a6a4000 of size 67584000\n",
            "2019-05-09 13:52:19.179678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77e718000 of size 127565824\n",
            "2019-05-09 13:52:19.179699: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7860c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179718: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179738: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179758: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179777: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x796290000 of size 67584000\n",
            "2019-05-09 13:52:19.179797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79e378000 of size 67584000\n",
            "2019-05-09 13:52:19.179835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a23ec000 of size 67584000\n",
            "2019-05-09 13:52:19.179854: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a6460000 of size 67584000\n",
            "2019-05-09 13:52:19.179885: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7aa4d4000 of size 67584000\n",
            "2019-05-09 13:52:19.179908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ae548000 of size 67584000\n",
            "2019-05-09 13:52:19.179928: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b25bc000 of size 67584000\n",
            "2019-05-09 13:52:19.179948: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b6630000 of size 67584000\n",
            "2019-05-09 13:52:19.179967: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ba6a4000 of size 67584000\n",
            "2019-05-09 13:52:19.179986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7be718000 of size 67584000\n",
            "2019-05-09 13:52:19.180004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c278c000 of size 67584000\n",
            "2019-05-09 13:52:19.180023: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c6800000 of size 67584000\n",
            "2019-05-09 13:52:19.180088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ca874000 of size 67584000\n",
            "2019-05-09 13:52:19.180109: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ce8e8000 of size 67584000\n",
            "2019-05-09 13:52:19.180128: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d295c000 of size 67584000\n",
            "2019-05-09 13:52:19.180147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d69d0000 of size 67584000\n",
            "2019-05-09 13:52:19.180167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7daa44000 of size 67584000\n",
            "2019-05-09 13:52:19.180201: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7deab8000 of size 67584000\n",
            "2019-05-09 13:52:19.180221: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e2b2c000 of size 67584000\n",
            "2019-05-09 13:52:19.180255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e6ba0000 of size 67584000\n",
            "2019-05-09 13:52:19.180273: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eac14000 of size 67584000\n",
            "2019-05-09 13:52:19.180291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eec88000 of size 67584000\n",
            "2019-05-09 13:52:19.180324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f2cfc000 of size 67584000\n",
            "2019-05-09 13:52:19.180360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f6d70000 of size 67584000\n",
            "2019-05-09 13:52:19.180382: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7fade4000 of size 67584000\n",
            "2019-05-09 13:52:19.180404: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fee58000 of size 119963648\n",
            "2019-05-09 13:52:19.180425: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8060c0000 of size 64512000\n",
            "2019-05-09 13:52:19.180445: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x809e46000 of size 67584000\n",
            "2019-05-09 13:52:19.180464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x80deba000 of size 67584000\n",
            "2019-05-09 13:52:19.180483: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x811f2e000 of size 67584000\n",
            "2019-05-09 13:52:19.180503: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x815fa2000 of size 64512000\n",
            "2019-05-09 13:52:19.180522: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x819d28000 of size 387072000\n",
            "2019-05-09 13:52:19.180541: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x830e4c000 of size 1612800000\n",
            "2019-05-09 13:52:19.180560: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x891062000 of size 1612800000\n",
            "2019-05-09 13:52:19.180581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8f1278000 of size 350519296\n",
            "2019-05-09 13:52:19.180601: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x9061c0000 of size 1612800000\n",
            "2019-05-09 13:52:19.180619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x9663d6000 of size 1041705984\n",
            "2019-05-09 13:52:19.180639: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \n",
            "2019-05-09 13:52:19.180671: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 114 Chunks of size 256 totalling 28.5KiB\n",
            "2019-05-09 13:52:19.180696: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2019-05-09 13:52:19.180720: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 377 Chunks of size 1536 totalling 565.5KiB\n",
            "2019-05-09 13:52:19.180742: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 2304 totalling 2.2KiB\n",
            "2019-05-09 13:52:19.180763: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 7680 totalling 15.0KiB\n",
            "2019-05-09 13:52:19.180785: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 8960 totalling 17.5KiB\n",
            "2019-05-09 13:52:19.180809: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 40 Chunks of size 495616 totalling 18.91MiB\n",
            "2019-05-09 13:52:19.180832: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 610304 totalling 596.0KiB\n",
            "2019-05-09 13:52:19.180854: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 724992 totalling 708.0KiB\n",
            "2019-05-09 13:52:19.180890: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 19 Chunks of size 921600 totalling 16.70MiB\n",
            "2019-05-09 13:52:19.180915: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 954368 totalling 932.0KiB\n",
            "2019-05-09 13:52:19.180938: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1555200 totalling 1.48MiB\n",
            "2019-05-09 13:52:19.180961: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1781760 totalling 3.40MiB\n",
            "2019-05-09 13:52:19.180983: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2703360 totalling 5.16MiB\n",
            "2019-05-09 13:52:19.181006: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 23040000 totalling 87.89MiB\n",
            "2019-05-09 13:52:19.181050: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 64512000 totalling 61.52MiB\n",
            "2019-05-09 13:52:19.181078: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 53 Chunks of size 67584000 totalling 3.34GiB\n",
            "2019-05-09 13:52:19.181102: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 119963648 totalling 114.41MiB\n",
            "2019-05-09 13:52:19.181125: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 127565824 totalling 121.66MiB\n",
            "2019-05-09 13:52:19.181147: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131366912 totalling 125.28MiB\n",
            "2019-05-09 13:52:19.181170: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 133267456 totalling 127.09MiB\n",
            "2019-05-09 13:52:19.181191: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 134217728 totalling 128.00MiB\n",
            "2019-05-09 13:52:19.181214: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 3 Chunks of size 1612800000 totalling 4.51GiB\n",
            "2019-05-09 13:52:19.181237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 8.64GiB\n",
            "2019-05-09 13:52:19.181263: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \n",
            "Limit:                 11276946637\n",
            "InUse:                  9274200832\n",
            "MaxInUse:               9466815232\n",
            "NumAllocs:                    1294\n",
            "MaxAllocSize:           1612800000\n",
            "\n",
            "2019-05-09 13:52:19.181331: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ******************************************__******************************__***************_________\n",
            "2019-05-09 13:52:19.181551: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at batch_matmul_op_impl.h:586 : Resource exhausted: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[{{node Adam/update}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "Caused by op 'gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 211, in run_training\n",
            "    aggregation_method=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\n",
            "    grad_loss=grad_loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\n",
            "    colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\n",
            "    unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\", line 1229, in _BatchMatMul\n",
            "    grad_y = math_ops.matmul(x, grad, adjoint_a=True, adjoint_b=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "...which was originally created as op 'func_map_loss/einsum_7/MatMul', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "[elided 1 identical lines from previous traceback]\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 200, in run_training\n",
            "    target_evecs_trans, target_evals\n",
            "  File \"/content/drive/unsupervisedfmnet/DFMnet.py\", line 83, in dfmnet_model\n",
            "    F, G\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 180, in func_map_layer\n",
            "    target_evecs, target_evecs_trans) +\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 116, in penalty_desc_commutativity\n",
            "    G_diag_reduce1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 262, in einsum\n",
            "    axes_to_sum)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 394, in _einsum_reduction\n",
            "    product = math_ops.matmul(t0, t1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kV_mS9G5_rM",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第三次 \n",
        "模型顶点数为3000，batch_size=8,溢出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R87O3veV4DmD",
        "colab_type": "code",
        "outputId": "5d42b36a-78fa-4106-cea8-4e648d5e43a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13107
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 13:55:24.073850: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 13:55:24.074117: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x19b7180 executing computations on platform Host. Devices:\n",
            "2019-05-09 13:55:24.074157: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 13:55:24.181504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 13:55:24.182102: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x19b7440 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 13:55:24.182148: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 13:55:24.182561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 13:55:24.182601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 13:55:24.584130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 13:55:24.584190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 13:55:24.584213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 13:55:24.584557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 14:02:06.927171: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 14:02:07.770591: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x7d938e0\n",
            "2019-05-09 14:02:19.197733: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.50GiB.  Current allocation summary follows.\n",
            "2019-05-09 14:02:19.197841: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 104, Chunks in use: 97. 26.0KiB allocated for chunks. 24.2KiB in use in bin. 396B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197868: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 4, Chunks in use: 0. 2.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197896: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 295, Chunks in use: 294. 442.5KiB allocated for chunks. 440.8KiB in use in bin. 403.9KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197919: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 1.4KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197939: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 3, Chunks in use: 2. 19.2KiB allocated for chunks. 15.0KiB in use in bin. 15.0KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197960: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 23.8KiB allocated for chunks. 23.8KiB in use in bin. 17.5KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197979: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197997: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198016: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198072: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198104: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 38, Chunks in use: 38. 17.96MiB allocated for chunks. 17.96MiB in use in bin. 17.96MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198124: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 23, Chunks in use: 22. 19.29MiB allocated for chunks. 18.41MiB in use in bin. 17.71MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198144: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 4.67MiB allocated for chunks. 2.97MiB in use in bin. 1.76MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198163: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 5.16MiB allocated for chunks. 5.16MiB in use in bin. 5.16MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198183: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198201: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198242: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 4, Chunks in use: 4. 87.89MiB allocated for chunks. 87.89MiB in use in bin. 87.89MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198265: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 123.05MiB allocated for chunks. 123.05MiB in use in bin. 123.05MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198285: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 60, Chunks in use: 58. 4.06GiB allocated for chunks. 3.93GiB in use in bin. 3.65GiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198305: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 64.45MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198324: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 3. 6.07GiB allocated for chunks. 4.51GiB in use in bin. 4.51GiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198343: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 1.50GiB was 256.00MiB, Chunk State: \n",
            "2019-05-09 14:02:19.198367: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 301.73MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198391: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 303.22MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 900.0KiB | Requested Size: 900.0KiB | in_use: 1, next:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198413: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 993.45MiB | Requested Size: 8.8KiB | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0000 of size 1280\n",
            "2019-05-09 14:02:19.198450: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0500 of size 1536\n",
            "2019-05-09 14:02:19.198477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0b00 of size 256\n",
            "2019-05-09 14:02:19.198519: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0c00 of size 256\n",
            "2019-05-09 14:02:19.198536: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0d00 of size 1536\n",
            "2019-05-09 14:02:19.198551: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1300 of size 1536\n",
            "2019-05-09 14:02:19.198581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1900 of size 1536\n",
            "2019-05-09 14:02:19.198597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1f00 of size 1536\n",
            "2019-05-09 14:02:19.198612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc2500 of size 1536\n",
            "2019-05-09 14:02:19.198627: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc2b00 of size 1536\n",
            "2019-05-09 14:02:19.198642: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3100 of size 1536\n",
            "2019-05-09 14:02:19.198673: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3700 of size 1536\n",
            "2019-05-09 14:02:19.198689: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3d00 of size 1536\n",
            "2019-05-09 14:02:19.198705: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4300 of size 1536\n",
            "2019-05-09 14:02:19.198720: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4900 of size 1536\n",
            "2019-05-09 14:02:19.198751: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4f00 of size 1536\n",
            "2019-05-09 14:02:19.198781: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc5500 of size 1536\n",
            "2019-05-09 14:02:19.198812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc5b00 of size 1536\n",
            "2019-05-09 14:02:19.198827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc6100 of size 495616\n",
            "2019-05-09 14:02:19.198843: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403f100 of size 528128\n",
            "2019-05-09 14:02:19.198859: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040c0000 of size 495616\n",
            "2019-05-09 14:02:19.198889: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139000 of size 1536\n",
            "2019-05-09 14:02:19.198904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139600 of size 1536\n",
            "2019-05-09 14:02:19.198920: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139c00 of size 1536\n",
            "2019-05-09 14:02:19.198936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413a200 of size 1536\n",
            "2019-05-09 14:02:19.198951: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413a800 of size 1536\n",
            "2019-05-09 14:02:19.198966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413ae00 of size 1536\n",
            "2019-05-09 14:02:19.198982: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413b400 of size 1536\n",
            "2019-05-09 14:02:19.198998: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413ba00 of size 1536\n",
            "2019-05-09 14:02:19.199014: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413c000 of size 495616\n",
            "2019-05-09 14:02:19.199045: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b5000 of size 1536\n",
            "2019-05-09 14:02:19.199063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b5600 of size 495616\n",
            "2019-05-09 14:02:19.199079: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70422e600 of size 596480\n",
            "2019-05-09 14:02:19.199096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0000 of size 1536\n",
            "2019-05-09 14:02:19.199112: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0600 of size 1536\n",
            "2019-05-09 14:02:19.199127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0c00 of size 1536\n",
            "2019-05-09 14:02:19.199143: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c1200 of size 495616\n",
            "2019-05-09 14:02:19.199158: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70433a200 of size 1536\n",
            "2019-05-09 14:02:19.199180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70433a800 of size 495616\n",
            "2019-05-09 14:02:19.199196: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b3800 of size 1536\n",
            "2019-05-09 14:02:19.199211: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b3e00 of size 495616\n",
            "2019-05-09 14:02:19.199227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442ce00 of size 1536\n",
            "2019-05-09 14:02:19.199243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442d400 of size 1536\n",
            "2019-05-09 14:02:19.199259: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442da00 of size 1536\n",
            "2019-05-09 14:02:19.199275: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442e000 of size 1536\n",
            "2019-05-09 14:02:19.199290: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442e600 of size 495616\n",
            "2019-05-09 14:02:19.199306: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7044a7600 of size 495616\n",
            "2019-05-09 14:02:19.199323: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704520600 of size 495616\n",
            "2019-05-09 14:02:19.199339: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704599600 of size 1536\n",
            "2019-05-09 14:02:19.199354: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704599c00 of size 1536\n",
            "2019-05-09 14:02:19.199370: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459a200 of size 1536\n",
            "2019-05-09 14:02:19.199385: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459a800 of size 1536\n",
            "2019-05-09 14:02:19.199401: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ae00 of size 1536\n",
            "2019-05-09 14:02:19.199416: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459b400 of size 1536\n",
            "2019-05-09 14:02:19.199432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ba00 of size 1536\n",
            "2019-05-09 14:02:19.199447: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459c000 of size 1536\n",
            "2019-05-09 14:02:19.199463: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459c600 of size 1536\n",
            "2019-05-09 14:02:19.199489: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459cc00 of size 1536\n",
            "2019-05-09 14:02:19.199516: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459d200 of size 1536\n",
            "2019-05-09 14:02:19.199533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459d800 of size 1536\n",
            "2019-05-09 14:02:19.199549: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459de00 of size 1536\n",
            "2019-05-09 14:02:19.199564: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459e400 of size 1536\n",
            "2019-05-09 14:02:19.199580: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ea00 of size 1536\n",
            "2019-05-09 14:02:19.199595: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459f000 of size 1536\n",
            "2019-05-09 14:02:19.199612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459f600 of size 1536\n",
            "2019-05-09 14:02:19.199628: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459fc00 of size 1536\n",
            "2019-05-09 14:02:19.199643: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0200 of size 1536\n",
            "2019-05-09 14:02:19.199659: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0800 of size 1536\n",
            "2019-05-09 14:02:19.199675: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0e00 of size 1536\n",
            "2019-05-09 14:02:19.199690: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a1400 of size 495616\n",
            "2019-05-09 14:02:19.199706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70461a400 of size 678912\n",
            "2019-05-09 14:02:19.199722: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7046c0000 of size 495616\n",
            "2019-05-09 14:02:19.199738: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739000 of size 1536\n",
            "2019-05-09 14:02:19.199753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739600 of size 1536\n",
            "2019-05-09 14:02:19.199770: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739c00 of size 1536\n",
            "2019-05-09 14:02:19.199787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473a200 of size 1536\n",
            "2019-05-09 14:02:19.199802: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473a800 of size 1536\n",
            "2019-05-09 14:02:19.199818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473ae00 of size 495616\n",
            "2019-05-09 14:02:19.199833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b3e00 of size 1536\n",
            "2019-05-09 14:02:19.199849: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b4400 of size 495616\n",
            "2019-05-09 14:02:19.199864: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482d400 of size 1536\n",
            "2019-05-09 14:02:19.199880: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482da00 of size 495616\n",
            "2019-05-09 14:02:19.199896: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a6a00 of size 1536\n",
            "2019-05-09 14:02:19.199911: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7000 of size 1536\n",
            "2019-05-09 14:02:19.199927: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7600 of size 1536\n",
            "2019-05-09 14:02:19.199942: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7c00 of size 1536\n",
            "2019-05-09 14:02:19.199958: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8200 of size 1536\n",
            "2019-05-09 14:02:19.199974: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8800 of size 1536\n",
            "2019-05-09 14:02:19.199989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8e00 of size 1536\n",
            "2019-05-09 14:02:19.200005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a9400 of size 1536\n",
            "2019-05-09 14:02:19.200021: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a9a00 of size 1536\n",
            "2019-05-09 14:02:19.200051: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aa000 of size 1536\n",
            "2019-05-09 14:02:19.200067: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aa600 of size 1536\n",
            "2019-05-09 14:02:19.200083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aac00 of size 1536\n",
            "2019-05-09 14:02:19.200099: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ab200 of size 1536\n",
            "2019-05-09 14:02:19.200116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ab800 of size 1536\n",
            "2019-05-09 14:02:19.200132: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048abe00 of size 1536\n",
            "2019-05-09 14:02:19.200148: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ac400 of size 1536\n",
            "2019-05-09 14:02:19.200163: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aca00 of size 1536\n",
            "2019-05-09 14:02:19.200179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ad000 of size 1536\n",
            "2019-05-09 14:02:19.200195: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ad600 of size 1536\n",
            "2019-05-09 14:02:19.200210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048adc00 of size 1536\n",
            "2019-05-09 14:02:19.200226: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ae200 of size 1536\n",
            "2019-05-09 14:02:19.200242: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ae800 of size 1536\n",
            "2019-05-09 14:02:19.200258: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aee00 of size 1536\n",
            "2019-05-09 14:02:19.200273: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048af400 of size 1536\n",
            "2019-05-09 14:02:19.200289: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048afa00 of size 1536\n",
            "2019-05-09 14:02:19.200305: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0000 of size 1536\n",
            "2019-05-09 14:02:19.200321: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0600 of size 1536\n",
            "2019-05-09 14:02:19.200336: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0c00 of size 1536\n",
            "2019-05-09 14:02:19.200352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b1200 of size 495616\n",
            "2019-05-09 14:02:19.200368: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70492a200 of size 495616\n",
            "2019-05-09 14:02:19.203720: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7049a3200 of size 495616\n",
            "2019-05-09 14:02:19.203746: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1c200 of size 1536\n",
            "2019-05-09 14:02:19.203757: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1c800 of size 1536\n",
            "2019-05-09 14:02:19.203768: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1ce00 of size 1536\n",
            "2019-05-09 14:02:19.203778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1d400 of size 495616\n",
            "2019-05-09 14:02:19.203787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a96400 of size 495616\n",
            "2019-05-09 14:02:19.203797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b0f400 of size 1536\n",
            "2019-05-09 14:02:19.203806: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b0fa00 of size 495616\n",
            "2019-05-09 14:02:19.203815: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b88a00 of size 1536\n",
            "2019-05-09 14:02:19.203826: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89000 of size 1536\n",
            "2019-05-09 14:02:19.203835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89600 of size 1536\n",
            "2019-05-09 14:02:19.203846: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89c00 of size 1536\n",
            "2019-05-09 14:02:19.203856: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8a200 of size 1536\n",
            "2019-05-09 14:02:19.203867: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8a800 of size 1536\n",
            "2019-05-09 14:02:19.203877: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ae00 of size 1536\n",
            "2019-05-09 14:02:19.203887: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8b400 of size 1536\n",
            "2019-05-09 14:02:19.203897: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ba00 of size 1536\n",
            "2019-05-09 14:02:19.203907: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8c000 of size 1536\n",
            "2019-05-09 14:02:19.203918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8c600 of size 1536\n",
            "2019-05-09 14:02:19.203929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8cc00 of size 1536\n",
            "2019-05-09 14:02:19.203939: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8d200 of size 1536\n",
            "2019-05-09 14:02:19.203949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8d800 of size 1536\n",
            "2019-05-09 14:02:19.203959: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8de00 of size 1536\n",
            "2019-05-09 14:02:19.203970: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8e400 of size 1536\n",
            "2019-05-09 14:02:19.203982: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ea00 of size 1536\n",
            "2019-05-09 14:02:19.203992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8f000 of size 1536\n",
            "2019-05-09 14:02:19.204003: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8f600 of size 1536\n",
            "2019-05-09 14:02:19.204015: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8fc00 of size 1536\n",
            "2019-05-09 14:02:19.204026: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90200 of size 1536\n",
            "2019-05-09 14:02:19.204059: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90800 of size 1536\n",
            "2019-05-09 14:02:19.204072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90e00 of size 1536\n",
            "2019-05-09 14:02:19.204083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b91400 of size 495616\n",
            "2019-05-09 14:02:19.204559: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c0a400 of size 495616\n",
            "2019-05-09 14:02:19.204591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c83400 of size 495616\n",
            "2019-05-09 14:02:19.204612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfc400 of size 1536\n",
            "2019-05-09 14:02:19.204632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfca00 of size 1536\n",
            "2019-05-09 14:02:19.204653: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfd000 of size 1536\n",
            "2019-05-09 14:02:19.204687: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfd600 of size 495616\n",
            "2019-05-09 14:02:19.204706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704d76600 of size 495616\n",
            "2019-05-09 14:02:19.204727: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704def600 of size 854528\n",
            "2019-05-09 14:02:19.204747: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0000 of size 1536\n",
            "2019-05-09 14:02:19.204768: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0600 of size 1536\n",
            "2019-05-09 14:02:19.204787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0c00 of size 1536\n",
            "2019-05-09 14:02:19.204807: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec1200 of size 495616\n",
            "2019-05-09 14:02:19.204842: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704f3a200 of size 495616\n",
            "2019-05-09 14:02:19.204860: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704fb3200 of size 495616\n",
            "2019-05-09 14:02:19.204879: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502c200 of size 1536\n",
            "2019-05-09 14:02:19.204898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502c800 of size 1536\n",
            "2019-05-09 14:02:19.204918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502ce00 of size 1536\n",
            "2019-05-09 14:02:19.204936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502d400 of size 1536\n",
            "2019-05-09 14:02:19.204949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502da00 of size 495616\n",
            "2019-05-09 14:02:19.204971: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7050a6a00 of size 495616\n",
            "2019-05-09 14:02:19.204992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70511fa00 of size 1536\n",
            "2019-05-09 14:02:19.205011: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705120000 of size 1536\n",
            "2019-05-09 14:02:19.205048: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705120600 of size 495616\n",
            "2019-05-09 14:02:19.205072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705199600 of size 1536\n",
            "2019-05-09 14:02:19.205097: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705199c00 of size 1536\n",
            "2019-05-09 14:02:19.205119: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519a200 of size 1536\n",
            "2019-05-09 14:02:19.205139: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519a800 of size 1536\n",
            "2019-05-09 14:02:19.205160: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519ae00 of size 1536\n",
            "2019-05-09 14:02:19.205179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519b400 of size 1536\n",
            "2019-05-09 14:02:19.205199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519ba00 of size 1536\n",
            "2019-05-09 14:02:19.205218: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519c000 of size 1536\n",
            "2019-05-09 14:02:19.205255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519c600 of size 1536\n",
            "2019-05-09 14:02:19.205276: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519cc00 of size 1536\n",
            "2019-05-09 14:02:19.205296: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519d200 of size 1536\n",
            "2019-05-09 14:02:19.205317: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519d800 of size 1536\n",
            "2019-05-09 14:02:19.205337: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519de00 of size 1536\n",
            "2019-05-09 14:02:19.205358: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519e400 of size 495616\n",
            "2019-05-09 14:02:19.205407: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705217400 of size 1536\n",
            "2019-05-09 14:02:19.205457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705217a00 of size 1536\n",
            "2019-05-09 14:02:19.205493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218000 of size 1536\n",
            "2019-05-09 14:02:19.205524: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218600 of size 1536\n",
            "2019-05-09 14:02:19.205545: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218c00 of size 495616\n",
            "2019-05-09 14:02:19.205566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705291c00 of size 1536\n",
            "2019-05-09 14:02:19.205588: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705292200 of size 495616\n",
            "2019-05-09 14:02:19.205609: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b200 of size 256\n",
            "2019-05-09 14:02:19.205630: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b300 of size 256\n",
            "2019-05-09 14:02:19.205650: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b400 of size 256\n",
            "2019-05-09 14:02:19.205669: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b500 of size 256\n",
            "2019-05-09 14:02:19.205690: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b600 of size 256\n",
            "2019-05-09 14:02:19.205710: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b700 of size 256\n",
            "2019-05-09 14:02:19.205730: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b800 of size 1536\n",
            "2019-05-09 14:02:19.205749: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530be00 of size 1536\n",
            "2019-05-09 14:02:19.205765: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530c400 of size 1536\n",
            "2019-05-09 14:02:19.205783: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530ca00 of size 1536\n",
            "2019-05-09 14:02:19.205803: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530d000 of size 495616\n",
            "2019-05-09 14:02:19.205822: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705386000 of size 495616\n",
            "2019-05-09 14:02:19.205843: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7053ff000 of size 495616\n",
            "2019-05-09 14:02:19.205863: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478000 of size 1536\n",
            "2019-05-09 14:02:19.205883: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478600 of size 1536\n",
            "2019-05-09 14:02:19.205903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478c00 of size 1536\n",
            "2019-05-09 14:02:19.205924: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479200 of size 1536\n",
            "2019-05-09 14:02:19.205944: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479800 of size 256\n",
            "2019-05-09 14:02:19.205964: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479900 of size 256\n",
            "2019-05-09 14:02:19.205986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479a00 of size 256\n",
            "2019-05-09 14:02:19.206007: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479b00 of size 256\n",
            "2019-05-09 14:02:19.206042: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479c00 of size 256\n",
            "2019-05-09 14:02:19.206066: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479d00 of size 1536\n",
            "2019-05-09 14:02:19.206087: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547a300 of size 1536\n",
            "2019-05-09 14:02:19.206107: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547a900 of size 1536\n",
            "2019-05-09 14:02:19.206126: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547af00 of size 1536\n",
            "2019-05-09 14:02:19.206147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547b500 of size 1536\n",
            "2019-05-09 14:02:19.206167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547bb00 of size 1536\n",
            "2019-05-09 14:02:19.206187: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547c100 of size 1536\n",
            "2019-05-09 14:02:19.206208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547c700 of size 1536\n",
            "2019-05-09 14:02:19.206229: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547cd00 of size 1536\n",
            "2019-05-09 14:02:19.206265: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547d300 of size 1536\n",
            "2019-05-09 14:02:19.206285: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547d900 of size 1536\n",
            "2019-05-09 14:02:19.206305: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547df00 of size 1536\n",
            "2019-05-09 14:02:19.206324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547e500 of size 1536\n",
            "2019-05-09 14:02:19.206344: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547eb00 of size 1536\n",
            "2019-05-09 14:02:19.206364: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547f100 of size 1536\n",
            "2019-05-09 14:02:19.206384: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547f700 of size 1536\n",
            "2019-05-09 14:02:19.206403: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547fd00 of size 1536\n",
            "2019-05-09 14:02:19.206423: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480300 of size 1536\n",
            "2019-05-09 14:02:19.206442: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480900 of size 1536\n",
            "2019-05-09 14:02:19.206462: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480f00 of size 1536\n",
            "2019-05-09 14:02:19.206520: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705481500 of size 1536\n",
            "2019-05-09 14:02:19.206542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705481b00 of size 1536\n",
            "2019-05-09 14:02:19.206562: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482100 of size 1536\n",
            "2019-05-09 14:02:19.206583: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482700 of size 1536\n",
            "2019-05-09 14:02:19.206604: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482d00 of size 1536\n",
            "2019-05-09 14:02:19.206624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483300 of size 1536\n",
            "2019-05-09 14:02:19.206645: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483900 of size 1536\n",
            "2019-05-09 14:02:19.206665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483f00 of size 1536\n",
            "2019-05-09 14:02:19.206685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705484500 of size 1536\n",
            "2019-05-09 14:02:19.206706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705484b00 of size 1536\n",
            "2019-05-09 14:02:19.206741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485100 of size 1536\n",
            "2019-05-09 14:02:19.206760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485700 of size 1536\n",
            "2019-05-09 14:02:19.206780: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485d00 of size 1536\n",
            "2019-05-09 14:02:19.206807: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486300 of size 1536\n",
            "2019-05-09 14:02:19.206827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486900 of size 1536\n",
            "2019-05-09 14:02:19.206847: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486f00 of size 1536\n",
            "2019-05-09 14:02:19.206868: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705487500 of size 1536\n",
            "2019-05-09 14:02:19.206888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705487b00 of size 1536\n",
            "2019-05-09 14:02:19.206908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488100 of size 1536\n",
            "2019-05-09 14:02:19.206927: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488700 of size 1536\n",
            "2019-05-09 14:02:19.206946: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488d00 of size 1536\n",
            "2019-05-09 14:02:19.206966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489300 of size 1536\n",
            "2019-05-09 14:02:19.206985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489900 of size 1536\n",
            "2019-05-09 14:02:19.207006: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489f00 of size 1536\n",
            "2019-05-09 14:02:19.207025: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548a500 of size 1536\n",
            "2019-05-09 14:02:19.207077: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ab00 of size 1536\n",
            "2019-05-09 14:02:19.207098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548b100 of size 1536\n",
            "2019-05-09 14:02:19.207118: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548b700 of size 1536\n",
            "2019-05-09 14:02:19.207138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548bd00 of size 1536\n",
            "2019-05-09 14:02:19.207159: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548c300 of size 1536\n",
            "2019-05-09 14:02:19.207180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548c900 of size 1536\n",
            "2019-05-09 14:02:19.207200: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548cf00 of size 1536\n",
            "2019-05-09 14:02:19.207235: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548d500 of size 1536\n",
            "2019-05-09 14:02:19.207255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548db00 of size 1536\n",
            "2019-05-09 14:02:19.207275: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548e100 of size 1536\n",
            "2019-05-09 14:02:19.207295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548e700 of size 1536\n",
            "2019-05-09 14:02:19.207331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ed00 of size 256\n",
            "2019-05-09 14:02:19.207352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ee00 of size 256\n",
            "2019-05-09 14:02:19.207372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ef00 of size 256\n",
            "2019-05-09 14:02:19.207392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f000 of size 256\n",
            "2019-05-09 14:02:19.207412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f100 of size 256\n",
            "2019-05-09 14:02:19.207437: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f200 of size 256\n",
            "2019-05-09 14:02:19.207457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f300 of size 256\n",
            "2019-05-09 14:02:19.207491: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f400 of size 256\n",
            "2019-05-09 14:02:19.207533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f500 of size 256\n",
            "2019-05-09 14:02:19.207555: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f600 of size 256\n",
            "2019-05-09 14:02:19.207576: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f700 of size 256\n",
            "2019-05-09 14:02:19.207597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f800 of size 256\n",
            "2019-05-09 14:02:19.207617: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f900 of size 256\n",
            "2019-05-09 14:02:19.207637: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fa00 of size 256\n",
            "2019-05-09 14:02:19.207657: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fb00 of size 256\n",
            "2019-05-09 14:02:19.207677: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fc00 of size 256\n",
            "2019-05-09 14:02:19.207696: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fd00 of size 256\n",
            "2019-05-09 14:02:19.207717: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fe00 of size 256\n",
            "2019-05-09 14:02:19.207736: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ff00 of size 256\n",
            "2019-05-09 14:02:19.207756: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490000 of size 256\n",
            "2019-05-09 14:02:19.207776: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490100 of size 256\n",
            "2019-05-09 14:02:19.207796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490200 of size 256\n",
            "2019-05-09 14:02:19.207816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490300 of size 256\n",
            "2019-05-09 14:02:19.207836: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490400 of size 256\n",
            "2019-05-09 14:02:19.207857: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490500 of size 256\n",
            "2019-05-09 14:02:19.207878: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490600 of size 256\n",
            "2019-05-09 14:02:19.207898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490700 of size 256\n",
            "2019-05-09 14:02:19.207919: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490800 of size 256\n",
            "2019-05-09 14:02:19.207940: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705490900 of size 256\n",
            "2019-05-09 14:02:19.207961: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490a00 of size 256\n",
            "2019-05-09 14:02:19.207981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490b00 of size 256\n",
            "2019-05-09 14:02:19.208002: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490c00 of size 256\n",
            "2019-05-09 14:02:19.208024: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490d00 of size 256\n",
            "2019-05-09 14:02:19.208062: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490e00 of size 7680\n",
            "2019-05-09 14:02:19.208083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705492c00 of size 256\n",
            "2019-05-09 14:02:19.208104: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705492d00 of size 7680\n",
            "2019-05-09 14:02:19.208125: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705494b00 of size 1536\n",
            "2019-05-09 14:02:19.208145: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495100 of size 1536\n",
            "2019-05-09 14:02:19.208167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495700 of size 1536\n",
            "2019-05-09 14:02:19.208187: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495d00 of size 256\n",
            "2019-05-09 14:02:19.208209: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495e00 of size 256\n",
            "2019-05-09 14:02:19.208230: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495f00 of size 256\n",
            "2019-05-09 14:02:19.208250: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496000 of size 256\n",
            "2019-05-09 14:02:19.208271: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496100 of size 256\n",
            "2019-05-09 14:02:19.208291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496200 of size 256\n",
            "2019-05-09 14:02:19.208311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496300 of size 1536\n",
            "2019-05-09 14:02:19.208331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496900 of size 1536\n",
            "2019-05-09 14:02:19.208352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496f00 of size 256\n",
            "2019-05-09 14:02:19.208372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497000 of size 256\n",
            "2019-05-09 14:02:19.208392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497100 of size 256\n",
            "2019-05-09 14:02:19.208412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497200 of size 256\n",
            "2019-05-09 14:02:19.208432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497300 of size 256\n",
            "2019-05-09 14:02:19.208453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497400 of size 1536\n",
            "2019-05-09 14:02:19.208486: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497a00 of size 1536\n",
            "2019-05-09 14:02:19.208515: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498000 of size 1536\n",
            "2019-05-09 14:02:19.208538: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498600 of size 1536\n",
            "2019-05-09 14:02:19.208570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498c00 of size 1536\n",
            "2019-05-09 14:02:19.208591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499200 of size 1536\n",
            "2019-05-09 14:02:19.208612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499800 of size 256\n",
            "2019-05-09 14:02:19.208632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499900 of size 256\n",
            "2019-05-09 14:02:19.208653: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499a00 of size 1536\n",
            "2019-05-09 14:02:19.208673: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549a000 of size 1536\n",
            "2019-05-09 14:02:19.208694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549a600 of size 1536\n",
            "2019-05-09 14:02:19.208714: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549ac00 of size 1536\n",
            "2019-05-09 14:02:19.208734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b200 of size 1536\n",
            "2019-05-09 14:02:19.208754: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b800 of size 256\n",
            "2019-05-09 14:02:19.208775: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b900 of size 1536\n",
            "2019-05-09 14:02:19.208796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549bf00 of size 256\n",
            "2019-05-09 14:02:19.208817: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c000 of size 256\n",
            "2019-05-09 14:02:19.208838: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c100 of size 256\n",
            "2019-05-09 14:02:19.208858: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c200 of size 256\n",
            "2019-05-09 14:02:19.208878: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c300 of size 1536\n",
            "2019-05-09 14:02:19.208898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c900 of size 1536\n",
            "2019-05-09 14:02:19.208917: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549cf00 of size 1536\n",
            "2019-05-09 14:02:19.208936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549d500 of size 1536\n",
            "2019-05-09 14:02:19.208956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549db00 of size 1536\n",
            "2019-05-09 14:02:19.208976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e100 of size 1536\n",
            "2019-05-09 14:02:19.208996: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e700 of size 256\n",
            "2019-05-09 14:02:19.209017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e800 of size 256\n",
            "2019-05-09 14:02:19.209057: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e900 of size 256\n",
            "2019-05-09 14:02:19.209080: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549ea00 of size 256\n",
            "2019-05-09 14:02:19.209102: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549eb00 of size 1536\n",
            "2019-05-09 14:02:19.209123: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549f100 of size 1536\n",
            "2019-05-09 14:02:19.209145: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549f700 of size 2560\n",
            "2019-05-09 14:02:19.209166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0100 of size 1536\n",
            "2019-05-09 14:02:19.209186: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0700 of size 1536\n",
            "2019-05-09 14:02:19.209207: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0d00 of size 1536\n",
            "2019-05-09 14:02:19.209227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1300 of size 1536\n",
            "2019-05-09 14:02:19.209249: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1900 of size 1536\n",
            "2019-05-09 14:02:19.209270: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1f00 of size 1536\n",
            "2019-05-09 14:02:19.209291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2500 of size 256\n",
            "2019-05-09 14:02:19.209311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2600 of size 1536\n",
            "2019-05-09 14:02:19.209332: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2c00 of size 256\n",
            "2019-05-09 14:02:19.209352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2d00 of size 1536\n",
            "2019-05-09 14:02:19.209372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3300 of size 1536\n",
            "2019-05-09 14:02:19.209393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3900 of size 1536\n",
            "2019-05-09 14:02:19.209413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3f00 of size 1536\n",
            "2019-05-09 14:02:19.209434: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4500 of size 1536\n",
            "2019-05-09 14:02:19.209455: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4b00 of size 256\n",
            "2019-05-09 14:02:19.209488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4c00 of size 256\n",
            "2019-05-09 14:02:19.209517: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054a4d00 of size 256\n",
            "2019-05-09 14:02:19.209532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4e00 of size 256\n",
            "2019-05-09 14:02:19.209553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4f00 of size 256\n",
            "2019-05-09 14:02:19.209572: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5000 of size 256\n",
            "2019-05-09 14:02:19.209592: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5100 of size 256\n",
            "2019-05-09 14:02:19.209612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5200 of size 256\n",
            "2019-05-09 14:02:19.209632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5300 of size 1536\n",
            "2019-05-09 14:02:19.209651: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5900 of size 1536\n",
            "2019-05-09 14:02:19.209671: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5f00 of size 1536\n",
            "2019-05-09 14:02:19.209691: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a6500 of size 1536\n",
            "2019-05-09 14:02:19.209711: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a6b00 of size 1536\n",
            "2019-05-09 14:02:19.209732: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7100 of size 1536\n",
            "2019-05-09 14:02:19.209752: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7700 of size 256\n",
            "2019-05-09 14:02:19.209772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7800 of size 256\n",
            "2019-05-09 14:02:19.209792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7900 of size 256\n",
            "2019-05-09 14:02:19.209813: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7a00 of size 256\n",
            "2019-05-09 14:02:19.209833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7b00 of size 1536\n",
            "2019-05-09 14:02:19.209853: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8100 of size 1536\n",
            "2019-05-09 14:02:19.209875: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8700 of size 1536\n",
            "2019-05-09 14:02:19.209895: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8d00 of size 1536\n",
            "2019-05-09 14:02:19.209916: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9300 of size 1536\n",
            "2019-05-09 14:02:19.209936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9900 of size 1536\n",
            "2019-05-09 14:02:19.209956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9f00 of size 256\n",
            "2019-05-09 14:02:19.209976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa000 of size 256\n",
            "2019-05-09 14:02:19.209996: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa100 of size 256\n",
            "2019-05-09 14:02:19.210017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa200 of size 256\n",
            "2019-05-09 14:02:19.210054: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa300 of size 1536\n",
            "2019-05-09 14:02:19.210077: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa900 of size 1536\n",
            "2019-05-09 14:02:19.210098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aaf00 of size 1536\n",
            "2019-05-09 14:02:19.210120: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ab500 of size 1536\n",
            "2019-05-09 14:02:19.210140: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054abb00 of size 1536\n",
            "2019-05-09 14:02:19.210161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac100 of size 1536\n",
            "2019-05-09 14:02:19.210175: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac700 of size 256\n",
            "2019-05-09 14:02:19.210193: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac800 of size 256\n",
            "2019-05-09 14:02:19.210212: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac900 of size 256\n",
            "2019-05-09 14:02:19.210233: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aca00 of size 256\n",
            "2019-05-09 14:02:19.210252: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054acb00 of size 1536\n",
            "2019-05-09 14:02:19.210269: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ad100 of size 1536\n",
            "2019-05-09 14:02:19.210288: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ad700 of size 1536\n",
            "2019-05-09 14:02:19.210308: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054add00 of size 1536\n",
            "2019-05-09 14:02:19.210327: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ae300 of size 1536\n",
            "2019-05-09 14:02:19.210348: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ae900 of size 1536\n",
            "2019-05-09 14:02:19.210369: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aef00 of size 1536\n",
            "2019-05-09 14:02:19.210390: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054af500 of size 256\n",
            "2019-05-09 14:02:19.210425: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054af600 of size 1536\n",
            "2019-05-09 14:02:19.210460: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054afc00 of size 1536\n",
            "2019-05-09 14:02:19.210533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0200 of size 1536\n",
            "2019-05-09 14:02:19.210556: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b0800 of size 512\n",
            "2019-05-09 14:02:19.210577: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0a00 of size 256\n",
            "2019-05-09 14:02:19.210597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0b00 of size 1536\n",
            "2019-05-09 14:02:19.210618: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1100 of size 1536\n",
            "2019-05-09 14:02:19.210638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1700 of size 1536\n",
            "2019-05-09 14:02:19.210658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1d00 of size 256\n",
            "2019-05-09 14:02:19.210679: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b1e00 of size 256\n",
            "2019-05-09 14:02:19.210698: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1f00 of size 1536\n",
            "2019-05-09 14:02:19.210719: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b2500 of size 1536\n",
            "2019-05-09 14:02:19.210740: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b2b00 of size 1536\n",
            "2019-05-09 14:02:19.210761: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3100 of size 1536\n",
            "2019-05-09 14:02:19.210781: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3700 of size 1536\n",
            "2019-05-09 14:02:19.210802: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3d00 of size 1536\n",
            "2019-05-09 14:02:19.210822: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b4300 of size 1536\n",
            "2019-05-09 14:02:19.210841: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b4900 of size 1536\n",
            "2019-05-09 14:02:19.210862: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b4f00 of size 512\n",
            "2019-05-09 14:02:19.210896: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5100 of size 256\n",
            "2019-05-09 14:02:19.210916: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b5200 of size 256\n",
            "2019-05-09 14:02:19.210935: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5300 of size 1536\n",
            "2019-05-09 14:02:19.210950: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5900 of size 1536\n",
            "2019-05-09 14:02:19.210969: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5f00 of size 1536\n",
            "2019-05-09 14:02:19.210988: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b6500 of size 256\n",
            "2019-05-09 14:02:19.211012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b6600 of size 1536\n",
            "2019-05-09 14:02:19.211045: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b6c00 of size 1536\n",
            "2019-05-09 14:02:19.211082: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b7200 of size 1536\n",
            "2019-05-09 14:02:19.211117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b7800 of size 4352\n",
            "2019-05-09 14:02:19.211138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8900 of size 256\n",
            "2019-05-09 14:02:19.211159: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b8a00 of size 256\n",
            "2019-05-09 14:02:19.211181: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8b00 of size 256\n",
            "2019-05-09 14:02:19.211201: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8c00 of size 8960\n",
            "2019-05-09 14:02:19.211222: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054baf00 of size 512\n",
            "2019-05-09 14:02:19.211257: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bb100 of size 15360\n",
            "2019-05-09 14:02:19.211278: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bed00 of size 1536\n",
            "2019-05-09 14:02:19.211299: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bf300 of size 921600\n",
            "2019-05-09 14:02:19.211331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7055a0300 of size 1781760\n",
            "2019-05-09 14:02:19.211351: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705753300 of size 2703360\n",
            "2019-05-09 14:02:19.211371: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7059e7300 of size 921600\n",
            "2019-05-09 14:02:19.211392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ac8300 of size 921600\n",
            "2019-05-09 14:02:19.211413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9300 of size 256\n",
            "2019-05-09 14:02:19.211433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ba9400 of size 256\n",
            "2019-05-09 14:02:19.211454: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9500 of size 768\n",
            "2019-05-09 14:02:19.211488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ba9800 of size 256\n",
            "2019-05-09 14:02:19.211518: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9900 of size 1792\n",
            "2019-05-09 14:02:19.211539: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705baa000 of size 921600\n",
            "2019-05-09 14:02:19.211562: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705c8b000 of size 979200\n",
            "2019-05-09 14:02:19.211584: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705d7a100 of size 1335040\n",
            "2019-05-09 14:02:19.211605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7060c0000 of size 134217728\n",
            "2019-05-09 14:02:19.211626: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70e0c0000 of size 23040000\n",
            "2019-05-09 14:02:19.211646: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70f6b9000 of size 111177728\n",
            "2019-05-09 14:02:19.211667: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7160c0000 of size 23040000\n",
            "2019-05-09 14:02:19.211688: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7176b9000 of size 23040000\n",
            "2019-05-09 14:02:19.211708: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x718cb2000 of size 23040000\n",
            "2019-05-09 14:02:19.211729: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71a2ab000 of size 67584000\n",
            "2019-05-09 14:02:19.211750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71e31f000 of size 131731456\n",
            "2019-05-09 14:02:19.211771: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7260c0000 of size 67584000\n",
            "2019-05-09 14:02:19.211792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72a134000 of size 67584000\n",
            "2019-05-09 14:02:19.211812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.211832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73221c000 of size 67584000\n",
            "2019-05-09 14:02:19.211852: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x736290000 of size 67584000\n",
            "2019-05-09 14:02:19.211881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73a304000 of size 67584000\n",
            "2019-05-09 14:02:19.211903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73e378000 of size 131366912\n",
            "2019-05-09 14:02:19.211924: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7460c0000 of size 67584000\n",
            "2019-05-09 14:02:19.211944: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74a134000 of size 67584000\n",
            "2019-05-09 14:02:19.211964: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.211985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75221c000 of size 67584000\n",
            "2019-05-09 14:02:19.212005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x756290000 of size 67584000\n",
            "2019-05-09 14:02:19.212025: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75a304000 of size 67584000\n",
            "2019-05-09 14:02:19.212063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75e378000 of size 67584000\n",
            "2019-05-09 14:02:19.212085: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7623ec000 of size 67584000\n",
            "2019-05-09 14:02:19.212106: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x766460000 of size 67584000\n",
            "2019-05-09 14:02:19.212127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76a4d4000 of size 67584000\n",
            "2019-05-09 14:02:19.212149: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76e548000 of size 67584000\n",
            "2019-05-09 14:02:19.212169: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7725bc000 of size 67584000\n",
            "2019-05-09 14:02:19.212189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x776630000 of size 67584000\n",
            "2019-05-09 14:02:19.212209: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77a6a4000 of size 67584000\n",
            "2019-05-09 14:02:19.212230: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77e718000 of size 127565824\n",
            "2019-05-09 14:02:19.212251: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7860c0000 of size 67584000\n",
            "2019-05-09 14:02:19.212271: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78a134000 of size 67584000\n",
            "2019-05-09 14:02:19.212291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.212311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79221c000 of size 67584000\n",
            "2019-05-09 14:02:19.212331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x796290000 of size 67584000\n",
            "2019-05-09 14:02:19.212350: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79a304000 of size 67584000\n",
            "2019-05-09 14:02:19.212371: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79e378000 of size 67584000\n",
            "2019-05-09 14:02:19.212392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a23ec000 of size 67584000\n",
            "2019-05-09 14:02:19.212412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a6460000 of size 67584000\n",
            "2019-05-09 14:02:19.212432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7aa4d4000 of size 67584000\n",
            "2019-05-09 14:02:19.212453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ae548000 of size 67584000\n",
            "2019-05-09 14:02:19.212488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b25bc000 of size 67584000\n",
            "2019-05-09 14:02:19.212519: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b6630000 of size 67584000\n",
            "2019-05-09 14:02:19.212542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ba6a4000 of size 67584000\n",
            "2019-05-09 14:02:19.212563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7be718000 of size 67584000\n",
            "2019-05-09 14:02:19.212584: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c278c000 of size 67584000\n",
            "2019-05-09 14:02:19.212604: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c6800000 of size 67584000\n",
            "2019-05-09 14:02:19.212624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ca874000 of size 67584000\n",
            "2019-05-09 14:02:19.212644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ce8e8000 of size 67584000\n",
            "2019-05-09 14:02:19.212665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d295c000 of size 67584000\n",
            "2019-05-09 14:02:19.212686: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d69d0000 of size 67584000\n",
            "2019-05-09 14:02:19.212707: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7daa44000 of size 67584000\n",
            "2019-05-09 14:02:19.212728: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7deab8000 of size 67584000\n",
            "2019-05-09 14:02:19.212750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e2b2c000 of size 67584000\n",
            "2019-05-09 14:02:19.212770: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e6ba0000 of size 67584000\n",
            "2019-05-09 14:02:19.212791: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eac14000 of size 67584000\n",
            "2019-05-09 14:02:19.212827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7eec88000 of size 67584000\n",
            "2019-05-09 14:02:19.212847: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f2cfc000 of size 67584000\n",
            "2019-05-09 14:02:19.212866: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f6d70000 of size 67584000\n",
            "2019-05-09 14:02:19.212901: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fade4000 of size 67584000\n",
            "2019-05-09 14:02:19.212922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fee58000 of size 119963648\n",
            "2019-05-09 14:02:19.212943: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8060c0000 of size 67584000\n",
            "2019-05-09 14:02:19.212963: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x80a134000 of size 67584000\n",
            "2019-05-09 14:02:19.212983: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x80e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.213004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81221c000 of size 2703360\n",
            "2019-05-09 14:02:19.213055: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8124b0000 of size 921600\n",
            "2019-05-09 14:02:19.213076: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x812591000 of size 1781760\n",
            "2019-05-09 14:02:19.213096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812744000 of size 921600\n",
            "2019-05-09 14:02:19.213117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812825000 of size 921600\n",
            "2019-05-09 14:02:19.213138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812906000 of size 921600\n",
            "2019-05-09 14:02:19.213158: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8129e7000 of size 921600\n",
            "2019-05-09 14:02:19.213179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812ac8000 of size 921600\n",
            "2019-05-09 14:02:19.213199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812ba9000 of size 921600\n",
            "2019-05-09 14:02:19.213220: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812c8a000 of size 921600\n",
            "2019-05-09 14:02:19.213240: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812d6b000 of size 921600\n",
            "2019-05-09 14:02:19.213261: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812e4c000 of size 85862400\n",
            "2019-05-09 14:02:19.213281: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81802e800 of size 64512000\n",
            "2019-05-09 14:02:19.213300: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81bdb4800 of size 64512000\n",
            "2019-05-09 14:02:19.213320: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x81fb3a800 of size 921600\n",
            "2019-05-09 14:02:19.213340: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fc1b800 of size 921600\n",
            "2019-05-09 14:02:19.213359: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fcfc800 of size 921600\n",
            "2019-05-09 14:02:19.213380: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fddd800 of size 921600\n",
            "2019-05-09 14:02:19.213402: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81febe800 of size 921600\n",
            "2019-05-09 14:02:19.213422: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x81ff9f800 of size 317952000\n",
            "2019-05-09 14:02:19.213443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x832ed8800 of size 1612800000\n",
            "2019-05-09 14:02:19.213463: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8930ee800 of size 1612800000\n",
            "2019-05-09 14:02:19.213496: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8f3304800 of size 316389376\n",
            "2019-05-09 14:02:19.213542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x9061c0000 of size 1612800000\n",
            "2019-05-09 14:02:19.213563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x9663d6000 of size 1041705984\n",
            "2019-05-09 14:02:19.213582: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \n",
            "2019-05-09 14:02:19.213608: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 97 Chunks of size 256 totalling 24.2KiB\n",
            "2019-05-09 14:02:19.213649: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2019-05-09 14:02:19.213673: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 293 Chunks of size 1536 totalling 439.5KiB\n",
            "2019-05-09 14:02:19.213696: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 2560 totalling 2.5KiB\n",
            "2019-05-09 14:02:19.213720: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 7680 totalling 15.0KiB\n",
            "2019-05-09 14:02:19.213743: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 8960 totalling 8.8KiB\n",
            "2019-05-09 14:02:19.213767: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 15360 totalling 15.0KiB\n",
            "2019-05-09 14:02:19.213791: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 38 Chunks of size 495616 totalling 17.96MiB\n",
            "2019-05-09 14:02:19.213814: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 528128 totalling 515.8KiB\n",
            "2019-05-09 14:02:19.213838: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 596480 totalling 582.5KiB\n",
            "2019-05-09 14:02:19.213876: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 678912 totalling 663.0KiB\n",
            "2019-05-09 14:02:19.213899: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 854528 totalling 834.5KiB\n",
            "2019-05-09 14:02:19.213923: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 17 Chunks of size 921600 totalling 14.94MiB\n",
            "2019-05-09 14:02:19.213945: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 979200 totalling 956.2KiB\n",
            "2019-05-09 14:02:19.213982: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1335040 totalling 1.27MiB\n",
            "2019-05-09 14:02:19.214004: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1781760 totalling 1.70MiB\n",
            "2019-05-09 14:02:19.214027: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2703360 totalling 5.16MiB\n",
            "2019-05-09 14:02:19.214067: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 23040000 totalling 87.89MiB\n",
            "2019-05-09 14:02:19.214092: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 64512000 totalling 123.05MiB\n",
            "2019-05-09 14:02:19.214132: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 52 Chunks of size 67584000 totalling 3.27GiB\n",
            "2019-05-09 14:02:19.214156: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 85862400 totalling 81.88MiB\n",
            "2019-05-09 14:02:19.214179: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 111177728 totalling 106.03MiB\n",
            "2019-05-09 14:02:19.214202: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 119963648 totalling 114.41MiB\n",
            "2019-05-09 14:02:19.214225: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 127565824 totalling 121.66MiB\n",
            "2019-05-09 14:02:19.214248: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131366912 totalling 125.28MiB\n",
            "2019-05-09 14:02:19.214271: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131731456 totalling 125.63MiB\n",
            "2019-05-09 14:02:19.214294: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 134217728 totalling 128.00MiB\n",
            "2019-05-09 14:02:19.214317: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 3 Chunks of size 1612800000 totalling 4.51GiB\n",
            "2019-05-09 14:02:19.214340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 8.81GiB\n",
            "2019-05-09 14:02:19.214383: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \n",
            "Limit:                 11276946637\n",
            "InUse:                  9463017472\n",
            "MaxInUse:               9530602240\n",
            "NumAllocs:                    1191\n",
            "MaxAllocSize:           1612800000\n",
            "\n",
            "2019-05-09 14:02:19.214486: W tensorflow/core/common_runtime/bfc_allocator.cc:271] *******************************************__*****************************__***************_________\n",
            "2019-05-09 14:02:19.214745: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at batch_matmul_op_impl.h:586 : Resource exhausted: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[{{node Adam/update}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "Caused by op 'gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 211, in run_training\n",
            "    aggregation_method=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\n",
            "    grad_loss=grad_loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\n",
            "    colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\n",
            "    unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\", line 1229, in _BatchMatMul\n",
            "    grad_y = math_ops.matmul(x, grad, adjoint_a=True, adjoint_b=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "...which was originally created as op 'func_map_loss/einsum_7/MatMul', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "[elided 1 identical lines from previous traceback]\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 200, in run_training\n",
            "    target_evecs_trans, target_evals\n",
            "  File \"/content/drive/unsupervisedfmnet/DFMnet.py\", line 83, in dfmnet_model\n",
            "    F, G\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 180, in func_map_layer\n",
            "    target_evecs, target_evecs_trans) +\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 116, in penalty_desc_commutativity\n",
            "    G_diag_reduce1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 262, in einsum\n",
            "    axes_to_sum)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 394, in _einsum_reduction\n",
            "    product = math_ops.matmul(t0, t1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewEIKURJ7jdD",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第四次 \n",
        "模型的顶点数调整为500，batch_size调整为16，用的显卡是特斯拉K80,但是loss太大，没有训练意义，且training中的文件是会覆盖的，不用管它，覆盖完的文件自动就跑到回收站了。迭代从第一步开始的。但是上午的使用3000个顶点，batch为4时跑的是从4929步开始的，loss在1000多到2000左右"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDAnx0Pe5xfM",
        "colab_type": "code",
        "outputId": "59b1b65c-3e14-4c55-f814-dcd8a8722a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train - step 959: loss = 2819470.75 (0.859 sec)\n",
            "train - step 960: loss = 12391454.00 (0.851 sec)\n",
            "train - step 961: loss = 3207419.00 (0.836 sec)\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFgwTPpCVl_",
        "colab_type": "text"
      },
      "source": [
        "#主题   第五次运行\n",
        "模型顶点数为6000，batch_size为4，go起来吧\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUylORl7AqIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84894
        },
        "outputId": "80049e38-4c54-47dc-b9ba-f96f707106ab"
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 14:39:15.498874: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 14:39:15.499135: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3162ec0 executing computations on platform Host. Devices:\n",
            "2019-05-09 14:39:15.499175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 14:39:15.599356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 14:39:15.599867: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3163180 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 14:39:15.599904: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 14:39:15.600392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 14:39:15.600427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 14:39:15.994116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 14:39:15.994214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 14:39:15.994233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 14:39:15.994529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 14:46:00.936236: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 14:46:01.468973: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x95498e0\n",
            "train - step 775: loss = 453146.94 (5.311 sec)\n",
            "train - step 776: loss = 506619.38 (1.802 sec)\n",
            "train - step 777: loss = 420729.03 (1.728 sec)\n",
            "train - step 778: loss = 369485.66 (2.564 sec)\n",
            "train - step 779: loss = 409475.81 (1.687 sec)\n",
            "train - step 780: loss = 413229.88 (1.683 sec)\n",
            "train - step 781: loss = 406036.97 (1.697 sec)\n",
            "train - step 782: loss = 514044.47 (1.708 sec)\n",
            "train - step 783: loss = 539288.94 (1.670 sec)\n",
            "train - step 784: loss = 558442.88 (1.687 sec)\n",
            "train - step 785: loss = 454052.50 (1.685 sec)\n",
            "train - step 786: loss = 367280.66 (1.717 sec)\n",
            "train - step 787: loss = 407553.44 (1.669 sec)\n",
            "train - step 788: loss = 526725.44 (1.695 sec)\n",
            "train - step 789: loss = 375453.81 (1.675 sec)\n",
            "train - step 790: loss = 388299.00 (1.676 sec)\n",
            "train - step 791: loss = 485336.94 (1.708 sec)\n",
            "train - step 792: loss = 414568.12 (1.658 sec)\n",
            "train - step 793: loss = 400970.59 (1.718 sec)\n",
            "train - step 794: loss = 458724.81 (1.685 sec)\n",
            "train - step 795: loss = 451180.12 (1.731 sec)\n",
            "train - step 796: loss = 429392.72 (1.706 sec)\n",
            "train - step 797: loss = 424181.41 (1.694 sec)\n",
            "train - step 798: loss = 289368.16 (1.691 sec)\n",
            "train - step 799: loss = 453268.31 (1.709 sec)\n",
            "train - step 800: loss = 608813.38 (1.670 sec)\n",
            "train - step 801: loss = 391998.41 (1.712 sec)\n",
            "train - step 802: loss = 305116.41 (1.684 sec)\n",
            "train - step 803: loss = 402009.81 (1.684 sec)\n",
            "train - step 804: loss = 531024.94 (1.700 sec)\n",
            "train - step 805: loss = 307740.78 (1.698 sec)\n",
            "train - step 806: loss = 522285.06 (1.700 sec)\n",
            "train - step 807: loss = 435155.28 (1.710 sec)\n",
            "train - step 808: loss = 245145.19 (1.728 sec)\n",
            "train - step 809: loss = 484228.62 (1.669 sec)\n",
            "train - step 810: loss = 517955.38 (1.697 sec)\n",
            "train - step 811: loss = 493316.88 (1.713 sec)\n",
            "train - step 812: loss = 641092.75 (1.705 sec)\n",
            "train - step 813: loss = 352564.41 (1.678 sec)\n",
            "train - step 814: loss = 444964.59 (2.820 sec)\n",
            "train - step 815: loss = 598864.25 (1.728 sec)\n",
            "train - step 816: loss = 348906.12 (1.696 sec)\n",
            "train - step 817: loss = 404744.84 (1.720 sec)\n",
            "train - step 818: loss = 462459.84 (1.672 sec)\n",
            "train - step 819: loss = 333766.19 (1.695 sec)\n",
            "train - step 820: loss = 423406.91 (1.649 sec)\n",
            "train - step 821: loss = 469373.03 (1.661 sec)\n",
            "train - step 822: loss = 333444.62 (1.644 sec)\n",
            "train - step 823: loss = 488871.91 (1.652 sec)\n",
            "train - step 824: loss = 441586.22 (1.676 sec)\n",
            "train - step 825: loss = 418657.16 (1.632 sec)\n",
            "train - step 826: loss = 578199.75 (1.681 sec)\n",
            "train - step 827: loss = 380458.88 (1.705 sec)\n",
            "train - step 828: loss = 404544.97 (1.691 sec)\n",
            "train - step 829: loss = 320112.34 (1.705 sec)\n",
            "train - step 830: loss = 437041.03 (1.688 sec)\n",
            "train - step 831: loss = 441706.41 (1.705 sec)\n",
            "train - step 832: loss = 466083.66 (1.705 sec)\n",
            "train - step 833: loss = 484033.59 (1.692 sec)\n",
            "train - step 834: loss = 498264.97 (1.697 sec)\n",
            "train - step 835: loss = 426453.72 (1.723 sec)\n",
            "train - step 836: loss = 555781.81 (1.705 sec)\n",
            "train - step 837: loss = 432876.78 (1.728 sec)\n",
            "train - step 838: loss = 376675.41 (1.704 sec)\n",
            "train - step 839: loss = 471088.00 (1.762 sec)\n",
            "train - step 840: loss = 407877.44 (1.676 sec)\n",
            "train - step 841: loss = 411700.78 (1.698 sec)\n",
            "train - step 842: loss = 509697.69 (1.734 sec)\n",
            "train - step 843: loss = 465694.06 (1.688 sec)\n",
            "train - step 844: loss = 383669.97 (1.740 sec)\n",
            "train - step 845: loss = 666123.38 (1.715 sec)\n",
            "train - step 846: loss = 519735.41 (1.711 sec)\n",
            "train - step 847: loss = 293406.56 (2.148 sec)\n",
            "train - step 848: loss = 405266.41 (1.711 sec)\n",
            "train - step 849: loss = 481961.06 (2.540 sec)\n",
            "train - step 850: loss = 671735.62 (1.709 sec)\n",
            "train - step 851: loss = 434100.81 (1.684 sec)\n",
            "train - step 852: loss = 474003.78 (1.648 sec)\n",
            "train - step 853: loss = 663206.06 (1.661 sec)\n",
            "train - step 854: loss = 425882.91 (1.664 sec)\n",
            "train - step 855: loss = 452056.06 (1.686 sec)\n",
            "train - step 856: loss = 429860.16 (1.681 sec)\n",
            "train - step 857: loss = 459202.47 (1.709 sec)\n",
            "train - step 858: loss = 518916.00 (1.686 sec)\n",
            "train - step 859: loss = 431829.41 (1.748 sec)\n",
            "train - step 860: loss = 434375.94 (1.677 sec)\n",
            "train - step 861: loss = 382375.66 (1.712 sec)\n",
            "train - step 862: loss = 322297.97 (1.704 sec)\n",
            "train - step 863: loss = 299331.06 (1.721 sec)\n",
            "train - step 864: loss = 391019.28 (1.682 sec)\n",
            "train - step 865: loss = 484541.00 (1.685 sec)\n",
            "train - step 866: loss = 361749.56 (1.695 sec)\n",
            "train - step 867: loss = 338939.00 (1.683 sec)\n",
            "train - step 868: loss = 394946.59 (1.717 sec)\n",
            "train - step 869: loss = 618932.62 (1.684 sec)\n",
            "train - step 870: loss = 305384.81 (1.686 sec)\n",
            "train - step 871: loss = 478540.88 (1.700 sec)\n",
            "train - step 872: loss = 543809.56 (1.706 sec)\n",
            "train - step 873: loss = 354112.28 (1.705 sec)\n",
            "train - step 874: loss = 363313.19 (1.683 sec)\n",
            "train - step 875: loss = 683450.12 (1.684 sec)\n",
            "train - step 876: loss = 365955.97 (1.687 sec)\n",
            "train - step 877: loss = 592501.94 (1.697 sec)\n",
            "train - step 878: loss = 449702.72 (1.684 sec)\n",
            "train - step 879: loss = 348273.31 (1.703 sec)\n",
            "train - step 880: loss = 500749.06 (1.682 sec)\n",
            "train - step 881: loss = 295672.47 (1.678 sec)\n",
            "train - step 882: loss = 607463.06 (1.682 sec)\n",
            "train - step 883: loss = 703325.44 (1.729 sec)\n",
            "train - step 884: loss = 408851.84 (1.689 sec)\n",
            "train - step 885: loss = 442423.62 (2.580 sec)\n",
            "train - step 886: loss = 309416.31 (1.717 sec)\n",
            "train - step 887: loss = 404484.03 (1.714 sec)\n",
            "train - step 888: loss = 474199.09 (1.693 sec)\n",
            "train - step 889: loss = 458743.91 (1.665 sec)\n",
            "train - step 890: loss = 443450.03 (1.700 sec)\n",
            "train - step 891: loss = 384472.41 (1.673 sec)\n",
            "train - step 892: loss = 310919.78 (1.673 sec)\n",
            "train - step 893: loss = 417029.16 (1.688 sec)\n",
            "train - step 894: loss = 455598.91 (1.679 sec)\n",
            "train - step 895: loss = 357793.03 (1.691 sec)\n",
            "train - step 896: loss = 340205.69 (1.689 sec)\n",
            "train - step 897: loss = 529065.25 (1.688 sec)\n",
            "train - step 898: loss = 374339.38 (1.693 sec)\n",
            "train - step 899: loss = 370311.72 (1.712 sec)\n",
            "train - step 900: loss = 394917.72 (1.697 sec)\n",
            "train - step 901: loss = 456552.47 (1.690 sec)\n",
            "train - step 902: loss = 651791.06 (1.701 sec)\n",
            "train - step 903: loss = 446567.84 (1.692 sec)\n",
            "train - step 904: loss = 474749.81 (1.699 sec)\n",
            "train - step 905: loss = 483795.84 (1.687 sec)\n",
            "train - step 906: loss = 401564.31 (1.676 sec)\n",
            "train - step 907: loss = 488264.41 (1.680 sec)\n",
            "train - step 908: loss = 325338.66 (1.711 sec)\n",
            "train - step 909: loss = 331574.72 (1.670 sec)\n",
            "train - step 910: loss = 359348.81 (1.690 sec)\n",
            "train - step 911: loss = 507224.00 (1.744 sec)\n",
            "train - step 912: loss = 313442.78 (1.700 sec)\n",
            "train - step 913: loss = 385773.78 (1.693 sec)\n",
            "train - step 914: loss = 563140.38 (1.691 sec)\n",
            "train - step 915: loss = 475707.53 (1.684 sec)\n",
            "train - step 916: loss = 629097.81 (1.699 sec)\n",
            "train - step 917: loss = 339531.72 (1.715 sec)\n",
            "train - step 918: loss = 533716.19 (1.688 sec)\n",
            "train - step 919: loss = 456770.72 (1.694 sec)\n",
            "train - step 920: loss = 567674.12 (1.683 sec)\n",
            "train - step 921: loss = 320178.28 (2.618 sec)\n",
            "train - step 922: loss = 343334.53 (1.709 sec)\n",
            "train - step 923: loss = 441538.19 (1.684 sec)\n",
            "train - step 924: loss = 403078.78 (1.717 sec)\n",
            "train - step 925: loss = 511383.88 (1.722 sec)\n",
            "train - step 926: loss = 476232.62 (1.688 sec)\n",
            "train - step 927: loss = 427345.84 (1.649 sec)\n",
            "train - step 928: loss = 530642.69 (1.696 sec)\n",
            "train - step 929: loss = 334743.31 (1.678 sec)\n",
            "train - step 930: loss = 426909.06 (1.700 sec)\n",
            "train - step 931: loss = 504205.97 (1.675 sec)\n",
            "train - step 932: loss = 411550.31 (1.695 sec)\n",
            "train - step 933: loss = 560156.56 (1.705 sec)\n",
            "train - step 934: loss = 466626.22 (1.705 sec)\n",
            "train - step 935: loss = 381072.78 (1.705 sec)\n",
            "train - step 936: loss = 466924.69 (1.681 sec)\n",
            "train - step 937: loss = 403586.47 (1.688 sec)\n",
            "train - step 938: loss = 519126.50 (1.677 sec)\n",
            "train - step 939: loss = 432650.41 (1.709 sec)\n",
            "train - step 940: loss = 541343.56 (1.666 sec)\n",
            "train - step 941: loss = 471664.50 (1.689 sec)\n",
            "train - step 942: loss = 558673.94 (1.695 sec)\n",
            "train - step 943: loss = 444619.88 (1.703 sec)\n",
            "train - step 944: loss = 441890.84 (1.690 sec)\n",
            "train - step 945: loss = 645164.94 (1.680 sec)\n",
            "train - step 946: loss = 395507.91 (1.696 sec)\n",
            "train - step 947: loss = 317461.62 (1.695 sec)\n",
            "train - step 948: loss = 397708.00 (1.719 sec)\n",
            "train - step 949: loss = 542614.31 (1.681 sec)\n",
            "train - step 950: loss = 369535.56 (1.671 sec)\n",
            "train - step 951: loss = 488756.34 (1.698 sec)\n",
            "train - step 952: loss = 527605.62 (1.698 sec)\n",
            "train - step 953: loss = 371316.56 (1.732 sec)\n",
            "train - step 954: loss = 384864.19 (1.689 sec)\n",
            "train - step 955: loss = 466580.56 (1.721 sec)\n",
            "train - step 956: loss = 407586.56 (1.698 sec)\n",
            "train - step 957: loss = 527495.31 (2.605 sec)\n",
            "train - step 958: loss = 493753.69 (1.727 sec)\n",
            "train - step 959: loss = 329360.66 (1.682 sec)\n",
            "train - step 960: loss = 501824.06 (1.713 sec)\n",
            "train - step 961: loss = 539477.25 (1.695 sec)\n",
            "train - step 962: loss = 399013.53 (1.685 sec)\n",
            "train - step 963: loss = 560219.69 (1.698 sec)\n",
            "train - step 964: loss = 490679.34 (1.667 sec)\n",
            "train - step 965: loss = 438868.62 (1.700 sec)\n",
            "train - step 966: loss = 595325.06 (1.679 sec)\n",
            "train - step 967: loss = 353568.00 (1.716 sec)\n",
            "train - step 968: loss = 550370.81 (1.678 sec)\n",
            "train - step 969: loss = 360542.97 (1.720 sec)\n",
            "train - step 970: loss = 373575.41 (1.693 sec)\n",
            "train - step 971: loss = 328580.97 (1.688 sec)\n",
            "train - step 972: loss = 341268.50 (1.693 sec)\n",
            "train - step 973: loss = 420320.34 (1.697 sec)\n",
            "train - step 974: loss = 490683.88 (1.680 sec)\n",
            "train - step 975: loss = 366336.19 (1.677 sec)\n",
            "train - step 976: loss = 373601.19 (1.687 sec)\n",
            "train - step 977: loss = 369259.81 (1.684 sec)\n",
            "train - step 978: loss = 376325.56 (1.686 sec)\n",
            "train - step 979: loss = 449680.03 (1.694 sec)\n",
            "train - step 980: loss = 434726.28 (1.702 sec)\n",
            "train - step 981: loss = 330161.31 (1.711 sec)\n",
            "train - step 982: loss = 481113.66 (1.685 sec)\n",
            "train - step 983: loss = 392224.69 (1.700 sec)\n",
            "train - step 984: loss = 396916.41 (1.684 sec)\n",
            "train - step 985: loss = 345830.56 (1.704 sec)\n",
            "train - step 986: loss = 538456.31 (1.691 sec)\n",
            "train - step 987: loss = 402848.88 (1.686 sec)\n",
            "train - step 988: loss = 573742.69 (1.667 sec)\n",
            "train - step 989: loss = 381874.41 (1.680 sec)\n",
            "train - step 990: loss = 445971.69 (1.703 sec)\n",
            "train - step 991: loss = 381119.34 (1.670 sec)\n",
            "train - step 992: loss = 352806.91 (1.701 sec)\n",
            "train - step 993: loss = 422001.06 (2.540 sec)\n",
            "train - step 994: loss = 603814.56 (1.698 sec)\n",
            "train - step 995: loss = 436271.72 (1.694 sec)\n",
            "train - step 996: loss = 357280.94 (1.714 sec)\n",
            "train - step 997: loss = 417522.38 (1.701 sec)\n",
            "train - step 998: loss = 440453.56 (1.672 sec)\n",
            "train - step 999: loss = 587650.38 (1.690 sec)\n",
            "train - step 1000: loss = 383447.62 (1.695 sec)\n",
            "train - step 1001: loss = 315973.41 (1.649 sec)\n",
            "train - step 1002: loss = 453755.91 (1.671 sec)\n",
            "train - step 1003: loss = 387422.12 (1.650 sec)\n",
            "train - step 1004: loss = 539067.44 (1.666 sec)\n",
            "train - step 1005: loss = 416332.72 (1.648 sec)\n",
            "train - step 1006: loss = 376390.66 (1.661 sec)\n",
            "train - step 1007: loss = 390484.22 (1.694 sec)\n",
            "train - step 1008: loss = 346912.44 (1.678 sec)\n",
            "train - step 1009: loss = 392309.69 (1.690 sec)\n",
            "train - step 1010: loss = 230596.22 (1.671 sec)\n",
            "train - step 1011: loss = 414682.03 (1.716 sec)\n",
            "train - step 1012: loss = 526813.19 (1.673 sec)\n",
            "train - step 1013: loss = 320668.03 (1.695 sec)\n",
            "train - step 1014: loss = 418977.38 (1.672 sec)\n",
            "train - step 1015: loss = 305448.47 (1.659 sec)\n",
            "train - step 1016: loss = 578776.94 (1.694 sec)\n",
            "train - step 1017: loss = 426348.09 (1.687 sec)\n",
            "train - step 1018: loss = 431934.28 (1.674 sec)\n",
            "train - step 1019: loss = 368576.91 (1.695 sec)\n",
            "train - step 1020: loss = 489402.28 (1.710 sec)\n",
            "train - step 1021: loss = 561800.06 (1.701 sec)\n",
            "train - step 1022: loss = 455883.69 (1.691 sec)\n",
            "train - step 1023: loss = 295577.34 (1.700 sec)\n",
            "train - step 1024: loss = 352627.88 (1.675 sec)\n",
            "train - step 1025: loss = 365165.84 (1.698 sec)\n",
            "train - step 1026: loss = 570622.69 (1.693 sec)\n",
            "train - step 1027: loss = 301952.62 (1.660 sec)\n",
            "train - step 1028: loss = 415014.28 (1.711 sec)\n",
            "train - step 1029: loss = 355892.66 (2.762 sec)\n",
            "train - step 1030: loss = 556081.06 (1.681 sec)\n",
            "train - step 1031: loss = 347489.50 (1.650 sec)\n",
            "train - step 1032: loss = 566983.69 (1.663 sec)\n",
            "train - step 1033: loss = 481540.97 (1.681 sec)\n",
            "train - step 1034: loss = 540400.31 (1.692 sec)\n",
            "train - step 1035: loss = 356574.72 (1.671 sec)\n",
            "train - step 1036: loss = 402765.06 (1.696 sec)\n",
            "train - step 1037: loss = 351389.38 (1.683 sec)\n",
            "train - step 1038: loss = 489789.81 (1.682 sec)\n",
            "train - step 1039: loss = 415209.56 (1.672 sec)\n",
            "train - step 1040: loss = 512672.06 (1.681 sec)\n",
            "train - step 1041: loss = 533912.25 (1.671 sec)\n",
            "train - step 1042: loss = 279709.44 (1.685 sec)\n",
            "train - step 1043: loss = 375785.44 (1.673 sec)\n",
            "train - step 1044: loss = 464548.16 (1.691 sec)\n",
            "train - step 1045: loss = 346004.81 (1.669 sec)\n",
            "train - step 1046: loss = 409171.50 (1.681 sec)\n",
            "train - step 1047: loss = 343569.56 (1.704 sec)\n",
            "train - step 1048: loss = 450964.28 (1.673 sec)\n",
            "train - step 1049: loss = 330223.94 (1.706 sec)\n",
            "train - step 1050: loss = 524627.25 (1.677 sec)\n",
            "train - step 1051: loss = 337536.06 (1.692 sec)\n",
            "train - step 1052: loss = 491544.28 (1.683 sec)\n",
            "train - step 1053: loss = 450086.72 (1.708 sec)\n",
            "train - step 1054: loss = 430916.66 (1.674 sec)\n",
            "train - step 1055: loss = 375098.56 (1.684 sec)\n",
            "train - step 1056: loss = 492405.72 (1.699 sec)\n",
            "train - step 1057: loss = 443545.03 (1.707 sec)\n",
            "train - step 1058: loss = 521929.88 (1.711 sec)\n",
            "train - step 1059: loss = 401185.38 (1.690 sec)\n",
            "train - step 1060: loss = 410726.00 (1.704 sec)\n",
            "train - step 1061: loss = 607002.94 (1.710 sec)\n",
            "train - step 1062: loss = 345541.41 (1.702 sec)\n",
            "train - step 1063: loss = 415383.53 (1.695 sec)\n",
            "train - step 1064: loss = 428625.22 (1.699 sec)\n",
            "train - step 1065: loss = 483084.78 (2.717 sec)\n",
            "train - step 1066: loss = 422542.19 (1.701 sec)\n",
            "train - step 1067: loss = 535514.75 (1.712 sec)\n",
            "train - step 1068: loss = 442585.38 (1.680 sec)\n",
            "train - step 1069: loss = 548315.06 (1.703 sec)\n",
            "train - step 1070: loss = 359684.72 (1.672 sec)\n",
            "train - step 1071: loss = 549652.69 (1.685 sec)\n",
            "train - step 1072: loss = 441435.16 (1.678 sec)\n",
            "train - step 1073: loss = 644869.81 (1.685 sec)\n",
            "train - step 1074: loss = 461583.19 (1.694 sec)\n",
            "train - step 1075: loss = 462400.06 (1.691 sec)\n",
            "train - step 1076: loss = 421014.28 (1.692 sec)\n",
            "train - step 1077: loss = 456235.84 (1.686 sec)\n",
            "train - step 1078: loss = 763452.88 (1.701 sec)\n",
            "train - step 1079: loss = 394319.44 (1.665 sec)\n",
            "train - step 1080: loss = 460044.62 (1.687 sec)\n",
            "train - step 1081: loss = 424946.44 (1.710 sec)\n",
            "train - step 1082: loss = 539922.19 (1.695 sec)\n",
            "train - step 1083: loss = 471638.41 (1.699 sec)\n",
            "train - step 1084: loss = 399951.44 (1.702 sec)\n",
            "train - step 1085: loss = 480019.06 (1.676 sec)\n",
            "train - step 1086: loss = 542287.94 (1.693 sec)\n",
            "train - step 1087: loss = 362306.97 (1.692 sec)\n",
            "train - step 1088: loss = 385413.41 (1.681 sec)\n",
            "train - step 1089: loss = 484866.81 (1.703 sec)\n",
            "train - step 1090: loss = 484700.84 (1.695 sec)\n",
            "train - step 1091: loss = 442644.88 (1.689 sec)\n",
            "train - step 1092: loss = 450297.97 (1.695 sec)\n",
            "train - step 1093: loss = 611591.00 (1.704 sec)\n",
            "train - step 1094: loss = 245958.69 (1.682 sec)\n",
            "train - step 1095: loss = 340670.00 (1.689 sec)\n",
            "train - step 1096: loss = 431602.00 (1.701 sec)\n",
            "train - step 1097: loss = 569963.69 (1.666 sec)\n",
            "train - step 1098: loss = 375568.12 (1.722 sec)\n",
            "train - step 1099: loss = 293360.03 (1.723 sec)\n",
            "train - step 1100: loss = 399630.31 (1.678 sec)\n",
            "train - step 1101: loss = 463802.59 (2.595 sec)\n",
            "train - step 1102: loss = 448937.06 (1.686 sec)\n",
            "train - step 1103: loss = 352466.09 (1.670 sec)\n",
            "train - step 1104: loss = 538428.94 (1.692 sec)\n",
            "train - step 1105: loss = 392460.16 (1.672 sec)\n",
            "train - step 1106: loss = 454082.81 (1.689 sec)\n",
            "train - step 1107: loss = 453480.22 (1.677 sec)\n",
            "train - step 1108: loss = 461457.00 (1.663 sec)\n",
            "train - step 1109: loss = 479212.62 (1.708 sec)\n",
            "train - step 1110: loss = 592019.12 (1.685 sec)\n",
            "train - step 1111: loss = 291676.69 (1.728 sec)\n",
            "train - step 1112: loss = 458112.00 (1.663 sec)\n",
            "train - step 1113: loss = 410964.88 (1.691 sec)\n",
            "train - step 1114: loss = 450192.66 (1.684 sec)\n",
            "train - step 1115: loss = 368865.19 (1.706 sec)\n",
            "train - step 1116: loss = 612897.38 (1.678 sec)\n",
            "train - step 1117: loss = 429601.34 (1.678 sec)\n",
            "train - step 1118: loss = 333169.50 (1.695 sec)\n",
            "train - step 1119: loss = 321377.34 (1.681 sec)\n",
            "train - step 1120: loss = 434379.47 (1.743 sec)\n",
            "train - step 1121: loss = 450829.78 (1.677 sec)\n",
            "train - step 1122: loss = 519117.53 (1.684 sec)\n",
            "train - step 1123: loss = 441543.53 (1.702 sec)\n",
            "train - step 1124: loss = 431085.72 (1.722 sec)\n",
            "train - step 1125: loss = 407233.59 (1.701 sec)\n",
            "train - step 1126: loss = 377767.06 (1.710 sec)\n",
            "train - step 1127: loss = 551744.19 (1.697 sec)\n",
            "train - step 1128: loss = 407641.66 (1.684 sec)\n",
            "train - step 1129: loss = 432107.78 (1.733 sec)\n",
            "train - step 1130: loss = 426087.97 (1.781 sec)\n",
            "train - step 1131: loss = 574380.81 (1.712 sec)\n",
            "train - step 1132: loss = 600538.06 (1.723 sec)\n",
            "train - step 1133: loss = 473425.12 (1.715 sec)\n",
            "train - step 1134: loss = 577815.69 (1.726 sec)\n",
            "train - step 1135: loss = 361266.47 (1.699 sec)\n",
            "train - step 1136: loss = 596591.62 (2.758 sec)\n",
            "train - step 1137: loss = 540797.88 (1.737 sec)\n",
            "train - step 1138: loss = 348718.00 (1.704 sec)\n",
            "train - step 1139: loss = 498781.44 (1.815 sec)\n",
            "train - step 1140: loss = 362942.28 (1.689 sec)\n",
            "train - step 1141: loss = 340516.97 (1.686 sec)\n",
            "train - step 1142: loss = 470879.94 (1.702 sec)\n",
            "train - step 1143: loss = 480210.47 (1.799 sec)\n",
            "train - step 1144: loss = 330129.88 (1.675 sec)\n",
            "train - step 1145: loss = 506594.66 (1.690 sec)\n",
            "train - step 1146: loss = 406707.38 (1.671 sec)\n",
            "train - step 1147: loss = 412547.41 (1.708 sec)\n",
            "train - step 1148: loss = 354343.72 (1.674 sec)\n",
            "train - step 1149: loss = 419340.34 (1.699 sec)\n",
            "train - step 1150: loss = 463711.38 (1.695 sec)\n",
            "train - step 1151: loss = 578363.00 (1.699 sec)\n",
            "train - step 1152: loss = 288011.44 (1.695 sec)\n",
            "train - step 1153: loss = 387715.78 (1.683 sec)\n",
            "train - step 1154: loss = 527845.56 (1.686 sec)\n",
            "train - step 1155: loss = 461766.09 (1.654 sec)\n",
            "train - step 1156: loss = 291144.22 (1.649 sec)\n",
            "train - step 1157: loss = 374294.28 (1.682 sec)\n",
            "train - step 1158: loss = 488170.41 (1.690 sec)\n",
            "train - step 1159: loss = 461486.03 (1.685 sec)\n",
            "train - step 1160: loss = 318411.12 (1.681 sec)\n",
            "train - step 1161: loss = 493435.06 (1.709 sec)\n",
            "train - step 1162: loss = 487990.72 (1.674 sec)\n",
            "train - step 1163: loss = 441641.53 (1.697 sec)\n",
            "train - step 1164: loss = 445847.78 (1.702 sec)\n",
            "train - step 1165: loss = 435836.50 (1.684 sec)\n",
            "train - step 1166: loss = 366837.78 (1.697 sec)\n",
            "train - step 1167: loss = 481906.12 (1.716 sec)\n",
            "train - step 1168: loss = 420114.94 (1.695 sec)\n",
            "train - step 1169: loss = 365067.50 (1.684 sec)\n",
            "train - step 1170: loss = 340750.41 (1.694 sec)\n",
            "train - step 1171: loss = 511874.91 (2.739 sec)\n",
            "train - step 1172: loss = 371629.50 (1.730 sec)\n",
            "train - step 1173: loss = 439554.97 (1.696 sec)\n",
            "train - step 1174: loss = 249651.42 (1.686 sec)\n",
            "train - step 1175: loss = 431984.28 (1.699 sec)\n",
            "train - step 1176: loss = 400052.81 (1.700 sec)\n",
            "train - step 1177: loss = 495524.38 (1.683 sec)\n",
            "train - step 1178: loss = 418421.94 (1.684 sec)\n",
            "train - step 1179: loss = 384902.00 (1.717 sec)\n",
            "train - step 1180: loss = 320134.97 (1.692 sec)\n",
            "train - step 1181: loss = 388647.28 (1.721 sec)\n",
            "train - step 1182: loss = 523571.19 (1.646 sec)\n",
            "train - step 1183: loss = 410198.69 (1.652 sec)\n",
            "train - step 1184: loss = 590039.56 (1.633 sec)\n",
            "train - step 1185: loss = 377191.12 (1.645 sec)\n",
            "train - step 1186: loss = 582317.75 (1.662 sec)\n",
            "train - step 1187: loss = 403931.94 (1.656 sec)\n",
            "train - step 1188: loss = 430097.31 (1.688 sec)\n",
            "train - step 1189: loss = 530538.75 (1.693 sec)\n",
            "train - step 1190: loss = 535007.12 (1.688 sec)\n",
            "train - step 1191: loss = 457950.28 (1.694 sec)\n",
            "train - step 1192: loss = 401828.06 (1.680 sec)\n",
            "train - step 1193: loss = 430857.69 (1.697 sec)\n",
            "train - step 1194: loss = 442999.06 (1.681 sec)\n",
            "train - step 1195: loss = 488205.62 (1.689 sec)\n",
            "train - step 1196: loss = 424122.69 (1.715 sec)\n",
            "train - step 1197: loss = 646749.69 (1.694 sec)\n",
            "train - step 1198: loss = 461682.06 (1.704 sec)\n",
            "train - step 1199: loss = 462563.22 (1.703 sec)\n",
            "train - step 1200: loss = 515672.97 (1.720 sec)\n",
            "train - step 1201: loss = 363461.97 (1.692 sec)\n",
            "train - step 1202: loss = 428638.38 (1.685 sec)\n",
            "train - step 1203: loss = 355150.47 (1.688 sec)\n",
            "train - step 1204: loss = 518376.38 (1.687 sec)\n",
            "train - step 1205: loss = 493388.22 (1.698 sec)\n",
            "train - step 1206: loss = 344018.00 (1.691 sec)\n",
            "train - step 1207: loss = 335088.31 (2.513 sec)\n",
            "train - step 1208: loss = 516737.56 (1.651 sec)\n",
            "train - step 1209: loss = 462709.09 (1.648 sec)\n",
            "train - step 1210: loss = 406092.66 (1.682 sec)\n",
            "train - step 1211: loss = 398207.62 (1.699 sec)\n",
            "train - step 1212: loss = 474026.16 (1.676 sec)\n",
            "train - step 1213: loss = 489719.41 (1.701 sec)\n",
            "train - step 1214: loss = 326093.09 (1.670 sec)\n",
            "train - step 1215: loss = 518720.94 (1.676 sec)\n",
            "train - step 1216: loss = 356304.19 (1.690 sec)\n",
            "train - step 1217: loss = 441501.31 (1.679 sec)\n",
            "train - step 1218: loss = 409193.62 (1.702 sec)\n",
            "train - step 1219: loss = 381603.84 (1.705 sec)\n",
            "train - step 1220: loss = 520435.66 (1.696 sec)\n",
            "train - step 1221: loss = 401042.97 (1.685 sec)\n",
            "train - step 1222: loss = 515774.66 (1.695 sec)\n",
            "train - step 1223: loss = 421869.34 (1.700 sec)\n",
            "train - step 1224: loss = 424747.72 (1.688 sec)\n",
            "train - step 1225: loss = 406312.12 (1.681 sec)\n",
            "train - step 1226: loss = 460813.94 (1.669 sec)\n",
            "train - step 1227: loss = 360139.97 (1.712 sec)\n",
            "train - step 1228: loss = 530217.88 (1.671 sec)\n",
            "train - step 1229: loss = 462142.81 (1.706 sec)\n",
            "train - step 1230: loss = 610202.25 (1.673 sec)\n",
            "train - step 1231: loss = 379158.72 (1.677 sec)\n",
            "train - step 1232: loss = 457826.34 (1.672 sec)\n",
            "train - step 1233: loss = 353568.72 (1.694 sec)\n",
            "train - step 1234: loss = 361636.72 (1.689 sec)\n",
            "train - step 1235: loss = 428031.41 (1.675 sec)\n",
            "train - step 1236: loss = 374525.19 (1.709 sec)\n",
            "train - step 1237: loss = 450444.72 (1.682 sec)\n",
            "train - step 1238: loss = 523442.59 (1.705 sec)\n",
            "train - step 1239: loss = 443326.00 (1.683 sec)\n",
            "train - step 1240: loss = 589305.31 (1.696 sec)\n",
            "train - step 1241: loss = 344147.41 (1.692 sec)\n",
            "train - step 1242: loss = 476320.44 (1.671 sec)\n",
            "train - step 1243: loss = 510548.88 (2.632 sec)\n",
            "train - step 1244: loss = 335749.50 (1.694 sec)\n",
            "train - step 1245: loss = 406969.59 (1.695 sec)\n",
            "train - step 1246: loss = 357414.09 (1.665 sec)\n",
            "train - step 1247: loss = 311103.97 (1.698 sec)\n",
            "train - step 1248: loss = 594808.44 (1.689 sec)\n",
            "train - step 1249: loss = 404516.59 (1.694 sec)\n",
            "train - step 1250: loss = 445179.84 (1.699 sec)\n",
            "train - step 1251: loss = 317900.34 (1.680 sec)\n",
            "train - step 1252: loss = 525045.75 (1.702 sec)\n",
            "train - step 1253: loss = 421846.12 (1.687 sec)\n",
            "train - step 1254: loss = 459011.53 (1.708 sec)\n",
            "train - step 1255: loss = 543132.81 (1.705 sec)\n",
            "train - step 1256: loss = 506070.50 (1.680 sec)\n",
            "train - step 1257: loss = 394810.84 (1.672 sec)\n",
            "train - step 1258: loss = 480619.34 (1.703 sec)\n",
            "train - step 1259: loss = 361428.94 (1.693 sec)\n",
            "train - step 1260: loss = 314989.53 (1.708 sec)\n",
            "train - step 1261: loss = 436357.81 (1.681 sec)\n",
            "train - step 1262: loss = 433141.97 (1.673 sec)\n",
            "train - step 1263: loss = 428727.72 (1.691 sec)\n",
            "train - step 1264: loss = 517693.81 (1.684 sec)\n",
            "train - step 1265: loss = 453878.22 (1.677 sec)\n",
            "train - step 1266: loss = 559258.31 (1.661 sec)\n",
            "train - step 1267: loss = 417604.66 (2.379 sec)\n",
            "train - step 1268: loss = 368065.47 (1.712 sec)\n",
            "train - step 1269: loss = 597728.06 (1.699 sec)\n",
            "train - step 1270: loss = 413049.94 (1.715 sec)\n",
            "train - step 1271: loss = 351948.94 (1.687 sec)\n",
            "train - step 1272: loss = 367428.34 (1.681 sec)\n",
            "train - step 1273: loss = 604494.56 (1.682 sec)\n",
            "train - step 1274: loss = 525847.62 (1.687 sec)\n",
            "train - step 1275: loss = 432212.41 (1.667 sec)\n",
            "train - step 1276: loss = 389299.59 (1.682 sec)\n",
            "train - step 1277: loss = 352986.88 (1.673 sec)\n",
            "train - step 1278: loss = 351434.94 (2.539 sec)\n",
            "train - step 1279: loss = 470362.62 (1.700 sec)\n",
            "train - step 1280: loss = 452884.59 (1.682 sec)\n",
            "train - step 1281: loss = 538017.12 (1.699 sec)\n",
            "train - step 1282: loss = 442739.47 (1.683 sec)\n",
            "train - step 1283: loss = 471884.84 (1.676 sec)\n",
            "train - step 1284: loss = 354731.34 (1.694 sec)\n",
            "train - step 1285: loss = 473826.81 (1.688 sec)\n",
            "train - step 1286: loss = 420004.72 (1.689 sec)\n",
            "train - step 1287: loss = 300694.41 (1.687 sec)\n",
            "train - step 1288: loss = 535150.19 (1.708 sec)\n",
            "train - step 1289: loss = 651214.00 (1.674 sec)\n",
            "train - step 1290: loss = 317839.41 (1.682 sec)\n",
            "train - step 1291: loss = 495179.03 (1.707 sec)\n",
            "train - step 1292: loss = 563479.25 (1.701 sec)\n",
            "train - step 1293: loss = 373634.59 (1.686 sec)\n",
            "train - step 1294: loss = 471321.97 (1.683 sec)\n",
            "train - step 1295: loss = 324964.28 (1.704 sec)\n",
            "train - step 1296: loss = 547445.88 (1.681 sec)\n",
            "train - step 1297: loss = 519948.94 (1.717 sec)\n",
            "train - step 1298: loss = 390901.56 (1.687 sec)\n",
            "train - step 1299: loss = 443597.38 (1.700 sec)\n",
            "train - step 1300: loss = 550466.81 (1.693 sec)\n",
            "train - step 1301: loss = 363582.66 (1.686 sec)\n",
            "train - step 1302: loss = 494965.59 (1.675 sec)\n",
            "train - step 1303: loss = 431865.47 (1.707 sec)\n",
            "train - step 1304: loss = 389082.47 (1.692 sec)\n",
            "train - step 1305: loss = 531662.94 (1.678 sec)\n",
            "train - step 1306: loss = 517605.66 (1.680 sec)\n",
            "train - step 1307: loss = 544629.25 (1.682 sec)\n",
            "train - step 1308: loss = 400404.31 (1.681 sec)\n",
            "train - step 1309: loss = 491458.06 (1.667 sec)\n",
            "train - step 1310: loss = 274397.06 (1.677 sec)\n",
            "train - step 1311: loss = 459548.06 (1.689 sec)\n",
            "train - step 1312: loss = 441491.88 (1.701 sec)\n",
            "train - step 1313: loss = 423598.72 (1.683 sec)\n",
            "train - step 1314: loss = 404079.81 (2.565 sec)\n",
            "train - step 1315: loss = 475898.16 (1.716 sec)\n",
            "train - step 1316: loss = 632892.00 (1.668 sec)\n",
            "train - step 1317: loss = 485355.72 (1.680 sec)\n",
            "train - step 1318: loss = 450978.59 (1.682 sec)\n",
            "train - step 1319: loss = 419831.97 (1.677 sec)\n",
            "train - step 1320: loss = 393749.94 (1.692 sec)\n",
            "train - step 1321: loss = 387681.47 (1.668 sec)\n",
            "train - step 1322: loss = 393149.66 (1.684 sec)\n",
            "train - step 1323: loss = 548426.06 (1.696 sec)\n",
            "train - step 1324: loss = 462974.59 (1.669 sec)\n",
            "train - step 1325: loss = 581554.31 (1.674 sec)\n",
            "train - step 1326: loss = 465394.84 (1.699 sec)\n",
            "train - step 1327: loss = 335483.62 (1.690 sec)\n",
            "train - step 1328: loss = 569015.94 (1.679 sec)\n",
            "train - step 1329: loss = 598241.38 (1.703 sec)\n",
            "train - step 1330: loss = 347274.09 (1.680 sec)\n",
            "train - step 1331: loss = 416198.53 (1.696 sec)\n",
            "train - step 1332: loss = 469603.53 (1.683 sec)\n",
            "train - step 1333: loss = 505894.81 (1.696 sec)\n",
            "train - step 1334: loss = 544749.88 (1.687 sec)\n",
            "train - step 1335: loss = 451543.19 (1.695 sec)\n",
            "train - step 1336: loss = 512289.84 (1.681 sec)\n",
            "train - step 1337: loss = 351492.81 (1.685 sec)\n",
            "train - step 1338: loss = 407429.38 (1.681 sec)\n",
            "train - step 1339: loss = 520961.62 (1.706 sec)\n",
            "train - step 1340: loss = 274838.59 (1.681 sec)\n",
            "train - step 1341: loss = 371840.72 (1.716 sec)\n",
            "train - step 1342: loss = 446852.31 (1.679 sec)\n",
            "train - step 1343: loss = 323314.22 (1.688 sec)\n",
            "train - step 1344: loss = 528288.38 (1.677 sec)\n",
            "train - step 1345: loss = 385364.72 (1.709 sec)\n",
            "train - step 1346: loss = 350057.72 (1.691 sec)\n",
            "train - step 1347: loss = 475083.47 (1.690 sec)\n",
            "train - step 1348: loss = 433920.56 (1.681 sec)\n",
            "train - step 1349: loss = 469312.72 (1.699 sec)\n",
            "train - step 1350: loss = 374735.62 (2.677 sec)\n",
            "train - step 1351: loss = 366343.28 (1.712 sec)\n",
            "train - step 1352: loss = 360761.69 (1.685 sec)\n",
            "train - step 1353: loss = 356697.59 (1.680 sec)\n",
            "train - step 1354: loss = 419506.28 (1.733 sec)\n",
            "train - step 1355: loss = 478399.00 (1.700 sec)\n",
            "train - step 1356: loss = 325131.00 (1.723 sec)\n",
            "train - step 1357: loss = 454849.94 (1.698 sec)\n",
            "train - step 1358: loss = 569959.31 (1.686 sec)\n",
            "train - step 1359: loss = 318915.28 (1.698 sec)\n",
            "train - step 1360: loss = 313689.44 (1.683 sec)\n",
            "train - step 1361: loss = 543127.38 (1.688 sec)\n",
            "train - step 1362: loss = 437382.16 (1.690 sec)\n",
            "train - step 1363: loss = 416535.94 (1.659 sec)\n",
            "train - step 1364: loss = 597074.94 (1.640 sec)\n",
            "train - step 1365: loss = 421867.81 (1.650 sec)\n",
            "train - step 1366: loss = 474993.94 (1.639 sec)\n",
            "train - step 1367: loss = 438304.28 (1.651 sec)\n",
            "train - step 1368: loss = 392367.09 (1.658 sec)\n",
            "train - step 1369: loss = 527830.06 (1.665 sec)\n",
            "train - step 1370: loss = 516240.81 (1.690 sec)\n",
            "train - step 1371: loss = 471732.53 (1.688 sec)\n",
            "train - step 1372: loss = 388593.84 (1.717 sec)\n",
            "train - step 1373: loss = 451814.81 (1.698 sec)\n",
            "train - step 1374: loss = 617659.00 (1.689 sec)\n",
            "train - step 1375: loss = 515205.88 (1.692 sec)\n",
            "train - step 1376: loss = 587576.19 (1.691 sec)\n",
            "train - step 1377: loss = 417412.66 (1.702 sec)\n",
            "train - step 1378: loss = 489953.56 (1.695 sec)\n",
            "train - step 1379: loss = 373728.53 (1.705 sec)\n",
            "train - step 1380: loss = 454836.09 (1.680 sec)\n",
            "train - step 1381: loss = 423606.72 (1.675 sec)\n",
            "train - step 1382: loss = 373443.44 (1.668 sec)\n",
            "train - step 1383: loss = 507774.72 (1.731 sec)\n",
            "train - step 1384: loss = 425674.66 (1.684 sec)\n",
            "train - step 1385: loss = 472495.50 (1.705 sec)\n",
            "train - step 1386: loss = 556157.69 (2.501 sec)\n",
            "train - step 1387: loss = 449697.34 (1.696 sec)\n",
            "train - step 1388: loss = 525615.31 (1.704 sec)\n",
            "train - step 1389: loss = 415558.84 (1.678 sec)\n",
            "train - step 1390: loss = 356324.53 (1.714 sec)\n",
            "train - step 1391: loss = 327505.47 (1.668 sec)\n",
            "train - step 1392: loss = 434525.91 (1.698 sec)\n",
            "train - step 1393: loss = 483425.34 (1.719 sec)\n",
            "train - step 1394: loss = 472232.12 (1.680 sec)\n",
            "train - step 1395: loss = 279064.06 (1.678 sec)\n",
            "train - step 1396: loss = 456258.06 (1.675 sec)\n",
            "train - step 1397: loss = 307227.19 (1.680 sec)\n",
            "train - step 1398: loss = 324331.41 (1.684 sec)\n",
            "train - step 1399: loss = 528675.19 (1.704 sec)\n",
            "train - step 1400: loss = 422670.31 (1.671 sec)\n",
            "train - step 1401: loss = 461184.16 (1.692 sec)\n",
            "train - step 1402: loss = 449936.31 (1.703 sec)\n",
            "train - step 1403: loss = 344560.31 (1.695 sec)\n",
            "train - step 1404: loss = 328881.53 (1.691 sec)\n",
            "train - step 1405: loss = 445755.00 (1.706 sec)\n",
            "train - step 1406: loss = 508752.94 (1.684 sec)\n",
            "train - step 1407: loss = 660469.81 (1.681 sec)\n",
            "train - step 1408: loss = 469068.03 (1.674 sec)\n",
            "train - step 1409: loss = 468471.38 (1.671 sec)\n",
            "train - step 1410: loss = 423025.81 (1.693 sec)\n",
            "train - step 1411: loss = 490607.78 (1.667 sec)\n",
            "train - step 1412: loss = 199697.77 (1.677 sec)\n",
            "train - step 1413: loss = 332552.91 (1.678 sec)\n",
            "train - step 1414: loss = 319918.72 (1.669 sec)\n",
            "train - step 1415: loss = 366258.97 (1.693 sec)\n",
            "train - step 1416: loss = 552879.69 (1.666 sec)\n",
            "train - step 1417: loss = 193016.80 (1.683 sec)\n",
            "train - step 1418: loss = 464233.34 (1.674 sec)\n",
            "train - step 1419: loss = 547916.25 (1.712 sec)\n",
            "train - step 1420: loss = 450011.59 (1.704 sec)\n",
            "train - step 1421: loss = 517529.66 (1.759 sec)\n",
            "train - step 1422: loss = 358908.19 (1.720 sec)\n",
            "train - step 1423: loss = 366787.72 (1.694 sec)\n",
            "train - step 1424: loss = 386275.12 (1.710 sec)\n",
            "train - step 1425: loss = 535393.88 (1.695 sec)\n",
            "train - step 1426: loss = 439911.97 (1.669 sec)\n",
            "train - step 1427: loss = 347936.94 (1.684 sec)\n",
            "train - step 1428: loss = 408103.28 (1.686 sec)\n",
            "train - step 1429: loss = 391655.78 (1.668 sec)\n",
            "train - step 1430: loss = 582305.38 (1.687 sec)\n",
            "train - step 1431: loss = 548124.44 (2.139 sec)\n",
            "train - step 1432: loss = 328992.47 (1.717 sec)\n",
            "train - step 1433: loss = 329049.59 (1.702 sec)\n",
            "train - step 1434: loss = 383847.00 (1.700 sec)\n",
            "train - step 1435: loss = 403458.47 (1.675 sec)\n",
            "train - step 1436: loss = 509997.03 (1.663 sec)\n",
            "train - step 1437: loss = 532594.19 (1.647 sec)\n",
            "train - step 1438: loss = 523897.66 (1.689 sec)\n",
            "train - step 1439: loss = 382449.56 (1.686 sec)\n",
            "train - step 1440: loss = 610762.00 (1.664 sec)\n",
            "train - step 1441: loss = 423545.41 (1.685 sec)\n",
            "train - step 1442: loss = 360963.56 (1.679 sec)\n",
            "train - step 1443: loss = 298670.09 (1.656 sec)\n",
            "train - step 1444: loss = 416577.16 (1.633 sec)\n",
            "train - step 1445: loss = 546949.75 (1.651 sec)\n",
            "train - step 1446: loss = 491796.16 (1.677 sec)\n",
            "train - step 1447: loss = 438830.66 (1.704 sec)\n",
            "train - step 1448: loss = 363772.41 (1.690 sec)\n",
            "train - step 1449: loss = 477473.06 (1.684 sec)\n",
            "train - step 1450: loss = 502121.53 (1.653 sec)\n",
            "train - step 1451: loss = 399376.66 (1.647 sec)\n",
            "train - step 1452: loss = 383471.28 (1.688 sec)\n",
            "train - step 1453: loss = 477940.94 (1.688 sec)\n",
            "train - step 1454: loss = 253917.33 (1.686 sec)\n",
            "train - step 1455: loss = 418551.56 (1.703 sec)\n",
            "train - step 1456: loss = 550854.62 (1.677 sec)\n",
            "train - step 1457: loss = 433916.91 (2.750 sec)\n",
            "train - step 1458: loss = 539466.69 (1.713 sec)\n",
            "train - step 1459: loss = 398935.09 (1.675 sec)\n",
            "train - step 1460: loss = 381415.03 (1.697 sec)\n",
            "train - step 1461: loss = 507384.56 (1.684 sec)\n",
            "train - step 1462: loss = 650614.62 (1.685 sec)\n",
            "train - step 1463: loss = 426304.47 (1.673 sec)\n",
            "train - step 1464: loss = 571061.56 (1.683 sec)\n",
            "train - step 1465: loss = 469100.50 (1.654 sec)\n",
            "train - step 1466: loss = 510158.34 (1.698 sec)\n",
            "train - step 1467: loss = 423310.22 (1.674 sec)\n",
            "train - step 1468: loss = 401930.50 (1.683 sec)\n",
            "train - step 1469: loss = 580761.12 (1.685 sec)\n",
            "train - step 1470: loss = 432290.78 (1.676 sec)\n",
            "train - step 1471: loss = 551950.31 (1.682 sec)\n",
            "train - step 1472: loss = 504195.09 (1.682 sec)\n",
            "train - step 1473: loss = 476182.16 (1.705 sec)\n",
            "train - step 1474: loss = 414296.03 (1.659 sec)\n",
            "train - step 1475: loss = 451980.34 (1.698 sec)\n",
            "train - step 1476: loss = 330296.12 (1.678 sec)\n",
            "train - step 1477: loss = 378407.62 (1.749 sec)\n",
            "train - step 1478: loss = 422916.12 (1.717 sec)\n",
            "train - step 1479: loss = 394539.97 (1.684 sec)\n",
            "train - step 1480: loss = 577221.44 (1.690 sec)\n",
            "train - step 1481: loss = 497073.47 (1.677 sec)\n",
            "train - step 1482: loss = 365242.34 (1.704 sec)\n",
            "train - step 1483: loss = 294383.66 (1.724 sec)\n",
            "train - step 1484: loss = 440863.59 (1.735 sec)\n",
            "train - step 1485: loss = 331055.28 (1.714 sec)\n",
            "train - step 1486: loss = 318303.88 (1.747 sec)\n",
            "train - step 1487: loss = 285088.59 (1.706 sec)\n",
            "train - step 1488: loss = 440137.91 (1.671 sec)\n",
            "train - step 1489: loss = 444319.69 (1.674 sec)\n",
            "train - step 1490: loss = 365187.38 (1.700 sec)\n",
            "train - step 1491: loss = 658367.88 (1.692 sec)\n",
            "train - step 1492: loss = 627580.69 (2.653 sec)\n",
            "train - step 1493: loss = 553817.06 (1.736 sec)\n",
            "train - step 1494: loss = 554807.69 (1.697 sec)\n",
            "train - step 1495: loss = 432432.12 (1.701 sec)\n",
            "train - step 1496: loss = 681608.25 (1.682 sec)\n",
            "train - step 1497: loss = 598874.88 (1.700 sec)\n",
            "train - step 1498: loss = 468123.97 (1.668 sec)\n",
            "train - step 1499: loss = 544070.38 (1.682 sec)\n",
            "train - step 1500: loss = 256486.78 (1.717 sec)\n",
            "train - step 1501: loss = 367472.28 (1.700 sec)\n",
            "train - step 1502: loss = 660601.62 (1.682 sec)\n",
            "train - step 1503: loss = 458535.22 (1.688 sec)\n",
            "train - step 1504: loss = 345323.16 (1.693 sec)\n",
            "train - step 1505: loss = 520759.66 (1.683 sec)\n",
            "train - step 1506: loss = 292941.94 (1.692 sec)\n",
            "train - step 1507: loss = 425857.41 (1.667 sec)\n",
            "train - step 1508: loss = 370380.84 (1.675 sec)\n",
            "train - step 1509: loss = 383679.44 (1.677 sec)\n",
            "train - step 1510: loss = 441846.88 (1.715 sec)\n",
            "train - step 1511: loss = 457584.12 (1.705 sec)\n",
            "train - step 1512: loss = 584273.31 (1.706 sec)\n",
            "train - step 1513: loss = 294359.41 (1.682 sec)\n",
            "train - step 1514: loss = 404250.19 (1.667 sec)\n",
            "train - step 1515: loss = 725376.62 (1.673 sec)\n",
            "train - step 1516: loss = 510918.31 (1.664 sec)\n",
            "train - step 1517: loss = 372815.81 (1.675 sec)\n",
            "train - step 1518: loss = 352391.78 (1.667 sec)\n",
            "train - step 1519: loss = 494810.28 (1.689 sec)\n",
            "train - step 1520: loss = 350244.28 (1.668 sec)\n",
            "train - step 1521: loss = 446460.28 (1.675 sec)\n",
            "train - step 1522: loss = 439916.47 (1.692 sec)\n",
            "train - step 1523: loss = 564739.00 (1.686 sec)\n",
            "train - step 1524: loss = 368678.41 (1.703 sec)\n",
            "train - step 1525: loss = 543566.19 (1.691 sec)\n",
            "train - step 1526: loss = 488834.41 (1.669 sec)\n",
            "train - step 1527: loss = 372140.16 (1.697 sec)\n",
            "train - step 1528: loss = 306684.19 (2.837 sec)\n",
            "train - step 1529: loss = 424846.81 (1.728 sec)\n",
            "train - step 1530: loss = 393544.28 (1.711 sec)\n",
            "train - step 1531: loss = 498309.19 (1.690 sec)\n",
            "train - step 1532: loss = 469615.62 (1.696 sec)\n",
            "train - step 1533: loss = 484669.47 (1.694 sec)\n",
            "train - step 1534: loss = 337346.41 (1.699 sec)\n",
            "train - step 1535: loss = 479556.03 (1.687 sec)\n",
            "train - step 1536: loss = 448947.62 (1.693 sec)\n",
            "train - step 1537: loss = 381643.53 (1.668 sec)\n",
            "train - step 1538: loss = 521859.00 (1.703 sec)\n",
            "train - step 1539: loss = 295129.34 (1.671 sec)\n",
            "train - step 1540: loss = 400464.41 (1.710 sec)\n",
            "train - step 1541: loss = 636840.75 (1.682 sec)\n",
            "train - step 1542: loss = 425940.38 (1.689 sec)\n",
            "train - step 1543: loss = 469075.66 (1.687 sec)\n",
            "train - step 1544: loss = 389939.31 (1.634 sec)\n",
            "train - step 1545: loss = 342980.59 (1.642 sec)\n",
            "train - step 1546: loss = 504160.59 (1.634 sec)\n",
            "train - step 1547: loss = 363805.59 (1.658 sec)\n",
            "train - step 1548: loss = 485715.16 (1.637 sec)\n",
            "train - step 1549: loss = 527881.81 (1.675 sec)\n",
            "train - step 1550: loss = 383293.06 (1.699 sec)\n",
            "train - step 1551: loss = 518866.09 (1.691 sec)\n",
            "train - step 1552: loss = 465540.34 (1.671 sec)\n",
            "train - step 1553: loss = 427088.22 (1.698 sec)\n",
            "train - step 1554: loss = 575748.75 (1.681 sec)\n",
            "train - step 1555: loss = 458926.44 (1.704 sec)\n",
            "train - step 1556: loss = 583190.62 (1.704 sec)\n",
            "train - step 1557: loss = 440001.34 (1.682 sec)\n",
            "train - step 1558: loss = 408673.91 (1.680 sec)\n",
            "train - step 1559: loss = 434765.12 (1.690 sec)\n",
            "train - step 1560: loss = 471280.41 (1.689 sec)\n",
            "train - step 1561: loss = 498419.72 (1.691 sec)\n",
            "train - step 1562: loss = 456799.69 (1.698 sec)\n",
            "train - step 1563: loss = 443085.78 (1.676 sec)\n",
            "train - step 1564: loss = 451876.66 (2.536 sec)\n",
            "train - step 1565: loss = 423413.56 (1.667 sec)\n",
            "train - step 1566: loss = 451492.59 (1.663 sec)\n",
            "train - step 1567: loss = 545765.44 (1.701 sec)\n",
            "train - step 1568: loss = 336814.34 (1.666 sec)\n",
            "train - step 1569: loss = 404131.19 (1.693 sec)\n",
            "train - step 1570: loss = 388046.19 (1.670 sec)\n",
            "train - step 1571: loss = 488871.41 (1.674 sec)\n",
            "train - step 1572: loss = 394670.06 (1.687 sec)\n",
            "train - step 1573: loss = 449721.50 (1.689 sec)\n",
            "train - step 1574: loss = 376034.81 (1.695 sec)\n",
            "train - step 1575: loss = 341446.22 (1.688 sec)\n",
            "train - step 1576: loss = 326405.62 (1.693 sec)\n",
            "train - step 1577: loss = 387491.66 (1.697 sec)\n",
            "train - step 1578: loss = 404806.38 (1.701 sec)\n",
            "train - step 1579: loss = 358857.53 (1.681 sec)\n",
            "train - step 1580: loss = 446730.62 (1.691 sec)\n",
            "train - step 1581: loss = 308036.44 (1.689 sec)\n",
            "train - step 1582: loss = 325121.72 (1.673 sec)\n",
            "train - step 1583: loss = 486767.41 (1.685 sec)\n",
            "train - step 1584: loss = 423028.38 (1.688 sec)\n",
            "train - step 1585: loss = 296535.28 (1.716 sec)\n",
            "train - step 1586: loss = 431013.88 (1.672 sec)\n",
            "train - step 1587: loss = 358485.84 (1.688 sec)\n",
            "train - step 1588: loss = 855481.12 (1.684 sec)\n",
            "train - step 1589: loss = 527024.81 (1.707 sec)\n",
            "train - step 1590: loss = 419251.12 (1.683 sec)\n",
            "train - step 1591: loss = 489872.84 (1.716 sec)\n",
            "train - step 1592: loss = 476516.47 (1.694 sec)\n",
            "train - step 1593: loss = 391798.16 (1.703 sec)\n",
            "train - step 1594: loss = 413120.66 (1.692 sec)\n",
            "train - step 1595: loss = 596191.88 (1.667 sec)\n",
            "train - step 1596: loss = 346672.12 (1.691 sec)\n",
            "train - step 1597: loss = 444737.31 (1.667 sec)\n",
            "train - step 1598: loss = 392118.94 (1.676 sec)\n",
            "train - step 1599: loss = 414262.34 (1.694 sec)\n",
            "train - step 1600: loss = 322526.91 (2.544 sec)\n",
            "train - step 1601: loss = 535077.62 (1.698 sec)\n",
            "train - step 1602: loss = 578638.12 (1.708 sec)\n",
            "train - step 1603: loss = 435558.06 (1.695 sec)\n",
            "train - step 1604: loss = 496504.50 (1.666 sec)\n",
            "train - step 1605: loss = 443789.00 (1.713 sec)\n",
            "train - step 1606: loss = 398759.50 (1.664 sec)\n",
            "train - step 1607: loss = 426880.03 (1.699 sec)\n",
            "train - step 1608: loss = 291785.94 (1.687 sec)\n",
            "train - step 1609: loss = 474105.50 (1.664 sec)\n",
            "train - step 1610: loss = 637714.88 (1.702 sec)\n",
            "train - step 1611: loss = 445647.69 (1.709 sec)\n",
            "train - step 1612: loss = 555455.81 (1.696 sec)\n",
            "train - step 1613: loss = 485026.41 (1.698 sec)\n",
            "train - step 1614: loss = 456533.78 (1.696 sec)\n",
            "train - step 1615: loss = 524673.25 (1.698 sec)\n",
            "train - step 1616: loss = 415486.03 (1.691 sec)\n",
            "train - step 1617: loss = 462962.41 (1.678 sec)\n",
            "train - step 1618: loss = 514039.97 (1.676 sec)\n",
            "train - step 1619: loss = 384217.84 (1.707 sec)\n",
            "train - step 1620: loss = 357158.47 (1.691 sec)\n",
            "train - step 1621: loss = 498137.03 (1.672 sec)\n",
            "train - step 1622: loss = 423256.00 (1.706 sec)\n",
            "train - step 1623: loss = 423042.34 (1.684 sec)\n",
            "train - step 1624: loss = 374415.94 (1.686 sec)\n",
            "train - step 1625: loss = 489927.19 (1.677 sec)\n",
            "train - step 1626: loss = 438978.19 (1.698 sec)\n",
            "train - step 1627: loss = 435901.81 (1.708 sec)\n",
            "train - step 1628: loss = 519447.19 (1.681 sec)\n",
            "train - step 1629: loss = 526975.12 (1.696 sec)\n",
            "train - step 1630: loss = 609532.94 (1.689 sec)\n",
            "train - step 1631: loss = 416427.03 (1.715 sec)\n",
            "train - step 1632: loss = 383681.69 (1.675 sec)\n",
            "train - step 1633: loss = 393144.56 (1.727 sec)\n",
            "train - step 1634: loss = 317351.12 (1.671 sec)\n",
            "train - step 1635: loss = 516052.16 (1.698 sec)\n",
            "train - step 1636: loss = 585478.12 (2.518 sec)\n",
            "train - step 1637: loss = 335729.06 (1.693 sec)\n",
            "train - step 1638: loss = 575756.56 (1.703 sec)\n",
            "train - step 1639: loss = 355461.97 (1.685 sec)\n",
            "train - step 1640: loss = 479977.22 (1.679 sec)\n",
            "train - step 1641: loss = 290921.09 (1.662 sec)\n",
            "train - step 1642: loss = 508092.69 (1.692 sec)\n",
            "train - step 1643: loss = 544006.31 (1.678 sec)\n",
            "train - step 1644: loss = 530043.06 (1.708 sec)\n",
            "train - step 1645: loss = 627911.88 (1.683 sec)\n",
            "train - step 1646: loss = 646631.31 (1.681 sec)\n",
            "train - step 1647: loss = 403036.34 (1.693 sec)\n",
            "train - step 1648: loss = 430056.09 (1.664 sec)\n",
            "train - step 1649: loss = 419634.53 (1.703 sec)\n",
            "train - step 1650: loss = 422123.53 (1.661 sec)\n",
            "train - step 1651: loss = 369487.47 (1.689 sec)\n",
            "train - step 1652: loss = 339321.84 (1.671 sec)\n",
            "train - step 1653: loss = 484417.50 (1.719 sec)\n",
            "train - step 1654: loss = 416435.00 (1.680 sec)\n",
            "train - step 1655: loss = 397118.44 (1.724 sec)\n",
            "train - step 1656: loss = 527133.31 (1.708 sec)\n",
            "train - step 1657: loss = 444869.94 (1.690 sec)\n",
            "train - step 1658: loss = 428339.59 (1.703 sec)\n",
            "train - step 1659: loss = 511309.47 (1.680 sec)\n",
            "train - step 1660: loss = 466520.81 (1.713 sec)\n",
            "train - step 1661: loss = 520769.50 (1.696 sec)\n",
            "train - step 1662: loss = 600857.94 (1.698 sec)\n",
            "train - step 1663: loss = 389586.59 (1.683 sec)\n",
            "train - step 1664: loss = 620492.69 (1.695 sec)\n",
            "train - step 1665: loss = 390256.56 (1.670 sec)\n",
            "train - step 1666: loss = 364473.56 (1.680 sec)\n",
            "train - step 1667: loss = 537108.62 (1.663 sec)\n",
            "train - step 1668: loss = 443699.12 (1.697 sec)\n",
            "train - step 1669: loss = 360385.78 (1.677 sec)\n",
            "train - step 1670: loss = 503132.81 (1.691 sec)\n",
            "train - step 1671: loss = 550533.62 (1.691 sec)\n",
            "train - step 1672: loss = 322768.94 (2.526 sec)\n",
            "train - step 1673: loss = 495847.88 (1.705 sec)\n",
            "train - step 1674: loss = 569519.69 (1.695 sec)\n",
            "train - step 1675: loss = 515711.34 (1.695 sec)\n",
            "train - step 1676: loss = 359530.81 (1.677 sec)\n",
            "train - step 1677: loss = 434682.16 (1.661 sec)\n",
            "train - step 1678: loss = 406375.22 (1.672 sec)\n",
            "train - step 1679: loss = 501224.56 (1.686 sec)\n",
            "train - step 1680: loss = 498919.09 (1.733 sec)\n",
            "train - step 1681: loss = 397039.12 (1.677 sec)\n",
            "train - step 1682: loss = 334882.38 (1.720 sec)\n",
            "train - step 1683: loss = 653075.81 (1.699 sec)\n",
            "train - step 1684: loss = 411481.53 (1.703 sec)\n",
            "train - step 1685: loss = 492826.72 (1.682 sec)\n",
            "train - step 1686: loss = 626166.56 (1.708 sec)\n",
            "train - step 1687: loss = 376187.03 (1.678 sec)\n",
            "train - step 1688: loss = 409015.38 (1.688 sec)\n",
            "train - step 1689: loss = 281822.16 (1.704 sec)\n",
            "train - step 1690: loss = 319276.19 (1.690 sec)\n",
            "train - step 1691: loss = 503470.53 (1.695 sec)\n",
            "train - step 1692: loss = 427437.31 (1.687 sec)\n",
            "train - step 1693: loss = 512696.78 (1.697 sec)\n",
            "train - step 1694: loss = 306399.78 (1.704 sec)\n",
            "train - step 1695: loss = 489018.50 (1.725 sec)\n",
            "train - step 1696: loss = 283706.66 (1.703 sec)\n",
            "train - step 1697: loss = 486115.16 (1.701 sec)\n",
            "train - step 1698: loss = 469746.47 (1.695 sec)\n",
            "train - step 1699: loss = 453001.78 (1.729 sec)\n",
            "train - step 1700: loss = 455439.38 (1.703 sec)\n",
            "train - step 1701: loss = 457502.34 (1.694 sec)\n",
            "train - step 1702: loss = 474392.91 (1.724 sec)\n",
            "train - step 1703: loss = 544461.94 (1.713 sec)\n",
            "train - step 1704: loss = 679292.69 (1.711 sec)\n",
            "train - step 1705: loss = 383347.84 (1.675 sec)\n",
            "train - step 1706: loss = 504094.19 (1.691 sec)\n",
            "train - step 1707: loss = 328132.44 (1.700 sec)\n",
            "train - step 1708: loss = 423959.66 (2.567 sec)\n",
            "train - step 1709: loss = 427198.91 (1.685 sec)\n",
            "train - step 1710: loss = 395175.47 (1.727 sec)\n",
            "train - step 1711: loss = 464626.72 (1.676 sec)\n",
            "train - step 1712: loss = 649124.81 (1.750 sec)\n",
            "train - step 1713: loss = 332648.94 (1.689 sec)\n",
            "train - step 1714: loss = 409766.28 (1.694 sec)\n",
            "train - step 1715: loss = 551213.56 (1.693 sec)\n",
            "train - step 1716: loss = 507335.06 (1.696 sec)\n",
            "train - step 1717: loss = 298744.72 (1.770 sec)\n",
            "train - step 1718: loss = 304825.66 (1.721 sec)\n",
            "train - step 1719: loss = 429088.91 (1.696 sec)\n",
            "train - step 1720: loss = 320790.62 (1.766 sec)\n",
            "train - step 1721: loss = 303601.38 (1.666 sec)\n",
            "train - step 1722: loss = 365173.12 (2.176 sec)\n",
            "train - step 1723: loss = 391133.88 (1.731 sec)\n",
            "train - step 1724: loss = 371868.62 (1.697 sec)\n",
            "train - step 1725: loss = 538967.12 (1.749 sec)\n",
            "train - step 1726: loss = 405513.78 (1.652 sec)\n",
            "train - step 1727: loss = 339555.06 (1.650 sec)\n",
            "train - step 1728: loss = 544033.38 (1.651 sec)\n",
            "train - step 1729: loss = 634240.94 (1.654 sec)\n",
            "train - step 1730: loss = 386407.72 (1.655 sec)\n",
            "train - step 1731: loss = 365700.53 (1.672 sec)\n",
            "train - step 1732: loss = 546749.38 (1.693 sec)\n",
            "train - step 1733: loss = 452839.78 (1.679 sec)\n",
            "train - step 1734: loss = 463362.56 (1.703 sec)\n",
            "train - step 1735: loss = 489435.50 (1.709 sec)\n",
            "train - step 1736: loss = 447874.38 (1.663 sec)\n",
            "train - step 1737: loss = 542793.25 (1.678 sec)\n",
            "train - step 1738: loss = 420736.88 (1.665 sec)\n",
            "train - step 1739: loss = 382003.19 (1.690 sec)\n",
            "train - step 1740: loss = 515585.53 (1.673 sec)\n",
            "train - step 1741: loss = 368391.28 (1.741 sec)\n",
            "train - step 1742: loss = 533352.19 (1.649 sec)\n",
            "train - step 1743: loss = 302691.72 (2.596 sec)\n",
            "train - step 1744: loss = 212040.41 (1.677 sec)\n",
            "train - step 1745: loss = 443677.44 (1.660 sec)\n",
            "train - step 1746: loss = 332458.94 (1.696 sec)\n",
            "train - step 1747: loss = 361212.69 (1.691 sec)\n",
            "train - step 1748: loss = 410697.78 (1.680 sec)\n",
            "train - step 1749: loss = 438117.78 (1.676 sec)\n",
            "train - step 1750: loss = 393074.12 (1.703 sec)\n",
            "train - step 1751: loss = 447664.81 (1.675 sec)\n",
            "train - step 1752: loss = 422772.06 (1.685 sec)\n",
            "train - step 1753: loss = 501173.78 (1.701 sec)\n",
            "train - step 1754: loss = 338246.31 (1.682 sec)\n",
            "train - step 1755: loss = 331833.72 (1.731 sec)\n",
            "train - step 1756: loss = 544873.50 (1.672 sec)\n",
            "train - step 1757: loss = 463982.50 (1.696 sec)\n",
            "train - step 1758: loss = 515358.31 (1.676 sec)\n",
            "train - step 1759: loss = 448700.09 (1.687 sec)\n",
            "train - step 1760: loss = 388114.88 (1.693 sec)\n",
            "train - step 1761: loss = 484387.88 (1.694 sec)\n",
            "train - step 1762: loss = 513406.88 (1.671 sec)\n",
            "train - step 1763: loss = 491000.69 (1.711 sec)\n",
            "train - step 1764: loss = 326196.41 (1.680 sec)\n",
            "train - step 1765: loss = 389159.34 (1.684 sec)\n",
            "train - step 1766: loss = 433538.28 (1.684 sec)\n",
            "train - step 1767: loss = 512092.94 (1.698 sec)\n",
            "train - step 1768: loss = 485266.94 (1.733 sec)\n",
            "train - step 1769: loss = 491576.28 (1.686 sec)\n",
            "train - step 1770: loss = 263115.47 (1.706 sec)\n",
            "train - step 1771: loss = 380017.16 (1.666 sec)\n",
            "train - step 1772: loss = 487653.09 (1.696 sec)\n",
            "train - step 1773: loss = 443528.72 (1.702 sec)\n",
            "train - step 1774: loss = 612695.94 (1.690 sec)\n",
            "train - step 1775: loss = 480639.12 (1.657 sec)\n",
            "train - step 1776: loss = 293477.00 (1.640 sec)\n",
            "train - step 1777: loss = 413803.78 (1.626 sec)\n",
            "train - step 1778: loss = 436897.50 (1.679 sec)\n",
            "train - step 1779: loss = 403345.06 (2.727 sec)\n",
            "train - step 1780: loss = 505862.72 (1.709 sec)\n",
            "train - step 1781: loss = 474729.91 (1.683 sec)\n",
            "train - step 1782: loss = 605170.00 (1.694 sec)\n",
            "train - step 1783: loss = 471079.81 (1.698 sec)\n",
            "train - step 1784: loss = 308242.62 (1.715 sec)\n",
            "train - step 1785: loss = 361885.88 (1.703 sec)\n",
            "train - step 1786: loss = 412813.78 (1.665 sec)\n",
            "train - step 1787: loss = 432541.62 (1.689 sec)\n",
            "train - step 1788: loss = 312973.56 (1.693 sec)\n",
            "train - step 1789: loss = 457928.12 (1.686 sec)\n",
            "train - step 1790: loss = 266767.25 (1.672 sec)\n",
            "train - step 1791: loss = 446489.53 (1.685 sec)\n",
            "train - step 1792: loss = 466895.72 (1.678 sec)\n",
            "train - step 1793: loss = 278964.41 (1.692 sec)\n",
            "train - step 1794: loss = 353968.09 (1.692 sec)\n",
            "train - step 1795: loss = 417358.81 (1.697 sec)\n",
            "train - step 1796: loss = 448158.94 (1.692 sec)\n",
            "train - step 1797: loss = 508119.31 (1.680 sec)\n",
            "train - step 1798: loss = 329103.19 (1.671 sec)\n",
            "train - step 1799: loss = 588929.06 (1.686 sec)\n",
            "train - step 1800: loss = 544803.12 (1.700 sec)\n",
            "train - step 1801: loss = 419823.88 (1.692 sec)\n",
            "train - step 1802: loss = 571800.69 (1.719 sec)\n",
            "train - step 1803: loss = 520056.69 (1.690 sec)\n",
            "train - step 1804: loss = 428811.03 (1.714 sec)\n",
            "train - step 1805: loss = 475661.06 (1.671 sec)\n",
            "train - step 1806: loss = 387493.03 (1.687 sec)\n",
            "train - step 1807: loss = 352269.41 (1.690 sec)\n",
            "train - step 1808: loss = 495421.47 (1.679 sec)\n",
            "train - step 1809: loss = 635098.81 (1.687 sec)\n",
            "train - step 1810: loss = 380035.28 (1.674 sec)\n",
            "train - step 1811: loss = 327964.84 (1.680 sec)\n",
            "train - step 1812: loss = 607097.75 (1.687 sec)\n",
            "train - step 1813: loss = 345274.94 (1.701 sec)\n",
            "train - step 1814: loss = 420116.91 (1.675 sec)\n",
            "train - step 1815: loss = 475176.66 (2.546 sec)\n",
            "train - step 1816: loss = 327904.44 (1.708 sec)\n",
            "train - step 1817: loss = 268878.06 (1.693 sec)\n",
            "train - step 1818: loss = 486122.44 (1.723 sec)\n",
            "train - step 1819: loss = 488350.81 (1.680 sec)\n",
            "train - step 1820: loss = 454387.22 (1.679 sec)\n",
            "train - step 1821: loss = 514554.88 (1.673 sec)\n",
            "train - step 1822: loss = 377974.56 (1.685 sec)\n",
            "train - step 1823: loss = 441610.47 (1.683 sec)\n",
            "train - step 1824: loss = 476581.19 (1.691 sec)\n",
            "train - step 1825: loss = 535611.00 (1.687 sec)\n",
            "train - step 1826: loss = 423905.69 (1.709 sec)\n",
            "train - step 1827: loss = 568755.25 (1.683 sec)\n",
            "train - step 1828: loss = 472175.88 (1.661 sec)\n",
            "train - step 1829: loss = 279534.34 (1.701 sec)\n",
            "train - step 1830: loss = 456966.28 (1.675 sec)\n",
            "train - step 1831: loss = 721380.00 (1.709 sec)\n",
            "train - step 1832: loss = 368602.03 (1.699 sec)\n",
            "train - step 1833: loss = 424023.84 (1.710 sec)\n",
            "train - step 1834: loss = 512941.62 (1.693 sec)\n",
            "train - step 1835: loss = 386126.22 (1.697 sec)\n",
            "train - step 1836: loss = 480646.78 (1.709 sec)\n",
            "train - step 1837: loss = 435932.56 (1.689 sec)\n",
            "train - step 1838: loss = 537310.19 (1.697 sec)\n",
            "train - step 1839: loss = 329941.47 (1.681 sec)\n",
            "train - step 1840: loss = 348066.59 (1.680 sec)\n",
            "train - step 1841: loss = 508085.09 (1.693 sec)\n",
            "train - step 1842: loss = 493667.72 (1.698 sec)\n",
            "train - step 1843: loss = 410790.88 (1.688 sec)\n",
            "train - step 1844: loss = 419084.41 (1.680 sec)\n",
            "train - step 1845: loss = 403664.44 (1.683 sec)\n",
            "train - step 1846: loss = 415242.84 (1.677 sec)\n",
            "train - step 1847: loss = 366312.19 (1.723 sec)\n",
            "train - step 1848: loss = 462464.94 (1.695 sec)\n",
            "train - step 1849: loss = 487809.03 (1.675 sec)\n",
            "train - step 1850: loss = 512980.78 (1.684 sec)\n",
            "train - step 1851: loss = 390804.38 (2.487 sec)\n",
            "train - step 1852: loss = 344141.94 (1.697 sec)\n",
            "train - step 1853: loss = 485142.38 (1.729 sec)\n",
            "train - step 1854: loss = 289120.16 (1.677 sec)\n",
            "train - step 1855: loss = 235388.25 (1.675 sec)\n",
            "train - step 1856: loss = 358656.31 (1.678 sec)\n",
            "train - step 1857: loss = 393889.22 (1.674 sec)\n",
            "train - step 1858: loss = 375060.66 (1.700 sec)\n",
            "train - step 1859: loss = 349452.09 (1.695 sec)\n",
            "train - step 1860: loss = 279833.50 (1.678 sec)\n",
            "train - step 1861: loss = 475606.72 (1.695 sec)\n",
            "train - step 1862: loss = 404008.12 (1.689 sec)\n",
            "train - step 1863: loss = 386496.41 (1.692 sec)\n",
            "train - step 1864: loss = 377693.78 (1.731 sec)\n",
            "train - step 1865: loss = 514816.31 (1.690 sec)\n",
            "train - step 1866: loss = 536709.12 (1.662 sec)\n",
            "train - step 1867: loss = 372952.81 (1.688 sec)\n",
            "train - step 1868: loss = 408974.88 (1.693 sec)\n",
            "train - step 1869: loss = 405372.19 (1.693 sec)\n",
            "train - step 1870: loss = 376239.44 (1.687 sec)\n",
            "train - step 1871: loss = 497496.00 (1.702 sec)\n",
            "train - step 1872: loss = 350173.66 (1.714 sec)\n",
            "train - step 1873: loss = 397276.62 (1.701 sec)\n",
            "train - step 1874: loss = 574101.25 (1.693 sec)\n",
            "train - step 1875: loss = 366659.53 (1.720 sec)\n",
            "train - step 1876: loss = 401376.62 (1.703 sec)\n",
            "train - step 1877: loss = 520541.00 (1.711 sec)\n",
            "train - step 1878: loss = 383367.34 (1.712 sec)\n",
            "train - step 1879: loss = 337742.03 (1.711 sec)\n",
            "train - step 1880: loss = 306220.09 (1.704 sec)\n",
            "train - step 1881: loss = 348329.66 (1.692 sec)\n",
            "train - step 1882: loss = 391825.94 (1.690 sec)\n",
            "train - step 1883: loss = 418063.69 (1.685 sec)\n",
            "train - step 1884: loss = 452743.50 (1.686 sec)\n",
            "train - step 1885: loss = 318101.22 (1.659 sec)\n",
            "train - step 1886: loss = 317234.38 (1.683 sec)\n",
            "train - step 1887: loss = 477227.19 (2.550 sec)\n",
            "train - step 1888: loss = 441119.97 (1.694 sec)\n",
            "train - step 1889: loss = 345635.69 (1.688 sec)\n",
            "train - step 1890: loss = 434924.84 (1.686 sec)\n",
            "train - step 1891: loss = 389253.91 (1.679 sec)\n",
            "train - step 1892: loss = 426665.22 (1.683 sec)\n",
            "train - step 1893: loss = 390328.34 (1.696 sec)\n",
            "train - step 1894: loss = 477094.44 (1.693 sec)\n",
            "train - step 1895: loss = 372473.12 (1.683 sec)\n",
            "train - step 1896: loss = 432543.59 (1.669 sec)\n",
            "train - step 1897: loss = 442702.50 (1.661 sec)\n",
            "train - step 1898: loss = 421533.19 (1.705 sec)\n",
            "train - step 1899: loss = 565001.69 (1.694 sec)\n",
            "train - step 1900: loss = 465782.19 (1.690 sec)\n",
            "train - step 1901: loss = 390039.41 (1.703 sec)\n",
            "train - step 1902: loss = 499791.53 (1.678 sec)\n",
            "train - step 1903: loss = 538217.31 (1.721 sec)\n",
            "train - step 1904: loss = 407833.12 (1.705 sec)\n",
            "train - step 1905: loss = 316606.91 (1.759 sec)\n",
            "train - step 1906: loss = 517816.62 (1.702 sec)\n",
            "train - step 1907: loss = 503236.56 (1.662 sec)\n",
            "train - step 1908: loss = 427154.28 (1.645 sec)\n",
            "train - step 1909: loss = 341950.97 (1.659 sec)\n",
            "train - step 1910: loss = 476654.81 (1.653 sec)\n",
            "train - step 1911: loss = 339184.38 (1.646 sec)\n",
            "train - step 1912: loss = 412068.69 (1.675 sec)\n",
            "train - step 1913: loss = 481584.84 (1.683 sec)\n",
            "train - step 1914: loss = 356304.94 (1.690 sec)\n",
            "train - step 1915: loss = 441454.47 (1.704 sec)\n",
            "train - step 1916: loss = 410734.53 (1.710 sec)\n",
            "train - step 1917: loss = 405733.69 (1.692 sec)\n",
            "train - step 1918: loss = 492458.06 (1.721 sec)\n",
            "train - step 1919: loss = 440190.41 (1.673 sec)\n",
            "train - step 1920: loss = 376135.28 (1.688 sec)\n",
            "train - step 1921: loss = 338014.31 (1.644 sec)\n",
            "train - step 1922: loss = 322254.69 (1.645 sec)\n",
            "train - step 1923: loss = 386081.28 (2.574 sec)\n",
            "train - step 1924: loss = 446248.69 (1.670 sec)\n",
            "train - step 1925: loss = 411664.69 (1.710 sec)\n",
            "train - step 1926: loss = 497127.94 (1.691 sec)\n",
            "train - step 1927: loss = 348735.03 (1.678 sec)\n",
            "train - step 1928: loss = 304397.69 (1.706 sec)\n",
            "train - step 1929: loss = 406616.91 (1.678 sec)\n",
            "train - step 1930: loss = 601366.81 (1.733 sec)\n",
            "train - step 1931: loss = 351475.50 (1.675 sec)\n",
            "train - step 1932: loss = 407939.00 (1.700 sec)\n",
            "train - step 1933: loss = 536841.00 (1.686 sec)\n",
            "train - step 1934: loss = 311265.72 (1.701 sec)\n",
            "train - step 1935: loss = 435509.28 (1.661 sec)\n",
            "train - step 1936: loss = 584623.19 (1.703 sec)\n",
            "train - step 1937: loss = 468177.56 (1.681 sec)\n",
            "train - step 1938: loss = 464260.88 (1.671 sec)\n",
            "train - step 1939: loss = 417842.78 (1.723 sec)\n",
            "train - step 1940: loss = 451602.28 (1.668 sec)\n",
            "train - step 1941: loss = 477611.53 (1.701 sec)\n",
            "train - step 1942: loss = 291878.94 (1.704 sec)\n",
            "train - step 1943: loss = 391732.97 (1.674 sec)\n",
            "train - step 1944: loss = 540537.94 (1.681 sec)\n",
            "train - step 1945: loss = 369555.59 (1.701 sec)\n",
            "train - step 1946: loss = 459562.88 (1.674 sec)\n",
            "train - step 1947: loss = 518625.88 (1.696 sec)\n",
            "train - step 1948: loss = 377262.59 (1.682 sec)\n",
            "train - step 1949: loss = 490150.59 (1.680 sec)\n",
            "train - step 1950: loss = 571367.38 (1.698 sec)\n",
            "train - step 1951: loss = 303862.31 (1.713 sec)\n",
            "train - step 1952: loss = 373653.59 (1.682 sec)\n",
            "train - step 1953: loss = 336101.00 (1.689 sec)\n",
            "train - step 1954: loss = 389173.28 (1.673 sec)\n",
            "train - step 1955: loss = 527378.12 (1.709 sec)\n",
            "train - step 1956: loss = 519145.84 (1.696 sec)\n",
            "train - step 1957: loss = 381344.41 (1.675 sec)\n",
            "train - step 1958: loss = 432466.31 (1.708 sec)\n",
            "train - step 1959: loss = 422017.78 (2.612 sec)\n",
            "train - step 1960: loss = 484054.81 (1.674 sec)\n",
            "train - step 1961: loss = 315332.41 (1.699 sec)\n",
            "train - step 1962: loss = 476753.16 (1.732 sec)\n",
            "train - step 1963: loss = 537618.31 (1.694 sec)\n",
            "train - step 1964: loss = 507142.31 (1.691 sec)\n",
            "train - step 1965: loss = 425041.91 (1.695 sec)\n",
            "train - step 1966: loss = 423842.12 (1.708 sec)\n",
            "train - step 1967: loss = 432734.72 (1.696 sec)\n",
            "train - step 1968: loss = 481217.28 (1.665 sec)\n",
            "train - step 1969: loss = 438409.38 (1.686 sec)\n",
            "train - step 1970: loss = 393946.94 (1.700 sec)\n",
            "train - step 1971: loss = 334577.41 (1.645 sec)\n",
            "train - step 1972: loss = 704131.25 (1.672 sec)\n",
            "train - step 1973: loss = 588488.31 (1.689 sec)\n",
            "train - step 1974: loss = 389782.09 (1.688 sec)\n",
            "train - step 1975: loss = 322529.03 (1.694 sec)\n",
            "train - step 1976: loss = 421338.94 (1.687 sec)\n",
            "train - step 1977: loss = 514485.47 (1.672 sec)\n",
            "train - step 1978: loss = 280545.34 (1.729 sec)\n",
            "train - step 1979: loss = 652900.56 (1.681 sec)\n",
            "train - step 1980: loss = 543760.25 (1.708 sec)\n",
            "train - step 1981: loss = 321313.88 (1.686 sec)\n",
            "train - step 1982: loss = 396502.97 (1.689 sec)\n",
            "train - step 1983: loss = 332555.56 (1.680 sec)\n",
            "train - step 1984: loss = 579752.12 (1.694 sec)\n",
            "train - step 1985: loss = 456292.41 (1.691 sec)\n",
            "train - step 1986: loss = 473985.59 (1.679 sec)\n",
            "train - step 1987: loss = 454697.41 (1.698 sec)\n",
            "train - step 1988: loss = 474456.56 (1.670 sec)\n",
            "train - step 1989: loss = 374428.38 (1.684 sec)\n",
            "train - step 1990: loss = 464628.38 (1.709 sec)\n",
            "train - step 1991: loss = 517005.72 (1.682 sec)\n",
            "train - step 1992: loss = 438008.00 (1.693 sec)\n",
            "train - step 1993: loss = 563513.38 (1.697 sec)\n",
            "train - step 1994: loss = 367813.16 (1.696 sec)\n",
            "train - step 1995: loss = 548400.50 (2.532 sec)\n",
            "train - step 1996: loss = 505516.84 (1.700 sec)\n",
            "train - step 1997: loss = 497420.94 (1.689 sec)\n",
            "train - step 1998: loss = 575429.38 (1.710 sec)\n",
            "train - step 1999: loss = 420511.72 (1.696 sec)\n",
            "train - step 2000: loss = 431735.34 (1.715 sec)\n",
            "train - step 2001: loss = 468123.91 (1.678 sec)\n",
            "train - step 2002: loss = 397196.81 (1.717 sec)\n",
            "train - step 2003: loss = 292181.56 (1.795 sec)\n",
            "train - step 2004: loss = 488407.84 (1.719 sec)\n",
            "train - step 2005: loss = 369864.88 (1.701 sec)\n",
            "train - step 2006: loss = 457872.44 (1.692 sec)\n",
            "train - step 2007: loss = 430825.06 (1.732 sec)\n",
            "train - step 2008: loss = 348855.53 (1.676 sec)\n",
            "train - step 2009: loss = 400542.69 (1.722 sec)\n",
            "train - step 2010: loss = 499766.47 (1.712 sec)\n",
            "train - step 2011: loss = 304445.84 (1.704 sec)\n",
            "train - step 2012: loss = 283732.56 (1.860 sec)\n",
            "train - step 2013: loss = 542122.75 (1.716 sec)\n",
            "train - step 2014: loss = 393318.50 (1.684 sec)\n",
            "train - step 2015: loss = 475782.00 (1.794 sec)\n",
            "train - step 2016: loss = 302575.38 (1.659 sec)\n",
            "train - step 2017: loss = 511326.03 (1.685 sec)\n",
            "train - step 2018: loss = 544843.19 (1.682 sec)\n",
            "train - step 2019: loss = 374657.88 (1.706 sec)\n",
            "train - step 2020: loss = 500453.12 (1.739 sec)\n",
            "train - step 2021: loss = 514068.59 (1.673 sec)\n",
            "train - step 2022: loss = 350846.78 (1.683 sec)\n",
            "train - step 2023: loss = 455513.34 (1.678 sec)\n",
            "train - step 2024: loss = 425177.31 (1.720 sec)\n",
            "train - step 2025: loss = 488199.41 (1.663 sec)\n",
            "train - step 2026: loss = 280335.94 (1.679 sec)\n",
            "train - step 2027: loss = 462816.81 (1.672 sec)\n",
            "train - step 2028: loss = 462592.34 (1.696 sec)\n",
            "train - step 2029: loss = 421730.09 (1.692 sec)\n",
            "train - step 2030: loss = 326795.84 (2.696 sec)\n",
            "train - step 2031: loss = 408453.19 (1.703 sec)\n",
            "train - step 2032: loss = 264852.72 (1.673 sec)\n",
            "train - step 2033: loss = 485461.53 (1.721 sec)\n",
            "train - step 2034: loss = 378948.59 (1.678 sec)\n",
            "train - step 2035: loss = 349414.06 (1.702 sec)\n",
            "train - step 2036: loss = 499606.00 (1.657 sec)\n",
            "train - step 2037: loss = 520920.59 (1.680 sec)\n",
            "train - step 2038: loss = 407040.12 (1.685 sec)\n",
            "train - step 2039: loss = 371710.31 (1.668 sec)\n",
            "train - step 2040: loss = 533108.06 (1.721 sec)\n",
            "train - step 2041: loss = 593208.62 (1.701 sec)\n",
            "train - step 2042: loss = 323003.19 (1.669 sec)\n",
            "train - step 2043: loss = 534391.88 (1.730 sec)\n",
            "train - step 2044: loss = 416207.31 (1.695 sec)\n",
            "train - step 2045: loss = 430219.12 (1.694 sec)\n",
            "train - step 2046: loss = 471634.06 (1.704 sec)\n",
            "train - step 2047: loss = 441501.47 (1.740 sec)\n",
            "train - step 2048: loss = 444250.81 (1.681 sec)\n",
            "train - step 2049: loss = 469522.81 (1.704 sec)\n",
            "train - step 2050: loss = 505298.81 (1.684 sec)\n",
            "train - step 2051: loss = 580728.62 (1.689 sec)\n",
            "train - step 2052: loss = 444044.22 (1.691 sec)\n",
            "train - step 2053: loss = 402833.84 (1.672 sec)\n",
            "train - step 2054: loss = 534298.81 (1.658 sec)\n",
            "train - step 2055: loss = 372803.22 (1.681 sec)\n",
            "train - step 2056: loss = 494060.88 (1.692 sec)\n",
            "train - step 2057: loss = 399783.94 (1.710 sec)\n",
            "train - step 2058: loss = 442563.31 (1.674 sec)\n",
            "train - step 2059: loss = 461655.91 (1.645 sec)\n",
            "train - step 2060: loss = 527262.12 (1.676 sec)\n",
            "train - step 2061: loss = 422399.81 (1.715 sec)\n",
            "train - step 2062: loss = 409301.38 (1.714 sec)\n",
            "train - step 2063: loss = 445986.12 (1.686 sec)\n",
            "train - step 2064: loss = 555003.88 (1.680 sec)\n",
            "train - step 2065: loss = 533499.94 (1.712 sec)\n",
            "train - step 2066: loss = 433735.00 (2.539 sec)\n",
            "train - step 2067: loss = 365850.22 (1.656 sec)\n",
            "train - step 2068: loss = 557355.81 (1.691 sec)\n",
            "train - step 2069: loss = 360907.97 (1.658 sec)\n",
            "train - step 2070: loss = 549212.12 (1.677 sec)\n",
            "train - step 2071: loss = 524927.75 (1.692 sec)\n",
            "train - step 2072: loss = 476624.62 (1.710 sec)\n",
            "train - step 2073: loss = 360033.12 (1.675 sec)\n",
            "train - step 2074: loss = 378691.97 (1.680 sec)\n",
            "train - step 2075: loss = 455088.59 (1.673 sec)\n",
            "train - step 2076: loss = 465584.91 (1.671 sec)\n",
            "train - step 2077: loss = 367256.81 (1.713 sec)\n",
            "train - step 2078: loss = 646335.19 (1.659 sec)\n",
            "train - step 2079: loss = 412496.84 (1.710 sec)\n",
            "train - step 2080: loss = 324538.19 (1.666 sec)\n",
            "train - step 2081: loss = 353892.03 (1.701 sec)\n",
            "train - step 2082: loss = 397495.53 (1.676 sec)\n",
            "train - step 2083: loss = 507104.84 (1.685 sec)\n",
            "train - step 2084: loss = 659059.31 (1.668 sec)\n",
            "train - step 2085: loss = 400443.50 (1.683 sec)\n",
            "train - step 2086: loss = 397381.34 (1.678 sec)\n",
            "train - step 2087: loss = 574458.75 (1.658 sec)\n",
            "train - step 2088: loss = 339082.44 (1.640 sec)\n",
            "train - step 2089: loss = 293037.81 (1.635 sec)\n",
            "train - step 2090: loss = 338705.12 (1.646 sec)\n",
            "train - step 2091: loss = 499643.88 (1.689 sec)\n",
            "train - step 2092: loss = 396218.03 (1.662 sec)\n",
            "train - step 2093: loss = 439068.47 (1.635 sec)\n",
            "train - step 2094: loss = 367628.72 (1.683 sec)\n",
            "train - step 2095: loss = 428458.59 (1.679 sec)\n",
            "train - step 2096: loss = 410465.66 (1.661 sec)\n",
            "train - step 2097: loss = 646174.31 (1.693 sec)\n",
            "train - step 2098: loss = 437038.69 (1.649 sec)\n",
            "train - step 2099: loss = 386168.72 (1.656 sec)\n",
            "train - step 2100: loss = 228656.56 (1.635 sec)\n",
            "train - step 2101: loss = 421055.62 (1.670 sec)\n",
            "train - step 2102: loss = 377935.94 (2.874 sec)\n",
            "train - step 2103: loss = 523008.38 (1.696 sec)\n",
            "train - step 2104: loss = 386125.19 (1.696 sec)\n",
            "train - step 2105: loss = 347859.94 (1.666 sec)\n",
            "train - step 2106: loss = 380710.47 (1.705 sec)\n",
            "train - step 2107: loss = 309260.00 (1.678 sec)\n",
            "train - step 2108: loss = 323379.97 (1.685 sec)\n",
            "train - step 2109: loss = 533313.88 (1.657 sec)\n",
            "train - step 2110: loss = 457819.94 (1.717 sec)\n",
            "train - step 2111: loss = 464994.22 (1.685 sec)\n",
            "train - step 2112: loss = 339404.31 (1.716 sec)\n",
            "train - step 2113: loss = 407266.50 (1.666 sec)\n",
            "train - step 2114: loss = 469210.66 (1.691 sec)\n",
            "train - step 2115: loss = 578703.69 (1.669 sec)\n",
            "train - step 2116: loss = 467574.47 (1.663 sec)\n",
            "train - step 2117: loss = 454333.28 (1.699 sec)\n",
            "train - step 2118: loss = 378339.91 (1.670 sec)\n",
            "train - step 2119: loss = 396151.28 (1.725 sec)\n",
            "train - step 2120: loss = 478851.28 (1.673 sec)\n",
            "train - step 2121: loss = 377615.03 (1.675 sec)\n",
            "train - step 2122: loss = 490085.22 (1.654 sec)\n",
            "train - step 2123: loss = 266938.56 (1.687 sec)\n",
            "train - step 2124: loss = 455678.97 (1.695 sec)\n",
            "train - step 2125: loss = 383721.16 (1.688 sec)\n",
            "train - step 2126: loss = 339117.78 (1.686 sec)\n",
            "train - step 2127: loss = 380883.94 (1.688 sec)\n",
            "train - step 2128: loss = 416289.31 (1.696 sec)\n",
            "train - step 2129: loss = 544306.62 (1.716 sec)\n",
            "train - step 2130: loss = 526907.44 (1.674 sec)\n",
            "train - step 2131: loss = 423210.34 (1.735 sec)\n",
            "train - step 2132: loss = 475976.50 (1.700 sec)\n",
            "train - step 2133: loss = 564249.94 (1.688 sec)\n",
            "train - step 2134: loss = 427499.59 (1.689 sec)\n",
            "train - step 2135: loss = 363188.03 (1.713 sec)\n",
            "train - step 2136: loss = 380417.22 (1.693 sec)\n",
            "train - step 2137: loss = 512724.62 (2.598 sec)\n",
            "train - step 2138: loss = 558859.06 (1.713 sec)\n",
            "train - step 2139: loss = 452439.69 (1.697 sec)\n",
            "train - step 2140: loss = 600668.31 (1.699 sec)\n",
            "train - step 2141: loss = 479856.97 (1.705 sec)\n",
            "train - step 2142: loss = 389256.06 (1.690 sec)\n",
            "train - step 2143: loss = 482225.91 (1.704 sec)\n",
            "train - step 2144: loss = 436477.06 (1.700 sec)\n",
            "train - step 2145: loss = 441621.72 (1.702 sec)\n",
            "train - step 2146: loss = 484286.16 (1.681 sec)\n",
            "train - step 2147: loss = 365601.41 (1.696 sec)\n",
            "train - step 2148: loss = 566994.81 (1.694 sec)\n",
            "train - step 2149: loss = 374005.16 (1.705 sec)\n",
            "train - step 2150: loss = 476708.41 (1.703 sec)\n",
            "train - step 2151: loss = 378692.53 (1.687 sec)\n",
            "train - step 2152: loss = 475499.38 (1.662 sec)\n",
            "train - step 2153: loss = 330372.34 (1.689 sec)\n",
            "train - step 2154: loss = 437354.06 (1.671 sec)\n",
            "train - step 2155: loss = 504785.88 (1.683 sec)\n",
            "train - step 2156: loss = 516152.12 (1.704 sec)\n",
            "train - step 2157: loss = 467249.00 (1.675 sec)\n",
            "train - step 2158: loss = 453320.78 (1.670 sec)\n",
            "train - step 2159: loss = 466209.44 (1.666 sec)\n",
            "train - step 2160: loss = 340515.19 (1.677 sec)\n",
            "train - step 2161: loss = 437163.94 (1.684 sec)\n",
            "train - step 2162: loss = 550983.69 (1.699 sec)\n",
            "train - step 2163: loss = 439148.66 (1.690 sec)\n",
            "train - step 2164: loss = 346069.28 (1.709 sec)\n",
            "train - step 2165: loss = 469833.53 (1.677 sec)\n",
            "train - step 2166: loss = 325819.94 (1.697 sec)\n",
            "train - step 2167: loss = 366682.88 (1.668 sec)\n",
            "train - step 2168: loss = 332661.16 (1.685 sec)\n",
            "train - step 2169: loss = 295306.78 (1.673 sec)\n",
            "train - step 2170: loss = 465759.06 (1.714 sec)\n",
            "train - step 2171: loss = 384253.06 (1.671 sec)\n",
            "train - step 2172: loss = 495323.69 (1.675 sec)\n",
            "train - step 2173: loss = 455086.81 (2.567 sec)\n",
            "train - step 2174: loss = 298312.56 (1.688 sec)\n",
            "train - step 2175: loss = 333045.22 (1.682 sec)\n",
            "train - step 2176: loss = 444892.69 (1.691 sec)\n",
            "train - step 2177: loss = 457012.56 (1.694 sec)\n",
            "train - step 2178: loss = 476539.34 (1.682 sec)\n",
            "train - step 2179: loss = 436205.78 (1.668 sec)\n",
            "train - step 2180: loss = 462047.91 (1.676 sec)\n",
            "train - step 2181: loss = 339687.69 (1.693 sec)\n",
            "train - step 2182: loss = 342166.81 (1.675 sec)\n",
            "train - step 2183: loss = 393650.16 (1.658 sec)\n",
            "train - step 2184: loss = 440507.38 (1.702 sec)\n",
            "train - step 2185: loss = 529564.06 (1.710 sec)\n",
            "train - step 2186: loss = 354228.09 (1.701 sec)\n",
            "train - step 2187: loss = 397004.91 (1.655 sec)\n",
            "train - step 2188: loss = 457715.91 (1.673 sec)\n",
            "train - step 2189: loss = 493868.16 (1.710 sec)\n",
            "train - step 2190: loss = 381813.19 (1.669 sec)\n",
            "train - step 2191: loss = 607727.19 (1.693 sec)\n",
            "train - step 2192: loss = 455121.34 (1.677 sec)\n",
            "train - step 2193: loss = 563131.62 (1.681 sec)\n",
            "train - step 2194: loss = 358189.78 (1.675 sec)\n",
            "train - step 2195: loss = 455943.44 (1.706 sec)\n",
            "train - step 2196: loss = 444294.06 (1.648 sec)\n",
            "train - step 2197: loss = 396055.66 (1.700 sec)\n",
            "train - step 2198: loss = 326711.88 (1.690 sec)\n",
            "train - step 2199: loss = 555615.94 (1.677 sec)\n",
            "train - step 2200: loss = 420288.41 (1.745 sec)\n",
            "train - step 2201: loss = 536016.75 (1.667 sec)\n",
            "train - step 2202: loss = 421184.69 (1.680 sec)\n",
            "train - step 2203: loss = 712483.25 (1.688 sec)\n",
            "train - step 2204: loss = 488171.50 (1.682 sec)\n",
            "train - step 2205: loss = 397542.00 (1.680 sec)\n",
            "train - step 2206: loss = 623940.94 (1.694 sec)\n",
            "train - step 2207: loss = 465636.81 (1.678 sec)\n",
            "train - step 2208: loss = 598855.44 (1.683 sec)\n",
            "train - step 2209: loss = 461833.34 (2.537 sec)\n",
            "train - step 2210: loss = 381986.56 (1.671 sec)\n",
            "train - step 2211: loss = 372025.03 (1.689 sec)\n",
            "train - step 2212: loss = 424556.44 (1.664 sec)\n",
            "train - step 2213: loss = 426455.50 (1.681 sec)\n",
            "train - step 2214: loss = 407641.28 (1.676 sec)\n",
            "train - step 2215: loss = 390095.66 (1.674 sec)\n",
            "train - step 2216: loss = 424400.16 (1.699 sec)\n",
            "train - step 2217: loss = 384757.66 (1.682 sec)\n",
            "train - step 2218: loss = 473349.72 (1.670 sec)\n",
            "train - step 2219: loss = 436708.84 (1.685 sec)\n",
            "train - step 2220: loss = 523618.28 (1.683 sec)\n",
            "train - step 2221: loss = 432729.94 (1.673 sec)\n",
            "train - step 2222: loss = 389831.03 (1.681 sec)\n",
            "train - step 2223: loss = 590938.62 (1.689 sec)\n",
            "train - step 2224: loss = 526103.75 (1.665 sec)\n",
            "train - step 2225: loss = 394605.59 (1.678 sec)\n",
            "train - step 2226: loss = 490619.81 (1.685 sec)\n",
            "train - step 2227: loss = 518533.41 (1.689 sec)\n",
            "train - step 2228: loss = 517384.03 (1.683 sec)\n",
            "train - step 2229: loss = 356336.06 (1.669 sec)\n",
            "train - step 2230: loss = 451469.88 (1.668 sec)\n",
            "train - step 2231: loss = 417112.88 (1.676 sec)\n",
            "train - step 2232: loss = 319901.38 (1.641 sec)\n",
            "train - step 2233: loss = 526452.81 (1.673 sec)\n",
            "train - step 2234: loss = 513408.53 (1.676 sec)\n",
            "train - step 2235: loss = 467630.12 (1.690 sec)\n",
            "train - step 2236: loss = 420428.31 (1.695 sec)\n",
            "train - step 2237: loss = 363534.91 (1.694 sec)\n",
            "train - step 2238: loss = 456911.28 (1.669 sec)\n",
            "train - step 2239: loss = 444832.84 (1.684 sec)\n",
            "train - step 2240: loss = 364345.72 (1.671 sec)\n",
            "train - step 2241: loss = 561240.88 (1.690 sec)\n",
            "train - step 2242: loss = 684758.69 (1.680 sec)\n",
            "train - step 2243: loss = 360697.06 (1.676 sec)\n",
            "train - step 2244: loss = 469996.03 (1.689 sec)\n",
            "train - step 2245: loss = 476187.06 (2.482 sec)\n",
            "train - step 2246: loss = 480107.94 (1.697 sec)\n",
            "train - step 2247: loss = 334025.94 (1.672 sec)\n",
            "train - step 2248: loss = 549768.69 (1.671 sec)\n",
            "train - step 2249: loss = 607272.38 (1.701 sec)\n",
            "train - step 2250: loss = 372980.09 (1.656 sec)\n",
            "train - step 2251: loss = 450441.00 (2.380 sec)\n",
            "train - step 2252: loss = 596012.75 (1.691 sec)\n",
            "train - step 2253: loss = 413387.94 (1.669 sec)\n",
            "train - step 2254: loss = 474623.56 (1.698 sec)\n",
            "train - step 2255: loss = 486091.03 (1.669 sec)\n",
            "train - step 2256: loss = 419994.72 (1.703 sec)\n",
            "train - step 2257: loss = 368096.69 (1.664 sec)\n",
            "train - step 2258: loss = 520681.16 (1.676 sec)\n",
            "train - step 2259: loss = 395851.38 (1.674 sec)\n",
            "train - step 2260: loss = 328539.28 (1.676 sec)\n",
            "train - step 2261: loss = 371036.94 (1.720 sec)\n",
            "train - step 2262: loss = 351239.16 (1.702 sec)\n",
            "train - step 2263: loss = 370700.81 (1.728 sec)\n",
            "train - step 2264: loss = 385150.50 (1.675 sec)\n",
            "train - step 2265: loss = 515713.97 (1.722 sec)\n",
            "train - step 2266: loss = 408008.22 (1.690 sec)\n",
            "train - step 2267: loss = 435858.94 (1.708 sec)\n",
            "train - step 2268: loss = 396031.88 (1.680 sec)\n",
            "train - step 2269: loss = 304647.28 (1.668 sec)\n",
            "train - step 2270: loss = 427526.31 (1.640 sec)\n",
            "train - step 2271: loss = 387348.88 (1.649 sec)\n",
            "train - step 2272: loss = 391080.59 (1.660 sec)\n",
            "train - step 2273: loss = 467835.97 (1.641 sec)\n",
            "train - step 2274: loss = 667629.44 (1.651 sec)\n",
            "train - step 2275: loss = 387354.31 (1.692 sec)\n",
            "train - step 2276: loss = 544455.31 (1.698 sec)\n",
            "train - step 2277: loss = 513400.66 (1.639 sec)\n",
            "train - step 2278: loss = 433528.12 (1.672 sec)\n",
            "train - step 2279: loss = 438016.03 (1.639 sec)\n",
            "train - step 2280: loss = 620122.81 (1.680 sec)\n",
            "train - step 2281: loss = 498745.81 (2.518 sec)\n",
            "train - step 2282: loss = 367738.88 (1.695 sec)\n",
            "train - step 2283: loss = 379456.34 (1.689 sec)\n",
            "train - step 2284: loss = 415852.84 (1.701 sec)\n",
            "train - step 2285: loss = 296516.88 (1.671 sec)\n",
            "train - step 2286: loss = 413930.72 (1.706 sec)\n",
            "train - step 2287: loss = 441622.47 (1.679 sec)\n",
            "train - step 2288: loss = 342130.47 (1.708 sec)\n",
            "train - step 2289: loss = 479443.97 (1.680 sec)\n",
            "train - step 2290: loss = 396733.34 (1.676 sec)\n",
            "train - step 2291: loss = 379732.69 (1.682 sec)\n",
            "train - step 2292: loss = 486342.88 (1.688 sec)\n",
            "train - step 2293: loss = 394526.38 (1.688 sec)\n",
            "train - step 2294: loss = 413049.28 (1.711 sec)\n",
            "train - step 2295: loss = 559133.75 (1.725 sec)\n",
            "train - step 2296: loss = 277280.47 (1.726 sec)\n",
            "train - step 2297: loss = 374936.28 (1.723 sec)\n",
            "train - step 2298: loss = 550663.81 (1.732 sec)\n",
            "train - step 2299: loss = 528051.31 (1.725 sec)\n",
            "train - step 2300: loss = 534316.25 (1.703 sec)\n",
            "train - step 2301: loss = 399704.72 (1.710 sec)\n",
            "train - step 2302: loss = 323892.84 (1.722 sec)\n",
            "train - step 2303: loss = 514353.41 (1.699 sec)\n",
            "train - step 2304: loss = 592066.44 (1.691 sec)\n",
            "train - step 2305: loss = 383357.00 (1.699 sec)\n",
            "train - step 2306: loss = 486987.12 (1.681 sec)\n",
            "train - step 2307: loss = 458266.66 (1.691 sec)\n",
            "train - step 2308: loss = 378105.94 (1.694 sec)\n",
            "train - step 2309: loss = 362066.66 (1.738 sec)\n",
            "train - step 2310: loss = 563095.38 (1.685 sec)\n",
            "train - step 2311: loss = 280923.47 (1.748 sec)\n",
            "train - step 2312: loss = 428066.19 (2.120 sec)\n",
            "train - step 2313: loss = 462177.69 (1.714 sec)\n",
            "train - step 2314: loss = 374793.53 (1.692 sec)\n",
            "train - step 2315: loss = 459782.91 (1.840 sec)\n",
            "train - step 2316: loss = 336446.09 (2.564 sec)\n",
            "train - step 2317: loss = 337279.31 (1.704 sec)\n",
            "train - step 2318: loss = 421635.84 (1.670 sec)\n",
            "train - step 2319: loss = 294783.38 (1.658 sec)\n",
            "train - step 2320: loss = 427913.47 (1.674 sec)\n",
            "train - step 2321: loss = 533021.69 (1.689 sec)\n",
            "train - step 2322: loss = 435278.31 (1.689 sec)\n",
            "train - step 2323: loss = 564064.19 (1.687 sec)\n",
            "train - step 2324: loss = 254545.75 (1.679 sec)\n",
            "train - step 2325: loss = 625538.38 (1.728 sec)\n",
            "train - step 2326: loss = 408191.72 (1.683 sec)\n",
            "train - step 2327: loss = 281402.31 (1.703 sec)\n",
            "train - step 2328: loss = 314103.53 (1.657 sec)\n",
            "train - step 2329: loss = 464777.00 (1.686 sec)\n",
            "train - step 2330: loss = 543856.75 (1.702 sec)\n",
            "train - step 2331: loss = 291965.78 (1.692 sec)\n",
            "train - step 2332: loss = 413358.53 (1.695 sec)\n",
            "train - step 2333: loss = 374376.41 (1.695 sec)\n",
            "train - step 2334: loss = 576728.00 (1.715 sec)\n",
            "train - step 2335: loss = 567674.00 (1.703 sec)\n",
            "train - step 2336: loss = 360168.38 (1.695 sec)\n",
            "train - step 2337: loss = 345252.00 (1.671 sec)\n",
            "train - step 2338: loss = 544790.12 (1.701 sec)\n",
            "train - step 2339: loss = 477422.69 (1.684 sec)\n",
            "train - step 2340: loss = 546865.62 (1.709 sec)\n",
            "train - step 2341: loss = 583846.62 (1.700 sec)\n",
            "train - step 2342: loss = 276943.88 (1.673 sec)\n",
            "train - step 2343: loss = 478820.19 (1.732 sec)\n",
            "train - step 2344: loss = 541900.25 (1.671 sec)\n",
            "train - step 2345: loss = 764997.56 (1.679 sec)\n",
            "train - step 2346: loss = 431544.19 (1.698 sec)\n",
            "train - step 2347: loss = 368823.28 (1.710 sec)\n",
            "train - step 2348: loss = 513324.91 (1.682 sec)\n",
            "train - step 2349: loss = 537747.81 (1.693 sec)\n",
            "train - step 2350: loss = 476244.41 (1.670 sec)\n",
            "train - step 2351: loss = 561893.62 (1.681 sec)\n",
            "train - step 2352: loss = 537857.31 (2.684 sec)\n",
            "train - step 2353: loss = 358666.47 (1.684 sec)\n",
            "train - step 2354: loss = 326285.00 (1.720 sec)\n",
            "train - step 2355: loss = 423814.56 (1.717 sec)\n",
            "train - step 2356: loss = 479090.72 (1.710 sec)\n",
            "train - step 2357: loss = 506850.56 (1.693 sec)\n",
            "train - step 2358: loss = 436568.06 (1.703 sec)\n",
            "train - step 2359: loss = 627121.56 (1.694 sec)\n",
            "train - step 2360: loss = 439180.78 (1.711 sec)\n",
            "train - step 2361: loss = 428518.34 (1.669 sec)\n",
            "train - step 2362: loss = 602528.38 (1.694 sec)\n",
            "train - step 2363: loss = 370577.62 (1.685 sec)\n",
            "train - step 2364: loss = 417147.97 (1.680 sec)\n",
            "train - step 2365: loss = 314058.28 (1.694 sec)\n",
            "train - step 2366: loss = 373073.59 (1.677 sec)\n",
            "train - step 2367: loss = 642905.94 (1.690 sec)\n",
            "train - step 2368: loss = 307833.19 (1.670 sec)\n",
            "train - step 2369: loss = 307597.09 (1.686 sec)\n",
            "train - step 2370: loss = 458176.16 (1.714 sec)\n",
            "train - step 2371: loss = 320177.47 (1.706 sec)\n",
            "train - step 2372: loss = 417372.59 (1.701 sec)\n",
            "train - step 2373: loss = 456670.94 (1.701 sec)\n",
            "train - step 2374: loss = 287848.06 (1.683 sec)\n",
            "train - step 2375: loss = 401926.41 (1.698 sec)\n",
            "train - step 2376: loss = 350737.34 (1.696 sec)\n",
            "train - step 2377: loss = 479099.84 (1.688 sec)\n",
            "train - step 2378: loss = 377677.41 (1.667 sec)\n",
            "train - step 2379: loss = 400577.41 (1.667 sec)\n",
            "train - step 2380: loss = 577173.62 (1.723 sec)\n",
            "train - step 2381: loss = 439017.34 (1.674 sec)\n",
            "train - step 2382: loss = 345761.91 (1.697 sec)\n",
            "train - step 2383: loss = 357338.78 (1.691 sec)\n",
            "train - step 2384: loss = 279830.06 (1.706 sec)\n",
            "train - step 2385: loss = 462176.72 (1.691 sec)\n",
            "train - step 2386: loss = 452618.03 (1.716 sec)\n",
            "train - step 2387: loss = 468636.50 (1.690 sec)\n",
            "train - step 2388: loss = 438775.94 (2.670 sec)\n",
            "train - step 2389: loss = 311054.50 (1.707 sec)\n",
            "train - step 2390: loss = 422824.00 (1.701 sec)\n",
            "train - step 2391: loss = 484315.50 (1.722 sec)\n",
            "train - step 2392: loss = 331674.09 (1.688 sec)\n",
            "train - step 2393: loss = 485757.59 (1.704 sec)\n",
            "train - step 2394: loss = 371440.41 (1.690 sec)\n",
            "train - step 2395: loss = 395791.59 (1.675 sec)\n",
            "train - step 2396: loss = 441404.06 (1.661 sec)\n",
            "train - step 2397: loss = 408482.97 (1.698 sec)\n",
            "train - step 2398: loss = 358964.66 (1.698 sec)\n",
            "train - step 2399: loss = 509006.19 (1.691 sec)\n",
            "train - step 2400: loss = 496628.78 (1.727 sec)\n",
            "train - step 2401: loss = 540432.75 (1.693 sec)\n",
            "train - step 2402: loss = 406984.22 (1.692 sec)\n",
            "train - step 2403: loss = 543840.56 (1.698 sec)\n",
            "train - step 2404: loss = 308470.00 (1.684 sec)\n",
            "train - step 2405: loss = 303070.69 (1.668 sec)\n",
            "train - step 2406: loss = 467932.66 (1.666 sec)\n",
            "train - step 2407: loss = 363527.53 (1.674 sec)\n",
            "train - step 2408: loss = 537518.94 (1.709 sec)\n",
            "train - step 2409: loss = 505037.06 (1.706 sec)\n",
            "train - step 2410: loss = 466708.19 (1.686 sec)\n",
            "train - step 2411: loss = 485341.06 (1.677 sec)\n",
            "train - step 2412: loss = 402342.69 (1.717 sec)\n",
            "train - step 2413: loss = 427751.50 (1.658 sec)\n",
            "train - step 2414: loss = 484211.09 (1.711 sec)\n",
            "train - step 2415: loss = 545429.38 (1.708 sec)\n",
            "train - step 2416: loss = 612107.19 (1.694 sec)\n",
            "train - step 2417: loss = 449902.72 (1.678 sec)\n",
            "train - step 2418: loss = 566449.19 (1.694 sec)\n",
            "train - step 2419: loss = 459802.81 (1.663 sec)\n",
            "train - step 2420: loss = 345926.03 (1.678 sec)\n",
            "train - step 2421: loss = 511165.28 (1.675 sec)\n",
            "train - step 2422: loss = 538628.88 (1.706 sec)\n",
            "train - step 2423: loss = 528290.94 (1.740 sec)\n",
            "train - step 2424: loss = 422483.06 (2.594 sec)\n",
            "train - step 2425: loss = 480581.81 (1.746 sec)\n",
            "train - step 2426: loss = 378869.19 (1.674 sec)\n",
            "train - step 2427: loss = 347193.53 (1.705 sec)\n",
            "train - step 2428: loss = 328175.62 (1.725 sec)\n",
            "train - step 2429: loss = 481859.59 (1.703 sec)\n",
            "train - step 2430: loss = 434359.78 (1.683 sec)\n",
            "train - step 2431: loss = 421977.03 (1.687 sec)\n",
            "train - step 2432: loss = 314694.41 (1.672 sec)\n",
            "train - step 2433: loss = 471188.50 (1.710 sec)\n",
            "train - step 2434: loss = 341653.41 (1.691 sec)\n",
            "train - step 2435: loss = 453450.38 (1.670 sec)\n",
            "train - step 2436: loss = 545144.19 (1.709 sec)\n",
            "train - step 2437: loss = 423635.59 (1.700 sec)\n",
            "train - step 2438: loss = 368299.88 (1.697 sec)\n",
            "train - step 2439: loss = 550718.88 (1.671 sec)\n",
            "train - step 2440: loss = 407222.53 (1.680 sec)\n",
            "train - step 2441: loss = 401952.72 (1.690 sec)\n",
            "train - step 2442: loss = 398710.81 (1.677 sec)\n",
            "train - step 2443: loss = 525099.31 (1.675 sec)\n",
            "train - step 2444: loss = 324720.53 (1.696 sec)\n",
            "train - step 2445: loss = 518881.88 (1.677 sec)\n",
            "train - step 2446: loss = 412805.53 (1.667 sec)\n",
            "train - step 2447: loss = 523652.28 (1.686 sec)\n",
            "train - step 2448: loss = 456233.62 (1.664 sec)\n",
            "train - step 2449: loss = 359174.44 (1.685 sec)\n",
            "train - step 2450: loss = 536454.06 (1.648 sec)\n",
            "train - step 2451: loss = 368880.91 (1.667 sec)\n",
            "train - step 2452: loss = 419385.28 (1.644 sec)\n",
            "train - step 2453: loss = 414833.72 (1.653 sec)\n",
            "train - step 2454: loss = 387772.00 (1.651 sec)\n",
            "train - step 2455: loss = 504718.00 (1.657 sec)\n",
            "train - step 2456: loss = 440985.06 (1.642 sec)\n",
            "train - step 2457: loss = 425270.97 (1.653 sec)\n",
            "train - step 2458: loss = 432825.16 (1.680 sec)\n",
            "train - step 2459: loss = 426230.00 (1.676 sec)\n",
            "train - step 2460: loss = 337805.81 (2.611 sec)\n",
            "train - step 2461: loss = 494394.66 (1.690 sec)\n",
            "train - step 2462: loss = 372159.88 (2.362 sec)\n",
            "train - step 2463: loss = 351587.38 (1.707 sec)\n",
            "train - step 2464: loss = 415522.69 (1.694 sec)\n",
            "train - step 2465: loss = 440958.16 (1.666 sec)\n",
            "train - step 2466: loss = 480583.19 (1.678 sec)\n",
            "train - step 2467: loss = 356213.50 (1.666 sec)\n",
            "train - step 2468: loss = 451963.66 (1.704 sec)\n",
            "train - step 2469: loss = 482753.16 (1.681 sec)\n",
            "train - step 2470: loss = 327525.28 (1.676 sec)\n",
            "train - step 2471: loss = 557338.75 (1.668 sec)\n",
            "train - step 2472: loss = 367589.22 (1.660 sec)\n",
            "train - step 2473: loss = 491754.16 (1.683 sec)\n",
            "train - step 2474: loss = 584009.38 (1.693 sec)\n",
            "train - step 2475: loss = 345822.47 (1.670 sec)\n",
            "train - step 2476: loss = 435480.22 (1.697 sec)\n",
            "train - step 2477: loss = 413549.34 (1.671 sec)\n",
            "train - step 2478: loss = 498691.53 (1.690 sec)\n",
            "train - step 2479: loss = 410816.41 (1.693 sec)\n",
            "train - step 2480: loss = 557817.31 (1.677 sec)\n",
            "train - step 2481: loss = 299804.03 (1.676 sec)\n",
            "train - step 2482: loss = 517245.22 (1.685 sec)\n",
            "train - step 2483: loss = 556833.31 (1.668 sec)\n",
            "train - step 2484: loss = 370020.91 (1.690 sec)\n",
            "train - step 2485: loss = 392327.69 (1.678 sec)\n",
            "train - step 2486: loss = 578875.31 (1.687 sec)\n",
            "train - step 2487: loss = 459376.03 (1.678 sec)\n",
            "train - step 2488: loss = 556362.12 (1.674 sec)\n",
            "train - step 2489: loss = 357201.53 (1.675 sec)\n",
            "train - step 2490: loss = 422566.72 (1.682 sec)\n",
            "train - step 2491: loss = 558210.38 (1.678 sec)\n",
            "train - step 2492: loss = 401869.97 (1.667 sec)\n",
            "train - step 2493: loss = 377963.88 (1.661 sec)\n",
            "train - step 2494: loss = 384692.56 (1.698 sec)\n",
            "train - step 2495: loss = 417545.84 (2.637 sec)\n",
            "train - step 2496: loss = 327778.22 (1.701 sec)\n",
            "train - step 2497: loss = 506807.31 (1.673 sec)\n",
            "train - step 2498: loss = 399051.44 (1.678 sec)\n",
            "train - step 2499: loss = 387487.31 (1.676 sec)\n",
            "train - step 2500: loss = 612828.06 (1.685 sec)\n",
            "train - step 2501: loss = 393159.88 (1.693 sec)\n",
            "train - step 2502: loss = 609640.62 (1.663 sec)\n",
            "train - step 2503: loss = 446376.44 (1.680 sec)\n",
            "train - step 2504: loss = 353532.31 (1.661 sec)\n",
            "train - step 2505: loss = 465593.12 (1.682 sec)\n",
            "train - step 2506: loss = 403184.09 (1.685 sec)\n",
            "train - step 2507: loss = 482773.06 (1.680 sec)\n",
            "train - step 2508: loss = 511175.72 (1.660 sec)\n",
            "train - step 2509: loss = 463258.28 (1.679 sec)\n",
            "train - step 2510: loss = 403605.50 (1.686 sec)\n",
            "train - step 2511: loss = 408677.53 (1.689 sec)\n",
            "train - step 2512: loss = 436181.41 (1.673 sec)\n",
            "train - step 2513: loss = 444651.28 (1.658 sec)\n",
            "train - step 2514: loss = 349841.66 (1.685 sec)\n",
            "train - step 2515: loss = 365513.12 (1.652 sec)\n",
            "train - step 2516: loss = 459745.41 (1.684 sec)\n",
            "train - step 2517: loss = 368761.59 (1.681 sec)\n",
            "train - step 2518: loss = 355843.88 (1.653 sec)\n",
            "train - step 2519: loss = 410794.03 (1.690 sec)\n",
            "train - step 2520: loss = 324379.88 (1.683 sec)\n",
            "train - step 2521: loss = 427394.09 (1.675 sec)\n",
            "train - step 2522: loss = 499712.31 (1.663 sec)\n",
            "train - step 2523: loss = 549777.25 (1.674 sec)\n",
            "train - step 2524: loss = 467907.19 (1.663 sec)\n",
            "train - step 2525: loss = 476796.72 (1.658 sec)\n",
            "train - step 2526: loss = 395395.97 (1.695 sec)\n",
            "train - step 2527: loss = 473521.06 (1.668 sec)\n",
            "train - step 2528: loss = 558093.56 (1.663 sec)\n",
            "train - step 2529: loss = 393464.97 (1.671 sec)\n",
            "train - step 2530: loss = 327305.59 (1.693 sec)\n",
            "train - step 2531: loss = 463941.97 (2.566 sec)\n",
            "train - step 2532: loss = 415693.22 (1.698 sec)\n",
            "train - step 2533: loss = 429511.50 (1.686 sec)\n",
            "train - step 2534: loss = 489635.44 (1.668 sec)\n",
            "train - step 2535: loss = 315084.66 (1.712 sec)\n",
            "train - step 2536: loss = 226417.20 (1.649 sec)\n",
            "train - step 2537: loss = 313831.84 (1.678 sec)\n",
            "train - step 2538: loss = 400887.47 (1.677 sec)\n",
            "train - step 2539: loss = 464989.94 (1.675 sec)\n",
            "train - step 2540: loss = 313813.50 (1.655 sec)\n",
            "train - step 2541: loss = 395737.06 (1.665 sec)\n",
            "train - step 2542: loss = 612998.94 (1.684 sec)\n",
            "train - step 2543: loss = 342230.81 (1.676 sec)\n",
            "train - step 2544: loss = 452524.09 (1.706 sec)\n",
            "train - step 2545: loss = 579415.25 (1.672 sec)\n",
            "train - step 2546: loss = 333998.81 (1.716 sec)\n",
            "train - step 2547: loss = 527670.94 (1.687 sec)\n",
            "train - step 2548: loss = 308245.91 (1.690 sec)\n",
            "train - step 2549: loss = 382795.12 (1.660 sec)\n",
            "train - step 2550: loss = 405577.84 (1.683 sec)\n",
            "train - step 2551: loss = 209933.59 (1.690 sec)\n",
            "train - step 2552: loss = 329497.53 (1.659 sec)\n",
            "train - step 2553: loss = 311151.38 (1.701 sec)\n",
            "train - step 2554: loss = 468964.91 (1.652 sec)\n",
            "train - step 2555: loss = 600893.56 (1.667 sec)\n",
            "train - step 2556: loss = 587530.38 (1.676 sec)\n",
            "train - step 2557: loss = 288265.16 (1.686 sec)\n",
            "train - step 2558: loss = 620569.94 (1.668 sec)\n",
            "train - step 2559: loss = 366208.53 (1.693 sec)\n",
            "train - step 2560: loss = 457344.31 (1.670 sec)\n",
            "train - step 2561: loss = 482963.47 (1.669 sec)\n",
            "train - step 2562: loss = 522446.47 (1.671 sec)\n",
            "train - step 2563: loss = 372459.19 (1.667 sec)\n",
            "train - step 2564: loss = 404779.66 (1.699 sec)\n",
            "train - step 2565: loss = 374172.72 (1.678 sec)\n",
            "train - step 2566: loss = 379025.88 (1.674 sec)\n",
            "train - step 2567: loss = 230748.45 (2.577 sec)\n",
            "train - step 2568: loss = 534541.56 (1.686 sec)\n",
            "train - step 2569: loss = 532746.06 (1.675 sec)\n",
            "train - step 2570: loss = 526165.56 (1.664 sec)\n",
            "train - step 2571: loss = 354580.47 (1.675 sec)\n",
            "train - step 2572: loss = 534481.12 (1.658 sec)\n",
            "train - step 2573: loss = 392884.66 (1.684 sec)\n",
            "train - step 2574: loss = 403052.03 (1.685 sec)\n",
            "train - step 2575: loss = 672847.31 (1.708 sec)\n",
            "train - step 2576: loss = 521781.69 (1.708 sec)\n",
            "train - step 2577: loss = 536092.88 (1.667 sec)\n",
            "train - step 2578: loss = 594844.06 (1.694 sec)\n",
            "train - step 2579: loss = 504174.88 (1.651 sec)\n",
            "train - step 2580: loss = 549249.19 (1.642 sec)\n",
            "train - step 2581: loss = 403492.91 (1.668 sec)\n",
            "train - step 2582: loss = 366247.50 (1.669 sec)\n",
            "train - step 2583: loss = 357015.59 (1.659 sec)\n",
            "train - step 2584: loss = 389482.78 (1.688 sec)\n",
            "train - step 2585: loss = 447133.34 (1.671 sec)\n",
            "train - step 2586: loss = 400831.69 (1.722 sec)\n",
            "train - step 2587: loss = 384912.59 (1.737 sec)\n",
            "train - step 2588: loss = 566904.88 (1.707 sec)\n",
            "train - step 2589: loss = 516597.69 (1.679 sec)\n",
            "train - step 2590: loss = 270851.00 (1.682 sec)\n",
            "train - step 2591: loss = 437130.28 (1.697 sec)\n",
            "train - step 2592: loss = 566044.56 (1.670 sec)\n",
            "train - step 2593: loss = 654234.25 (1.710 sec)\n",
            "train - step 2594: loss = 389021.19 (1.665 sec)\n",
            "train - step 2595: loss = 307646.09 (1.711 sec)\n",
            "train - step 2596: loss = 325739.38 (1.669 sec)\n",
            "train - step 2597: loss = 389036.88 (1.667 sec)\n",
            "train - step 2598: loss = 295410.97 (1.684 sec)\n",
            "train - step 2599: loss = 345090.19 (1.693 sec)\n",
            "train - step 2600: loss = 505146.12 (2.051 sec)\n",
            "train - step 2601: loss = 332100.28 (1.686 sec)\n",
            "train - step 2602: loss = 373405.34 (1.660 sec)\n",
            "train - step 2603: loss = 336451.62 (2.850 sec)\n",
            "train - step 2604: loss = 424466.94 (1.707 sec)\n",
            "train - step 2605: loss = 318995.19 (1.707 sec)\n",
            "train - step 2606: loss = 402715.44 (1.688 sec)\n",
            "train - step 2607: loss = 447950.00 (1.680 sec)\n",
            "train - step 2608: loss = 386897.00 (1.668 sec)\n",
            "train - step 2609: loss = 485447.97 (1.683 sec)\n",
            "train - step 2610: loss = 519302.94 (1.697 sec)\n",
            "train - step 2611: loss = 514316.97 (1.708 sec)\n",
            "train - step 2612: loss = 538869.75 (1.674 sec)\n",
            "train - step 2613: loss = 406537.50 (1.696 sec)\n",
            "train - step 2614: loss = 333140.12 (1.694 sec)\n",
            "train - step 2615: loss = 350560.09 (1.679 sec)\n",
            "train - step 2616: loss = 362128.72 (1.684 sec)\n",
            "train - step 2617: loss = 549796.25 (1.685 sec)\n",
            "train - step 2618: loss = 312184.91 (1.694 sec)\n",
            "train - step 2619: loss = 331462.22 (1.677 sec)\n",
            "train - step 2620: loss = 470857.06 (1.671 sec)\n",
            "train - step 2621: loss = 517465.50 (1.686 sec)\n",
            "train - step 2622: loss = 343341.72 (1.646 sec)\n",
            "train - step 2623: loss = 380386.34 (1.678 sec)\n",
            "train - step 2624: loss = 526365.88 (1.672 sec)\n",
            "train - step 2625: loss = 360186.06 (1.665 sec)\n",
            "train - step 2626: loss = 512722.47 (1.662 sec)\n",
            "train - step 2627: loss = 543687.94 (1.700 sec)\n",
            "train - step 2628: loss = 456648.41 (1.669 sec)\n",
            "train - step 2629: loss = 417958.09 (1.687 sec)\n",
            "train - step 2630: loss = 543825.25 (1.671 sec)\n",
            "train - step 2631: loss = 586036.75 (1.645 sec)\n",
            "train - step 2632: loss = 316750.09 (1.653 sec)\n",
            "train - step 2633: loss = 477462.66 (1.639 sec)\n",
            "train - step 2634: loss = 549729.12 (1.686 sec)\n",
            "train - step 2635: loss = 328651.81 (1.637 sec)\n",
            "train - step 2636: loss = 420597.72 (1.658 sec)\n",
            "train - step 2637: loss = 406167.91 (1.651 sec)\n",
            "train - step 2638: loss = 478569.50 (1.684 sec)\n",
            "train - step 2639: loss = 479240.59 (2.520 sec)\n",
            "train - step 2640: loss = 378632.84 (1.700 sec)\n",
            "train - step 2641: loss = 387388.03 (1.678 sec)\n",
            "train - step 2642: loss = 475800.97 (1.662 sec)\n",
            "train - step 2643: loss = 478723.81 (1.673 sec)\n",
            "train - step 2644: loss = 438754.53 (1.683 sec)\n",
            "train - step 2645: loss = 482263.56 (1.700 sec)\n",
            "train - step 2646: loss = 420633.81 (1.677 sec)\n",
            "train - step 2647: loss = 743513.69 (1.665 sec)\n",
            "train - step 2648: loss = 332632.19 (1.649 sec)\n",
            "train - step 2649: loss = 592905.56 (1.675 sec)\n",
            "train - step 2650: loss = 615930.44 (1.679 sec)\n",
            "train - step 2651: loss = 464991.97 (1.686 sec)\n",
            "train - step 2652: loss = 320970.19 (1.694 sec)\n",
            "train - step 2653: loss = 496020.06 (1.694 sec)\n",
            "train - step 2654: loss = 432836.47 (1.676 sec)\n",
            "train - step 2655: loss = 528875.00 (1.677 sec)\n",
            "train - step 2656: loss = 607311.25 (1.667 sec)\n",
            "train - step 2657: loss = 463508.94 (1.688 sec)\n",
            "train - step 2658: loss = 481763.56 (1.687 sec)\n",
            "train - step 2659: loss = 418028.78 (1.674 sec)\n",
            "train - step 2660: loss = 378425.62 (1.691 sec)\n",
            "train - step 2661: loss = 470916.16 (1.695 sec)\n",
            "train - step 2662: loss = 475367.56 (1.682 sec)\n",
            "train - step 2663: loss = 518576.47 (1.688 sec)\n",
            "train - step 2664: loss = 268885.47 (1.668 sec)\n",
            "train - step 2665: loss = 328357.69 (1.698 sec)\n",
            "train - step 2666: loss = 437895.78 (1.657 sec)\n",
            "train - step 2667: loss = 474489.66 (1.648 sec)\n",
            "train - step 2668: loss = 444622.97 (1.656 sec)\n",
            "train - step 2669: loss = 559791.38 (1.675 sec)\n",
            "train - step 2670: loss = 397020.44 (1.668 sec)\n",
            "train - step 2671: loss = 324457.38 (1.686 sec)\n",
            "train - step 2672: loss = 437992.22 (1.673 sec)\n",
            "train - step 2673: loss = 320212.34 (1.670 sec)\n",
            "train - step 2674: loss = 604828.12 (1.679 sec)\n",
            "train - step 2675: loss = 489141.44 (2.639 sec)\n",
            "train - step 2676: loss = 538994.00 (1.708 sec)\n",
            "train - step 2677: loss = 374027.28 (1.686 sec)\n",
            "train - step 2678: loss = 430835.38 (1.681 sec)\n",
            "train - step 2679: loss = 513446.53 (1.710 sec)\n",
            "train - step 2680: loss = 365485.88 (1.675 sec)\n",
            "train - step 2681: loss = 415345.38 (1.675 sec)\n",
            "train - step 2682: loss = 456615.22 (1.678 sec)\n",
            "train - step 2683: loss = 304437.59 (1.676 sec)\n",
            "train - step 2684: loss = 358550.34 (1.685 sec)\n",
            "train - step 2685: loss = 637089.00 (1.674 sec)\n",
            "train - step 2686: loss = 461027.00 (1.683 sec)\n",
            "train - step 2687: loss = 503199.12 (1.663 sec)\n",
            "train - step 2688: loss = 449593.53 (1.683 sec)\n",
            "train - step 2689: loss = 378539.47 (1.662 sec)\n",
            "train - step 2690: loss = 483265.72 (1.678 sec)\n",
            "train - step 2691: loss = 471385.38 (1.678 sec)\n",
            "train - step 2692: loss = 510969.91 (1.663 sec)\n",
            "train - step 2693: loss = 389325.91 (1.688 sec)\n",
            "train - step 2694: loss = 453656.22 (1.674 sec)\n",
            "train - step 2695: loss = 431383.84 (1.685 sec)\n",
            "train - step 2696: loss = 638980.94 (1.669 sec)\n",
            "train - step 2697: loss = 378947.41 (1.689 sec)\n",
            "train - step 2698: loss = 548276.75 (1.667 sec)\n",
            "train - step 2699: loss = 347393.72 (1.707 sec)\n",
            "train - step 2700: loss = 393549.62 (1.682 sec)\n",
            "train - step 2701: loss = 373366.66 (1.655 sec)\n",
            "train - step 2702: loss = 445591.34 (1.705 sec)\n",
            "train - step 2703: loss = 326608.97 (1.671 sec)\n",
            "train - step 2704: loss = 426508.66 (1.667 sec)\n",
            "train - step 2705: loss = 377746.00 (1.657 sec)\n",
            "train - step 2706: loss = 541938.44 (1.677 sec)\n",
            "train - step 2707: loss = 301900.91 (1.656 sec)\n",
            "train - step 2708: loss = 409648.34 (1.657 sec)\n",
            "train - step 2709: loss = 343971.50 (1.685 sec)\n",
            "train - step 2710: loss = 417385.66 (1.677 sec)\n",
            "train - step 2711: loss = 369771.72 (2.523 sec)\n",
            "train - step 2712: loss = 388286.97 (1.667 sec)\n",
            "train - step 2713: loss = 391225.59 (1.674 sec)\n",
            "train - step 2714: loss = 404881.41 (1.678 sec)\n",
            "train - step 2715: loss = 465796.69 (1.655 sec)\n",
            "train - step 2716: loss = 315501.34 (1.669 sec)\n",
            "train - step 2717: loss = 446337.47 (1.680 sec)\n",
            "train - step 2718: loss = 368011.38 (1.670 sec)\n",
            "train - step 2719: loss = 341393.84 (1.665 sec)\n",
            "train - step 2720: loss = 403393.12 (1.659 sec)\n",
            "train - step 2721: loss = 539570.19 (1.667 sec)\n",
            "train - step 2722: loss = 496584.78 (1.656 sec)\n",
            "train - step 2723: loss = 583982.75 (1.675 sec)\n",
            "train - step 2724: loss = 475740.22 (1.657 sec)\n",
            "train - step 2725: loss = 339528.41 (1.701 sec)\n",
            "train - step 2726: loss = 465691.00 (1.671 sec)\n",
            "train - step 2727: loss = 371298.78 (1.697 sec)\n",
            "train - step 2728: loss = 378291.66 (1.707 sec)\n",
            "train - step 2729: loss = 402236.88 (1.656 sec)\n",
            "train - step 2730: loss = 360761.22 (1.659 sec)\n",
            "train - step 2731: loss = 478684.00 (1.647 sec)\n",
            "train - step 2732: loss = 472561.47 (1.688 sec)\n",
            "train - step 2733: loss = 455876.66 (1.704 sec)\n",
            "train - step 2734: loss = 621026.69 (1.659 sec)\n",
            "train - step 2735: loss = 475592.12 (1.668 sec)\n",
            "train - step 2736: loss = 579207.75 (1.658 sec)\n",
            "train - step 2737: loss = 473291.16 (1.676 sec)\n",
            "train - step 2738: loss = 616558.06 (1.700 sec)\n",
            "train - step 2739: loss = 423610.03 (1.704 sec)\n",
            "train - step 2740: loss = 388489.28 (1.683 sec)\n",
            "train - step 2741: loss = 277904.19 (1.667 sec)\n",
            "train - step 2742: loss = 298114.44 (1.694 sec)\n",
            "train - step 2743: loss = 479939.53 (1.701 sec)\n",
            "train - step 2744: loss = 433032.44 (1.671 sec)\n",
            "train - step 2745: loss = 437151.12 (1.695 sec)\n",
            "train - step 2746: loss = 518673.72 (1.702 sec)\n",
            "train - step 2747: loss = 529743.69 (2.604 sec)\n",
            "train - step 2748: loss = 350854.06 (1.669 sec)\n",
            "train - step 2749: loss = 475697.12 (1.676 sec)\n",
            "train - step 2750: loss = 480280.94 (1.706 sec)\n",
            "train - step 2751: loss = 443457.59 (1.706 sec)\n",
            "train - step 2752: loss = 430689.69 (1.667 sec)\n",
            "train - step 2753: loss = 356539.59 (1.668 sec)\n",
            "train - step 2754: loss = 341735.22 (1.671 sec)\n",
            "train - step 2755: loss = 276678.53 (1.642 sec)\n",
            "train - step 2756: loss = 441395.59 (1.673 sec)\n",
            "train - step 2757: loss = 526034.94 (1.673 sec)\n",
            "train - step 2758: loss = 399889.44 (1.667 sec)\n",
            "train - step 2759: loss = 495018.31 (1.685 sec)\n",
            "train - step 2760: loss = 294586.50 (1.666 sec)\n",
            "train - step 2761: loss = 605834.12 (1.689 sec)\n",
            "train - step 2762: loss = 384117.56 (1.653 sec)\n",
            "train - step 2763: loss = 351748.38 (1.679 sec)\n",
            "train - step 2764: loss = 351952.84 (1.657 sec)\n",
            "train - step 2765: loss = 317432.53 (1.689 sec)\n",
            "train - step 2766: loss = 292516.00 (1.655 sec)\n",
            "train - step 2767: loss = 439888.00 (1.670 sec)\n",
            "train - step 2768: loss = 266019.69 (1.700 sec)\n",
            "train - step 2769: loss = 329733.53 (1.648 sec)\n",
            "train - step 2770: loss = 616416.19 (1.673 sec)\n",
            "train - step 2771: loss = 369676.56 (1.668 sec)\n",
            "train - step 2772: loss = 422909.22 (1.681 sec)\n",
            "train - step 2773: loss = 467726.41 (1.673 sec)\n",
            "train - step 2774: loss = 375706.00 (1.663 sec)\n",
            "train - step 2775: loss = 577435.75 (1.692 sec)\n",
            "train - step 2776: loss = 556151.44 (1.674 sec)\n",
            "train - step 2777: loss = 207032.80 (1.697 sec)\n",
            "train - step 2778: loss = 482923.47 (1.700 sec)\n",
            "train - step 2779: loss = 361840.09 (1.675 sec)\n",
            "train - step 2780: loss = 586541.75 (1.673 sec)\n",
            "train - step 2781: loss = 558175.06 (1.683 sec)\n",
            "train - step 2782: loss = 481754.59 (1.705 sec)\n",
            "train - step 2783: loss = 332969.16 (2.488 sec)\n",
            "train - step 2784: loss = 357460.91 (1.700 sec)\n",
            "train - step 2785: loss = 318021.94 (1.674 sec)\n",
            "train - step 2786: loss = 496601.62 (1.663 sec)\n",
            "train - step 2787: loss = 380674.59 (1.700 sec)\n",
            "train - step 2788: loss = 443918.00 (1.683 sec)\n",
            "train - step 2789: loss = 613991.00 (1.689 sec)\n",
            "train - step 2790: loss = 493458.47 (1.667 sec)\n",
            "train - step 2791: loss = 515887.78 (1.679 sec)\n",
            "train - step 2792: loss = 440712.22 (1.680 sec)\n",
            "train - step 2793: loss = 651166.12 (1.683 sec)\n",
            "train - step 2794: loss = 372257.72 (1.680 sec)\n",
            "train - step 2795: loss = 378528.62 (1.673 sec)\n",
            "train - step 2796: loss = 529016.75 (1.701 sec)\n",
            "train - step 2797: loss = 465557.16 (1.669 sec)\n",
            "train - step 2798: loss = 646695.56 (1.686 sec)\n",
            "train - step 2799: loss = 487241.66 (1.673 sec)\n",
            "train - step 2800: loss = 235748.06 (1.674 sec)\n",
            "train - step 2801: loss = 437364.19 (1.714 sec)\n",
            "train - step 2802: loss = 423850.81 (1.693 sec)\n",
            "train - step 2803: loss = 526138.19 (1.678 sec)\n",
            "train - step 2804: loss = 595837.31 (1.666 sec)\n",
            "train - step 2805: loss = 446595.44 (1.665 sec)\n",
            "train - step 2806: loss = 606784.19 (1.674 sec)\n",
            "train - step 2807: loss = 699479.69 (1.662 sec)\n",
            "train - step 2808: loss = 319490.28 (1.704 sec)\n",
            "train - step 2809: loss = 570044.06 (1.683 sec)\n",
            "train - step 2810: loss = 356652.03 (1.678 sec)\n",
            "train - step 2811: loss = 433986.38 (1.686 sec)\n",
            "train - step 2812: loss = 401650.59 (1.662 sec)\n",
            "train - step 2813: loss = 554801.50 (1.650 sec)\n",
            "train - step 2814: loss = 447198.22 (1.661 sec)\n",
            "train - step 2815: loss = 389155.66 (1.648 sec)\n",
            "train - step 2816: loss = 515115.34 (1.656 sec)\n",
            "train - step 2817: loss = 445968.56 (1.642 sec)\n",
            "train - step 2818: loss = 505514.62 (1.642 sec)\n",
            "train - step 2819: loss = 511915.97 (2.490 sec)\n",
            "train - step 2820: loss = 372475.53 (1.695 sec)\n",
            "train - step 2821: loss = 326784.66 (1.684 sec)\n",
            "train - step 2822: loss = 354499.34 (1.660 sec)\n",
            "train - step 2823: loss = 380222.19 (1.693 sec)\n",
            "train - step 2824: loss = 332154.62 (1.669 sec)\n",
            "train - step 2825: loss = 637492.38 (1.721 sec)\n",
            "train - step 2826: loss = 485762.91 (1.676 sec)\n",
            "train - step 2827: loss = 529082.81 (1.685 sec)\n",
            "train - step 2828: loss = 441505.22 (1.665 sec)\n",
            "train - step 2829: loss = 509134.22 (1.676 sec)\n",
            "train - step 2830: loss = 358068.06 (1.673 sec)\n",
            "train - step 2831: loss = 215771.47 (1.678 sec)\n",
            "train - step 2832: loss = 507922.47 (1.693 sec)\n",
            "train - step 2833: loss = 576015.00 (1.656 sec)\n",
            "train - step 2834: loss = 402984.41 (1.692 sec)\n",
            "train - step 2835: loss = 366205.66 (1.688 sec)\n",
            "train - step 2836: loss = 555514.19 (1.679 sec)\n",
            "train - step 2837: loss = 371680.19 (1.664 sec)\n",
            "train - step 2838: loss = 493348.12 (1.671 sec)\n",
            "train - step 2839: loss = 423846.19 (1.698 sec)\n",
            "train - step 2840: loss = 331737.34 (1.686 sec)\n",
            "train - step 2841: loss = 253870.08 (1.727 sec)\n",
            "train - step 2842: loss = 418015.53 (1.651 sec)\n",
            "train - step 2843: loss = 323496.91 (1.661 sec)\n",
            "train - step 2844: loss = 452379.56 (1.658 sec)\n",
            "train - step 2845: loss = 453747.78 (1.674 sec)\n",
            "train - step 2846: loss = 571386.19 (1.705 sec)\n",
            "train - step 2847: loss = 550218.19 (1.658 sec)\n",
            "train - step 2848: loss = 415365.41 (1.676 sec)\n",
            "train - step 2849: loss = 334659.06 (1.677 sec)\n",
            "train - step 2850: loss = 357522.47 (1.675 sec)\n",
            "train - step 2851: loss = 368584.84 (1.660 sec)\n",
            "train - step 2852: loss = 519447.06 (1.670 sec)\n",
            "train - step 2853: loss = 659918.19 (1.698 sec)\n",
            "train - step 2854: loss = 409066.44 (1.654 sec)\n",
            "train - step 2855: loss = 355504.91 (2.473 sec)\n",
            "train - step 2856: loss = 570995.19 (1.683 sec)\n",
            "train - step 2857: loss = 591332.38 (1.668 sec)\n",
            "train - step 2858: loss = 385853.16 (1.676 sec)\n",
            "train - step 2859: loss = 501726.16 (1.692 sec)\n",
            "train - step 2860: loss = 400326.94 (1.694 sec)\n",
            "train - step 2861: loss = 664755.75 (1.678 sec)\n",
            "train - step 2862: loss = 343684.59 (1.662 sec)\n",
            "train - step 2863: loss = 437474.97 (1.665 sec)\n",
            "train - step 2864: loss = 335435.56 (1.667 sec)\n",
            "train - step 2865: loss = 616487.12 (1.673 sec)\n",
            "train - step 2866: loss = 620966.12 (1.654 sec)\n",
            "train - step 2867: loss = 446466.06 (1.664 sec)\n",
            "train - step 2868: loss = 459086.50 (1.681 sec)\n",
            "train - step 2869: loss = 474744.78 (1.703 sec)\n",
            "train - step 2870: loss = 538963.62 (1.653 sec)\n",
            "train - step 2871: loss = 342122.72 (1.680 sec)\n",
            "train - step 2872: loss = 376917.66 (1.681 sec)\n",
            "train - step 2873: loss = 281608.81 (1.661 sec)\n",
            "train - step 2874: loss = 441516.69 (1.685 sec)\n",
            "train - step 2875: loss = 426987.38 (1.670 sec)\n",
            "train - step 2876: loss = 265915.34 (1.685 sec)\n",
            "train - step 2877: loss = 490444.00 (1.657 sec)\n",
            "train - step 2878: loss = 545684.94 (1.706 sec)\n",
            "train - step 2879: loss = 346296.38 (1.703 sec)\n",
            "train - step 2880: loss = 476342.12 (1.699 sec)\n",
            "train - step 2881: loss = 482287.97 (1.681 sec)\n",
            "train - step 2882: loss = 409234.91 (1.667 sec)\n",
            "train - step 2883: loss = 577011.44 (1.703 sec)\n",
            "train - step 2884: loss = 394879.22 (1.676 sec)\n",
            "train - step 2885: loss = 455830.44 (1.692 sec)\n",
            "train - step 2886: loss = 441881.09 (1.705 sec)\n",
            "train - step 2887: loss = 425521.94 (1.700 sec)\n",
            "train - step 2888: loss = 485054.50 (1.694 sec)\n",
            "train - step 2889: loss = 493314.31 (1.688 sec)\n",
            "train - step 2890: loss = 495057.38 (1.713 sec)\n",
            "train - step 2891: loss = 628608.88 (1.985 sec)\n",
            "train - step 2892: loss = 520799.03 (1.671 sec)\n",
            "train - step 2893: loss = 425984.12 (1.782 sec)\n",
            "train - step 2894: loss = 501933.22 (1.671 sec)\n",
            "train - step 2895: loss = 459411.00 (1.708 sec)\n",
            "train - step 2896: loss = 394695.94 (1.671 sec)\n",
            "train - step 2897: loss = 356954.69 (1.686 sec)\n",
            "train - step 2898: loss = 485378.22 (1.683 sec)\n",
            "train - step 2899: loss = 423202.69 (1.657 sec)\n",
            "train - step 2900: loss = 444399.41 (1.677 sec)\n",
            "train - step 2901: loss = 286164.09 (1.663 sec)\n",
            "train - step 2902: loss = 413630.06 (1.676 sec)\n",
            "train - step 2903: loss = 416753.41 (1.684 sec)\n",
            "train - step 2904: loss = 330756.91 (1.703 sec)\n",
            "train - step 2905: loss = 325777.53 (1.651 sec)\n",
            "train - step 2906: loss = 527690.38 (1.657 sec)\n",
            "train - step 2907: loss = 466087.00 (1.654 sec)\n",
            "train - step 2908: loss = 369917.00 (1.668 sec)\n",
            "train - step 2909: loss = 295938.47 (1.696 sec)\n",
            "train - step 2910: loss = 503914.69 (1.666 sec)\n",
            "train - step 2911: loss = 493472.84 (1.669 sec)\n",
            "train - step 2912: loss = 317285.06 (1.677 sec)\n",
            "train - step 2913: loss = 539919.06 (1.685 sec)\n",
            "train - step 2914: loss = 339228.66 (1.663 sec)\n",
            "train - step 2915: loss = 530346.31 (1.666 sec)\n",
            "train - step 2916: loss = 327185.78 (1.682 sec)\n",
            "train - step 2917: loss = 354970.78 (1.672 sec)\n",
            "train - step 2918: loss = 450340.34 (1.692 sec)\n",
            "train - step 2919: loss = 487010.12 (1.676 sec)\n",
            "train - step 2920: loss = 539743.25 (1.653 sec)\n",
            "train - step 2921: loss = 519959.00 (1.704 sec)\n",
            "train - step 2922: loss = 522224.47 (1.682 sec)\n",
            "train - step 2923: loss = 394805.56 (1.667 sec)\n",
            "train - step 2924: loss = 442209.03 (1.680 sec)\n",
            "train - step 2925: loss = 357452.97 (1.663 sec)\n",
            "train - step 2926: loss = 408633.59 (1.686 sec)\n",
            "train - step 2927: loss = 431092.59 (2.490 sec)\n",
            "train - step 2928: loss = 299946.28 (1.680 sec)\n",
            "train - step 2929: loss = 512797.91 (1.677 sec)\n",
            "train - step 2930: loss = 394370.00 (1.643 sec)\n",
            "train - step 2931: loss = 513632.53 (1.677 sec)\n",
            "train - step 2932: loss = 355386.88 (1.683 sec)\n",
            "train - step 2933: loss = 402496.66 (1.676 sec)\n",
            "train - step 2934: loss = 345637.53 (1.670 sec)\n",
            "train - step 2935: loss = 437567.12 (1.668 sec)\n",
            "train - step 2936: loss = 295396.62 (1.683 sec)\n",
            "train - step 2937: loss = 526031.75 (1.662 sec)\n",
            "train - step 2938: loss = 554212.25 (1.692 sec)\n",
            "train - step 2939: loss = 397216.19 (1.681 sec)\n",
            "train - step 2940: loss = 396702.78 (1.703 sec)\n",
            "train - step 2941: loss = 451474.28 (1.666 sec)\n",
            "train - step 2942: loss = 372058.03 (1.695 sec)\n",
            "train - step 2943: loss = 577284.12 (1.673 sec)\n",
            "train - step 2944: loss = 401400.34 (1.649 sec)\n",
            "train - step 2945: loss = 520318.94 (1.673 sec)\n",
            "train - step 2946: loss = 464356.09 (1.663 sec)\n",
            "train - step 2947: loss = 346609.47 (1.730 sec)\n",
            "train - step 2948: loss = 378644.38 (1.650 sec)\n",
            "train - step 2949: loss = 448261.38 (1.666 sec)\n",
            "train - step 2950: loss = 356222.22 (1.677 sec)\n",
            "train - step 2951: loss = 457993.28 (1.704 sec)\n",
            "train - step 2952: loss = 451693.28 (1.667 sec)\n",
            "train - step 2953: loss = 363978.62 (1.672 sec)\n",
            "train - step 2954: loss = 369848.59 (1.677 sec)\n",
            "train - step 2955: loss = 301776.28 (1.658 sec)\n",
            "train - step 2956: loss = 403652.47 (1.673 sec)\n",
            "train - step 2957: loss = 307076.69 (1.655 sec)\n",
            "train - step 2958: loss = 422360.53 (1.671 sec)\n",
            "train - step 2959: loss = 472358.41 (1.682 sec)\n",
            "train - step 2960: loss = 406909.59 (1.689 sec)\n",
            "train - step 2961: loss = 464310.28 (1.657 sec)\n",
            "train - step 2962: loss = 357189.97 (1.678 sec)\n",
            "train - step 2963: loss = 348634.72 (2.528 sec)\n",
            "train - step 2964: loss = 440429.19 (1.689 sec)\n",
            "train - step 2965: loss = 626330.19 (1.692 sec)\n",
            "train - step 2966: loss = 415646.53 (1.718 sec)\n",
            "train - step 2967: loss = 356048.12 (1.702 sec)\n",
            "train - step 2968: loss = 359622.28 (1.646 sec)\n",
            "train - step 2969: loss = 316844.09 (1.683 sec)\n",
            "train - step 2970: loss = 382084.38 (1.686 sec)\n",
            "train - step 2971: loss = 300476.47 (1.642 sec)\n",
            "train - step 2972: loss = 479961.94 (1.680 sec)\n",
            "train - step 2973: loss = 563241.75 (1.685 sec)\n",
            "train - step 2974: loss = 477410.56 (1.684 sec)\n",
            "train - step 2975: loss = 492431.91 (1.668 sec)\n",
            "train - step 2976: loss = 385300.31 (1.662 sec)\n",
            "train - step 2977: loss = 467774.06 (1.685 sec)\n",
            "train - step 2978: loss = 312466.47 (1.669 sec)\n",
            "train - step 2979: loss = 425835.81 (1.673 sec)\n",
            "train - step 2980: loss = 557358.00 (1.671 sec)\n",
            "train - step 2981: loss = 504593.28 (1.689 sec)\n",
            "train - step 2982: loss = 452294.06 (1.692 sec)\n",
            "train - step 2983: loss = 477622.03 (1.671 sec)\n",
            "train - step 2984: loss = 348287.88 (1.669 sec)\n",
            "train - step 2985: loss = 437245.34 (1.674 sec)\n",
            "train - step 2986: loss = 233616.73 (1.671 sec)\n",
            "train - step 2987: loss = 408069.72 (1.661 sec)\n",
            "train - step 2988: loss = 253248.69 (1.668 sec)\n",
            "train - step 2989: loss = 517568.41 (1.693 sec)\n",
            "train - step 2990: loss = 380426.12 (1.667 sec)\n",
            "train - step 2991: loss = 418931.72 (1.672 sec)\n",
            "train - step 2992: loss = 402755.94 (1.655 sec)\n",
            "train - step 2993: loss = 348015.44 (1.636 sec)\n",
            "train - step 2994: loss = 536228.75 (1.634 sec)\n",
            "train - step 2995: loss = 432711.94 (1.646 sec)\n",
            "train - step 2996: loss = 337557.94 (1.648 sec)\n",
            "train - step 2997: loss = 318904.94 (1.635 sec)\n",
            "train - step 2998: loss = 421202.81 (1.635 sec)\n",
            "train - step 2999: loss = 426695.69 (2.456 sec)\n",
            "train - step 3000: loss = 529501.38 (1.670 sec)\n",
            "train - step 3001: loss = 404793.69 (1.654 sec)\n",
            "train - step 3002: loss = 556218.31 (1.646 sec)\n",
            "train - step 3003: loss = 472715.56 (1.679 sec)\n",
            "train - step 3004: loss = 511597.88 (1.672 sec)\n",
            "train - step 3005: loss = 370505.47 (1.665 sec)\n",
            "train - step 3006: loss = 705096.12 (1.676 sec)\n",
            "train - step 3007: loss = 508792.69 (1.663 sec)\n",
            "train - step 3008: loss = 247344.47 (1.672 sec)\n",
            "train - step 3009: loss = 439969.19 (1.685 sec)\n",
            "train - step 3010: loss = 527993.56 (1.672 sec)\n",
            "train - step 3011: loss = 454699.66 (1.693 sec)\n",
            "train - step 3012: loss = 322461.31 (1.683 sec)\n",
            "train - step 3013: loss = 538448.25 (1.672 sec)\n",
            "train - step 3014: loss = 486682.88 (1.669 sec)\n",
            "train - step 3015: loss = 591426.88 (1.668 sec)\n",
            "train - step 3016: loss = 421855.78 (1.666 sec)\n",
            "train - step 3017: loss = 393801.16 (1.662 sec)\n",
            "train - step 3018: loss = 545943.81 (1.667 sec)\n",
            "train - step 3019: loss = 347859.94 (1.674 sec)\n",
            "train - step 3020: loss = 497040.62 (1.673 sec)\n",
            "train - step 3021: loss = 390481.03 (1.687 sec)\n",
            "train - step 3022: loss = 325031.81 (1.703 sec)\n",
            "train - step 3023: loss = 394735.50 (1.647 sec)\n",
            "train - step 3024: loss = 481794.72 (1.674 sec)\n",
            "train - step 3025: loss = 529984.81 (1.654 sec)\n",
            "train - step 3026: loss = 354722.66 (1.669 sec)\n",
            "train - step 3027: loss = 451410.28 (1.666 sec)\n",
            "train - step 3028: loss = 492037.31 (1.652 sec)\n",
            "train - step 3029: loss = 439012.69 (1.680 sec)\n",
            "train - step 3030: loss = 573328.44 (1.658 sec)\n",
            "train - step 3031: loss = 320489.78 (1.676 sec)\n",
            "train - step 3032: loss = 389450.81 (1.656 sec)\n",
            "train - step 3033: loss = 433230.12 (1.678 sec)\n",
            "train - step 3034: loss = 574098.44 (1.675 sec)\n",
            "train - step 3035: loss = 400913.53 (2.505 sec)\n",
            "train - step 3036: loss = 425357.28 (1.696 sec)\n",
            "train - step 3037: loss = 573992.94 (1.680 sec)\n",
            "train - step 3038: loss = 251896.58 (1.683 sec)\n",
            "train - step 3039: loss = 399107.34 (1.664 sec)\n",
            "train - step 3040: loss = 363670.53 (1.663 sec)\n",
            "train - step 3041: loss = 446257.34 (1.675 sec)\n",
            "train - step 3042: loss = 381260.19 (1.665 sec)\n",
            "train - step 3043: loss = 404240.03 (1.662 sec)\n",
            "train - step 3044: loss = 538828.06 (1.656 sec)\n",
            "train - step 3045: loss = 429345.47 (1.690 sec)\n",
            "train - step 3046: loss = 458713.34 (1.669 sec)\n",
            "train - step 3047: loss = 574036.00 (1.695 sec)\n",
            "train - step 3048: loss = 516384.09 (1.679 sec)\n",
            "train - step 3049: loss = 563793.75 (1.665 sec)\n",
            "train - step 3050: loss = 390069.09 (1.676 sec)\n",
            "train - step 3051: loss = 353029.56 (1.674 sec)\n",
            "train - step 3052: loss = 506050.69 (1.677 sec)\n",
            "train - step 3053: loss = 282283.28 (1.676 sec)\n",
            "train - step 3054: loss = 492819.78 (1.687 sec)\n",
            "train - step 3055: loss = 359046.31 (1.658 sec)\n",
            "train - step 3056: loss = 488279.03 (1.682 sec)\n",
            "train - step 3057: loss = 331540.34 (1.668 sec)\n",
            "train - step 3058: loss = 484487.19 (1.674 sec)\n",
            "train - step 3059: loss = 424409.56 (1.678 sec)\n",
            "train - step 3060: loss = 335882.66 (1.685 sec)\n",
            "train - step 3061: loss = 392181.28 (1.677 sec)\n",
            "train - step 3062: loss = 507986.28 (1.670 sec)\n",
            "train - step 3063: loss = 553854.62 (1.675 sec)\n",
            "train - step 3064: loss = 391664.53 (1.687 sec)\n",
            "train - step 3065: loss = 425638.59 (1.677 sec)\n",
            "train - step 3066: loss = 403259.16 (1.680 sec)\n",
            "train - step 3067: loss = 443723.44 (1.696 sec)\n",
            "train - step 3068: loss = 468742.81 (1.667 sec)\n",
            "train - step 3069: loss = 522109.94 (1.683 sec)\n",
            "train - step 3070: loss = 508011.12 (1.690 sec)\n",
            "train - step 3071: loss = 444479.94 (2.742 sec)\n",
            "train - step 3072: loss = 570520.31 (1.730 sec)\n",
            "train - step 3073: loss = 384998.69 (1.683 sec)\n",
            "train - step 3074: loss = 505388.38 (1.673 sec)\n",
            "train - step 3075: loss = 450023.88 (1.673 sec)\n",
            "train - step 3076: loss = 385784.38 (1.666 sec)\n",
            "train - step 3077: loss = 492024.06 (1.671 sec)\n",
            "train - step 3078: loss = 432996.72 (1.670 sec)\n",
            "train - step 3079: loss = 328079.41 (1.677 sec)\n",
            "train - step 3080: loss = 576564.81 (1.686 sec)\n",
            "train - step 3081: loss = 463852.19 (1.670 sec)\n",
            "train - step 3082: loss = 508989.47 (1.666 sec)\n",
            "train - step 3083: loss = 541005.25 (1.686 sec)\n",
            "train - step 3084: loss = 393595.00 (1.669 sec)\n",
            "train - step 3085: loss = 442728.34 (1.693 sec)\n",
            "train - step 3086: loss = 453662.12 (1.677 sec)\n",
            "train - step 3087: loss = 395739.81 (1.657 sec)\n",
            "train - step 3088: loss = 331924.66 (1.690 sec)\n",
            "train - step 3089: loss = 413405.78 (1.664 sec)\n",
            "train - step 3090: loss = 478047.97 (1.673 sec)\n",
            "train - step 3091: loss = 339612.44 (1.703 sec)\n",
            "train - step 3092: loss = 343139.69 (1.693 sec)\n",
            "train - step 3093: loss = 419147.47 (1.701 sec)\n",
            "train - step 3094: loss = 486421.22 (1.670 sec)\n",
            "train - step 3095: loss = 426823.78 (1.682 sec)\n",
            "train - step 3096: loss = 529020.19 (1.668 sec)\n",
            "train - step 3097: loss = 378565.84 (1.722 sec)\n",
            "train - step 3098: loss = 386267.44 (1.674 sec)\n",
            "train - step 3099: loss = 433445.12 (1.682 sec)\n",
            "train - step 3100: loss = 435726.34 (1.673 sec)\n",
            "train - step 3101: loss = 405055.72 (1.658 sec)\n",
            "train - step 3102: loss = 581902.31 (1.708 sec)\n",
            "train - step 3103: loss = 388095.78 (1.715 sec)\n",
            "train - step 3104: loss = 353897.97 (1.675 sec)\n",
            "train - step 3105: loss = 413241.22 (1.675 sec)\n",
            "train - step 3106: loss = 425255.47 (1.675 sec)\n",
            "train - step 3107: loss = 282723.91 (2.503 sec)\n",
            "train - step 3108: loss = 401007.69 (1.673 sec)\n",
            "train - step 3109: loss = 395561.38 (1.697 sec)\n",
            "train - step 3110: loss = 349200.06 (1.663 sec)\n",
            "train - step 3111: loss = 429452.88 (1.668 sec)\n",
            "train - step 3112: loss = 365557.09 (1.673 sec)\n",
            "train - step 3113: loss = 377436.34 (1.664 sec)\n",
            "train - step 3114: loss = 506990.03 (1.670 sec)\n",
            "train - step 3115: loss = 568100.00 (1.673 sec)\n",
            "train - step 3116: loss = 607038.25 (1.695 sec)\n",
            "train - step 3117: loss = 507223.06 (1.676 sec)\n",
            "train - step 3118: loss = 461063.38 (1.678 sec)\n",
            "train - step 3119: loss = 346057.62 (1.681 sec)\n",
            "train - step 3120: loss = 432612.16 (1.692 sec)\n",
            "train - step 3121: loss = 441833.94 (1.685 sec)\n",
            "train - step 3122: loss = 332412.66 (1.676 sec)\n",
            "train - step 3123: loss = 409733.97 (1.663 sec)\n",
            "train - step 3124: loss = 521986.88 (1.660 sec)\n",
            "train - step 3125: loss = 372954.53 (1.661 sec)\n",
            "train - step 3126: loss = 569164.81 (1.665 sec)\n",
            "train - step 3127: loss = 326932.19 (1.681 sec)\n",
            "train - step 3128: loss = 468414.62 (1.696 sec)\n",
            "train - step 3129: loss = 395531.09 (1.684 sec)\n",
            "train - step 3130: loss = 459641.34 (1.662 sec)\n",
            "train - step 3131: loss = 549120.75 (1.667 sec)\n",
            "train - step 3132: loss = 407482.72 (1.694 sec)\n",
            "train - step 3133: loss = 394048.47 (1.685 sec)\n",
            "train - step 3134: loss = 380257.47 (1.696 sec)\n",
            "train - step 3135: loss = 273000.28 (1.669 sec)\n",
            "train - step 3136: loss = 497217.31 (1.672 sec)\n",
            "train - step 3137: loss = 539355.44 (1.668 sec)\n",
            "train - step 3138: loss = 351695.41 (1.674 sec)\n",
            "train - step 3139: loss = 465409.50 (1.683 sec)\n",
            "train - step 3140: loss = 430055.00 (1.694 sec)\n",
            "train - step 3141: loss = 425057.28 (1.660 sec)\n",
            "train - step 3142: loss = 530838.00 (1.686 sec)\n",
            "train - step 3143: loss = 344659.53 (2.691 sec)\n",
            "train - step 3144: loss = 514346.56 (1.689 sec)\n",
            "train - step 3145: loss = 369558.28 (1.740 sec)\n",
            "train - step 3146: loss = 518153.91 (1.688 sec)\n",
            "train - step 3147: loss = 422642.41 (1.702 sec)\n",
            "train - step 3148: loss = 447629.28 (1.680 sec)\n",
            "train - step 3149: loss = 525926.75 (1.691 sec)\n",
            "train - step 3150: loss = 348405.78 (1.679 sec)\n",
            "train - step 3151: loss = 586051.69 (1.707 sec)\n",
            "train - step 3152: loss = 409741.97 (1.679 sec)\n",
            "train - step 3153: loss = 514702.12 (1.673 sec)\n",
            "train - step 3154: loss = 515812.06 (1.742 sec)\n",
            "train - step 3155: loss = 400840.06 (1.660 sec)\n",
            "train - step 3156: loss = 412414.78 (1.710 sec)\n",
            "train - step 3157: loss = 424556.62 (1.695 sec)\n",
            "train - step 3158: loss = 436723.81 (1.710 sec)\n",
            "train - step 3159: loss = 449026.53 (1.693 sec)\n",
            "train - step 3160: loss = 538621.19 (1.701 sec)\n",
            "train - step 3161: loss = 326279.12 (1.673 sec)\n",
            "train - step 3162: loss = 406868.81 (1.684 sec)\n",
            "train - step 3163: loss = 343682.91 (1.652 sec)\n",
            "train - step 3164: loss = 430440.66 (1.691 sec)\n",
            "train - step 3165: loss = 487115.78 (1.675 sec)\n",
            "train - step 3166: loss = 603560.38 (1.681 sec)\n",
            "train - step 3167: loss = 431980.50 (1.676 sec)\n",
            "train - step 3168: loss = 471658.34 (1.659 sec)\n",
            "train - step 3169: loss = 361735.81 (1.672 sec)\n",
            "train - step 3170: loss = 404182.09 (1.699 sec)\n",
            "train - step 3171: loss = 433136.81 (1.679 sec)\n",
            "train - step 3172: loss = 353649.31 (1.667 sec)\n",
            "train - step 3173: loss = 542510.38 (1.702 sec)\n",
            "train - step 3174: loss = 521944.41 (1.726 sec)\n",
            "train - step 3175: loss = 393550.03 (1.657 sec)\n",
            "train - step 3176: loss = 582230.31 (1.702 sec)\n",
            "train - step 3177: loss = 657517.19 (1.711 sec)\n",
            "train - step 3178: loss = 402621.81 (1.684 sec)\n",
            "train - step 3179: loss = 476104.59 (1.679 sec)\n",
            "train - step 3180: loss = 407698.19 (1.643 sec)\n",
            "train - step 3181: loss = 426237.97 (1.667 sec)\n",
            "train - step 3182: loss = 355455.81 (1.678 sec)\n",
            "train - step 3183: loss = 401783.72 (1.668 sec)\n",
            "train - step 3184: loss = 264332.06 (1.646 sec)\n",
            "train - step 3185: loss = 581328.81 (1.647 sec)\n",
            "train - step 3186: loss = 496422.62 (1.693 sec)\n",
            "train - step 3187: loss = 580984.81 (1.695 sec)\n",
            "train - step 3188: loss = 513020.78 (1.705 sec)\n",
            "train - step 3189: loss = 405935.34 (1.688 sec)\n",
            "train - step 3190: loss = 491699.81 (1.687 sec)\n",
            "train - step 3191: loss = 491621.59 (1.671 sec)\n",
            "train - step 3192: loss = 468724.56 (1.671 sec)\n",
            "train - step 3193: loss = 417392.78 (1.659 sec)\n",
            "train - step 3194: loss = 387762.47 (1.657 sec)\n",
            "train - step 3195: loss = 629055.31 (1.693 sec)\n",
            "train - step 3196: loss = 515325.12 (1.670 sec)\n",
            "train - step 3197: loss = 504595.88 (1.706 sec)\n",
            "train - step 3198: loss = 672505.81 (1.675 sec)\n",
            "train - step 3199: loss = 351020.00 (1.680 sec)\n",
            "train - step 3200: loss = 360981.28 (1.693 sec)\n",
            "train - step 3201: loss = 260046.92 (1.667 sec)\n",
            "train - step 3202: loss = 329163.69 (1.661 sec)\n",
            "train - step 3203: loss = 669760.56 (1.681 sec)\n",
            "train - step 3204: loss = 457317.78 (1.699 sec)\n",
            "train - step 3205: loss = 330304.00 (1.678 sec)\n",
            "train - step 3206: loss = 407095.88 (1.693 sec)\n",
            "train - step 3207: loss = 317108.53 (1.667 sec)\n",
            "train - step 3208: loss = 388434.94 (1.685 sec)\n",
            "train - step 3209: loss = 426460.94 (1.662 sec)\n",
            "train - step 3210: loss = 392413.00 (1.683 sec)\n",
            "train - step 3211: loss = 416098.88 (1.676 sec)\n",
            "train - step 3212: loss = 314656.91 (1.699 sec)\n",
            "train - step 3213: loss = 406378.78 (1.689 sec)\n",
            "train - step 3214: loss = 350908.62 (1.680 sec)\n",
            "train - step 3215: loss = 558537.00 (2.987 sec)\n",
            "train - step 3216: loss = 403071.88 (1.695 sec)\n",
            "train - step 3217: loss = 336706.47 (1.684 sec)\n",
            "train - step 3218: loss = 642935.31 (1.653 sec)\n",
            "train - step 3219: loss = 444667.06 (1.688 sec)\n",
            "train - step 3220: loss = 352577.06 (1.645 sec)\n",
            "train - step 3221: loss = 372206.47 (1.660 sec)\n",
            "train - step 3222: loss = 532252.56 (1.680 sec)\n",
            "train - step 3223: loss = 488780.34 (1.687 sec)\n",
            "train - step 3224: loss = 630501.19 (1.685 sec)\n",
            "train - step 3225: loss = 449084.22 (1.673 sec)\n",
            "train - step 3226: loss = 435258.88 (1.703 sec)\n",
            "train - step 3227: loss = 403687.16 (1.684 sec)\n",
            "train - step 3228: loss = 452606.41 (1.689 sec)\n",
            "train - step 3229: loss = 525853.69 (1.687 sec)\n",
            "train - step 3230: loss = 279814.56 (1.726 sec)\n",
            "train - step 3231: loss = 399599.72 (1.667 sec)\n",
            "train - step 3232: loss = 406415.22 (1.698 sec)\n",
            "train - step 3233: loss = 547185.19 (1.694 sec)\n",
            "train - step 3234: loss = 395357.00 (1.695 sec)\n",
            "train - step 3235: loss = 319546.81 (1.680 sec)\n",
            "train - step 3236: loss = 470097.97 (1.691 sec)\n",
            "train - step 3237: loss = 450583.34 (1.711 sec)\n",
            "train - step 3238: loss = 372304.12 (1.718 sec)\n",
            "train - step 3239: loss = 360241.19 (1.699 sec)\n",
            "train - step 3240: loss = 493610.28 (1.706 sec)\n",
            "train - step 3241: loss = 448177.91 (1.694 sec)\n",
            "train - step 3242: loss = 483289.47 (1.681 sec)\n",
            "train - step 3243: loss = 433203.06 (1.694 sec)\n",
            "train - step 3244: loss = 429427.47 (1.680 sec)\n",
            "train - step 3245: loss = 577066.75 (1.685 sec)\n",
            "train - step 3246: loss = 417839.62 (1.663 sec)\n",
            "train - step 3247: loss = 373456.09 (1.695 sec)\n",
            "train - step 3248: loss = 264452.72 (1.671 sec)\n",
            "train - step 3249: loss = 516442.44 (1.685 sec)\n",
            "train - step 3250: loss = 555207.94 (1.703 sec)\n",
            "train - step 3251: loss = 443874.34 (2.501 sec)\n",
            "train - step 3252: loss = 358493.22 (1.671 sec)\n",
            "train - step 3253: loss = 296927.84 (1.695 sec)\n",
            "train - step 3254: loss = 409427.38 (1.676 sec)\n",
            "train - step 3255: loss = 733981.88 (1.707 sec)\n",
            "train - step 3256: loss = 461313.28 (1.681 sec)\n",
            "train - step 3257: loss = 350577.16 (1.677 sec)\n",
            "train - step 3258: loss = 380216.44 (1.692 sec)\n",
            "train - step 3259: loss = 438114.06 (1.685 sec)\n",
            "train - step 3260: loss = 339443.34 (1.670 sec)\n",
            "train - step 3261: loss = 579047.00 (1.678 sec)\n",
            "train - step 3262: loss = 561092.69 (1.668 sec)\n",
            "train - step 3263: loss = 555507.38 (1.687 sec)\n",
            "train - step 3264: loss = 353865.47 (1.661 sec)\n",
            "train - step 3265: loss = 361595.97 (1.684 sec)\n",
            "train - step 3266: loss = 499493.41 (1.702 sec)\n",
            "train - step 3267: loss = 396896.47 (1.674 sec)\n",
            "train - step 3268: loss = 462931.62 (1.661 sec)\n",
            "train - step 3269: loss = 563405.38 (1.692 sec)\n",
            "train - step 3270: loss = 400334.09 (1.677 sec)\n",
            "train - step 3271: loss = 438979.12 (1.699 sec)\n",
            "train - step 3272: loss = 553704.44 (1.693 sec)\n",
            "train - step 3273: loss = 242852.84 (1.698 sec)\n",
            "train - step 3274: loss = 571886.31 (1.686 sec)\n",
            "train - step 3275: loss = 443720.44 (1.697 sec)\n",
            "train - step 3276: loss = 493311.34 (1.696 sec)\n",
            "train - step 3277: loss = 550252.12 (1.721 sec)\n",
            "train - step 3278: loss = 539443.06 (1.706 sec)\n",
            "train - step 3279: loss = 408727.06 (1.708 sec)\n",
            "train - step 3280: loss = 337068.62 (1.672 sec)\n",
            "train - step 3281: loss = 477378.31 (1.656 sec)\n",
            "train - step 3282: loss = 420878.03 (1.662 sec)\n",
            "train - step 3283: loss = 467366.66 (1.709 sec)\n",
            "train - step 3284: loss = 501379.81 (1.665 sec)\n",
            "train - step 3285: loss = 541083.69 (1.699 sec)\n",
            "train - step 3286: loss = 379260.53 (1.655 sec)\n",
            "train - step 3287: loss = 502006.00 (2.490 sec)\n",
            "train - step 3288: loss = 511123.03 (1.699 sec)\n",
            "train - step 3289: loss = 566539.31 (1.686 sec)\n",
            "train - step 3290: loss = 490492.06 (1.708 sec)\n",
            "train - step 3291: loss = 373190.69 (1.664 sec)\n",
            "train - step 3292: loss = 584608.81 (1.706 sec)\n",
            "train - step 3293: loss = 341148.84 (1.672 sec)\n",
            "train - step 3294: loss = 352968.72 (1.685 sec)\n",
            "train - step 3295: loss = 548164.12 (1.674 sec)\n",
            "train - step 3296: loss = 499316.00 (1.684 sec)\n",
            "train - step 3297: loss = 460081.72 (1.668 sec)\n",
            "train - step 3298: loss = 462353.66 (1.653 sec)\n",
            "train - step 3299: loss = 512722.06 (1.721 sec)\n",
            "train - step 3300: loss = 407507.88 (1.677 sec)\n",
            "train - step 3301: loss = 298199.66 (1.679 sec)\n",
            "train - step 3302: loss = 463028.03 (1.668 sec)\n",
            "train - step 3303: loss = 306530.66 (1.698 sec)\n",
            "train - step 3304: loss = 467820.06 (1.671 sec)\n",
            "train - step 3305: loss = 461260.59 (1.669 sec)\n",
            "train - step 3306: loss = 247936.17 (1.665 sec)\n",
            "train - step 3307: loss = 590533.69 (1.675 sec)\n",
            "train - step 3308: loss = 450435.19 (1.702 sec)\n",
            "train - step 3309: loss = 331796.59 (1.674 sec)\n",
            "train - step 3310: loss = 388229.88 (1.681 sec)\n",
            "train - step 3311: loss = 370644.22 (1.662 sec)\n",
            "train - step 3312: loss = 505143.34 (1.660 sec)\n",
            "train - step 3313: loss = 350623.19 (1.677 sec)\n",
            "train - step 3314: loss = 473972.72 (1.676 sec)\n",
            "train - step 3315: loss = 253657.70 (1.684 sec)\n",
            "train - step 3316: loss = 299673.59 (1.663 sec)\n",
            "train - step 3317: loss = 440998.34 (1.682 sec)\n",
            "train - step 3318: loss = 223291.59 (1.683 sec)\n",
            "train - step 3319: loss = 448568.69 (1.679 sec)\n",
            "train - step 3320: loss = 425197.09 (1.686 sec)\n",
            "train - step 3321: loss = 543319.38 (1.678 sec)\n",
            "train - step 3322: loss = 673931.31 (1.699 sec)\n",
            "train - step 3323: loss = 437661.59 (2.469 sec)\n",
            "train - step 3324: loss = 370058.12 (1.701 sec)\n",
            "train - step 3325: loss = 548109.06 (1.669 sec)\n",
            "train - step 3326: loss = 441341.88 (1.694 sec)\n",
            "train - step 3327: loss = 389296.59 (1.665 sec)\n",
            "train - step 3328: loss = 468808.47 (1.672 sec)\n",
            "train - step 3329: loss = 508128.34 (1.669 sec)\n",
            "train - step 3330: loss = 428925.53 (1.661 sec)\n",
            "train - step 3331: loss = 391448.22 (1.703 sec)\n",
            "train - step 3332: loss = 501802.50 (1.686 sec)\n",
            "train - step 3333: loss = 516224.72 (1.710 sec)\n",
            "train - step 3334: loss = 351647.94 (1.705 sec)\n",
            "train - step 3335: loss = 388268.88 (1.690 sec)\n",
            "train - step 3336: loss = 424600.84 (1.699 sec)\n",
            "train - step 3337: loss = 521385.72 (1.714 sec)\n",
            "train - step 3338: loss = 426750.38 (1.664 sec)\n",
            "train - step 3339: loss = 686592.00 (1.689 sec)\n",
            "train - step 3340: loss = 500994.66 (1.699 sec)\n",
            "train - step 3341: loss = 509394.44 (1.686 sec)\n",
            "train - step 3342: loss = 485708.97 (1.684 sec)\n",
            "train - step 3343: loss = 449126.28 (1.677 sec)\n",
            "train - step 3344: loss = 573676.31 (1.678 sec)\n",
            "train - step 3345: loss = 214377.67 (1.662 sec)\n",
            "train - step 3346: loss = 365827.44 (1.682 sec)\n",
            "train - step 3347: loss = 507201.59 (1.673 sec)\n",
            "train - step 3348: loss = 354216.28 (1.690 sec)\n",
            "train - step 3349: loss = 667269.56 (1.673 sec)\n",
            "train - step 3350: loss = 477650.59 (1.675 sec)\n",
            "train - step 3351: loss = 369798.72 (1.662 sec)\n",
            "train - step 3352: loss = 518689.31 (1.643 sec)\n",
            "train - step 3353: loss = 499855.88 (1.651 sec)\n",
            "train - step 3354: loss = 524256.97 (1.643 sec)\n",
            "train - step 3355: loss = 348758.66 (1.664 sec)\n",
            "train - step 3356: loss = 419697.81 (1.679 sec)\n",
            "train - step 3357: loss = 375329.81 (1.665 sec)\n",
            "train - step 3358: loss = 423543.09 (1.693 sec)\n",
            "train - step 3359: loss = 399314.97 (2.587 sec)\n",
            "train - step 3360: loss = 394214.59 (1.701 sec)\n",
            "train - step 3361: loss = 501150.94 (1.669 sec)\n",
            "train - step 3362: loss = 531332.44 (1.628 sec)\n",
            "train - step 3363: loss = 538236.94 (1.657 sec)\n",
            "train - step 3364: loss = 459869.41 (1.657 sec)\n",
            "train - step 3365: loss = 354919.62 (1.653 sec)\n",
            "train - step 3366: loss = 477266.88 (1.657 sec)\n",
            "train - step 3367: loss = 370010.88 (1.624 sec)\n",
            "train - step 3368: loss = 403628.97 (1.678 sec)\n",
            "train - step 3369: loss = 341151.28 (1.651 sec)\n",
            "train - step 3370: loss = 462444.97 (1.680 sec)\n",
            "train - step 3371: loss = 546997.12 (1.678 sec)\n",
            "train - step 3372: loss = 534418.50 (1.680 sec)\n",
            "train - step 3373: loss = 438262.91 (1.692 sec)\n",
            "train - step 3374: loss = 464991.44 (1.685 sec)\n",
            "train - step 3375: loss = 447854.03 (1.674 sec)\n",
            "train - step 3376: loss = 328311.62 (1.686 sec)\n",
            "train - step 3377: loss = 342618.16 (1.660 sec)\n",
            "train - step 3378: loss = 539461.81 (1.671 sec)\n",
            "train - step 3379: loss = 520629.09 (1.688 sec)\n",
            "train - step 3380: loss = 343645.91 (1.679 sec)\n",
            "train - step 3381: loss = 351114.19 (1.703 sec)\n",
            "train - step 3382: loss = 431998.19 (1.704 sec)\n",
            "train - step 3383: loss = 474530.72 (1.660 sec)\n",
            "train - step 3384: loss = 556439.19 (1.678 sec)\n",
            "train - step 3385: loss = 401620.34 (1.662 sec)\n",
            "train - step 3386: loss = 487150.59 (1.682 sec)\n",
            "train - step 3387: loss = 411737.44 (1.669 sec)\n",
            "train - step 3388: loss = 537485.62 (1.678 sec)\n",
            "train - step 3389: loss = 427218.81 (1.690 sec)\n",
            "train - step 3390: loss = 513100.09 (1.694 sec)\n",
            "train - step 3391: loss = 380316.47 (1.686 sec)\n",
            "train - step 3392: loss = 422805.22 (1.668 sec)\n",
            "train - step 3393: loss = 482418.00 (1.696 sec)\n",
            "train - step 3394: loss = 618122.38 (1.666 sec)\n",
            "train - step 3395: loss = 422484.41 (2.518 sec)\n",
            "train - step 3396: loss = 298613.94 (1.687 sec)\n",
            "train - step 3397: loss = 455280.38 (1.663 sec)\n",
            "train - step 3398: loss = 419870.16 (1.685 sec)\n",
            "train - step 3399: loss = 513676.38 (1.662 sec)\n",
            "train - step 3400: loss = 356855.72 (1.677 sec)\n",
            "train - step 3401: loss = 465457.19 (1.670 sec)\n",
            "train - step 3402: loss = 492696.69 (1.673 sec)\n",
            "train - step 3403: loss = 465062.81 (1.705 sec)\n",
            "train - step 3404: loss = 400645.09 (1.664 sec)\n",
            "train - step 3405: loss = 341558.78 (1.710 sec)\n",
            "train - step 3406: loss = 456107.78 (1.676 sec)\n",
            "train - step 3407: loss = 388950.28 (1.664 sec)\n",
            "train - step 3408: loss = 292911.06 (1.658 sec)\n",
            "train - step 3409: loss = 442507.47 (1.670 sec)\n",
            "train - step 3410: loss = 525676.19 (1.659 sec)\n",
            "train - step 3411: loss = 552365.75 (1.680 sec)\n",
            "train - step 3412: loss = 384416.41 (1.669 sec)\n",
            "train - step 3413: loss = 453468.62 (1.662 sec)\n",
            "train - step 3414: loss = 546138.31 (1.691 sec)\n",
            "train - step 3415: loss = 418247.72 (1.666 sec)\n",
            "train - step 3416: loss = 317811.00 (1.721 sec)\n",
            "train - step 3417: loss = 455822.53 (1.659 sec)\n",
            "train - step 3418: loss = 452396.62 (1.659 sec)\n",
            "train - step 3419: loss = 441964.66 (1.670 sec)\n",
            "train - step 3420: loss = 429467.31 (1.646 sec)\n",
            "train - step 3421: loss = 547357.62 (1.695 sec)\n",
            "train - step 3422: loss = 577701.00 (1.664 sec)\n",
            "train - step 3423: loss = 471357.16 (1.663 sec)\n",
            "train - step 3424: loss = 315829.41 (1.654 sec)\n",
            "train - step 3425: loss = 404588.72 (1.674 sec)\n",
            "train - step 3426: loss = 494663.62 (1.669 sec)\n",
            "train - step 3427: loss = 474212.72 (1.656 sec)\n",
            "train - step 3428: loss = 414660.41 (1.681 sec)\n",
            "train - step 3429: loss = 469681.47 (1.691 sec)\n",
            "train - step 3430: loss = 552968.56 (1.702 sec)\n",
            "train - step 3431: loss = 316590.47 (2.512 sec)\n",
            "train - step 3432: loss = 462358.47 (1.692 sec)\n",
            "train - step 3433: loss = 497623.53 (1.680 sec)\n",
            "train - step 3434: loss = 386568.41 (1.678 sec)\n",
            "train - step 3435: loss = 503015.03 (1.708 sec)\n",
            "train - step 3436: loss = 300790.16 (1.653 sec)\n",
            "train - step 3437: loss = 466921.12 (1.698 sec)\n",
            "train - step 3438: loss = 334241.53 (1.689 sec)\n",
            "train - step 3439: loss = 512340.31 (1.663 sec)\n",
            "train - step 3440: loss = 301372.66 (1.664 sec)\n",
            "train - step 3441: loss = 299223.97 (1.661 sec)\n",
            "train - step 3442: loss = 377276.84 (1.655 sec)\n",
            "train - step 3443: loss = 525856.25 (1.662 sec)\n",
            "train - step 3444: loss = 555515.75 (1.679 sec)\n",
            "train - step 3445: loss = 434072.62 (1.673 sec)\n",
            "train - step 3446: loss = 532816.94 (1.675 sec)\n",
            "train - step 3447: loss = 417184.72 (1.655 sec)\n",
            "train - step 3448: loss = 245043.53 (1.665 sec)\n",
            "train - step 3449: loss = 393129.56 (1.679 sec)\n",
            "train - step 3450: loss = 500326.34 (1.685 sec)\n",
            "train - step 3451: loss = 407444.59 (1.670 sec)\n",
            "train - step 3452: loss = 361250.56 (1.678 sec)\n",
            "train - step 3453: loss = 567627.31 (1.701 sec)\n",
            "train - step 3454: loss = 431229.84 (1.665 sec)\n",
            "train - step 3455: loss = 441458.47 (1.672 sec)\n",
            "train - step 3456: loss = 497269.69 (1.667 sec)\n",
            "train - step 3457: loss = 432038.31 (1.688 sec)\n",
            "train - step 3458: loss = 365764.78 (1.656 sec)\n",
            "train - step 3459: loss = 530964.88 (1.688 sec)\n",
            "train - step 3460: loss = 444898.78 (1.681 sec)\n",
            "train - step 3461: loss = 544283.06 (1.668 sec)\n",
            "train - step 3462: loss = 411249.06 (1.665 sec)\n",
            "train - step 3463: loss = 524180.22 (1.656 sec)\n",
            "train - step 3464: loss = 447153.19 (1.683 sec)\n",
            "train - step 3465: loss = 419984.91 (1.656 sec)\n",
            "train - step 3466: loss = 348799.22 (1.701 sec)\n",
            "train - step 3467: loss = 439032.59 (2.743 sec)\n",
            "train - step 3468: loss = 477010.69 (1.743 sec)\n",
            "train - step 3469: loss = 401298.81 (1.696 sec)\n",
            "train - step 3470: loss = 548868.31 (1.665 sec)\n",
            "train - step 3471: loss = 374172.62 (1.710 sec)\n",
            "train - step 3472: loss = 537806.56 (1.701 sec)\n",
            "train - step 3473: loss = 317233.91 (1.692 sec)\n",
            "train - step 3474: loss = 408946.09 (1.682 sec)\n",
            "train - step 3475: loss = 308237.88 (1.683 sec)\n",
            "train - step 3476: loss = 437742.28 (1.662 sec)\n",
            "train - step 3477: loss = 342959.19 (1.717 sec)\n",
            "train - step 3478: loss = 347667.53 (1.702 sec)\n",
            "train - step 3479: loss = 530291.94 (1.674 sec)\n",
            "train - step 3480: loss = 457798.78 (1.687 sec)\n",
            "train - step 3481: loss = 471349.28 (1.805 sec)\n",
            "train - step 3482: loss = 454560.31 (1.653 sec)\n",
            "train - step 3483: loss = 483593.44 (1.688 sec)\n",
            "train - step 3484: loss = 597659.75 (1.672 sec)\n",
            "train - step 3485: loss = 334395.50 (1.665 sec)\n",
            "train - step 3486: loss = 406619.12 (1.687 sec)\n",
            "train - step 3487: loss = 559704.25 (1.686 sec)\n",
            "train - step 3488: loss = 527609.19 (1.690 sec)\n",
            "train - step 3489: loss = 556694.94 (1.686 sec)\n",
            "train - step 3490: loss = 609439.00 (1.665 sec)\n",
            "train - step 3491: loss = 338276.34 (1.684 sec)\n",
            "train - step 3492: loss = 414954.12 (1.682 sec)\n",
            "train - step 3493: loss = 537280.88 (1.687 sec)\n",
            "train - step 3494: loss = 365859.59 (1.668 sec)\n",
            "train - step 3495: loss = 472998.88 (1.701 sec)\n",
            "train - step 3496: loss = 370306.84 (1.658 sec)\n",
            "train - step 3497: loss = 511427.47 (1.712 sec)\n",
            "train - step 3498: loss = 332249.09 (1.667 sec)\n",
            "train - step 3499: loss = 506468.44 (1.690 sec)\n",
            "train - step 3500: loss = 535444.12 (1.662 sec)\n",
            "train - step 3501: loss = 340014.88 (1.649 sec)\n",
            "train - step 3502: loss = 365991.31 (1.673 sec)\n",
            "train - step 3503: loss = 511652.28 (2.544 sec)\n",
            "train - step 3504: loss = 455485.00 (1.693 sec)\n",
            "train - step 3505: loss = 449115.03 (1.677 sec)\n",
            "train - step 3506: loss = 482869.28 (1.679 sec)\n",
            "train - step 3507: loss = 520809.66 (1.699 sec)\n",
            "train - step 3508: loss = 411396.81 (1.675 sec)\n",
            "train - step 3509: loss = 457725.47 (1.718 sec)\n",
            "train - step 3510: loss = 313255.16 (1.690 sec)\n",
            "train - step 3511: loss = 318872.09 (1.692 sec)\n",
            "train - step 3512: loss = 532907.69 (1.687 sec)\n",
            "train - step 3513: loss = 404619.94 (1.666 sec)\n",
            "train - step 3514: loss = 439329.12 (1.674 sec)\n",
            "train - step 3515: loss = 531493.69 (1.653 sec)\n",
            "train - step 3516: loss = 452011.50 (1.696 sec)\n",
            "train - step 3517: loss = 414555.53 (1.662 sec)\n",
            "train - step 3518: loss = 439301.34 (1.716 sec)\n",
            "train - step 3519: loss = 494314.84 (1.675 sec)\n",
            "train - step 3520: loss = 530442.19 (1.684 sec)\n",
            "train - step 3521: loss = 487682.44 (1.669 sec)\n",
            "train - step 3522: loss = 395542.88 (1.659 sec)\n",
            "train - step 3523: loss = 354442.47 (1.664 sec)\n",
            "train - step 3524: loss = 316311.50 (1.667 sec)\n",
            "train - step 3525: loss = 369150.66 (1.702 sec)\n",
            "train - step 3526: loss = 345896.59 (1.688 sec)\n",
            "train - step 3527: loss = 517296.34 (1.670 sec)\n",
            "train - step 3528: loss = 342097.22 (1.676 sec)\n",
            "train - step 3529: loss = 403984.53 (1.675 sec)\n",
            "train - step 3530: loss = 497543.28 (1.677 sec)\n",
            "train - step 3531: loss = 456362.06 (1.663 sec)\n",
            "train - step 3532: loss = 444116.06 (1.643 sec)\n",
            "train - step 3533: loss = 443488.97 (1.650 sec)\n",
            "train - step 3534: loss = 415867.88 (1.701 sec)\n",
            "train - step 3535: loss = 442278.00 (1.707 sec)\n",
            "train - step 3536: loss = 514813.59 (1.680 sec)\n",
            "train - step 3537: loss = 415283.97 (1.712 sec)\n",
            "train - step 3538: loss = 277622.47 (1.704 sec)\n",
            "train - step 3539: loss = 311239.28 (2.552 sec)\n",
            "train - step 3540: loss = 358198.59 (1.676 sec)\n",
            "train - step 3541: loss = 510592.56 (1.687 sec)\n",
            "train - step 3542: loss = 414540.41 (1.699 sec)\n",
            "train - step 3543: loss = 572626.12 (1.680 sec)\n",
            "train - step 3544: loss = 421240.69 (1.633 sec)\n",
            "train - step 3545: loss = 410484.97 (1.640 sec)\n",
            "train - step 3546: loss = 577877.56 (1.638 sec)\n",
            "train - step 3547: loss = 372150.31 (1.666 sec)\n",
            "train - step 3548: loss = 501431.19 (1.655 sec)\n",
            "train - step 3549: loss = 317893.81 (1.669 sec)\n",
            "train - step 3550: loss = 546044.00 (1.682 sec)\n",
            "train - step 3551: loss = 293654.38 (1.669 sec)\n",
            "train - step 3552: loss = 400866.16 (1.683 sec)\n",
            "train - step 3553: loss = 382233.06 (1.690 sec)\n",
            "train - step 3554: loss = 357281.22 (1.686 sec)\n",
            "train - step 3555: loss = 305416.78 (1.679 sec)\n",
            "train - step 3556: loss = 332113.12 (1.685 sec)\n",
            "train - step 3557: loss = 508384.59 (1.691 sec)\n",
            "train - step 3558: loss = 515492.31 (1.689 sec)\n",
            "train - step 3559: loss = 384484.06 (1.661 sec)\n",
            "train - step 3560: loss = 474007.59 (1.707 sec)\n",
            "train - step 3561: loss = 510136.47 (1.677 sec)\n",
            "train - step 3562: loss = 635118.38 (1.657 sec)\n",
            "train - step 3563: loss = 338303.19 (1.668 sec)\n",
            "train - step 3564: loss = 596079.75 (1.669 sec)\n",
            "train - step 3565: loss = 576504.56 (1.677 sec)\n",
            "train - step 3566: loss = 618641.81 (1.655 sec)\n",
            "train - step 3567: loss = 527254.62 (1.684 sec)\n",
            "train - step 3568: loss = 417272.16 (1.677 sec)\n",
            "train - step 3569: loss = 404336.84 (1.683 sec)\n",
            "train - step 3570: loss = 420972.41 (1.681 sec)\n",
            "train - step 3571: loss = 335244.47 (1.655 sec)\n",
            "train - step 3572: loss = 540238.56 (1.671 sec)\n",
            "train - step 3573: loss = 403533.31 (1.649 sec)\n",
            "train - step 3574: loss = 435554.28 (1.675 sec)\n",
            "train - step 3575: loss = 395874.22 (2.806 sec)\n",
            "train - step 3576: loss = 403009.66 (1.687 sec)\n",
            "train - step 3577: loss = 384294.72 (1.657 sec)\n",
            "train - step 3578: loss = 518208.19 (1.696 sec)\n",
            "train - step 3579: loss = 314153.69 (1.676 sec)\n",
            "train - step 3580: loss = 370442.03 (1.670 sec)\n",
            "train - step 3581: loss = 329709.84 (1.686 sec)\n",
            "train - step 3582: loss = 378750.22 (1.660 sec)\n",
            "train - step 3583: loss = 412977.59 (1.678 sec)\n",
            "train - step 3584: loss = 416377.62 (1.699 sec)\n",
            "train - step 3585: loss = 529743.25 (1.694 sec)\n",
            "train - step 3586: loss = 348314.84 (1.667 sec)\n",
            "train - step 3587: loss = 402433.53 (1.674 sec)\n",
            "train - step 3588: loss = 431924.62 (1.681 sec)\n",
            "train - step 3589: loss = 523018.22 (1.700 sec)\n",
            "train - step 3590: loss = 399774.31 (1.667 sec)\n",
            "train - step 3591: loss = 505581.19 (1.678 sec)\n",
            "train - step 3592: loss = 520957.28 (1.684 sec)\n",
            "train - step 3593: loss = 397549.09 (1.673 sec)\n",
            "train - step 3594: loss = 609949.81 (1.698 sec)\n",
            "train - step 3595: loss = 318023.94 (1.687 sec)\n",
            "train - step 3596: loss = 490126.00 (1.672 sec)\n",
            "train - step 3597: loss = 544763.75 (1.676 sec)\n",
            "train - step 3598: loss = 349139.81 (1.661 sec)\n",
            "train - step 3599: loss = 457668.50 (1.662 sec)\n",
            "train - step 3600: loss = 415234.06 (1.682 sec)\n",
            "train - step 3601: loss = 434812.06 (1.708 sec)\n",
            "train - step 3602: loss = 521883.69 (1.699 sec)\n",
            "train - step 3603: loss = 449569.66 (1.658 sec)\n",
            "train - step 3604: loss = 393269.09 (1.678 sec)\n",
            "train - step 3605: loss = 455935.34 (1.715 sec)\n",
            "train - step 3606: loss = 382367.31 (1.656 sec)\n",
            "train - step 3607: loss = 453181.53 (1.685 sec)\n",
            "train - step 3608: loss = 425191.06 (1.664 sec)\n",
            "train - step 3609: loss = 494476.41 (1.672 sec)\n",
            "train - step 3610: loss = 628197.56 (1.713 sec)\n",
            "train - step 3611: loss = 444599.09 (2.466 sec)\n",
            "train - step 3612: loss = 516650.72 (1.690 sec)\n",
            "train - step 3613: loss = 385678.53 (1.681 sec)\n",
            "train - step 3614: loss = 433724.94 (1.666 sec)\n",
            "train - step 3615: loss = 299495.03 (1.672 sec)\n",
            "train - step 3616: loss = 465825.34 (1.672 sec)\n",
            "train - step 3617: loss = 489762.00 (1.670 sec)\n",
            "train - step 3618: loss = 390991.62 (1.674 sec)\n",
            "train - step 3619: loss = 472273.09 (1.671 sec)\n",
            "train - step 3620: loss = 466771.53 (1.668 sec)\n",
            "train - step 3621: loss = 406357.12 (1.684 sec)\n",
            "train - step 3622: loss = 434886.69 (1.675 sec)\n",
            "train - step 3623: loss = 519152.47 (1.663 sec)\n",
            "train - step 3624: loss = 377592.53 (1.689 sec)\n",
            "train - step 3625: loss = 368181.66 (1.686 sec)\n",
            "train - step 3626: loss = 363153.47 (1.703 sec)\n",
            "train - step 3627: loss = 382069.78 (1.665 sec)\n",
            "train - step 3628: loss = 392832.19 (1.687 sec)\n",
            "train - step 3629: loss = 446035.22 (1.684 sec)\n",
            "train - step 3630: loss = 439658.59 (1.688 sec)\n",
            "train - step 3631: loss = 452898.34 (1.683 sec)\n",
            "train - step 3632: loss = 514605.91 (1.652 sec)\n",
            "train - step 3633: loss = 460605.28 (1.685 sec)\n",
            "train - step 3634: loss = 288338.22 (1.672 sec)\n",
            "train - step 3635: loss = 511997.66 (1.658 sec)\n",
            "train - step 3636: loss = 410672.94 (1.661 sec)\n",
            "train - step 3637: loss = 355241.50 (1.651 sec)\n",
            "train - step 3638: loss = 454718.34 (1.650 sec)\n",
            "train - step 3639: loss = 581161.00 (1.654 sec)\n",
            "train - step 3640: loss = 369037.50 (1.663 sec)\n",
            "train - step 3641: loss = 460764.72 (1.669 sec)\n",
            "train - step 3642: loss = 380337.12 (1.633 sec)\n",
            "train - step 3643: loss = 339345.00 (1.641 sec)\n",
            "train - step 3644: loss = 376650.56 (1.657 sec)\n",
            "train - step 3645: loss = 465941.09 (1.646 sec)\n",
            "train - step 3646: loss = 360254.28 (1.651 sec)\n",
            "train - step 3647: loss = 473531.50 (2.816 sec)\n",
            "train - step 3648: loss = 444897.28 (1.669 sec)\n",
            "train - step 3649: loss = 467094.62 (1.658 sec)\n",
            "train - step 3650: loss = 369329.84 (1.657 sec)\n",
            "train - step 3651: loss = 219839.80 (1.641 sec)\n",
            "train - step 3652: loss = 358987.72 (1.671 sec)\n",
            "train - step 3653: loss = 330924.09 (1.659 sec)\n",
            "train - step 3654: loss = 478052.00 (1.653 sec)\n",
            "train - step 3655: loss = 338370.03 (1.679 sec)\n",
            "train - step 3656: loss = 448663.81 (1.652 sec)\n",
            "train - step 3657: loss = 339553.84 (1.675 sec)\n",
            "train - step 3658: loss = 492282.88 (1.656 sec)\n",
            "train - step 3659: loss = 411114.81 (1.673 sec)\n",
            "train - step 3660: loss = 303181.09 (1.653 sec)\n",
            "train - step 3661: loss = 443391.81 (1.665 sec)\n",
            "train - step 3662: loss = 371273.59 (1.655 sec)\n",
            "train - step 3663: loss = 456069.16 (1.665 sec)\n",
            "train - step 3664: loss = 596013.88 (1.655 sec)\n",
            "train - step 3665: loss = 399368.69 (1.677 sec)\n",
            "train - step 3666: loss = 366863.78 (1.655 sec)\n",
            "train - step 3667: loss = 327831.50 (1.682 sec)\n",
            "train - step 3668: loss = 491061.41 (1.659 sec)\n",
            "train - step 3669: loss = 470562.69 (1.643 sec)\n",
            "train - step 3670: loss = 531803.88 (1.673 sec)\n",
            "train - step 3671: loss = 399637.03 (1.676 sec)\n",
            "train - step 3672: loss = 579350.56 (1.648 sec)\n",
            "train - step 3673: loss = 356519.88 (1.675 sec)\n",
            "train - step 3674: loss = 484042.00 (1.662 sec)\n",
            "train - step 3675: loss = 426146.81 (1.660 sec)\n",
            "train - step 3676: loss = 625807.88 (1.676 sec)\n",
            "train - step 3677: loss = 317062.22 (1.648 sec)\n",
            "train - step 3678: loss = 331832.56 (1.663 sec)\n",
            "train - step 3679: loss = 351947.06 (1.664 sec)\n",
            "train - step 3680: loss = 320058.88 (1.657 sec)\n",
            "train - step 3681: loss = 350019.22 (1.685 sec)\n",
            "train - step 3682: loss = 356918.28 (1.654 sec)\n",
            "train - step 3683: loss = 423185.22 (2.516 sec)\n",
            "train - step 3684: loss = 467813.91 (1.657 sec)\n",
            "train - step 3685: loss = 512711.81 (1.698 sec)\n",
            "train - step 3686: loss = 443308.03 (1.664 sec)\n",
            "train - step 3687: loss = 416408.88 (1.666 sec)\n",
            "train - step 3688: loss = 506916.31 (1.681 sec)\n",
            "train - step 3689: loss = 394973.97 (1.671 sec)\n",
            "train - step 3690: loss = 390430.22 (1.661 sec)\n",
            "train - step 3691: loss = 502657.41 (1.654 sec)\n",
            "train - step 3692: loss = 624330.81 (1.645 sec)\n",
            "train - step 3693: loss = 182333.61 (1.680 sec)\n",
            "train - step 3694: loss = 372543.12 (1.647 sec)\n",
            "train - step 3695: loss = 385035.47 (1.664 sec)\n",
            "train - step 3696: loss = 355483.81 (1.671 sec)\n",
            "train - step 3697: loss = 395949.88 (1.661 sec)\n",
            "train - step 3698: loss = 444163.00 (1.669 sec)\n",
            "train - step 3699: loss = 431720.09 (1.672 sec)\n",
            "train - step 3700: loss = 251777.94 (1.666 sec)\n",
            "train - step 3701: loss = 404789.91 (1.661 sec)\n",
            "train - step 3702: loss = 357312.12 (1.653 sec)\n",
            "train - step 3703: loss = 508726.47 (1.654 sec)\n",
            "train - step 3704: loss = 514971.22 (1.660 sec)\n",
            "train - step 3705: loss = 346867.16 (1.681 sec)\n",
            "train - step 3706: loss = 538458.62 (1.673 sec)\n",
            "train - step 3707: loss = 440180.41 (1.668 sec)\n",
            "train - step 3708: loss = 457933.22 (1.678 sec)\n",
            "train - step 3709: loss = 515771.59 (1.676 sec)\n",
            "train - step 3710: loss = 440093.66 (1.685 sec)\n",
            "train - step 3711: loss = 333580.84 (1.693 sec)\n",
            "train - step 3712: loss = 590164.31 (1.630 sec)\n",
            "train - step 3713: loss = 559737.75 (1.642 sec)\n",
            "train - step 3714: loss = 354031.47 (1.638 sec)\n",
            "train - step 3715: loss = 371514.06 (1.691 sec)\n",
            "train - step 3716: loss = 466561.19 (1.651 sec)\n",
            "train - step 3717: loss = 367743.00 (1.668 sec)\n",
            "train - step 3718: loss = 460612.47 (1.662 sec)\n",
            "train - step 3719: loss = 450410.97 (2.566 sec)\n",
            "train - step 3720: loss = 350696.56 (1.675 sec)\n",
            "train - step 3721: loss = 442775.06 (1.680 sec)\n",
            "train - step 3722: loss = 541441.75 (1.691 sec)\n",
            "train - step 3723: loss = 283911.16 (1.694 sec)\n",
            "train - step 3724: loss = 512926.66 (1.654 sec)\n",
            "train - step 3725: loss = 451855.72 (1.668 sec)\n",
            "train - step 3726: loss = 368172.38 (1.662 sec)\n",
            "train - step 3727: loss = 523786.41 (1.651 sec)\n",
            "train - step 3728: loss = 385177.12 (1.651 sec)\n",
            "train - step 3729: loss = 428399.44 (1.646 sec)\n",
            "train - step 3730: loss = 413922.88 (1.656 sec)\n",
            "train - step 3731: loss = 390998.78 (1.664 sec)\n",
            "train - step 3732: loss = 334768.88 (1.652 sec)\n",
            "train - step 3733: loss = 437421.28 (1.683 sec)\n",
            "train - step 3734: loss = 437472.47 (1.685 sec)\n",
            "train - step 3735: loss = 301748.06 (1.696 sec)\n",
            "train - step 3736: loss = 481871.28 (1.675 sec)\n",
            "train - step 3737: loss = 590825.69 (1.669 sec)\n",
            "train - step 3738: loss = 471310.50 (1.677 sec)\n",
            "train - step 3739: loss = 457858.59 (1.672 sec)\n",
            "train - step 3740: loss = 393677.94 (1.680 sec)\n",
            "train - step 3741: loss = 483336.47 (1.653 sec)\n",
            "train - step 3742: loss = 542888.00 (1.638 sec)\n",
            "train - step 3743: loss = 481701.44 (1.676 sec)\n",
            "train - step 3744: loss = 427567.72 (1.672 sec)\n",
            "train - step 3745: loss = 359128.16 (1.658 sec)\n",
            "train - step 3746: loss = 420799.31 (1.680 sec)\n",
            "train - step 3747: loss = 534253.38 (1.643 sec)\n",
            "train - step 3748: loss = 344426.22 (1.670 sec)\n",
            "train - step 3749: loss = 374800.84 (1.666 sec)\n",
            "train - step 3750: loss = 459077.97 (1.670 sec)\n",
            "train - step 3751: loss = 386795.12 (1.663 sec)\n",
            "train - step 3752: loss = 445219.56 (1.654 sec)\n",
            "train - step 3753: loss = 401626.19 (1.670 sec)\n",
            "train - step 3754: loss = 623245.00 (1.658 sec)\n",
            "train - step 3755: loss = 366088.59 (2.578 sec)\n",
            "train - step 3756: loss = 320336.59 (1.687 sec)\n",
            "train - step 3757: loss = 386121.59 (1.647 sec)\n",
            "train - step 3758: loss = 359482.12 (1.685 sec)\n",
            "train - step 3759: loss = 500349.31 (1.659 sec)\n",
            "train - step 3760: loss = 515144.28 (1.702 sec)\n",
            "train - step 3761: loss = 414612.88 (1.674 sec)\n",
            "train - step 3762: loss = 497081.50 (1.706 sec)\n",
            "train - step 3763: loss = 443655.34 (1.668 sec)\n",
            "train - step 3764: loss = 639503.06 (1.684 sec)\n",
            "train - step 3765: loss = 511373.28 (1.703 sec)\n",
            "train - step 3766: loss = 521073.34 (1.663 sec)\n",
            "train - step 3767: loss = 441974.72 (1.685 sec)\n",
            "train - step 3768: loss = 501607.66 (1.651 sec)\n",
            "train - step 3769: loss = 342763.44 (1.669 sec)\n",
            "train - step 3770: loss = 454116.50 (1.687 sec)\n",
            "train - step 3771: loss = 542974.81 (1.693 sec)\n",
            "train - step 3772: loss = 393522.09 (2.044 sec)\n",
            "train - step 3773: loss = 530944.44 (1.681 sec)\n",
            "train - step 3774: loss = 434044.41 (1.684 sec)\n",
            "train - step 3775: loss = 345394.94 (1.777 sec)\n",
            "train - step 3776: loss = 490079.88 (1.666 sec)\n",
            "train - step 3777: loss = 354201.41 (1.660 sec)\n",
            "train - step 3778: loss = 345397.41 (1.645 sec)\n",
            "train - step 3779: loss = 427560.66 (1.673 sec)\n",
            "train - step 3780: loss = 351129.03 (1.691 sec)\n",
            "train - step 3781: loss = 511932.69 (1.688 sec)\n",
            "train - step 3782: loss = 429230.84 (1.680 sec)\n",
            "train - step 3783: loss = 389698.22 (1.663 sec)\n",
            "train - step 3784: loss = 462686.09 (1.705 sec)\n",
            "train - step 3785: loss = 437831.22 (1.692 sec)\n",
            "train - step 3786: loss = 329247.16 (1.660 sec)\n",
            "train - step 3787: loss = 468419.12 (1.703 sec)\n",
            "train - step 3788: loss = 380832.84 (1.682 sec)\n",
            "train - step 3789: loss = 276367.38 (1.668 sec)\n",
            "train - step 3790: loss = 473880.34 (1.662 sec)\n",
            "train - step 3791: loss = 479622.31 (2.499 sec)\n",
            "train - step 3792: loss = 235684.61 (1.675 sec)\n",
            "train - step 3793: loss = 442626.81 (1.693 sec)\n",
            "train - step 3794: loss = 382782.50 (1.669 sec)\n",
            "train - step 3795: loss = 360491.06 (1.680 sec)\n",
            "train - step 3796: loss = 438540.72 (1.667 sec)\n",
            "train - step 3797: loss = 621818.75 (1.671 sec)\n",
            "train - step 3798: loss = 471217.69 (1.668 sec)\n",
            "train - step 3799: loss = 312379.00 (1.666 sec)\n",
            "train - step 3800: loss = 761441.44 (1.702 sec)\n",
            "train - step 3801: loss = 420453.41 (1.680 sec)\n",
            "train - step 3802: loss = 450128.88 (1.696 sec)\n",
            "train - step 3803: loss = 438105.59 (1.679 sec)\n",
            "train - step 3804: loss = 342599.03 (1.675 sec)\n",
            "train - step 3805: loss = 474115.28 (1.669 sec)\n",
            "train - step 3806: loss = 432959.59 (1.684 sec)\n",
            "train - step 3807: loss = 449698.34 (1.688 sec)\n",
            "train - step 3808: loss = 351427.72 (1.679 sec)\n",
            "train - step 3809: loss = 587068.62 (1.680 sec)\n",
            "train - step 3810: loss = 489395.88 (1.679 sec)\n",
            "train - step 3811: loss = 297339.22 (1.701 sec)\n",
            "train - step 3812: loss = 281868.94 (1.680 sec)\n",
            "train - step 3813: loss = 505537.53 (1.685 sec)\n",
            "train - step 3814: loss = 476628.88 (1.685 sec)\n",
            "train - step 3815: loss = 398510.84 (1.681 sec)\n",
            "train - step 3816: loss = 469850.47 (1.693 sec)\n",
            "train - step 3817: loss = 606562.69 (1.691 sec)\n",
            "train - step 3818: loss = 372970.72 (1.709 sec)\n",
            "train - step 3819: loss = 450556.72 (1.699 sec)\n",
            "train - step 3820: loss = 312544.03 (1.672 sec)\n",
            "train - step 3821: loss = 451942.28 (1.686 sec)\n",
            "train - step 3822: loss = 537111.81 (1.684 sec)\n",
            "train - step 3823: loss = 642705.38 (1.685 sec)\n",
            "train - step 3824: loss = 404926.97 (1.670 sec)\n",
            "train - step 3825: loss = 326541.53 (1.679 sec)\n",
            "train - step 3826: loss = 479548.47 (1.679 sec)\n",
            "train - step 3827: loss = 431736.28 (2.511 sec)\n",
            "train - step 3828: loss = 286801.03 (1.674 sec)\n",
            "train - step 3829: loss = 522915.59 (1.672 sec)\n",
            "train - step 3830: loss = 371429.19 (1.668 sec)\n",
            "train - step 3831: loss = 541279.81 (1.649 sec)\n",
            "train - step 3832: loss = 445678.03 (1.666 sec)\n",
            "train - step 3833: loss = 336928.88 (1.687 sec)\n",
            "train - step 3834: loss = 430453.56 (1.663 sec)\n",
            "train - step 3835: loss = 611235.44 (1.675 sec)\n",
            "train - step 3836: loss = 296130.38 (1.666 sec)\n",
            "train - step 3837: loss = 380884.19 (1.670 sec)\n",
            "train - step 3838: loss = 603926.19 (1.677 sec)\n",
            "train - step 3839: loss = 508285.66 (1.664 sec)\n",
            "train - step 3840: loss = 354843.28 (1.671 sec)\n",
            "train - step 3841: loss = 266305.00 (1.650 sec)\n",
            "train - step 3842: loss = 395749.44 (1.657 sec)\n",
            "train - step 3843: loss = 548094.62 (1.684 sec)\n",
            "train - step 3844: loss = 387530.34 (1.644 sec)\n",
            "train - step 3845: loss = 304375.44 (1.664 sec)\n",
            "train - step 3846: loss = 402187.19 (1.644 sec)\n",
            "train - step 3847: loss = 240652.11 (1.660 sec)\n",
            "train - step 3848: loss = 365527.94 (1.653 sec)\n",
            "train - step 3849: loss = 371055.78 (1.671 sec)\n",
            "train - step 3850: loss = 394345.53 (1.656 sec)\n",
            "train - step 3851: loss = 523510.59 (1.676 sec)\n",
            "train - step 3852: loss = 359084.03 (1.662 sec)\n",
            "train - step 3853: loss = 363489.69 (1.702 sec)\n",
            "train - step 3854: loss = 322138.28 (1.643 sec)\n",
            "train - step 3855: loss = 506580.66 (1.656 sec)\n",
            "train - step 3856: loss = 284890.28 (1.664 sec)\n",
            "train - step 3857: loss = 471881.12 (1.642 sec)\n",
            "train - step 3858: loss = 566811.12 (1.696 sec)\n",
            "train - step 3859: loss = 375411.81 (1.661 sec)\n",
            "train - step 3860: loss = 466555.62 (1.646 sec)\n",
            "train - step 3861: loss = 435551.97 (1.669 sec)\n",
            "train - step 3862: loss = 396413.34 (1.648 sec)\n",
            "train - step 3863: loss = 372289.56 (2.512 sec)\n",
            "train - step 3864: loss = 469911.28 (1.652 sec)\n",
            "train - step 3865: loss = 339578.94 (1.647 sec)\n",
            "train - step 3866: loss = 358687.00 (1.673 sec)\n",
            "train - step 3867: loss = 401059.88 (1.659 sec)\n",
            "train - step 3868: loss = 384231.19 (1.655 sec)\n",
            "train - step 3869: loss = 450586.72 (1.677 sec)\n",
            "train - step 3870: loss = 574284.75 (1.659 sec)\n",
            "train - step 3871: loss = 421818.19 (1.672 sec)\n",
            "train - step 3872: loss = 455754.28 (1.680 sec)\n",
            "train - step 3873: loss = 291073.56 (1.659 sec)\n",
            "train - step 3874: loss = 410440.41 (1.672 sec)\n",
            "train - step 3875: loss = 347837.09 (1.649 sec)\n",
            "train - step 3876: loss = 375468.88 (1.664 sec)\n",
            "train - step 3877: loss = 532102.31 (1.694 sec)\n",
            "train - step 3878: loss = 455018.50 (1.657 sec)\n",
            "train - step 3879: loss = 522579.94 (1.677 sec)\n",
            "train - step 3880: loss = 455818.84 (1.640 sec)\n",
            "train - step 3881: loss = 333528.12 (1.672 sec)\n",
            "train - step 3882: loss = 414477.91 (1.644 sec)\n",
            "train - step 3883: loss = 433174.41 (1.665 sec)\n",
            "train - step 3884: loss = 490796.41 (1.667 sec)\n",
            "train - step 3885: loss = 508582.06 (1.656 sec)\n",
            "train - step 3886: loss = 447603.47 (1.657 sec)\n",
            "train - step 3887: loss = 245462.58 (1.677 sec)\n",
            "train - step 3888: loss = 531522.56 (1.687 sec)\n",
            "train - step 3889: loss = 479522.66 (1.673 sec)\n",
            "train - step 3890: loss = 556031.00 (1.671 sec)\n",
            "train - step 3891: loss = 463161.66 (1.663 sec)\n",
            "train - step 3892: loss = 456561.66 (1.656 sec)\n",
            "train - step 3893: loss = 400416.72 (1.624 sec)\n",
            "train - step 3894: loss = 493332.81 (1.646 sec)\n",
            "train - step 3895: loss = 376417.69 (1.672 sec)\n",
            "train - step 3896: loss = 382742.09 (1.656 sec)\n",
            "train - step 3897: loss = 392748.59 (1.653 sec)\n",
            "train - step 3898: loss = 503012.66 (1.667 sec)\n",
            "train - step 3899: loss = 416356.44 (2.650 sec)\n",
            "train - step 3900: loss = 557460.62 (1.670 sec)\n",
            "train - step 3901: loss = 486209.22 (1.665 sec)\n",
            "train - step 3902: loss = 452691.59 (1.656 sec)\n",
            "train - step 3903: loss = 542805.38 (1.663 sec)\n",
            "train - step 3904: loss = 360003.94 (1.655 sec)\n",
            "train - step 3905: loss = 441890.09 (1.669 sec)\n",
            "train - step 3906: loss = 289671.81 (1.660 sec)\n",
            "train - step 3907: loss = 564541.31 (1.666 sec)\n",
            "train - step 3908: loss = 314938.59 (1.669 sec)\n",
            "train - step 3909: loss = 501623.09 (1.673 sec)\n",
            "train - step 3910: loss = 607785.31 (1.655 sec)\n",
            "train - step 3911: loss = 335880.31 (1.632 sec)\n",
            "train - step 3912: loss = 419384.47 (1.631 sec)\n",
            "train - step 3913: loss = 461342.50 (1.647 sec)\n",
            "train - step 3914: loss = 439951.88 (1.630 sec)\n",
            "train - step 3915: loss = 362330.22 (1.644 sec)\n",
            "train - step 3916: loss = 343006.72 (1.634 sec)\n",
            "train - step 3917: loss = 495610.34 (1.709 sec)\n",
            "train - step 3918: loss = 442361.28 (1.679 sec)\n",
            "train - step 3919: loss = 501854.88 (1.690 sec)\n",
            "train - step 3920: loss = 368359.94 (1.699 sec)\n",
            "train - step 3921: loss = 491296.44 (1.678 sec)\n",
            "train - step 3922: loss = 452080.56 (1.677 sec)\n",
            "train - step 3923: loss = 416671.56 (1.696 sec)\n",
            "train - step 3924: loss = 413539.38 (1.685 sec)\n",
            "train - step 3925: loss = 381249.41 (1.652 sec)\n",
            "train - step 3926: loss = 460799.59 (1.657 sec)\n",
            "train - step 3927: loss = 271703.78 (1.685 sec)\n",
            "train - step 3928: loss = 409154.22 (1.676 sec)\n",
            "train - step 3929: loss = 638452.69 (1.695 sec)\n",
            "train - step 3930: loss = 399416.41 (1.661 sec)\n",
            "train - step 3931: loss = 442926.03 (1.669 sec)\n",
            "train - step 3932: loss = 385272.47 (1.671 sec)\n",
            "train - step 3933: loss = 321985.19 (1.649 sec)\n",
            "train - step 3934: loss = 237289.33 (1.676 sec)\n",
            "train - step 3935: loss = 459760.53 (2.481 sec)\n",
            "train - step 3936: loss = 430661.12 (1.689 sec)\n",
            "train - step 3937: loss = 439143.00 (1.666 sec)\n",
            "train - step 3938: loss = 376467.22 (1.670 sec)\n",
            "train - step 3939: loss = 435523.94 (1.691 sec)\n",
            "train - step 3940: loss = 482564.72 (1.683 sec)\n",
            "train - step 3941: loss = 332780.16 (1.667 sec)\n",
            "train - step 3942: loss = 446421.62 (1.700 sec)\n",
            "train - step 3943: loss = 266801.62 (1.691 sec)\n",
            "train - step 3944: loss = 342728.34 (1.688 sec)\n",
            "train - step 3945: loss = 422492.41 (1.691 sec)\n",
            "train - step 3946: loss = 388493.09 (1.677 sec)\n",
            "train - step 3947: loss = 410212.19 (1.659 sec)\n",
            "train - step 3948: loss = 443273.06 (1.680 sec)\n",
            "train - step 3949: loss = 376010.81 (1.679 sec)\n",
            "train - step 3950: loss = 430490.66 (1.688 sec)\n",
            "train - step 3951: loss = 391579.59 (1.698 sec)\n",
            "train - step 3952: loss = 398012.16 (1.689 sec)\n",
            "train - step 3953: loss = 484014.97 (1.687 sec)\n",
            "train - step 3954: loss = 326487.41 (1.662 sec)\n",
            "train - step 3955: loss = 448755.53 (2.984 sec)\n",
            "train - step 3956: loss = 557722.38 (1.714 sec)\n",
            "train - step 3957: loss = 341581.59 (1.674 sec)\n",
            "train - step 3958: loss = 426907.84 (1.695 sec)\n",
            "train - step 3959: loss = 399615.81 (1.675 sec)\n",
            "train - step 3960: loss = 471796.34 (1.688 sec)\n",
            "train - step 3961: loss = 442228.47 (1.702 sec)\n",
            "train - step 3962: loss = 468820.09 (1.695 sec)\n",
            "train - step 3963: loss = 436472.28 (1.692 sec)\n",
            "train - step 3964: loss = 451839.94 (1.699 sec)\n",
            "train - step 3965: loss = 370304.47 (1.691 sec)\n",
            "train - step 3966: loss = 511027.06 (1.676 sec)\n",
            "train - step 3967: loss = 499494.50 (1.708 sec)\n",
            "train - step 3968: loss = 547055.25 (1.673 sec)\n",
            "train - step 3969: loss = 445738.88 (1.678 sec)\n",
            "train - step 3970: loss = 436410.19 (2.631 sec)\n",
            "train - step 3971: loss = 461059.22 (1.693 sec)\n",
            "train - step 3972: loss = 489860.84 (1.702 sec)\n",
            "train - step 3973: loss = 343964.22 (1.707 sec)\n",
            "train - step 3974: loss = 340764.28 (1.700 sec)\n",
            "train - step 3975: loss = 383246.44 (1.714 sec)\n",
            "train - step 3976: loss = 451316.62 (1.685 sec)\n",
            "train - step 3977: loss = 394624.84 (1.661 sec)\n",
            "train - step 3978: loss = 569498.31 (1.670 sec)\n",
            "train - step 3979: loss = 336161.12 (1.684 sec)\n",
            "train - step 3980: loss = 542782.31 (1.657 sec)\n",
            "train - step 3981: loss = 506524.62 (1.669 sec)\n",
            "train - step 3982: loss = 424379.16 (1.673 sec)\n",
            "train - step 3983: loss = 522674.56 (1.694 sec)\n",
            "train - step 3984: loss = 449950.38 (1.680 sec)\n",
            "train - step 3985: loss = 332414.44 (1.670 sec)\n",
            "train - step 3986: loss = 501317.00 (1.689 sec)\n",
            "train - step 3987: loss = 381308.72 (1.660 sec)\n",
            "train - step 3988: loss = 515525.69 (1.690 sec)\n",
            "train - step 3989: loss = 282365.47 (1.682 sec)\n",
            "train - step 3990: loss = 499879.91 (1.666 sec)\n",
            "train - step 3991: loss = 447948.84 (1.680 sec)\n",
            "train - step 3992: loss = 467902.69 (1.679 sec)\n",
            "train - step 3993: loss = 413818.56 (1.706 sec)\n",
            "train - step 3994: loss = 496530.50 (1.668 sec)\n",
            "train - step 3995: loss = 419471.12 (1.656 sec)\n",
            "train - step 3996: loss = 687665.00 (1.668 sec)\n",
            "train - step 3997: loss = 290282.09 (1.676 sec)\n",
            "train - step 3998: loss = 433517.34 (1.678 sec)\n",
            "train - step 3999: loss = 357645.00 (1.673 sec)\n",
            "train - step 4000: loss = 568892.38 (1.660 sec)\n",
            "train - step 4001: loss = 496684.06 (1.664 sec)\n",
            "train - step 4002: loss = 393469.22 (1.667 sec)\n",
            "train - step 4003: loss = 424255.56 (1.669 sec)\n",
            "train - step 4004: loss = 423131.06 (1.675 sec)\n",
            "train - step 4005: loss = 397246.88 (1.679 sec)\n",
            "train - step 4006: loss = 338797.81 (2.527 sec)\n",
            "train - step 4007: loss = 381875.47 (1.650 sec)\n",
            "train - step 4008: loss = 552737.25 (1.632 sec)\n",
            "train - step 4009: loss = 333155.53 (1.660 sec)\n",
            "train - step 4010: loss = 434405.50 (1.657 sec)\n",
            "train - step 4011: loss = 355435.62 (1.673 sec)\n",
            "train - step 4012: loss = 463187.97 (1.682 sec)\n",
            "train - step 4013: loss = 404269.69 (1.673 sec)\n",
            "train - step 4014: loss = 341643.91 (1.662 sec)\n",
            "train - step 4015: loss = 387213.09 (1.660 sec)\n",
            "train - step 4016: loss = 449845.38 (1.680 sec)\n",
            "train - step 4017: loss = 521777.12 (1.644 sec)\n",
            "train - step 4018: loss = 459993.06 (1.656 sec)\n",
            "train - step 4019: loss = 479840.84 (1.668 sec)\n",
            "train - step 4020: loss = 338545.19 (1.659 sec)\n",
            "train - step 4021: loss = 508175.22 (1.636 sec)\n",
            "train - step 4022: loss = 507911.09 (1.647 sec)\n",
            "train - step 4023: loss = 232284.17 (1.618 sec)\n",
            "train - step 4024: loss = 443403.97 (1.640 sec)\n",
            "train - step 4025: loss = 452106.97 (1.644 sec)\n",
            "train - step 4026: loss = 322708.44 (1.636 sec)\n",
            "train - step 4027: loss = 546523.56 (1.673 sec)\n",
            "train - step 4028: loss = 533519.81 (1.677 sec)\n",
            "train - step 4029: loss = 407977.59 (1.629 sec)\n",
            "train - step 4030: loss = 413080.62 (1.676 sec)\n",
            "train - step 4031: loss = 323683.53 (1.664 sec)\n",
            "train - step 4032: loss = 352976.03 (1.656 sec)\n",
            "train - step 4033: loss = 399423.78 (1.665 sec)\n",
            "train - step 4034: loss = 416104.50 (1.685 sec)\n",
            "train - step 4035: loss = 481988.72 (1.671 sec)\n",
            "train - step 4036: loss = 359806.12 (1.684 sec)\n",
            "train - step 4037: loss = 549679.94 (1.665 sec)\n",
            "train - step 4038: loss = 453694.94 (1.686 sec)\n",
            "train - step 4039: loss = 361552.81 (1.676 sec)\n",
            "train - step 4040: loss = 261376.80 (1.673 sec)\n",
            "train - step 4041: loss = 524577.06 (1.677 sec)\n",
            "train - step 4042: loss = 533986.38 (2.523 sec)\n",
            "train - step 4043: loss = 402215.19 (1.664 sec)\n",
            "train - step 4044: loss = 573218.00 (1.636 sec)\n",
            "train - step 4045: loss = 446016.38 (1.646 sec)\n",
            "train - step 4046: loss = 507134.38 (1.644 sec)\n",
            "train - step 4047: loss = 529646.19 (1.637 sec)\n",
            "train - step 4048: loss = 375977.38 (1.648 sec)\n",
            "train - step 4049: loss = 457942.41 (1.628 sec)\n",
            "train - step 4050: loss = 567526.56 (1.662 sec)\n",
            "train - step 4051: loss = 319617.12 (1.655 sec)\n",
            "train - step 4052: loss = 571537.38 (1.655 sec)\n",
            "train - step 4053: loss = 520101.47 (1.679 sec)\n",
            "train - step 4054: loss = 270828.53 (1.703 sec)\n",
            "train - step 4055: loss = 505488.31 (1.692 sec)\n",
            "train - step 4056: loss = 222523.45 (1.697 sec)\n",
            "train - step 4057: loss = 488720.59 (1.710 sec)\n",
            "train - step 4058: loss = 449213.66 (1.674 sec)\n",
            "train - step 4059: loss = 451568.12 (1.693 sec)\n",
            "train - step 4060: loss = 278103.59 (1.719 sec)\n",
            "train - step 4061: loss = 408236.28 (1.690 sec)\n",
            "train - step 4062: loss = 476031.56 (1.690 sec)\n",
            "train - step 4063: loss = 523357.22 (1.680 sec)\n",
            "train - step 4064: loss = 439609.38 (1.668 sec)\n",
            "train - step 4065: loss = 403007.84 (1.816 sec)\n",
            "train - step 4066: loss = 333295.41 (1.687 sec)\n",
            "train - step 4067: loss = 480365.97 (1.697 sec)\n",
            "train - step 4068: loss = 389683.41 (1.700 sec)\n",
            "train - step 4069: loss = 345751.09 (1.668 sec)\n",
            "train - step 4070: loss = 316355.38 (1.654 sec)\n",
            "train - step 4071: loss = 385655.94 (1.639 sec)\n",
            "train - step 4072: loss = 310124.03 (1.656 sec)\n",
            "train - step 4073: loss = 420444.97 (1.634 sec)\n",
            "train - step 4074: loss = 374623.09 (1.660 sec)\n",
            "train - step 4075: loss = 394309.88 (1.664 sec)\n",
            "train - step 4076: loss = 470009.78 (1.671 sec)\n",
            "train - step 4077: loss = 454333.72 (1.663 sec)\n",
            "train - step 4078: loss = 360675.31 (2.818 sec)\n",
            "train - step 4079: loss = 412683.00 (1.664 sec)\n",
            "train - step 4080: loss = 361272.16 (1.665 sec)\n",
            "train - step 4081: loss = 305629.78 (1.673 sec)\n",
            "train - step 4082: loss = 355385.00 (1.675 sec)\n",
            "train - step 4083: loss = 472511.38 (1.667 sec)\n",
            "train - step 4084: loss = 517998.09 (1.681 sec)\n",
            "train - step 4085: loss = 365421.88 (1.690 sec)\n",
            "train - step 4086: loss = 438817.00 (1.679 sec)\n",
            "train - step 4087: loss = 378153.53 (1.672 sec)\n",
            "train - step 4088: loss = 317362.38 (1.680 sec)\n",
            "train - step 4089: loss = 308041.81 (1.679 sec)\n",
            "train - step 4090: loss = 323264.31 (1.676 sec)\n",
            "train - step 4091: loss = 386399.66 (1.688 sec)\n",
            "train - step 4092: loss = 500412.66 (1.648 sec)\n",
            "train - step 4093: loss = 363841.56 (1.634 sec)\n",
            "train - step 4094: loss = 423176.66 (1.651 sec)\n",
            "train - step 4095: loss = 563987.12 (1.647 sec)\n",
            "train - step 4096: loss = 403110.66 (1.655 sec)\n",
            "train - step 4097: loss = 481151.53 (1.644 sec)\n",
            "train - step 4098: loss = 460302.38 (1.640 sec)\n",
            "train - step 4099: loss = 390477.22 (1.673 sec)\n",
            "train - step 4100: loss = 608215.31 (1.677 sec)\n",
            "train - step 4101: loss = 459967.03 (1.696 sec)\n",
            "train - step 4102: loss = 429985.28 (1.677 sec)\n",
            "train - step 4103: loss = 409482.44 (1.658 sec)\n",
            "train - step 4104: loss = 289275.84 (1.675 sec)\n",
            "train - step 4105: loss = 404011.41 (1.670 sec)\n",
            "train - step 4106: loss = 419825.09 (1.677 sec)\n",
            "train - step 4107: loss = 228415.30 (1.667 sec)\n",
            "train - step 4108: loss = 505031.06 (1.678 sec)\n",
            "train - step 4109: loss = 497149.47 (1.692 sec)\n",
            "train - step 4110: loss = 466247.09 (1.675 sec)\n",
            "train - step 4111: loss = 377459.94 (1.656 sec)\n",
            "train - step 4112: loss = 514921.12 (1.683 sec)\n",
            "train - step 4113: loss = 436916.59 (1.661 sec)\n",
            "train - step 4114: loss = 373298.16 (2.500 sec)\n",
            "train - step 4115: loss = 446755.16 (1.683 sec)\n",
            "train - step 4116: loss = 498769.53 (1.662 sec)\n",
            "train - step 4117: loss = 406020.59 (1.665 sec)\n",
            "train - step 4118: loss = 501783.03 (1.680 sec)\n",
            "train - step 4119: loss = 365098.56 (1.682 sec)\n",
            "train - step 4120: loss = 351671.91 (1.672 sec)\n",
            "train - step 4121: loss = 721492.19 (1.683 sec)\n",
            "train - step 4122: loss = 478472.88 (1.668 sec)\n",
            "train - step 4123: loss = 390923.47 (1.653 sec)\n",
            "train - step 4124: loss = 477187.38 (1.672 sec)\n",
            "train - step 4125: loss = 415751.78 (1.670 sec)\n",
            "train - step 4126: loss = 288956.88 (1.689 sec)\n",
            "train - step 4127: loss = 661426.56 (1.669 sec)\n",
            "train - step 4128: loss = 332837.81 (1.678 sec)\n",
            "train - step 4129: loss = 504245.94 (1.665 sec)\n",
            "train - step 4130: loss = 373108.56 (1.673 sec)\n",
            "train - step 4131: loss = 484441.84 (1.670 sec)\n",
            "train - step 4132: loss = 446894.31 (1.648 sec)\n",
            "train - step 4133: loss = 417247.16 (1.683 sec)\n",
            "train - step 4134: loss = 433550.34 (1.664 sec)\n",
            "train - step 4135: loss = 543466.94 (1.691 sec)\n",
            "train - step 4136: loss = 436904.88 (1.663 sec)\n",
            "train - step 4137: loss = 411085.38 (1.655 sec)\n",
            "train - step 4138: loss = 360676.56 (1.661 sec)\n",
            "train - step 4139: loss = 298194.72 (1.690 sec)\n",
            "train - step 4140: loss = 478375.34 (1.677 sec)\n",
            "train - step 4141: loss = 333038.41 (1.676 sec)\n",
            "train - step 4142: loss = 379433.81 (1.666 sec)\n",
            "train - step 4143: loss = 339107.06 (1.685 sec)\n",
            "train - step 4144: loss = 401622.88 (1.668 sec)\n",
            "train - step 4145: loss = 378976.38 (1.674 sec)\n",
            "train - step 4146: loss = 424133.78 (1.670 sec)\n",
            "train - step 4147: loss = 419866.78 (1.659 sec)\n",
            "train - step 4148: loss = 350080.22 (1.669 sec)\n",
            "train - step 4149: loss = 382383.41 (1.681 sec)\n",
            "train - step 4150: loss = 354072.59 (2.517 sec)\n",
            "train - step 4151: loss = 575180.44 (1.669 sec)\n",
            "train - step 4152: loss = 345564.34 (1.686 sec)\n",
            "train - step 4153: loss = 436009.62 (1.686 sec)\n",
            "train - step 4154: loss = 394648.22 (1.673 sec)\n",
            "train - step 4155: loss = 385298.09 (1.667 sec)\n",
            "train - step 4156: loss = 407784.34 (1.700 sec)\n",
            "train - step 4157: loss = 470729.22 (1.665 sec)\n",
            "train - step 4158: loss = 394301.12 (1.688 sec)\n",
            "train - step 4159: loss = 504361.94 (1.692 sec)\n",
            "train - step 4160: loss = 623770.75 (1.663 sec)\n",
            "train - step 4161: loss = 311297.91 (1.667 sec)\n",
            "train - step 4162: loss = 474547.28 (1.683 sec)\n",
            "train - step 4163: loss = 297596.22 (1.666 sec)\n",
            "train - step 4164: loss = 536536.50 (1.677 sec)\n",
            "train - step 4165: loss = 388455.31 (1.654 sec)\n",
            "train - step 4166: loss = 370563.38 (1.684 sec)\n",
            "train - step 4167: loss = 430445.19 (1.663 sec)\n",
            "train - step 4168: loss = 526440.06 (1.654 sec)\n",
            "train - step 4169: loss = 389438.31 (1.696 sec)\n",
            "train - step 4170: loss = 466674.94 (1.669 sec)\n",
            "train - step 4171: loss = 464887.56 (1.662 sec)\n",
            "train - step 4172: loss = 480942.94 (1.692 sec)\n",
            "train - step 4173: loss = 479807.06 (1.671 sec)\n",
            "train - step 4174: loss = 401323.34 (1.683 sec)\n",
            "train - step 4175: loss = 369733.91 (1.684 sec)\n",
            "train - step 4176: loss = 625285.25 (1.680 sec)\n",
            "train - step 4177: loss = 531937.00 (1.696 sec)\n",
            "train - step 4178: loss = 490000.16 (1.685 sec)\n",
            "train - step 4179: loss = 459764.19 (1.692 sec)\n",
            "train - step 4180: loss = 363647.28 (1.663 sec)\n",
            "train - step 4181: loss = 443813.09 (1.675 sec)\n",
            "train - step 4182: loss = 422199.03 (1.671 sec)\n",
            "train - step 4183: loss = 474029.09 (1.680 sec)\n",
            "train - step 4184: loss = 432104.28 (1.682 sec)\n",
            "train - step 4185: loss = 504578.44 (1.678 sec)\n",
            "train - step 4186: loss = 509293.34 (2.583 sec)\n",
            "train - step 4187: loss = 462267.50 (1.693 sec)\n",
            "train - step 4188: loss = 462973.09 (1.663 sec)\n",
            "train - step 4189: loss = 320552.44 (1.662 sec)\n",
            "train - step 4190: loss = 449325.50 (1.693 sec)\n",
            "train - step 4191: loss = 437578.94 (1.672 sec)\n",
            "train - step 4192: loss = 448435.41 (1.682 sec)\n",
            "train - step 4193: loss = 409199.66 (1.687 sec)\n",
            "train - step 4194: loss = 301080.31 (1.680 sec)\n",
            "train - step 4195: loss = 362694.56 (1.670 sec)\n",
            "train - step 4196: loss = 451267.22 (1.676 sec)\n",
            "train - step 4197: loss = 303257.19 (1.691 sec)\n",
            "train - step 4198: loss = 384259.69 (1.668 sec)\n",
            "train - step 4199: loss = 314431.72 (1.683 sec)\n",
            "train - step 4200: loss = 490922.66 (1.681 sec)\n",
            "train - step 4201: loss = 531057.31 (1.693 sec)\n",
            "train - step 4202: loss = 518475.91 (1.694 sec)\n",
            "train - step 4203: loss = 425591.59 (1.682 sec)\n",
            "train - step 4204: loss = 365368.47 (1.679 sec)\n",
            "train - step 4205: loss = 623784.88 (1.662 sec)\n",
            "train - step 4206: loss = 396882.88 (1.680 sec)\n",
            "train - step 4207: loss = 436257.41 (1.691 sec)\n",
            "train - step 4208: loss = 434754.91 (1.673 sec)\n",
            "train - step 4209: loss = 281872.19 (1.691 sec)\n",
            "train - step 4210: loss = 445382.00 (1.702 sec)\n",
            "train - step 4211: loss = 473784.94 (1.688 sec)\n",
            "train - step 4212: loss = 493642.47 (1.695 sec)\n",
            "train - step 4213: loss = 433914.66 (1.675 sec)\n",
            "train - step 4214: loss = 327066.59 (1.684 sec)\n",
            "train - step 4215: loss = 454368.09 (1.690 sec)\n",
            "train - step 4216: loss = 450967.34 (1.708 sec)\n",
            "train - step 4217: loss = 325889.91 (1.683 sec)\n",
            "train - step 4218: loss = 413005.38 (1.685 sec)\n",
            "train - step 4219: loss = 441198.00 (1.671 sec)\n",
            "train - step 4220: loss = 436399.34 (1.672 sec)\n",
            "train - step 4221: loss = 406226.66 (1.698 sec)\n",
            "train - step 4222: loss = 335572.84 (2.758 sec)\n",
            "train - step 4223: loss = 433442.44 (1.693 sec)\n",
            "train - step 4224: loss = 494235.34 (1.679 sec)\n",
            "train - step 4225: loss = 360961.38 (1.677 sec)\n",
            "train - step 4226: loss = 497087.66 (1.675 sec)\n",
            "train - step 4227: loss = 259669.16 (1.680 sec)\n",
            "train - step 4228: loss = 460513.88 (1.662 sec)\n",
            "train - step 4229: loss = 358216.88 (1.705 sec)\n",
            "train - step 4230: loss = 356089.38 (1.682 sec)\n",
            "train - step 4231: loss = 578662.12 (1.689 sec)\n",
            "train - step 4232: loss = 345834.88 (1.683 sec)\n",
            "train - step 4233: loss = 464519.56 (1.694 sec)\n",
            "train - step 4234: loss = 375463.81 (1.676 sec)\n",
            "train - step 4235: loss = 356024.88 (1.710 sec)\n",
            "train - step 4236: loss = 545480.50 (1.683 sec)\n",
            "train - step 4237: loss = 435893.88 (1.690 sec)\n",
            "train - step 4238: loss = 469563.72 (1.705 sec)\n",
            "train - step 4239: loss = 369559.56 (1.697 sec)\n",
            "train - step 4240: loss = 413799.19 (1.685 sec)\n",
            "train - step 4241: loss = 646584.25 (1.675 sec)\n",
            "train - step 4242: loss = 374429.81 (1.700 sec)\n",
            "train - step 4243: loss = 448552.72 (1.699 sec)\n",
            "train - step 4244: loss = 380687.44 (1.670 sec)\n",
            "train - step 4245: loss = 523803.16 (1.655 sec)\n",
            "train - step 4246: loss = 536680.25 (1.677 sec)\n",
            "train - step 4247: loss = 320664.78 (1.681 sec)\n",
            "train - step 4248: loss = 530499.75 (1.678 sec)\n",
            "train - step 4249: loss = 588851.88 (1.705 sec)\n",
            "train - step 4250: loss = 484916.91 (1.649 sec)\n",
            "train - step 4251: loss = 602287.94 (1.630 sec)\n",
            "train - step 4252: loss = 463028.97 (1.637 sec)\n",
            "train - step 4253: loss = 528505.38 (1.654 sec)\n",
            "train - step 4254: loss = 462466.53 (1.691 sec)\n",
            "train - step 4255: loss = 390245.41 (1.689 sec)\n",
            "train - step 4256: loss = 679021.88 (1.696 sec)\n",
            "train - step 4257: loss = 399776.38 (1.678 sec)\n",
            "train - step 4258: loss = 443531.00 (2.636 sec)\n",
            "train - step 4259: loss = 483195.78 (1.679 sec)\n",
            "train - step 4260: loss = 373648.94 (1.681 sec)\n",
            "train - step 4261: loss = 627463.44 (1.696 sec)\n",
            "train - step 4262: loss = 369328.12 (1.692 sec)\n",
            "train - step 4263: loss = 332925.47 (1.662 sec)\n",
            "train - step 4264: loss = 526547.75 (1.682 sec)\n",
            "train - step 4265: loss = 468974.03 (1.704 sec)\n",
            "train - step 4266: loss = 438829.34 (1.684 sec)\n",
            "train - step 4267: loss = 372426.00 (1.667 sec)\n",
            "train - step 4268: loss = 418524.69 (1.669 sec)\n",
            "train - step 4269: loss = 399502.72 (1.671 sec)\n",
            "train - step 4270: loss = 342373.19 (1.661 sec)\n",
            "train - step 4271: loss = 450980.28 (1.686 sec)\n",
            "train - step 4272: loss = 288634.06 (1.689 sec)\n",
            "train - step 4273: loss = 408247.00 (1.687 sec)\n",
            "train - step 4274: loss = 516748.19 (1.681 sec)\n",
            "train - step 4275: loss = 414842.72 (1.660 sec)\n",
            "train - step 4276: loss = 478332.03 (1.646 sec)\n",
            "train - step 4277: loss = 413026.81 (1.650 sec)\n",
            "train - step 4278: loss = 320455.28 (1.646 sec)\n",
            "train - step 4279: loss = 402452.12 (1.648 sec)\n",
            "train - step 4280: loss = 497706.41 (1.663 sec)\n",
            "train - step 4281: loss = 380242.53 (1.672 sec)\n",
            "train - step 4282: loss = 408815.72 (1.680 sec)\n",
            "train - step 4283: loss = 493791.41 (1.709 sec)\n",
            "train - step 4284: loss = 411425.66 (1.668 sec)\n",
            "train - step 4285: loss = 436072.81 (1.675 sec)\n",
            "train - step 4286: loss = 427011.31 (1.677 sec)\n",
            "train - step 4287: loss = 469895.28 (1.666 sec)\n",
            "train - step 4288: loss = 501787.44 (1.684 sec)\n",
            "train - step 4289: loss = 558898.69 (1.669 sec)\n",
            "train - step 4290: loss = 501431.69 (1.695 sec)\n",
            "train - step 4291: loss = 381784.78 (1.677 sec)\n",
            "train - step 4292: loss = 396820.41 (1.667 sec)\n",
            "train - step 4293: loss = 504750.31 (1.687 sec)\n",
            "train - step 4294: loss = 242918.56 (2.600 sec)\n",
            "train - step 4295: loss = 440308.88 (1.682 sec)\n",
            "train - step 4296: loss = 360712.00 (1.691 sec)\n",
            "train - step 4297: loss = 434039.47 (1.680 sec)\n",
            "train - step 4298: loss = 419304.59 (1.710 sec)\n",
            "train - step 4299: loss = 332398.72 (1.673 sec)\n",
            "train - step 4300: loss = 498540.56 (1.658 sec)\n",
            "train - step 4301: loss = 451475.88 (1.674 sec)\n",
            "train - step 4302: loss = 485452.62 (1.663 sec)\n",
            "train - step 4303: loss = 509259.16 (1.666 sec)\n",
            "train - step 4304: loss = 500138.41 (1.683 sec)\n",
            "train - step 4305: loss = 408115.94 (1.682 sec)\n",
            "train - step 4306: loss = 439817.78 (1.697 sec)\n",
            "train - step 4307: loss = 484786.41 (1.677 sec)\n",
            "train - step 4308: loss = 372044.47 (1.671 sec)\n",
            "train - step 4309: loss = 388210.19 (1.668 sec)\n",
            "train - step 4310: loss = 405394.84 (1.673 sec)\n",
            "train - step 4311: loss = 462132.38 (1.675 sec)\n",
            "train - step 4312: loss = 457015.97 (1.678 sec)\n",
            "train - step 4313: loss = 390049.53 (1.688 sec)\n",
            "train - step 4314: loss = 382862.81 (1.683 sec)\n",
            "train - step 4315: loss = 359022.47 (1.688 sec)\n",
            "train - step 4316: loss = 489789.38 (1.662 sec)\n",
            "train - step 4317: loss = 454155.62 (1.695 sec)\n",
            "train - step 4318: loss = 401607.62 (1.691 sec)\n",
            "train - step 4319: loss = 471986.09 (1.679 sec)\n",
            "train - step 4320: loss = 422385.44 (1.680 sec)\n",
            "train - step 4321: loss = 508062.09 (1.690 sec)\n",
            "train - step 4322: loss = 409729.09 (1.686 sec)\n",
            "train - step 4323: loss = 304587.59 (1.679 sec)\n",
            "train - step 4324: loss = 406400.19 (1.673 sec)\n",
            "train - step 4325: loss = 436985.47 (1.686 sec)\n",
            "train - step 4326: loss = 409071.41 (1.667 sec)\n",
            "train - step 4327: loss = 570665.25 (1.668 sec)\n",
            "train - step 4328: loss = 434160.97 (1.674 sec)\n",
            "train - step 4329: loss = 478365.06 (1.653 sec)\n",
            "train - step 4330: loss = 451791.16 (2.565 sec)\n",
            "train - step 4331: loss = 492660.88 (1.682 sec)\n",
            "train - step 4332: loss = 380839.78 (1.674 sec)\n",
            "train - step 4333: loss = 500027.88 (1.683 sec)\n",
            "train - step 4334: loss = 320996.97 (1.680 sec)\n",
            "train - step 4335: loss = 507287.59 (1.675 sec)\n",
            "train - step 4336: loss = 408065.81 (1.676 sec)\n",
            "train - step 4337: loss = 415196.88 (1.676 sec)\n",
            "train - step 4338: loss = 354379.59 (1.665 sec)\n",
            "train - step 4339: loss = 322418.91 (1.670 sec)\n",
            "train - step 4340: loss = 415713.97 (1.679 sec)\n",
            "train - step 4341: loss = 422773.91 (1.673 sec)\n",
            "train - step 4342: loss = 369532.84 (1.665 sec)\n",
            "train - step 4343: loss = 402116.88 (1.667 sec)\n",
            "train - step 4344: loss = 493502.88 (1.665 sec)\n",
            "train - step 4345: loss = 459270.41 (1.662 sec)\n",
            "train - step 4346: loss = 467404.62 (1.682 sec)\n",
            "train - step 4347: loss = 308083.31 (1.698 sec)\n",
            "train - step 4348: loss = 486338.12 (1.706 sec)\n",
            "train - step 4349: loss = 376914.91 (1.714 sec)\n",
            "train - step 4350: loss = 369491.66 (1.689 sec)\n",
            "train - step 4351: loss = 498063.34 (1.678 sec)\n",
            "train - step 4352: loss = 424123.69 (1.709 sec)\n",
            "train - step 4353: loss = 517748.72 (1.703 sec)\n",
            "train - step 4354: loss = 344603.06 (1.694 sec)\n",
            "train - step 4355: loss = 301272.59 (1.666 sec)\n",
            "train - step 4356: loss = 437990.84 (1.669 sec)\n",
            "train - step 4357: loss = 555559.31 (1.671 sec)\n",
            "train - step 4358: loss = 393345.34 (1.659 sec)\n",
            "train - step 4359: loss = 335502.94 (1.672 sec)\n",
            "train - step 4360: loss = 440969.91 (1.682 sec)\n",
            "train - step 4361: loss = 345454.41 (1.834 sec)\n",
            "train - step 4362: loss = 318462.97 (1.669 sec)\n",
            "train - step 4363: loss = 303850.34 (1.677 sec)\n",
            "train - step 4364: loss = 375153.31 (1.690 sec)\n",
            "train - step 4365: loss = 531144.12 (1.691 sec)\n",
            "train - step 4366: loss = 458578.16 (2.502 sec)\n",
            "train - step 4367: loss = 408630.31 (1.667 sec)\n",
            "train - step 4368: loss = 482152.00 (1.705 sec)\n",
            "train - step 4369: loss = 443077.66 (1.677 sec)\n",
            "train - step 4370: loss = 370136.88 (1.676 sec)\n",
            "train - step 4371: loss = 295473.12 (1.679 sec)\n",
            "train - step 4372: loss = 394298.31 (1.680 sec)\n",
            "train - step 4373: loss = 399677.53 (1.706 sec)\n",
            "train - step 4374: loss = 425631.19 (1.667 sec)\n",
            "train - step 4375: loss = 341652.81 (1.711 sec)\n",
            "train - step 4376: loss = 445032.19 (1.708 sec)\n",
            "train - step 4377: loss = 393043.69 (1.681 sec)\n",
            "train - step 4378: loss = 498810.88 (1.672 sec)\n",
            "train - step 4379: loss = 367150.56 (1.688 sec)\n",
            "train - step 4380: loss = 374587.50 (1.665 sec)\n",
            "train - step 4381: loss = 489089.06 (1.675 sec)\n",
            "train - step 4382: loss = 538646.94 (1.674 sec)\n",
            "train - step 4383: loss = 542983.81 (1.699 sec)\n",
            "train - step 4384: loss = 342291.44 (1.684 sec)\n",
            "train - step 4385: loss = 453400.28 (1.674 sec)\n",
            "train - step 4386: loss = 480916.81 (1.668 sec)\n",
            "train - step 4387: loss = 434583.66 (1.694 sec)\n",
            "train - step 4388: loss = 450422.94 (1.672 sec)\n",
            "train - step 4389: loss = 281447.53 (1.675 sec)\n",
            "train - step 4390: loss = 313170.62 (1.686 sec)\n",
            "train - step 4391: loss = 390407.53 (1.665 sec)\n",
            "train - step 4392: loss = 312723.78 (1.690 sec)\n",
            "train - step 4393: loss = 374958.59 (1.692 sec)\n",
            "train - step 4394: loss = 460494.06 (1.667 sec)\n",
            "train - step 4395: loss = 434666.28 (1.700 sec)\n",
            "train - step 4396: loss = 347198.44 (1.655 sec)\n",
            "train - step 4397: loss = 368546.94 (1.674 sec)\n",
            "train - step 4398: loss = 527942.00 (1.693 sec)\n",
            "train - step 4399: loss = 357253.59 (1.667 sec)\n",
            "train - step 4400: loss = 332734.28 (1.676 sec)\n",
            "train - step 4401: loss = 387313.53 (1.687 sec)\n",
            "train - step 4402: loss = 355595.09 (2.550 sec)\n",
            "train - step 4403: loss = 430626.91 (1.685 sec)\n",
            "train - step 4404: loss = 547937.88 (1.699 sec)\n",
            "train - step 4405: loss = 442013.34 (1.672 sec)\n",
            "train - step 4406: loss = 684803.31 (1.677 sec)\n",
            "train - step 4407: loss = 487574.47 (1.694 sec)\n",
            "train - step 4408: loss = 400752.22 (1.717 sec)\n",
            "train - step 4409: loss = 545734.75 (1.677 sec)\n",
            "train - step 4410: loss = 449245.53 (1.676 sec)\n",
            "train - step 4411: loss = 308167.94 (1.683 sec)\n",
            "train - step 4412: loss = 422290.16 (1.685 sec)\n",
            "train - step 4413: loss = 445392.38 (1.683 sec)\n",
            "train - step 4414: loss = 364938.88 (1.691 sec)\n",
            "train - step 4415: loss = 434994.56 (1.682 sec)\n",
            "train - step 4416: loss = 440715.16 (1.714 sec)\n",
            "train - step 4417: loss = 320728.84 (1.694 sec)\n",
            "train - step 4418: loss = 375241.12 (1.679 sec)\n",
            "train - step 4419: loss = 548212.56 (1.692 sec)\n",
            "train - step 4420: loss = 453548.72 (1.688 sec)\n",
            "train - step 4421: loss = 400826.28 (1.691 sec)\n",
            "train - step 4422: loss = 356512.31 (1.677 sec)\n",
            "train - step 4423: loss = 403906.88 (1.684 sec)\n",
            "train - step 4424: loss = 375543.78 (1.673 sec)\n",
            "train - step 4425: loss = 503415.62 (1.690 sec)\n",
            "train - step 4426: loss = 338537.59 (1.678 sec)\n",
            "train - step 4427: loss = 476647.12 (1.688 sec)\n",
            "train - step 4428: loss = 433139.91 (1.679 sec)\n",
            "train - step 4429: loss = 468724.84 (1.682 sec)\n",
            "train - step 4430: loss = 373680.38 (1.643 sec)\n",
            "train - step 4431: loss = 374410.91 (1.650 sec)\n",
            "train - step 4432: loss = 247721.02 (1.636 sec)\n",
            "train - step 4433: loss = 375222.09 (1.656 sec)\n",
            "train - step 4434: loss = 573942.81 (1.697 sec)\n",
            "train - step 4435: loss = 380176.94 (1.695 sec)\n",
            "train - step 4436: loss = 531512.38 (1.713 sec)\n",
            "train - step 4437: loss = 522041.53 (1.683 sec)\n",
            "train - step 4438: loss = 537184.00 (2.199 sec)\n",
            "train - step 4439: loss = 383730.66 (1.687 sec)\n",
            "train - step 4440: loss = 398914.72 (1.666 sec)\n",
            "train - step 4441: loss = 650090.75 (1.666 sec)\n",
            "train - step 4442: loss = 444226.88 (1.689 sec)\n",
            "train - step 4443: loss = 241591.16 (1.684 sec)\n",
            "train - step 4444: loss = 428629.44 (1.667 sec)\n",
            "train - step 4445: loss = 426683.00 (1.683 sec)\n",
            "train - step 4446: loss = 423008.19 (1.653 sec)\n",
            "train - step 4447: loss = 472793.06 (1.693 sec)\n",
            "train - step 4448: loss = 390821.69 (1.661 sec)\n",
            "train - step 4449: loss = 524020.97 (1.663 sec)\n",
            "train - step 4450: loss = 369522.97 (1.670 sec)\n",
            "train - step 4451: loss = 352353.00 (1.687 sec)\n",
            "train - step 4452: loss = 463100.41 (1.667 sec)\n",
            "train - step 4453: loss = 365327.62 (1.664 sec)\n",
            "train - step 4454: loss = 280192.28 (1.685 sec)\n",
            "train - step 4455: loss = 495905.41 (1.677 sec)\n",
            "train - step 4456: loss = 381772.69 (1.691 sec)\n",
            "train - step 4457: loss = 190671.59 (1.660 sec)\n",
            "train - step 4458: loss = 405606.78 (1.652 sec)\n",
            "train - step 4459: loss = 406178.47 (1.649 sec)\n",
            "train - step 4460: loss = 366061.22 (1.647 sec)\n",
            "train - step 4461: loss = 302313.06 (1.655 sec)\n",
            "train - step 4462: loss = 445070.94 (1.635 sec)\n",
            "train - step 4463: loss = 625025.00 (1.660 sec)\n",
            "train - step 4464: loss = 528180.75 (1.685 sec)\n",
            "train - step 4465: loss = 436553.31 (1.670 sec)\n",
            "train - step 4466: loss = 510449.34 (1.669 sec)\n",
            "train - step 4467: loss = 423409.22 (1.694 sec)\n",
            "train - step 4468: loss = 455517.84 (1.681 sec)\n",
            "train - step 4469: loss = 243726.70 (1.673 sec)\n",
            "train - step 4470: loss = 623970.25 (1.672 sec)\n",
            "train - step 4471: loss = 557385.31 (1.689 sec)\n",
            "train - step 4472: loss = 317300.88 (1.672 sec)\n",
            "train - step 4473: loss = 568824.69 (1.689 sec)\n",
            "train - step 4474: loss = 502695.19 (2.569 sec)\n",
            "train - step 4475: loss = 396702.03 (1.686 sec)\n",
            "train - step 4476: loss = 403557.59 (1.679 sec)\n",
            "train - step 4477: loss = 628178.94 (1.698 sec)\n",
            "train - step 4478: loss = 340385.06 (1.679 sec)\n",
            "train - step 4479: loss = 427504.03 (1.664 sec)\n",
            "train - step 4480: loss = 379708.28 (1.697 sec)\n",
            "train - step 4481: loss = 366148.78 (1.688 sec)\n",
            "train - step 4482: loss = 565021.12 (1.681 sec)\n",
            "train - step 4483: loss = 399768.19 (1.664 sec)\n",
            "train - step 4484: loss = 461209.81 (1.683 sec)\n",
            "train - step 4485: loss = 379924.78 (1.671 sec)\n",
            "train - step 4486: loss = 394015.56 (1.666 sec)\n",
            "train - step 4487: loss = 424646.94 (1.673 sec)\n",
            "train - step 4488: loss = 462115.66 (1.687 sec)\n",
            "train - step 4489: loss = 531753.75 (1.687 sec)\n",
            "train - step 4490: loss = 282053.84 (1.688 sec)\n",
            "train - step 4491: loss = 553027.12 (1.678 sec)\n",
            "train - step 4492: loss = 398606.94 (1.675 sec)\n",
            "train - step 4493: loss = 321978.78 (1.672 sec)\n",
            "train - step 4494: loss = 351971.09 (1.671 sec)\n",
            "train - step 4495: loss = 378094.62 (1.694 sec)\n",
            "train - step 4496: loss = 338379.19 (1.670 sec)\n",
            "train - step 4497: loss = 454547.78 (1.682 sec)\n",
            "train - step 4498: loss = 164375.44 (1.675 sec)\n",
            "train - step 4499: loss = 340769.41 (1.687 sec)\n",
            "train - step 4500: loss = 592674.75 (1.677 sec)\n",
            "train - step 4501: loss = 523751.97 (1.663 sec)\n",
            "train - step 4502: loss = 673878.00 (1.681 sec)\n",
            "train - step 4503: loss = 439043.56 (1.675 sec)\n",
            "train - step 4504: loss = 473497.53 (1.686 sec)\n",
            "train - step 4505: loss = 268899.84 (1.672 sec)\n",
            "train - step 4506: loss = 380676.91 (1.671 sec)\n",
            "train - step 4507: loss = 454679.53 (1.681 sec)\n",
            "train - step 4508: loss = 289966.91 (1.693 sec)\n",
            "train - step 4509: loss = 501823.47 (1.685 sec)\n",
            "train - step 4510: loss = 307620.19 (2.645 sec)\n",
            "train - step 4511: loss = 479198.94 (1.707 sec)\n",
            "train - step 4512: loss = 358551.03 (1.683 sec)\n",
            "train - step 4513: loss = 388030.81 (1.688 sec)\n",
            "train - step 4514: loss = 494740.22 (1.685 sec)\n",
            "train - step 4515: loss = 459498.88 (1.687 sec)\n",
            "train - step 4516: loss = 438080.31 (1.698 sec)\n",
            "train - step 4517: loss = 583675.06 (1.675 sec)\n",
            "train - step 4518: loss = 495621.34 (1.678 sec)\n",
            "train - step 4519: loss = 492780.62 (1.690 sec)\n",
            "train - step 4520: loss = 420730.16 (1.702 sec)\n",
            "train - step 4521: loss = 474939.47 (1.666 sec)\n",
            "train - step 4522: loss = 625472.00 (1.704 sec)\n",
            "train - step 4523: loss = 324751.41 (1.674 sec)\n",
            "train - step 4524: loss = 398504.19 (1.670 sec)\n",
            "train - step 4525: loss = 366127.56 (1.668 sec)\n",
            "train - step 4526: loss = 358934.88 (1.670 sec)\n",
            "train - step 4527: loss = 451157.38 (1.682 sec)\n",
            "train - step 4528: loss = 480795.53 (1.672 sec)\n",
            "train - step 4529: loss = 522402.19 (1.676 sec)\n",
            "train - step 4530: loss = 401951.72 (1.686 sec)\n",
            "train - step 4531: loss = 375318.91 (1.684 sec)\n",
            "train - step 4532: loss = 377392.97 (1.666 sec)\n",
            "train - step 4533: loss = 369857.22 (1.674 sec)\n",
            "train - step 4534: loss = 428768.44 (1.667 sec)\n",
            "train - step 4535: loss = 473868.41 (1.685 sec)\n",
            "train - step 4536: loss = 380440.91 (1.707 sec)\n",
            "train - step 4537: loss = 478278.44 (1.677 sec)\n",
            "train - step 4538: loss = 479234.06 (1.671 sec)\n",
            "train - step 4539: loss = 614012.75 (1.656 sec)\n",
            "train - step 4540: loss = 515280.38 (1.664 sec)\n",
            "train - step 4541: loss = 468891.03 (1.679 sec)\n",
            "train - step 4542: loss = 549530.62 (1.677 sec)\n",
            "train - step 4543: loss = 362484.59 (1.686 sec)\n",
            "train - step 4544: loss = 439224.44 (1.695 sec)\n",
            "train - step 4545: loss = 421822.62 (1.723 sec)\n",
            "train - step 4546: loss = 308142.59 (2.501 sec)\n",
            "train - step 4547: loss = 411273.12 (1.687 sec)\n",
            "train - step 4548: loss = 296885.16 (1.675 sec)\n",
            "train - step 4549: loss = 304679.81 (1.691 sec)\n",
            "train - step 4550: loss = 454524.91 (1.666 sec)\n",
            "train - step 4551: loss = 315517.16 (1.678 sec)\n",
            "train - step 4552: loss = 564822.81 (1.678 sec)\n",
            "train - step 4553: loss = 342574.19 (1.701 sec)\n",
            "train - step 4554: loss = 353786.47 (1.686 sec)\n",
            "train - step 4555: loss = 242806.39 (1.705 sec)\n",
            "train - step 4556: loss = 340982.06 (1.687 sec)\n",
            "train - step 4557: loss = 353652.03 (1.679 sec)\n",
            "train - step 4558: loss = 444626.44 (1.671 sec)\n",
            "train - step 4559: loss = 494333.00 (1.696 sec)\n",
            "train - step 4560: loss = 534493.25 (1.699 sec)\n",
            "train - step 4561: loss = 312005.81 (1.678 sec)\n",
            "train - step 4562: loss = 481273.72 (1.685 sec)\n",
            "train - step 4563: loss = 468530.34 (1.698 sec)\n",
            "train - step 4564: loss = 331391.34 (1.694 sec)\n",
            "train - step 4565: loss = 362484.34 (1.694 sec)\n",
            "train - step 4566: loss = 298983.12 (1.676 sec)\n",
            "train - step 4567: loss = 434309.53 (1.678 sec)\n",
            "train - step 4568: loss = 456870.31 (1.728 sec)\n",
            "train - step 4569: loss = 440102.94 (1.681 sec)\n",
            "train - step 4570: loss = 428364.38 (1.697 sec)\n",
            "train - step 4571: loss = 391425.91 (1.678 sec)\n",
            "train - step 4572: loss = 412647.59 (1.692 sec)\n",
            "train - step 4573: loss = 292023.41 (1.685 sec)\n",
            "train - step 4574: loss = 495631.91 (1.681 sec)\n",
            "train - step 4575: loss = 510607.19 (1.729 sec)\n",
            "train - step 4576: loss = 448863.56 (1.691 sec)\n",
            "train - step 4577: loss = 295759.94 (1.706 sec)\n",
            "train - step 4578: loss = 330120.41 (1.687 sec)\n",
            "train - step 4579: loss = 431576.34 (1.722 sec)\n",
            "train - step 4580: loss = 414713.88 (1.692 sec)\n",
            "train - step 4581: loss = 404585.78 (1.658 sec)\n",
            "train - step 4582: loss = 494547.59 (3.084 sec)\n",
            "train - step 4583: loss = 413380.06 (1.685 sec)\n",
            "train - step 4584: loss = 433366.31 (1.671 sec)\n",
            "train - step 4585: loss = 435904.72 (1.665 sec)\n",
            "train - step 4586: loss = 396538.16 (1.680 sec)\n",
            "train - step 4587: loss = 259375.66 (1.664 sec)\n",
            "train - step 4588: loss = 522494.00 (1.676 sec)\n",
            "train - step 4589: loss = 409249.16 (1.677 sec)\n",
            "train - step 4590: loss = 453448.72 (1.693 sec)\n",
            "train - step 4591: loss = 441378.22 (1.679 sec)\n",
            "train - step 4592: loss = 586184.31 (1.695 sec)\n",
            "train - step 4593: loss = 318434.00 (1.674 sec)\n",
            "train - step 4594: loss = 380262.09 (1.669 sec)\n",
            "train - step 4595: loss = 485159.97 (1.665 sec)\n",
            "train - step 4596: loss = 474327.22 (1.683 sec)\n",
            "train - step 4597: loss = 321587.47 (1.670 sec)\n",
            "train - step 4598: loss = 346049.22 (1.655 sec)\n",
            "train - step 4599: loss = 398360.38 (1.675 sec)\n",
            "train - step 4600: loss = 519875.44 (1.669 sec)\n",
            "train - step 4601: loss = 433609.06 (1.688 sec)\n",
            "train - step 4602: loss = 428720.78 (1.680 sec)\n",
            "train - step 4603: loss = 259715.86 (1.668 sec)\n",
            "train - step 4604: loss = 407745.19 (1.673 sec)\n",
            "train - step 4605: loss = 286373.91 (1.705 sec)\n",
            "train - step 4606: loss = 459469.19 (1.663 sec)\n",
            "train - step 4607: loss = 539033.38 (1.675 sec)\n",
            "train - step 4608: loss = 546368.25 (1.664 sec)\n",
            "train - step 4609: loss = 328551.22 (1.677 sec)\n",
            "train - step 4610: loss = 327384.53 (1.647 sec)\n",
            "train - step 4611: loss = 319078.34 (1.658 sec)\n",
            "train - step 4612: loss = 589742.94 (1.651 sec)\n",
            "train - step 4613: loss = 494786.34 (1.685 sec)\n",
            "train - step 4614: loss = 364785.34 (1.675 sec)\n",
            "train - step 4615: loss = 436694.19 (1.711 sec)\n",
            "train - step 4616: loss = 519046.88 (1.685 sec)\n",
            "train - step 4617: loss = 600003.06 (1.667 sec)\n",
            "train - step 4618: loss = 456408.53 (2.756 sec)\n",
            "train - step 4619: loss = 429687.22 (1.687 sec)\n",
            "train - step 4620: loss = 500662.34 (1.668 sec)\n",
            "train - step 4621: loss = 264823.09 (1.666 sec)\n",
            "train - step 4622: loss = 455063.19 (1.669 sec)\n",
            "train - step 4623: loss = 542313.12 (1.671 sec)\n",
            "train - step 4624: loss = 303659.41 (1.675 sec)\n",
            "train - step 4625: loss = 450990.19 (1.673 sec)\n",
            "train - step 4626: loss = 455821.31 (1.683 sec)\n",
            "train - step 4627: loss = 401954.91 (1.662 sec)\n",
            "train - step 4628: loss = 299060.53 (1.667 sec)\n",
            "train - step 4629: loss = 468979.53 (1.674 sec)\n",
            "train - step 4630: loss = 439548.00 (1.670 sec)\n",
            "train - step 4631: loss = 463165.81 (1.689 sec)\n",
            "train - step 4632: loss = 331677.41 (1.691 sec)\n",
            "train - step 4633: loss = 509054.03 (1.666 sec)\n",
            "train - step 4634: loss = 380072.09 (1.683 sec)\n",
            "train - step 4635: loss = 469387.69 (1.686 sec)\n",
            "train - step 4636: loss = 438501.06 (1.699 sec)\n",
            "train - step 4637: loss = 426324.19 (1.710 sec)\n",
            "train - step 4638: loss = 393608.78 (1.679 sec)\n",
            "train - step 4639: loss = 303960.88 (1.696 sec)\n",
            "train - step 4640: loss = 397465.31 (1.629 sec)\n",
            "train - step 4641: loss = 423229.59 (1.674 sec)\n",
            "train - step 4642: loss = 496377.62 (1.652 sec)\n",
            "train - step 4643: loss = 352017.34 (1.739 sec)\n",
            "train - step 4644: loss = 300703.91 (1.671 sec)\n",
            "train - step 4645: loss = 451141.78 (1.661 sec)\n",
            "train - step 4646: loss = 362402.38 (1.686 sec)\n",
            "train - step 4647: loss = 438024.09 (1.742 sec)\n",
            "train - step 4648: loss = 552030.94 (1.744 sec)\n",
            "train - step 4649: loss = 558751.50 (1.700 sec)\n",
            "train - step 4650: loss = 464184.12 (1.697 sec)\n",
            "train - step 4651: loss = 335454.12 (1.713 sec)\n",
            "train - step 4652: loss = 328025.78 (1.674 sec)\n",
            "train - step 4653: loss = 464592.44 (1.767 sec)\n",
            "train - step 4654: loss = 363096.84 (1.691 sec)\n",
            "train - step 4655: loss = 398096.34 (1.691 sec)\n",
            "train - step 4656: loss = 430940.16 (1.697 sec)\n",
            "train - step 4657: loss = 384920.12 (1.671 sec)\n",
            "train - step 4658: loss = 380904.28 (1.670 sec)\n",
            "train - step 4659: loss = 422946.06 (1.712 sec)\n",
            "train - step 4660: loss = 594030.88 (1.684 sec)\n",
            "train - step 4661: loss = 272356.59 (1.676 sec)\n",
            "train - step 4662: loss = 345142.59 (1.677 sec)\n",
            "train - step 4663: loss = 483681.28 (1.675 sec)\n",
            "train - step 4664: loss = 530048.69 (1.690 sec)\n",
            "train - step 4665: loss = 363570.56 (1.692 sec)\n",
            "train - step 4666: loss = 343176.53 (1.659 sec)\n",
            "train - step 4667: loss = 414367.44 (1.700 sec)\n",
            "train - step 4668: loss = 487649.34 (1.707 sec)\n",
            "train - step 4669: loss = 502222.94 (1.682 sec)\n",
            "train - step 4670: loss = 707609.75 (1.681 sec)\n",
            "train - step 4671: loss = 437994.31 (1.684 sec)\n",
            "train - step 4672: loss = 369128.09 (1.696 sec)\n",
            "train - step 4673: loss = 443048.34 (1.676 sec)\n",
            "train - step 4674: loss = 421928.84 (1.669 sec)\n",
            "train - step 4675: loss = 498537.88 (1.670 sec)\n",
            "train - step 4676: loss = 471816.72 (1.671 sec)\n",
            "train - step 4677: loss = 373218.62 (1.680 sec)\n",
            "train - step 4678: loss = 478581.34 (1.686 sec)\n",
            "train - step 4679: loss = 440474.88 (1.716 sec)\n",
            "train - step 4680: loss = 260277.23 (1.652 sec)\n",
            "train - step 4681: loss = 438463.53 (1.697 sec)\n",
            "train - step 4682: loss = 416774.84 (1.680 sec)\n",
            "train - step 4683: loss = 395612.78 (1.670 sec)\n",
            "train - step 4684: loss = 565247.44 (1.692 sec)\n",
            "train - step 4685: loss = 371444.41 (1.689 sec)\n",
            "train - step 4686: loss = 502399.34 (1.686 sec)\n",
            "train - step 4687: loss = 341352.19 (1.662 sec)\n",
            "train - step 4688: loss = 365831.59 (1.671 sec)\n",
            "train - step 4689: loss = 528269.94 (1.686 sec)\n",
            "train - step 4690: loss = 510321.53 (2.561 sec)\n",
            "train - step 4691: loss = 514521.88 (1.697 sec)\n",
            "train - step 4692: loss = 419291.00 (1.680 sec)\n",
            "train - step 4693: loss = 320653.09 (1.676 sec)\n",
            "train - step 4694: loss = 465514.88 (1.690 sec)\n",
            "train - step 4695: loss = 420324.94 (1.691 sec)\n",
            "train - step 4696: loss = 436552.44 (1.666 sec)\n",
            "train - step 4697: loss = 356456.91 (1.688 sec)\n",
            "train - step 4698: loss = 435284.78 (1.679 sec)\n",
            "train - step 4699: loss = 461566.66 (1.657 sec)\n",
            "train - step 4700: loss = 375646.22 (1.685 sec)\n",
            "train - step 4701: loss = 466555.00 (1.666 sec)\n",
            "train - step 4702: loss = 604239.88 (1.665 sec)\n",
            "train - step 4703: loss = 409456.41 (1.656 sec)\n",
            "train - step 4704: loss = 356268.59 (1.684 sec)\n",
            "train - step 4705: loss = 485940.16 (1.679 sec)\n",
            "train - step 4706: loss = 360620.34 (1.659 sec)\n",
            "train - step 4707: loss = 315158.19 (1.693 sec)\n",
            "train - step 4708: loss = 420303.19 (1.701 sec)\n",
            "train - step 4709: loss = 466328.28 (1.662 sec)\n",
            "train - step 4710: loss = 359212.41 (1.678 sec)\n",
            "train - step 4711: loss = 486768.34 (1.669 sec)\n",
            "train - step 4712: loss = 451370.78 (1.662 sec)\n",
            "train - step 4713: loss = 310090.53 (1.676 sec)\n",
            "train - step 4714: loss = 505670.88 (1.672 sec)\n",
            "train - step 4715: loss = 449303.88 (1.679 sec)\n",
            "train - step 4716: loss = 493122.12 (1.672 sec)\n",
            "train - step 4717: loss = 429034.41 (1.692 sec)\n",
            "train - step 4718: loss = 438190.88 (1.689 sec)\n",
            "train - step 4719: loss = 434528.31 (1.704 sec)\n",
            "train - step 4720: loss = 408153.66 (1.684 sec)\n",
            "train - step 4721: loss = 426210.16 (1.681 sec)\n",
            "train - step 4722: loss = 367823.22 (1.682 sec)\n",
            "train - step 4723: loss = 443076.53 (1.683 sec)\n",
            "train - step 4724: loss = 423258.22 (1.679 sec)\n",
            "train - step 4725: loss = 419375.34 (1.685 sec)\n",
            "train - step 4726: loss = 414281.38 (2.609 sec)\n",
            "train - step 4727: loss = 465433.62 (1.682 sec)\n",
            "train - step 4728: loss = 530418.38 (1.705 sec)\n",
            "train - step 4729: loss = 607265.06 (1.703 sec)\n",
            "train - step 4730: loss = 328205.19 (1.690 sec)\n",
            "train - step 4731: loss = 372018.03 (1.688 sec)\n",
            "train - step 4732: loss = 331459.81 (1.684 sec)\n",
            "train - step 4733: loss = 544437.69 (1.669 sec)\n",
            "train - step 4734: loss = 311642.53 (1.684 sec)\n",
            "train - step 4735: loss = 453520.94 (1.702 sec)\n",
            "train - step 4736: loss = 320299.00 (1.690 sec)\n",
            "train - step 4737: loss = 329370.50 (1.679 sec)\n",
            "train - step 4738: loss = 406463.59 (1.662 sec)\n",
            "train - step 4739: loss = 410683.19 (1.700 sec)\n",
            "train - step 4740: loss = 485201.56 (1.680 sec)\n",
            "train - step 4741: loss = 581678.38 (1.678 sec)\n",
            "train - step 4742: loss = 361071.56 (1.688 sec)\n",
            "train - step 4743: loss = 324172.19 (1.681 sec)\n",
            "train - step 4744: loss = 322588.81 (1.677 sec)\n",
            "train - step 4745: loss = 371361.78 (1.677 sec)\n",
            "train - step 4746: loss = 373863.53 (1.683 sec)\n",
            "train - step 4747: loss = 411922.81 (1.676 sec)\n",
            "train - step 4748: loss = 403223.81 (1.671 sec)\n",
            "train - step 4749: loss = 541909.88 (1.684 sec)\n",
            "train - step 4750: loss = 459847.62 (1.676 sec)\n",
            "train - step 4751: loss = 415167.78 (1.681 sec)\n",
            "train - step 4752: loss = 479017.16 (1.660 sec)\n",
            "train - step 4753: loss = 398492.94 (1.703 sec)\n",
            "train - step 4754: loss = 342812.84 (1.705 sec)\n",
            "train - step 4755: loss = 536070.06 (1.690 sec)\n",
            "train - step 4756: loss = 430584.06 (1.695 sec)\n",
            "train - step 4757: loss = 268382.03 (1.672 sec)\n",
            "train - step 4758: loss = 370038.06 (1.676 sec)\n",
            "train - step 4759: loss = 373158.34 (1.695 sec)\n",
            "train - step 4760: loss = 486649.56 (1.696 sec)\n",
            "train - step 4761: loss = 574098.62 (1.690 sec)\n",
            "train - step 4762: loss = 474409.09 (2.636 sec)\n",
            "train - step 4763: loss = 355435.66 (1.715 sec)\n",
            "train - step 4764: loss = 386637.94 (1.669 sec)\n",
            "train - step 4765: loss = 440518.22 (1.687 sec)\n",
            "train - step 4766: loss = 268689.03 (1.696 sec)\n",
            "train - step 4767: loss = 359588.12 (1.687 sec)\n",
            "train - step 4768: loss = 385984.97 (1.676 sec)\n",
            "train - step 4769: loss = 400035.50 (1.674 sec)\n",
            "train - step 4770: loss = 393794.38 (1.695 sec)\n",
            "train - step 4771: loss = 364810.59 (1.667 sec)\n",
            "train - step 4772: loss = 371227.84 (1.687 sec)\n",
            "train - step 4773: loss = 421523.12 (1.680 sec)\n",
            "train - step 4774: loss = 492044.19 (1.686 sec)\n",
            "train - step 4775: loss = 423739.88 (1.708 sec)\n",
            "train - step 4776: loss = 363701.00 (1.675 sec)\n",
            "train - step 4777: loss = 357911.53 (1.665 sec)\n",
            "train - step 4778: loss = 560829.31 (1.678 sec)\n",
            "train - step 4779: loss = 407180.59 (1.687 sec)\n",
            "train - step 4780: loss = 460780.97 (1.681 sec)\n",
            "train - step 4781: loss = 415595.66 (1.672 sec)\n",
            "train - step 4782: loss = 418273.62 (1.683 sec)\n",
            "train - step 4783: loss = 342153.41 (1.675 sec)\n",
            "train - step 4784: loss = 289544.16 (1.672 sec)\n",
            "train - step 4785: loss = 571070.56 (1.661 sec)\n",
            "train - step 4786: loss = 420355.69 (1.670 sec)\n",
            "train - step 4787: loss = 332875.84 (1.674 sec)\n",
            "train - step 4788: loss = 348966.66 (1.680 sec)\n",
            "train - step 4789: loss = 357047.91 (1.655 sec)\n",
            "train - step 4790: loss = 393340.28 (1.646 sec)\n",
            "train - step 4791: loss = 423670.19 (1.651 sec)\n",
            "train - step 4792: loss = 412664.38 (1.682 sec)\n",
            "train - step 4793: loss = 366661.44 (1.682 sec)\n",
            "train - step 4794: loss = 501177.19 (1.666 sec)\n",
            "train - step 4795: loss = 500709.56 (1.686 sec)\n",
            "train - step 4796: loss = 392594.19 (1.696 sec)\n",
            "train - step 4797: loss = 312957.06 (1.695 sec)\n",
            "train - step 4798: loss = 377810.34 (2.501 sec)\n",
            "train - step 4799: loss = 526544.38 (1.681 sec)\n",
            "train - step 4800: loss = 360100.94 (1.675 sec)\n",
            "train - step 4801: loss = 443816.50 (1.692 sec)\n",
            "train - step 4802: loss = 379254.94 (1.672 sec)\n",
            "train - step 4803: loss = 447376.00 (1.670 sec)\n",
            "train - step 4804: loss = 425284.56 (1.661 sec)\n",
            "train - step 4805: loss = 342907.69 (1.688 sec)\n",
            "train - step 4806: loss = 358248.72 (1.681 sec)\n",
            "train - step 4807: loss = 400424.88 (1.689 sec)\n",
            "train - step 4808: loss = 624487.44 (1.692 sec)\n",
            "train - step 4809: loss = 432886.09 (1.659 sec)\n",
            "train - step 4810: loss = 448763.12 (1.676 sec)\n",
            "train - step 4811: loss = 337733.47 (1.669 sec)\n",
            "train - step 4812: loss = 519755.47 (1.692 sec)\n",
            "train - step 4813: loss = 435759.94 (1.675 sec)\n",
            "train - step 4814: loss = 420961.69 (1.666 sec)\n",
            "train - step 4815: loss = 380487.94 (1.679 sec)\n",
            "train - step 4816: loss = 427972.22 (1.675 sec)\n",
            "train - step 4817: loss = 313544.94 (1.677 sec)\n",
            "train - step 4818: loss = 341562.53 (1.685 sec)\n",
            "train - step 4819: loss = 558953.56 (1.691 sec)\n",
            "train - step 4820: loss = 478423.72 (1.698 sec)\n",
            "train - step 4821: loss = 400528.38 (1.653 sec)\n",
            "train - step 4822: loss = 246580.27 (1.643 sec)\n",
            "train - step 4823: loss = 328831.69 (1.639 sec)\n",
            "train - step 4824: loss = 321084.22 (1.654 sec)\n",
            "train - step 4825: loss = 391314.47 (1.631 sec)\n",
            "train - step 4826: loss = 339647.78 (1.665 sec)\n",
            "train - step 4827: loss = 385546.72 (1.642 sec)\n",
            "train - step 4828: loss = 343631.41 (1.648 sec)\n",
            "train - step 4829: loss = 288481.09 (1.689 sec)\n",
            "train - step 4830: loss = 267091.59 (1.672 sec)\n",
            "train - step 4831: loss = 362776.88 (1.714 sec)\n",
            "train - step 4832: loss = 348305.91 (1.674 sec)\n",
            "train - step 4833: loss = 349977.81 (1.669 sec)\n",
            "train - step 4834: loss = 536024.44 (2.814 sec)\n",
            "train - step 4835: loss = 297557.00 (1.690 sec)\n",
            "train - step 4836: loss = 305021.78 (1.696 sec)\n",
            "train - step 4837: loss = 340045.34 (1.688 sec)\n",
            "train - step 4838: loss = 477981.47 (1.693 sec)\n",
            "train - step 4839: loss = 445751.28 (1.663 sec)\n",
            "train - step 4840: loss = 495764.12 (1.674 sec)\n",
            "train - step 4841: loss = 344545.66 (1.657 sec)\n",
            "train - step 4842: loss = 485708.84 (1.672 sec)\n",
            "train - step 4843: loss = 535066.69 (1.678 sec)\n",
            "train - step 4844: loss = 363173.59 (1.662 sec)\n",
            "train - step 4845: loss = 307283.97 (1.678 sec)\n",
            "train - step 4846: loss = 542276.31 (1.679 sec)\n",
            "train - step 4847: loss = 286424.16 (1.678 sec)\n",
            "train - step 4848: loss = 404994.06 (1.659 sec)\n",
            "train - step 4849: loss = 342312.69 (1.664 sec)\n",
            "train - step 4850: loss = 511027.34 (1.700 sec)\n",
            "train - step 4851: loss = 353834.81 (1.684 sec)\n",
            "train - step 4852: loss = 350968.19 (1.666 sec)\n",
            "train - step 4853: loss = 405933.66 (1.679 sec)\n",
            "train - step 4854: loss = 522045.53 (1.650 sec)\n",
            "train - step 4855: loss = 578069.06 (1.686 sec)\n",
            "train - step 4856: loss = 423519.47 (1.672 sec)\n",
            "train - step 4857: loss = 416567.34 (1.670 sec)\n",
            "train - step 4858: loss = 603161.00 (1.665 sec)\n",
            "train - step 4859: loss = 515770.06 (1.704 sec)\n",
            "train - step 4860: loss = 464539.03 (1.672 sec)\n",
            "train - step 4861: loss = 343214.81 (1.672 sec)\n",
            "train - step 4862: loss = 329055.97 (1.693 sec)\n",
            "train - step 4863: loss = 316641.56 (1.662 sec)\n",
            "train - step 4864: loss = 436624.16 (1.703 sec)\n",
            "train - step 4865: loss = 496251.47 (1.672 sec)\n",
            "train - step 4866: loss = 279789.78 (1.677 sec)\n",
            "train - step 4867: loss = 413031.59 (1.686 sec)\n",
            "train - step 4868: loss = 386783.69 (1.667 sec)\n",
            "train - step 4869: loss = 432382.00 (1.684 sec)\n",
            "train - step 4870: loss = 372991.00 (2.801 sec)\n",
            "train - step 4871: loss = 299171.16 (1.696 sec)\n",
            "train - step 4872: loss = 466131.97 (1.677 sec)\n",
            "train - step 4873: loss = 372519.28 (1.677 sec)\n",
            "train - step 4874: loss = 554333.56 (1.680 sec)\n",
            "train - step 4875: loss = 496754.91 (1.675 sec)\n",
            "train - step 4876: loss = 415292.03 (1.680 sec)\n",
            "train - step 4877: loss = 503529.47 (1.690 sec)\n",
            "train - step 4878: loss = 363427.81 (1.673 sec)\n",
            "train - step 4879: loss = 401952.69 (1.664 sec)\n",
            "train - step 4880: loss = 347668.47 (1.699 sec)\n",
            "train - step 4881: loss = 362021.47 (1.679 sec)\n",
            "train - step 4882: loss = 368872.09 (1.687 sec)\n",
            "train - step 4883: loss = 605987.12 (1.660 sec)\n",
            "train - step 4884: loss = 270933.47 (1.696 sec)\n",
            "train - step 4885: loss = 388621.72 (1.694 sec)\n",
            "train - step 4886: loss = 534317.81 (1.662 sec)\n",
            "train - step 4887: loss = 462755.97 (1.666 sec)\n",
            "train - step 4888: loss = 378207.78 (1.679 sec)\n",
            "train - step 4889: loss = 267475.84 (1.668 sec)\n",
            "train - step 4890: loss = 381901.41 (1.672 sec)\n",
            "train - step 4891: loss = 441084.81 (1.681 sec)\n",
            "train - step 4892: loss = 499419.22 (1.659 sec)\n",
            "train - step 4893: loss = 478351.16 (1.660 sec)\n",
            "train - step 4894: loss = 399018.19 (1.666 sec)\n",
            "train - step 4895: loss = 322961.91 (1.668 sec)\n",
            "train - step 4896: loss = 393257.69 (1.676 sec)\n",
            "train - step 4897: loss = 466214.09 (1.659 sec)\n",
            "train - step 4898: loss = 394930.00 (1.675 sec)\n",
            "train - step 4899: loss = 385089.66 (1.666 sec)\n",
            "train - step 4900: loss = 444311.97 (1.679 sec)\n",
            "train - step 4901: loss = 365306.22 (1.675 sec)\n",
            "train - step 4902: loss = 444811.22 (1.695 sec)\n",
            "train - step 4903: loss = 424082.84 (1.658 sec)\n",
            "train - step 4904: loss = 394101.50 (1.675 sec)\n",
            "train - step 4905: loss = 416449.69 (1.681 sec)\n",
            "train - step 4906: loss = 350485.50 (2.643 sec)\n",
            "train - step 4907: loss = 471162.47 (1.696 sec)\n",
            "train - step 4908: loss = 364366.81 (1.666 sec)\n",
            "train - step 4909: loss = 396936.12 (1.669 sec)\n",
            "train - step 4910: loss = 476272.38 (1.687 sec)\n",
            "train - step 4911: loss = 456804.94 (1.673 sec)\n",
            "train - step 4912: loss = 473302.00 (1.693 sec)\n",
            "train - step 4913: loss = 385663.12 (1.669 sec)\n",
            "train - step 4914: loss = 513984.16 (1.702 sec)\n",
            "train - step 4915: loss = 355400.62 (1.674 sec)\n",
            "train - step 4916: loss = 549352.81 (1.680 sec)\n",
            "train - step 4917: loss = 333482.84 (1.696 sec)\n",
            "train - step 4918: loss = 339923.66 (1.674 sec)\n",
            "train - step 4919: loss = 370014.88 (1.706 sec)\n",
            "train - step 4920: loss = 521055.91 (1.707 sec)\n",
            "train - step 4921: loss = 398906.09 (1.672 sec)\n",
            "train - step 4922: loss = 377120.16 (1.683 sec)\n",
            "train - step 4923: loss = 325326.59 (1.668 sec)\n",
            "train - step 4924: loss = 288994.09 (1.677 sec)\n",
            "train - step 4925: loss = 377705.41 (1.689 sec)\n",
            "train - step 4926: loss = 354265.88 (1.688 sec)\n",
            "train - step 4927: loss = 332287.59 (1.673 sec)\n",
            "train - step 4928: loss = 225593.28 (1.680 sec)\n",
            "train - step 4929: loss = 423007.62 (1.676 sec)\n",
            "train - step 4930: loss = 364064.66 (1.671 sec)\n",
            "train - step 4931: loss = 247230.36 (1.693 sec)\n",
            "train - step 4932: loss = 394778.38 (1.695 sec)\n",
            "train - step 4933: loss = 364668.00 (1.699 sec)\n",
            "train - step 4934: loss = 359682.41 (1.681 sec)\n",
            "train - step 4935: loss = 473107.69 (1.723 sec)\n",
            "train - step 4936: loss = 341608.66 (1.692 sec)\n",
            "train - step 4937: loss = 385455.62 (1.693 sec)\n",
            "train - step 4938: loss = 483531.44 (1.687 sec)\n",
            "train - step 4939: loss = 418546.81 (1.711 sec)\n",
            "train - step 4940: loss = 365948.06 (1.702 sec)\n",
            "train - step 4941: loss = 407245.78 (1.686 sec)\n",
            "train - step 4942: loss = 475315.03 (1.697 sec)\n",
            "train - step 4943: loss = 299230.59 (1.679 sec)\n",
            "train - step 4944: loss = 451721.62 (1.668 sec)\n",
            "train - step 4945: loss = 392086.19 (1.672 sec)\n",
            "train - step 4946: loss = 391741.34 (1.682 sec)\n",
            "train - step 4947: loss = 442934.78 (1.699 sec)\n",
            "train - step 4948: loss = 494370.19 (1.678 sec)\n",
            "train - step 4949: loss = 694143.19 (1.714 sec)\n",
            "train - step 4950: loss = 477136.56 (1.806 sec)\n",
            "train - step 4951: loss = 449727.88 (1.689 sec)\n",
            "train - step 4952: loss = 387097.59 (1.660 sec)\n",
            "train - step 4953: loss = 416813.94 (1.701 sec)\n",
            "train - step 4954: loss = 436233.09 (1.680 sec)\n",
            "train - step 4955: loss = 344654.41 (1.659 sec)\n",
            "train - step 4956: loss = 354481.53 (1.682 sec)\n",
            "train - step 4957: loss = 388897.81 (1.680 sec)\n",
            "train - step 4958: loss = 427053.50 (1.698 sec)\n",
            "train - step 4959: loss = 343049.31 (1.689 sec)\n",
            "train - step 4960: loss = 395182.28 (1.679 sec)\n",
            "train - step 4961: loss = 347941.31 (1.676 sec)\n",
            "train - step 4962: loss = 377561.00 (1.680 sec)\n",
            "train - step 4963: loss = 325594.09 (1.667 sec)\n",
            "train - step 4964: loss = 414138.22 (1.677 sec)\n",
            "train - step 4965: loss = 455766.03 (1.683 sec)\n",
            "train - step 4966: loss = 522371.03 (1.680 sec)\n",
            "train - step 4967: loss = 356269.28 (1.708 sec)\n",
            "train - step 4968: loss = 550878.19 (1.664 sec)\n",
            "train - step 4969: loss = 346586.00 (1.659 sec)\n",
            "train - step 4970: loss = 251253.41 (1.639 sec)\n",
            "train - step 4971: loss = 537146.94 (1.670 sec)\n",
            "train - step 4972: loss = 306257.97 (1.671 sec)\n",
            "train - step 4973: loss = 422801.53 (1.665 sec)\n",
            "train - step 4974: loss = 435140.28 (1.682 sec)\n",
            "train - step 4975: loss = 332363.59 (1.675 sec)\n",
            "train - step 4976: loss = 420675.84 (1.669 sec)\n",
            "train - step 4977: loss = 289636.97 (1.671 sec)\n",
            "train - step 4978: loss = 377781.28 (3.002 sec)\n",
            "train - step 4979: loss = 401004.62 (1.699 sec)\n",
            "train - step 4980: loss = 351969.28 (1.693 sec)\n",
            "train - step 4981: loss = 504822.81 (1.682 sec)\n",
            "train - step 4982: loss = 468990.91 (1.688 sec)\n",
            "train - step 4983: loss = 427043.47 (1.692 sec)\n",
            "train - step 4984: loss = 354646.53 (1.686 sec)\n",
            "train - step 4985: loss = 430223.12 (1.672 sec)\n",
            "train - step 4986: loss = 641047.62 (1.676 sec)\n",
            "train - step 4987: loss = 358671.50 (1.667 sec)\n",
            "train - step 4988: loss = 472901.69 (1.665 sec)\n",
            "train - step 4989: loss = 496249.03 (1.680 sec)\n",
            "train - step 4990: loss = 512028.06 (1.673 sec)\n",
            "train - step 4991: loss = 441014.28 (1.680 sec)\n",
            "train - step 4992: loss = 284521.66 (1.678 sec)\n",
            "train - step 4993: loss = 305063.12 (1.667 sec)\n",
            "train - step 4994: loss = 457690.84 (1.703 sec)\n",
            "train - step 4995: loss = 502017.88 (1.664 sec)\n",
            "train - step 4996: loss = 497561.88 (1.689 sec)\n",
            "train - step 4997: loss = 298370.91 (1.689 sec)\n",
            "train - step 4998: loss = 407225.53 (1.683 sec)\n",
            "train - step 4999: loss = 388284.31 (1.695 sec)\n",
            "train - step 5000: loss = 425852.50 (1.702 sec)\n",
            "train - step 5001: loss = 359271.06 (1.680 sec)\n",
            "train - step 5002: loss = 509353.19 (1.690 sec)\n",
            "train - step 5003: loss = 399380.62 (1.666 sec)\n",
            "train - step 5004: loss = 347671.03 (1.667 sec)\n",
            "train - step 5005: loss = 513552.47 (1.658 sec)\n",
            "train - step 5006: loss = 395937.66 (1.622 sec)\n",
            "train - step 5007: loss = 364410.19 (1.644 sec)\n",
            "train - step 5008: loss = 353878.78 (1.642 sec)\n",
            "train - step 5009: loss = 401533.66 (1.638 sec)\n",
            "train - step 5010: loss = 418802.31 (1.655 sec)\n",
            "train - step 5011: loss = 420181.00 (1.664 sec)\n",
            "train - step 5012: loss = 685356.12 (1.690 sec)\n",
            "train - step 5013: loss = 456747.72 (1.673 sec)\n",
            "train - step 5014: loss = 293266.56 (2.221 sec)\n",
            "train - step 5015: loss = 303168.94 (1.669 sec)\n",
            "train - step 5016: loss = 335417.47 (1.668 sec)\n",
            "train - step 5017: loss = 483548.50 (1.686 sec)\n",
            "train - step 5018: loss = 425252.56 (1.676 sec)\n",
            "train - step 5019: loss = 416865.66 (1.684 sec)\n",
            "train - step 5020: loss = 337801.91 (1.672 sec)\n",
            "train - step 5021: loss = 533062.56 (1.665 sec)\n",
            "train - step 5022: loss = 315206.53 (1.684 sec)\n",
            "train - step 5023: loss = 280271.50 (1.675 sec)\n",
            "train - step 5024: loss = 398841.22 (1.664 sec)\n",
            "train - step 5025: loss = 435688.41 (1.686 sec)\n",
            "train - step 5026: loss = 352170.22 (1.672 sec)\n",
            "train - step 5027: loss = 347729.16 (1.714 sec)\n",
            "train - step 5028: loss = 407274.88 (1.681 sec)\n",
            "train - step 5029: loss = 510348.97 (1.665 sec)\n",
            "train - step 5030: loss = 405244.44 (1.696 sec)\n",
            "train - step 5031: loss = 450105.62 (1.666 sec)\n",
            "train - step 5032: loss = 311716.00 (1.673 sec)\n",
            "train - step 5033: loss = 417819.72 (1.685 sec)\n",
            "train - step 5034: loss = 393769.56 (1.687 sec)\n",
            "train - step 5035: loss = 468491.19 (1.682 sec)\n",
            "train - step 5036: loss = 313972.66 (1.677 sec)\n",
            "train - step 5037: loss = 298622.06 (1.676 sec)\n",
            "train - step 5038: loss = 432263.72 (1.679 sec)\n",
            "train - step 5039: loss = 408680.50 (1.673 sec)\n",
            "train - step 5040: loss = 459650.62 (1.676 sec)\n",
            "train - step 5041: loss = 385850.38 (1.678 sec)\n",
            "train - step 5042: loss = 287582.03 (1.667 sec)\n",
            "train - step 5043: loss = 415007.22 (1.673 sec)\n",
            "train - step 5044: loss = 290318.28 (1.686 sec)\n",
            "train - step 5045: loss = 408237.34 (1.675 sec)\n",
            "train - step 5046: loss = 359962.47 (1.678 sec)\n",
            "train - step 5047: loss = 344999.06 (1.684 sec)\n",
            "train - step 5048: loss = 512880.62 (1.670 sec)\n",
            "train - step 5049: loss = 494271.12 (1.682 sec)\n",
            "train - step 5050: loss = 267257.28 (2.528 sec)\n",
            "train - step 5051: loss = 358163.59 (1.708 sec)\n",
            "train - step 5052: loss = 182846.14 (1.663 sec)\n",
            "train - step 5053: loss = 578753.62 (1.695 sec)\n",
            "train - step 5054: loss = 352005.00 (1.677 sec)\n",
            "train - step 5055: loss = 345340.94 (1.688 sec)\n",
            "train - step 5056: loss = 376159.03 (1.669 sec)\n",
            "train - step 5057: loss = 323046.00 (1.682 sec)\n",
            "train - step 5058: loss = 548566.44 (1.688 sec)\n",
            "train - step 5059: loss = 458521.44 (1.677 sec)\n",
            "train - step 5060: loss = 408852.84 (1.678 sec)\n",
            "train - step 5061: loss = 606303.00 (1.676 sec)\n",
            "train - step 5062: loss = 392823.22 (1.668 sec)\n",
            "train - step 5063: loss = 422421.62 (1.680 sec)\n",
            "train - step 5064: loss = 461762.38 (1.671 sec)\n",
            "train - step 5065: loss = 317294.50 (1.684 sec)\n",
            "train - step 5066: loss = 481115.97 (1.700 sec)\n",
            "train - step 5067: loss = 318725.41 (1.694 sec)\n",
            "train - step 5068: loss = 570122.38 (1.666 sec)\n",
            "train - step 5069: loss = 357899.66 (1.685 sec)\n",
            "train - step 5070: loss = 485852.72 (1.686 sec)\n",
            "train - step 5071: loss = 405468.91 (1.675 sec)\n",
            "train - step 5072: loss = 354057.66 (1.698 sec)\n",
            "train - step 5073: loss = 422233.44 (1.682 sec)\n",
            "train - step 5074: loss = 511117.91 (1.680 sec)\n",
            "train - step 5075: loss = 559524.06 (1.689 sec)\n",
            "train - step 5076: loss = 517308.16 (1.676 sec)\n",
            "train - step 5077: loss = 439199.66 (1.681 sec)\n",
            "train - step 5078: loss = 422736.06 (1.697 sec)\n",
            "train - step 5079: loss = 386779.69 (1.668 sec)\n",
            "train - step 5080: loss = 426648.09 (1.684 sec)\n",
            "train - step 5081: loss = 409137.44 (1.676 sec)\n",
            "train - step 5082: loss = 449516.91 (1.675 sec)\n",
            "train - step 5083: loss = 346219.09 (1.698 sec)\n",
            "train - step 5084: loss = 374829.28 (1.708 sec)\n",
            "train - step 5085: loss = 656573.62 (1.673 sec)\n",
            "train - step 5086: loss = 447412.97 (2.525 sec)\n",
            "train - step 5087: loss = 468703.34 (1.677 sec)\n",
            "train - step 5088: loss = 540907.31 (1.668 sec)\n",
            "train - step 5089: loss = 548504.06 (1.681 sec)\n",
            "train - step 5090: loss = 404120.47 (1.695 sec)\n",
            "train - step 5091: loss = 464284.69 (1.697 sec)\n",
            "train - step 5092: loss = 456691.56 (1.673 sec)\n",
            "train - step 5093: loss = 480400.62 (1.684 sec)\n",
            "train - step 5094: loss = 404017.41 (1.679 sec)\n",
            "train - step 5095: loss = 432869.53 (1.679 sec)\n",
            "train - step 5096: loss = 330824.03 (1.673 sec)\n",
            "train - step 5097: loss = 359673.97 (1.669 sec)\n",
            "train - step 5098: loss = 299764.59 (1.668 sec)\n",
            "train - step 5099: loss = 604045.56 (1.675 sec)\n",
            "train - step 5100: loss = 341339.72 (1.679 sec)\n",
            "train - step 5101: loss = 371162.19 (1.692 sec)\n",
            "train - step 5102: loss = 394505.47 (1.680 sec)\n",
            "train - step 5103: loss = 351349.44 (1.703 sec)\n",
            "train - step 5104: loss = 456990.53 (1.689 sec)\n",
            "train - step 5105: loss = 335947.81 (1.674 sec)\n",
            "train - step 5106: loss = 388843.50 (1.676 sec)\n",
            "train - step 5107: loss = 361285.81 (1.690 sec)\n",
            "train - step 5108: loss = 390354.91 (1.681 sec)\n",
            "train - step 5109: loss = 469473.06 (1.664 sec)\n",
            "train - step 5110: loss = 388087.34 (1.686 sec)\n",
            "train - step 5111: loss = 431918.66 (1.672 sec)\n",
            "train - step 5112: loss = 394904.56 (1.672 sec)\n",
            "train - step 5113: loss = 449133.38 (1.683 sec)\n",
            "train - step 5114: loss = 340406.41 (1.664 sec)\n",
            "train - step 5115: loss = 439157.41 (1.683 sec)\n",
            "train - step 5116: loss = 431566.47 (1.684 sec)\n",
            "train - step 5117: loss = 440506.16 (1.686 sec)\n",
            "train - step 5118: loss = 494733.97 (1.696 sec)\n",
            "train - step 5119: loss = 424653.47 (1.681 sec)\n",
            "train - step 5120: loss = 427007.72 (1.681 sec)\n",
            "train - step 5121: loss = 591223.06 (1.686 sec)\n",
            "train - step 5122: loss = 403989.22 (2.604 sec)\n",
            "train - step 5123: loss = 385462.28 (1.690 sec)\n",
            "train - step 5124: loss = 360199.91 (1.707 sec)\n",
            "train - step 5125: loss = 441032.53 (1.678 sec)\n",
            "train - step 5126: loss = 364757.97 (1.679 sec)\n",
            "train - step 5127: loss = 420753.53 (1.674 sec)\n",
            "train - step 5128: loss = 535242.19 (1.690 sec)\n",
            "train - step 5129: loss = 494323.50 (1.672 sec)\n",
            "train - step 5130: loss = 396675.88 (1.671 sec)\n",
            "train - step 5131: loss = 467909.19 (1.682 sec)\n",
            "train - step 5132: loss = 538918.56 (1.672 sec)\n",
            "train - step 5133: loss = 417996.88 (1.676 sec)\n",
            "train - step 5134: loss = 633218.75 (1.684 sec)\n",
            "train - step 5135: loss = 371455.50 (1.685 sec)\n",
            "train - step 5136: loss = 402988.53 (1.664 sec)\n",
            "train - step 5137: loss = 458609.00 (1.679 sec)\n",
            "train - step 5138: loss = 250193.53 (1.655 sec)\n",
            "train - step 5139: loss = 673690.00 (1.690 sec)\n",
            "train - step 5140: loss = 540231.12 (1.671 sec)\n",
            "train - step 5141: loss = 427903.31 (1.699 sec)\n",
            "train - step 5142: loss = 495579.28 (1.687 sec)\n",
            "train - step 5143: loss = 448571.84 (1.668 sec)\n",
            "train - step 5144: loss = 479145.78 (1.672 sec)\n",
            "train - step 5145: loss = 343796.38 (1.670 sec)\n",
            "train - step 5146: loss = 395624.97 (1.679 sec)\n",
            "train - step 5147: loss = 357854.94 (1.677 sec)\n",
            "train - step 5148: loss = 354520.44 (1.640 sec)\n",
            "train - step 5149: loss = 394910.97 (1.632 sec)\n",
            "train - step 5150: loss = 408074.91 (1.632 sec)\n",
            "train - step 5151: loss = 335982.28 (1.666 sec)\n",
            "train - step 5152: loss = 474112.06 (1.679 sec)\n",
            "train - step 5153: loss = 562896.31 (1.705 sec)\n",
            "train - step 5154: loss = 378689.69 (1.683 sec)\n",
            "train - step 5155: loss = 414013.44 (1.668 sec)\n",
            "train - step 5156: loss = 501557.19 (1.676 sec)\n",
            "train - step 5157: loss = 374974.91 (1.691 sec)\n",
            "train - step 5158: loss = 445786.88 (3.041 sec)\n",
            "train - step 5159: loss = 412947.22 (1.685 sec)\n",
            "train - step 5160: loss = 288374.72 (1.682 sec)\n",
            "train - step 5161: loss = 450078.16 (1.683 sec)\n",
            "train - step 5162: loss = 486176.78 (1.684 sec)\n",
            "train - step 5163: loss = 493662.47 (1.679 sec)\n",
            "train - step 5164: loss = 349859.28 (1.686 sec)\n",
            "train - step 5165: loss = 393160.22 (1.665 sec)\n",
            "train - step 5166: loss = 399846.09 (1.698 sec)\n",
            "train - step 5167: loss = 332109.97 (1.690 sec)\n",
            "train - step 5168: loss = 250545.58 (1.679 sec)\n",
            "train - step 5169: loss = 327932.22 (1.661 sec)\n",
            "train - step 5170: loss = 454215.97 (1.681 sec)\n",
            "train - step 5171: loss = 352075.03 (1.681 sec)\n",
            "train - step 5172: loss = 256881.75 (1.677 sec)\n",
            "train - step 5173: loss = 497501.53 (1.666 sec)\n",
            "train - step 5174: loss = 406251.03 (1.677 sec)\n",
            "train - step 5175: loss = 408323.69 (1.675 sec)\n",
            "train - step 5176: loss = 454711.72 (1.679 sec)\n",
            "train - step 5177: loss = 459628.91 (1.670 sec)\n",
            "train - step 5178: loss = 438572.81 (1.688 sec)\n",
            "train - step 5179: loss = 374744.00 (1.683 sec)\n",
            "train - step 5180: loss = 511678.38 (1.675 sec)\n",
            "train - step 5181: loss = 508315.44 (1.685 sec)\n",
            "train - step 5182: loss = 279418.19 (1.673 sec)\n",
            "train - step 5183: loss = 332194.88 (1.680 sec)\n",
            "train - step 5184: loss = 445838.06 (1.667 sec)\n",
            "train - step 5185: loss = 377619.00 (1.684 sec)\n",
            "train - step 5186: loss = 407541.38 (1.672 sec)\n",
            "train - step 5187: loss = 303513.38 (1.648 sec)\n",
            "train - step 5188: loss = 387776.28 (1.647 sec)\n",
            "train - step 5189: loss = 480852.12 (1.637 sec)\n",
            "train - step 5190: loss = 532515.94 (1.637 sec)\n",
            "train - step 5191: loss = 399454.00 (1.653 sec)\n",
            "train - step 5192: loss = 310456.22 (1.644 sec)\n",
            "train - step 5193: loss = 366333.03 (1.669 sec)\n",
            "train - step 5194: loss = 388846.47 (2.578 sec)\n",
            "train - step 5195: loss = 427099.94 (1.695 sec)\n",
            "train - step 5196: loss = 545226.25 (1.672 sec)\n",
            "train - step 5197: loss = 414912.50 (1.669 sec)\n",
            "train - step 5198: loss = 357574.94 (1.683 sec)\n",
            "train - step 5199: loss = 377102.00 (1.683 sec)\n",
            "train - step 5200: loss = 268427.53 (1.682 sec)\n",
            "train - step 5201: loss = 355121.19 (1.708 sec)\n",
            "train - step 5202: loss = 562213.12 (1.674 sec)\n",
            "train - step 5203: loss = 293010.00 (1.664 sec)\n",
            "train - step 5204: loss = 440312.00 (1.691 sec)\n",
            "train - step 5205: loss = 393749.00 (1.669 sec)\n",
            "train - step 5206: loss = 491230.56 (1.683 sec)\n",
            "train - step 5207: loss = 414480.66 (1.664 sec)\n",
            "train - step 5208: loss = 320549.06 (1.672 sec)\n",
            "train - step 5209: loss = 367901.81 (1.685 sec)\n",
            "train - step 5210: loss = 501835.03 (1.673 sec)\n",
            "train - step 5211: loss = 294528.88 (1.693 sec)\n",
            "train - step 5212: loss = 293183.44 (1.680 sec)\n",
            "train - step 5213: loss = 398516.34 (1.680 sec)\n",
            "train - step 5214: loss = 558478.38 (1.686 sec)\n",
            "train - step 5215: loss = 492882.59 (1.703 sec)\n",
            "train - step 5216: loss = 380841.03 (1.673 sec)\n",
            "train - step 5217: loss = 384144.38 (1.675 sec)\n",
            "train - step 5218: loss = 440193.38 (1.671 sec)\n",
            "train - step 5219: loss = 284542.72 (1.681 sec)\n",
            "train - step 5220: loss = 260226.02 (1.679 sec)\n",
            "train - step 5221: loss = 341429.91 (1.699 sec)\n",
            "train - step 5222: loss = 429935.62 (1.692 sec)\n",
            "train - step 5223: loss = 490479.56 (1.689 sec)\n",
            "train - step 5224: loss = 340136.44 (1.694 sec)\n",
            "train - step 5225: loss = 269312.50 (1.670 sec)\n",
            "train - step 5226: loss = 261834.23 (1.668 sec)\n",
            "train - step 5227: loss = 346608.41 (1.658 sec)\n",
            "train - step 5228: loss = 513154.97 (1.688 sec)\n",
            "train - step 5229: loss = 467339.09 (1.694 sec)\n",
            "train - step 5230: loss = 445706.44 (2.657 sec)\n",
            "train - step 5231: loss = 399211.91 (1.741 sec)\n",
            "train - step 5232: loss = 361254.00 (1.683 sec)\n",
            "train - step 5233: loss = 353830.97 (1.709 sec)\n",
            "train - step 5234: loss = 486117.16 (1.694 sec)\n",
            "train - step 5235: loss = 332975.16 (1.722 sec)\n",
            "train - step 5236: loss = 382807.28 (1.703 sec)\n",
            "train - step 5237: loss = 448581.69 (1.695 sec)\n",
            "train - step 5238: loss = 315802.28 (1.684 sec)\n",
            "train - step 5239: loss = 453411.78 (1.681 sec)\n",
            "train - step 5240: loss = 317699.22 (1.874 sec)\n",
            "train - step 5241: loss = 465879.94 (1.682 sec)\n",
            "train - step 5242: loss = 445619.19 (1.693 sec)\n",
            "train - step 5243: loss = 467498.88 (1.666 sec)\n",
            "train - step 5244: loss = 252055.47 (1.671 sec)\n",
            "train - step 5245: loss = 608883.00 (1.681 sec)\n",
            "train - step 5246: loss = 424165.97 (1.678 sec)\n",
            "train - step 5247: loss = 525925.62 (1.669 sec)\n",
            "train - step 5248: loss = 419525.38 (1.687 sec)\n",
            "train - step 5249: loss = 434546.72 (1.660 sec)\n",
            "train - step 5250: loss = 507956.50 (1.675 sec)\n",
            "train - step 5251: loss = 424884.09 (1.669 sec)\n",
            "train - step 5252: loss = 297994.69 (1.675 sec)\n",
            "train - step 5253: loss = 415824.69 (1.671 sec)\n",
            "train - step 5254: loss = 228329.80 (1.672 sec)\n",
            "train - step 5255: loss = 445975.59 (1.696 sec)\n",
            "train - step 5256: loss = 408563.09 (1.669 sec)\n",
            "train - step 5257: loss = 416598.41 (1.692 sec)\n",
            "train - step 5258: loss = 377251.78 (1.692 sec)\n",
            "train - step 5259: loss = 352964.59 (1.685 sec)\n",
            "train - step 5260: loss = 349651.59 (1.676 sec)\n",
            "train - step 5261: loss = 290005.44 (1.683 sec)\n",
            "train - step 5262: loss = 428860.78 (1.668 sec)\n",
            "train - step 5263: loss = 435900.12 (1.675 sec)\n",
            "train - step 5264: loss = 379660.72 (1.700 sec)\n",
            "train - step 5265: loss = 428296.44 (1.680 sec)\n",
            "train - step 5266: loss = 494934.28 (2.252 sec)\n",
            "train - step 5267: loss = 474057.59 (1.691 sec)\n",
            "train - step 5268: loss = 342836.78 (1.693 sec)\n",
            "train - step 5269: loss = 459524.12 (1.659 sec)\n",
            "train - step 5270: loss = 409227.81 (1.676 sec)\n",
            "train - step 5271: loss = 412863.34 (1.671 sec)\n",
            "train - step 5272: loss = 422353.06 (1.673 sec)\n",
            "train - step 5273: loss = 282087.03 (1.695 sec)\n",
            "train - step 5274: loss = 504541.97 (1.681 sec)\n",
            "train - step 5275: loss = 384054.97 (1.683 sec)\n",
            "train - step 5276: loss = 462231.91 (1.684 sec)\n",
            "train - step 5277: loss = 359869.94 (1.667 sec)\n",
            "train - step 5278: loss = 421777.50 (1.666 sec)\n",
            "train - step 5279: loss = 424968.12 (1.658 sec)\n",
            "train - step 5280: loss = 361888.41 (1.689 sec)\n",
            "train - step 5281: loss = 332128.72 (1.672 sec)\n",
            "train - step 5282: loss = 441663.31 (1.692 sec)\n",
            "train - step 5283: loss = 495314.81 (1.689 sec)\n",
            "train - step 5284: loss = 393962.56 (1.687 sec)\n",
            "train - step 5285: loss = 351236.66 (1.718 sec)\n",
            "train - step 5286: loss = 414312.59 (1.666 sec)\n",
            "train - step 5287: loss = 507626.38 (1.707 sec)\n",
            "train - step 5288: loss = 281447.22 (1.672 sec)\n",
            "train - step 5289: loss = 375833.72 (1.684 sec)\n",
            "train - step 5290: loss = 541606.31 (1.688 sec)\n",
            "train - step 5291: loss = 383200.47 (1.703 sec)\n",
            "train - step 5292: loss = 436361.91 (1.705 sec)\n",
            "train - step 5293: loss = 231563.27 (1.680 sec)\n",
            "train - step 5294: loss = 370945.88 (1.692 sec)\n",
            "train - step 5295: loss = 420541.56 (1.686 sec)\n",
            "train - step 5296: loss = 536887.81 (1.705 sec)\n",
            "train - step 5297: loss = 401910.78 (1.664 sec)\n",
            "train - step 5298: loss = 384338.03 (1.678 sec)\n",
            "train - step 5299: loss = 309240.94 (1.702 sec)\n",
            "train - step 5300: loss = 405523.00 (1.674 sec)\n",
            "train - step 5301: loss = 379097.91 (1.681 sec)\n",
            "train - step 5302: loss = 455007.88 (2.777 sec)\n",
            "train - step 5303: loss = 461202.38 (1.677 sec)\n",
            "train - step 5304: loss = 378501.56 (1.682 sec)\n",
            "train - step 5305: loss = 398823.34 (1.658 sec)\n",
            "train - step 5306: loss = 359551.44 (1.666 sec)\n",
            "train - step 5307: loss = 384200.03 (1.689 sec)\n",
            "train - step 5308: loss = 521824.59 (1.682 sec)\n",
            "train - step 5309: loss = 443570.09 (1.687 sec)\n",
            "train - step 5310: loss = 403644.34 (1.673 sec)\n",
            "train - step 5311: loss = 526566.62 (1.693 sec)\n",
            "train - step 5312: loss = 379392.41 (1.658 sec)\n",
            "train - step 5313: loss = 394650.19 (1.668 sec)\n",
            "train - step 5314: loss = 423554.28 (1.691 sec)\n",
            "train - step 5315: loss = 474936.00 (1.671 sec)\n",
            "train - step 5316: loss = 305957.62 (1.662 sec)\n",
            "train - step 5317: loss = 381437.69 (1.686 sec)\n",
            "train - step 5318: loss = 544328.00 (1.681 sec)\n",
            "train - step 5319: loss = 429303.81 (1.681 sec)\n",
            "train - step 5320: loss = 381735.12 (1.676 sec)\n",
            "train - step 5321: loss = 399524.00 (1.679 sec)\n",
            "train - step 5322: loss = 401599.53 (1.681 sec)\n",
            "train - step 5323: loss = 370614.31 (1.669 sec)\n",
            "train - step 5324: loss = 318086.91 (1.650 sec)\n",
            "train - step 5325: loss = 572163.56 (1.670 sec)\n",
            "train - step 5326: loss = 608481.75 (1.665 sec)\n",
            "train - step 5327: loss = 392816.38 (1.650 sec)\n",
            "train - step 5328: loss = 483176.81 (1.624 sec)\n",
            "train - step 5329: loss = 451405.09 (1.636 sec)\n",
            "train - step 5330: loss = 457144.00 (1.656 sec)\n",
            "train - step 5331: loss = 365870.16 (1.675 sec)\n",
            "train - step 5332: loss = 362587.19 (1.686 sec)\n",
            "train - step 5333: loss = 390556.66 (1.676 sec)\n",
            "train - step 5334: loss = 475318.78 (1.672 sec)\n",
            "train - step 5335: loss = 366357.53 (1.704 sec)\n",
            "train - step 5336: loss = 507686.66 (1.678 sec)\n",
            "train - step 5337: loss = 244881.83 (1.653 sec)\n",
            "train - step 5338: loss = 399265.00 (2.535 sec)\n",
            "train - step 5339: loss = 417977.19 (1.665 sec)\n",
            "train - step 5340: loss = 417400.94 (1.682 sec)\n",
            "train - step 5341: loss = 250163.98 (1.686 sec)\n",
            "train - step 5342: loss = 445479.19 (1.688 sec)\n",
            "train - step 5343: loss = 256360.30 (1.689 sec)\n",
            "train - step 5344: loss = 399174.34 (1.676 sec)\n",
            "train - step 5345: loss = 475748.78 (1.691 sec)\n",
            "train - step 5346: loss = 357401.28 (1.695 sec)\n",
            "train - step 5347: loss = 544787.38 (1.668 sec)\n",
            "train - step 5348: loss = 418668.69 (1.676 sec)\n",
            "train - step 5349: loss = 441369.78 (1.665 sec)\n",
            "train - step 5350: loss = 412082.12 (1.682 sec)\n",
            "train - step 5351: loss = 346625.06 (1.680 sec)\n",
            "train - step 5352: loss = 600953.69 (1.685 sec)\n",
            "train - step 5353: loss = 365950.84 (1.678 sec)\n",
            "train - step 5354: loss = 450394.06 (1.667 sec)\n",
            "train - step 5355: loss = 426188.44 (1.674 sec)\n",
            "train - step 5356: loss = 283536.19 (1.670 sec)\n",
            "train - step 5357: loss = 399621.09 (1.668 sec)\n",
            "train - step 5358: loss = 466280.50 (1.685 sec)\n",
            "train - step 5359: loss = 389210.81 (1.709 sec)\n",
            "train - step 5360: loss = 248905.14 (1.673 sec)\n",
            "train - step 5361: loss = 490644.56 (1.686 sec)\n",
            "train - step 5362: loss = 458839.47 (1.670 sec)\n",
            "train - step 5363: loss = 315386.00 (1.698 sec)\n",
            "train - step 5364: loss = 307930.16 (1.677 sec)\n",
            "train - step 5365: loss = 564386.12 (1.691 sec)\n",
            "train - step 5366: loss = 285328.72 (1.681 sec)\n",
            "train - step 5367: loss = 347409.38 (1.667 sec)\n",
            "train - step 5368: loss = 439666.00 (1.680 sec)\n",
            "train - step 5369: loss = 471443.41 (1.656 sec)\n",
            "train - step 5370: loss = 358245.50 (1.655 sec)\n",
            "train - step 5371: loss = 380079.47 (1.650 sec)\n",
            "train - step 5372: loss = 563502.25 (1.678 sec)\n",
            "train - step 5373: loss = 397706.06 (1.651 sec)\n",
            "train - step 5374: loss = 399466.34 (2.537 sec)\n",
            "train - step 5375: loss = 468998.88 (1.691 sec)\n",
            "train - step 5376: loss = 413137.69 (1.657 sec)\n",
            "train - step 5377: loss = 431810.94 (1.711 sec)\n",
            "train - step 5378: loss = 485879.69 (1.678 sec)\n",
            "train - step 5379: loss = 391975.00 (1.658 sec)\n",
            "train - step 5380: loss = 330702.94 (1.675 sec)\n",
            "train - step 5381: loss = 422066.31 (1.686 sec)\n",
            "train - step 5382: loss = 515253.09 (1.667 sec)\n",
            "train - step 5383: loss = 427940.62 (1.691 sec)\n",
            "train - step 5384: loss = 380999.03 (1.683 sec)\n",
            "train - step 5385: loss = 458610.19 (1.702 sec)\n",
            "train - step 5386: loss = 518769.19 (1.655 sec)\n",
            "train - step 5387: loss = 437785.59 (1.670 sec)\n",
            "train - step 5388: loss = 397334.72 (1.712 sec)\n",
            "train - step 5389: loss = 439312.94 (1.674 sec)\n",
            "train - step 5390: loss = 361900.69 (1.677 sec)\n",
            "train - step 5391: loss = 421016.69 (1.690 sec)\n",
            "train - step 5392: loss = 427561.31 (1.667 sec)\n",
            "train - step 5393: loss = 383604.72 (1.679 sec)\n",
            "train - step 5394: loss = 488053.91 (1.674 sec)\n",
            "train - step 5395: loss = 386895.22 (1.679 sec)\n",
            "train - step 5396: loss = 452470.94 (1.689 sec)\n",
            "train - step 5397: loss = 346805.16 (1.686 sec)\n",
            "train - step 5398: loss = 303751.12 (1.690 sec)\n",
            "train - step 5399: loss = 411991.66 (1.675 sec)\n",
            "train - step 5400: loss = 532357.69 (1.674 sec)\n",
            "train - step 5401: loss = 464915.88 (1.689 sec)\n",
            "train - step 5402: loss = 359299.34 (1.670 sec)\n",
            "train - step 5403: loss = 219887.48 (1.662 sec)\n",
            "train - step 5404: loss = 405253.06 (1.687 sec)\n",
            "train - step 5405: loss = 545395.50 (1.671 sec)\n",
            "train - step 5406: loss = 451186.66 (1.687 sec)\n",
            "train - step 5407: loss = 469334.94 (1.691 sec)\n",
            "train - step 5408: loss = 427492.00 (1.687 sec)\n",
            "train - step 5409: loss = 485944.22 (1.686 sec)\n",
            "train - step 5410: loss = 406750.59 (2.532 sec)\n",
            "train - step 5411: loss = 555709.81 (1.710 sec)\n",
            "train - step 5412: loss = 252876.00 (1.670 sec)\n",
            "train - step 5413: loss = 304215.66 (1.690 sec)\n",
            "train - step 5414: loss = 520475.84 (1.681 sec)\n",
            "train - step 5415: loss = 420770.72 (1.670 sec)\n",
            "train - step 5416: loss = 441305.88 (1.690 sec)\n",
            "train - step 5417: loss = 359336.38 (1.689 sec)\n",
            "train - step 5418: loss = 534775.75 (1.669 sec)\n",
            "train - step 5419: loss = 542674.81 (1.668 sec)\n",
            "train - step 5420: loss = 467736.06 (1.692 sec)\n",
            "train - step 5421: loss = 452293.72 (1.666 sec)\n",
            "train - step 5422: loss = 435226.31 (1.679 sec)\n",
            "train - step 5423: loss = 467481.62 (1.702 sec)\n",
            "train - step 5424: loss = 322755.47 (1.696 sec)\n",
            "train - step 5425: loss = 533517.44 (1.702 sec)\n",
            "train - step 5426: loss = 359217.19 (1.675 sec)\n",
            "train - step 5427: loss = 387880.41 (1.684 sec)\n",
            "train - step 5428: loss = 346904.59 (1.666 sec)\n",
            "train - step 5429: loss = 467048.69 (1.665 sec)\n",
            "train - step 5430: loss = 449697.41 (1.685 sec)\n",
            "train - step 5431: loss = 418918.78 (1.684 sec)\n",
            "train - step 5432: loss = 383509.28 (1.701 sec)\n",
            "train - step 5433: loss = 414975.44 (1.695 sec)\n",
            "train - step 5434: loss = 392425.59 (1.687 sec)\n",
            "train - step 5435: loss = 453821.22 (1.703 sec)\n",
            "train - step 5436: loss = 502408.12 (1.683 sec)\n",
            "train - step 5437: loss = 353383.44 (1.708 sec)\n",
            "train - step 5438: loss = 512436.41 (1.688 sec)\n",
            "train - step 5439: loss = 352061.81 (1.708 sec)\n",
            "train - step 5440: loss = 315504.12 (1.676 sec)\n",
            "train - step 5441: loss = 320199.88 (1.685 sec)\n",
            "train - step 5442: loss = 330523.97 (1.709 sec)\n",
            "train - step 5443: loss = 449973.84 (1.674 sec)\n",
            "train - step 5444: loss = 438494.81 (1.682 sec)\n",
            "train - step 5445: loss = 394257.06 (1.713 sec)\n",
            "train - step 5446: loss = 359149.62 (3.104 sec)\n",
            "train - step 5447: loss = 429412.94 (1.665 sec)\n",
            "train - step 5448: loss = 287477.78 (1.679 sec)\n",
            "train - step 5449: loss = 411855.62 (1.692 sec)\n",
            "train - step 5450: loss = 452758.31 (1.662 sec)\n",
            "train - step 5451: loss = 376291.47 (1.697 sec)\n",
            "train - step 5452: loss = 549214.75 (1.672 sec)\n",
            "train - step 5453: loss = 399647.72 (1.673 sec)\n",
            "train - step 5454: loss = 374538.47 (1.692 sec)\n",
            "train - step 5455: loss = 420958.53 (1.672 sec)\n",
            "train - step 5456: loss = 470606.19 (1.689 sec)\n",
            "train - step 5457: loss = 385695.53 (1.696 sec)\n",
            "train - step 5458: loss = 474402.84 (1.679 sec)\n",
            "train - step 5459: loss = 284357.59 (1.673 sec)\n",
            "train - step 5460: loss = 468947.41 (1.672 sec)\n",
            "train - step 5461: loss = 369910.28 (1.706 sec)\n",
            "train - step 5462: loss = 520263.97 (1.668 sec)\n",
            "train - step 5463: loss = 453835.22 (1.682 sec)\n",
            "train - step 5464: loss = 516822.38 (1.693 sec)\n",
            "train - step 5465: loss = 302199.00 (1.694 sec)\n",
            "train - step 5466: loss = 232398.48 (1.676 sec)\n",
            "train - step 5467: loss = 553184.44 (1.689 sec)\n",
            "train - step 5468: loss = 383853.78 (1.683 sec)\n",
            "train - step 5469: loss = 509778.16 (1.675 sec)\n",
            "train - step 5470: loss = 422023.81 (1.689 sec)\n",
            "train - step 5471: loss = 443236.19 (1.698 sec)\n",
            "train - step 5472: loss = 430461.94 (1.674 sec)\n",
            "train - step 5473: loss = 415931.16 (1.705 sec)\n",
            "train - step 5474: loss = 521100.03 (1.666 sec)\n",
            "train - step 5475: loss = 456076.16 (1.667 sec)\n",
            "train - step 5476: loss = 418967.34 (1.697 sec)\n",
            "train - step 5477: loss = 357426.88 (1.683 sec)\n",
            "train - step 5478: loss = 384756.66 (1.667 sec)\n",
            "train - step 5479: loss = 320519.62 (1.679 sec)\n",
            "train - step 5480: loss = 354839.81 (1.668 sec)\n",
            "train - step 5481: loss = 577259.81 (1.680 sec)\n",
            "train - step 5482: loss = 501663.69 (2.553 sec)\n",
            "train - step 5483: loss = 431195.69 (1.684 sec)\n",
            "train - step 5484: loss = 446465.28 (1.678 sec)\n",
            "train - step 5485: loss = 453386.06 (1.694 sec)\n",
            "train - step 5486: loss = 342380.38 (1.684 sec)\n",
            "train - step 5487: loss = 482032.94 (1.687 sec)\n",
            "train - step 5488: loss = 442358.88 (1.681 sec)\n",
            "train - step 5489: loss = 460508.62 (1.696 sec)\n",
            "train - step 5490: loss = 406751.38 (1.672 sec)\n",
            "train - step 5491: loss = 360168.47 (1.680 sec)\n",
            "train - step 5492: loss = 336100.19 (1.679 sec)\n",
            "train - step 5493: loss = 296665.78 (1.681 sec)\n",
            "train - step 5494: loss = 402943.72 (1.677 sec)\n",
            "train - step 5495: loss = 507036.47 (1.673 sec)\n",
            "train - step 5496: loss = 403125.19 (1.663 sec)\n",
            "train - step 5497: loss = 487001.00 (1.700 sec)\n",
            "train - step 5498: loss = 568481.00 (1.671 sec)\n",
            "train - step 5499: loss = 333644.81 (1.674 sec)\n",
            "train - step 5500: loss = 410085.22 (1.677 sec)\n",
            "train - step 5501: loss = 403780.81 (1.683 sec)\n",
            "train - step 5502: loss = 410219.06 (1.679 sec)\n",
            "train - step 5503: loss = 440430.44 (1.710 sec)\n",
            "train - step 5504: loss = 339028.91 (1.670 sec)\n",
            "train - step 5505: loss = 327739.66 (1.683 sec)\n",
            "train - step 5506: loss = 416260.41 (1.639 sec)\n",
            "train - step 5507: loss = 248926.03 (1.654 sec)\n",
            "train - step 5508: loss = 291283.88 (1.635 sec)\n",
            "train - step 5509: loss = 446984.78 (1.674 sec)\n",
            "train - step 5510: loss = 497721.22 (1.678 sec)\n",
            "train - step 5511: loss = 365223.19 (1.681 sec)\n",
            "train - step 5512: loss = 527068.19 (1.680 sec)\n",
            "train - step 5513: loss = 366874.69 (1.692 sec)\n",
            "train - step 5514: loss = 421717.59 (1.684 sec)\n",
            "train - step 5515: loss = 381004.78 (1.680 sec)\n",
            "train - step 5516: loss = 427972.41 (1.690 sec)\n",
            "train - step 5517: loss = 504557.91 (1.673 sec)\n",
            "train - step 5518: loss = 450452.84 (2.568 sec)\n",
            "train - step 5519: loss = 300913.00 (1.671 sec)\n",
            "train - step 5520: loss = 541132.69 (1.691 sec)\n",
            "train - step 5521: loss = 278331.59 (1.742 sec)\n",
            "train - step 5522: loss = 472366.66 (1.699 sec)\n",
            "train - step 5523: loss = 446676.47 (1.698 sec)\n",
            "train - step 5524: loss = 376684.91 (1.696 sec)\n",
            "train - step 5525: loss = 433850.00 (1.676 sec)\n",
            "train - step 5526: loss = 572081.69 (1.676 sec)\n",
            "train - step 5527: loss = 346284.91 (1.720 sec)\n",
            "train - step 5528: loss = 296519.69 (1.696 sec)\n",
            "train - step 5529: loss = 347229.12 (1.685 sec)\n",
            "train - step 5530: loss = 346530.69 (1.683 sec)\n",
            "train - step 5531: loss = 462810.97 (1.689 sec)\n",
            "train - step 5532: loss = 338819.53 (1.678 sec)\n",
            "train - step 5533: loss = 426190.62 (1.719 sec)\n",
            "train - step 5534: loss = 358372.28 (1.690 sec)\n",
            "train - step 5535: loss = 436993.69 (1.681 sec)\n",
            "train - step 5536: loss = 282481.09 (1.669 sec)\n",
            "train - step 5537: loss = 413555.41 (1.693 sec)\n",
            "train - step 5538: loss = 403660.50 (1.679 sec)\n",
            "train - step 5539: loss = 414595.28 (1.681 sec)\n",
            "train - step 5540: loss = 392542.84 (1.676 sec)\n",
            "train - step 5541: loss = 371114.38 (1.655 sec)\n",
            "train - step 5542: loss = 442316.66 (1.681 sec)\n",
            "train - step 5543: loss = 385947.28 (1.684 sec)\n",
            "train - step 5544: loss = 348771.19 (1.677 sec)\n",
            "train - step 5545: loss = 303146.84 (1.679 sec)\n",
            "train - step 5546: loss = 292976.06 (1.693 sec)\n",
            "train - step 5547: loss = 477817.28 (1.696 sec)\n",
            "train - step 5548: loss = 438674.06 (1.701 sec)\n",
            "train - step 5549: loss = 286730.94 (1.668 sec)\n",
            "train - step 5550: loss = 292450.56 (1.690 sec)\n",
            "train - step 5551: loss = 322265.50 (1.655 sec)\n",
            "train - step 5552: loss = 294029.91 (1.630 sec)\n",
            "train - step 5553: loss = 443596.88 (1.676 sec)\n",
            "train - step 5554: loss = 462844.31 (2.606 sec)\n",
            "train - step 5555: loss = 312708.41 (1.642 sec)\n",
            "train - step 5556: loss = 255693.52 (1.645 sec)\n",
            "train - step 5557: loss = 308575.12 (1.668 sec)\n",
            "train - step 5558: loss = 261559.50 (1.682 sec)\n",
            "train - step 5559: loss = 327675.81 (1.671 sec)\n",
            "train - step 5560: loss = 457692.34 (1.677 sec)\n",
            "train - step 5561: loss = 189098.09 (1.686 sec)\n",
            "train - step 5562: loss = 377201.81 (1.668 sec)\n",
            "train - step 5563: loss = 304786.41 (1.679 sec)\n",
            "train - step 5564: loss = 268525.06 (1.679 sec)\n",
            "train - step 5565: loss = 282569.84 (1.688 sec)\n",
            "train - step 5566: loss = 280294.97 (1.678 sec)\n",
            "train - step 5567: loss = 288326.03 (1.658 sec)\n",
            "train - step 5568: loss = 506381.34 (1.684 sec)\n",
            "train - step 5569: loss = 478161.56 (1.692 sec)\n",
            "train - step 5570: loss = 492710.88 (1.668 sec)\n",
            "train - step 5571: loss = 342866.78 (1.683 sec)\n",
            "train - step 5572: loss = 374054.97 (1.668 sec)\n",
            "train - step 5573: loss = 469542.59 (1.677 sec)\n",
            "train - step 5574: loss = 405710.81 (1.677 sec)\n",
            "train - step 5575: loss = 468543.94 (1.699 sec)\n",
            "train - step 5576: loss = 600584.31 (1.678 sec)\n",
            "train - step 5577: loss = 363060.16 (1.687 sec)\n",
            "train - step 5578: loss = 457770.62 (1.674 sec)\n",
            "train - step 5579: loss = 348096.41 (1.693 sec)\n",
            "train - step 5580: loss = 354208.28 (1.668 sec)\n",
            "train - step 5581: loss = 368336.91 (1.689 sec)\n",
            "train - step 5582: loss = 357692.00 (1.675 sec)\n",
            "train - step 5583: loss = 391719.84 (1.672 sec)\n",
            "train - step 5584: loss = 517765.91 (1.686 sec)\n",
            "train - step 5585: loss = 351397.97 (1.689 sec)\n",
            "train - step 5586: loss = 348912.97 (1.672 sec)\n",
            "train - step 5587: loss = 405972.94 (1.669 sec)\n",
            "train - step 5588: loss = 462124.16 (1.666 sec)\n",
            "train - step 5589: loss = 357003.97 (1.669 sec)\n",
            "train - step 5590: loss = 346688.56 (2.540 sec)\n",
            "train - step 5591: loss = 364252.78 (1.680 sec)\n",
            "train - step 5592: loss = 488553.94 (1.669 sec)\n",
            "train - step 5593: loss = 388314.34 (1.692 sec)\n",
            "train - step 5594: loss = 532327.88 (1.669 sec)\n",
            "train - step 5595: loss = 317201.31 (1.716 sec)\n",
            "train - step 5596: loss = 269341.94 (1.712 sec)\n",
            "train - step 5597: loss = 475082.00 (1.677 sec)\n",
            "train - step 5598: loss = 314130.59 (1.676 sec)\n",
            "train - step 5599: loss = 455688.50 (1.675 sec)\n",
            "train - step 5600: loss = 364642.22 (1.675 sec)\n",
            "train - step 5601: loss = 326297.12 (1.682 sec)\n",
            "train - step 5602: loss = 372494.53 (1.684 sec)\n",
            "train - step 5603: loss = 436853.34 (1.671 sec)\n",
            "train - step 5604: loss = 339452.59 (1.681 sec)\n",
            "train - step 5605: loss = 544895.94 (1.664 sec)\n",
            "train - step 5606: loss = 292773.62 (1.681 sec)\n",
            "train - step 5607: loss = 297155.81 (1.679 sec)\n",
            "train - step 5608: loss = 443180.47 (1.668 sec)\n",
            "train - step 5609: loss = 383494.41 (1.686 sec)\n",
            "train - step 5610: loss = 238443.70 (1.689 sec)\n",
            "train - step 5611: loss = 376258.31 (1.688 sec)\n",
            "train - step 5612: loss = 396999.41 (1.689 sec)\n",
            "train - step 5613: loss = 520210.22 (1.685 sec)\n",
            "train - step 5614: loss = 531466.31 (1.678 sec)\n",
            "train - step 5615: loss = 302538.31 (1.703 sec)\n",
            "train - step 5616: loss = 391419.12 (1.668 sec)\n",
            "train - step 5617: loss = 353050.50 (1.683 sec)\n",
            "train - step 5618: loss = 468860.56 (1.676 sec)\n",
            "train - step 5619: loss = 376518.69 (1.688 sec)\n",
            "train - step 5620: loss = 517219.28 (1.680 sec)\n",
            "train - step 5621: loss = 398888.19 (1.660 sec)\n",
            "train - step 5622: loss = 439295.59 (1.678 sec)\n",
            "train - step 5623: loss = 376123.09 (1.680 sec)\n",
            "train - step 5624: loss = 333245.69 (1.673 sec)\n",
            "train - step 5625: loss = 449453.34 (1.677 sec)\n",
            "train - step 5626: loss = 390162.22 (2.697 sec)\n",
            "train - step 5627: loss = 497095.44 (1.698 sec)\n",
            "train - step 5628: loss = 364769.03 (1.669 sec)\n",
            "train - step 5629: loss = 451278.06 (1.677 sec)\n",
            "train - step 5630: loss = 327693.72 (1.675 sec)\n",
            "train - step 5631: loss = 468645.94 (1.700 sec)\n",
            "train - step 5632: loss = 283949.97 (1.666 sec)\n",
            "train - step 5633: loss = 380183.97 (1.679 sec)\n",
            "train - step 5634: loss = 296837.50 (1.682 sec)\n",
            "train - step 5635: loss = 308774.94 (1.679 sec)\n",
            "train - step 5636: loss = 361187.22 (1.690 sec)\n",
            "train - step 5637: loss = 378766.22 (1.683 sec)\n",
            "train - step 5638: loss = 394410.22 (1.689 sec)\n",
            "train - step 5639: loss = 426448.12 (1.683 sec)\n",
            "train - step 5640: loss = 295832.59 (1.681 sec)\n",
            "train - step 5641: loss = 337044.69 (1.689 sec)\n",
            "train - step 5642: loss = 345117.00 (1.672 sec)\n",
            "train - step 5643: loss = 303470.81 (1.673 sec)\n",
            "train - step 5644: loss = 363405.47 (1.698 sec)\n",
            "train - step 5645: loss = 397449.44 (1.703 sec)\n",
            "train - step 5646: loss = 426218.44 (1.682 sec)\n",
            "train - step 5647: loss = 382441.31 (1.680 sec)\n",
            "train - step 5648: loss = 318089.72 (1.685 sec)\n",
            "train - step 5649: loss = 376779.53 (1.686 sec)\n",
            "train - step 5650: loss = 421857.94 (1.682 sec)\n",
            "train - step 5651: loss = 362331.59 (1.683 sec)\n",
            "train - step 5652: loss = 559473.75 (1.653 sec)\n",
            "train - step 5653: loss = 434159.03 (1.667 sec)\n",
            "train - step 5654: loss = 488487.19 (1.657 sec)\n",
            "train - step 5655: loss = 368525.69 (1.685 sec)\n",
            "train - step 5656: loss = 345498.16 (1.674 sec)\n",
            "train - step 5657: loss = 414140.09 (1.670 sec)\n",
            "train - step 5658: loss = 445776.69 (1.678 sec)\n",
            "train - step 5659: loss = 446864.69 (1.673 sec)\n",
            "train - step 5660: loss = 520779.69 (1.666 sec)\n",
            "train - step 5661: loss = 266550.88 (1.667 sec)\n",
            "train - step 5662: loss = 247774.91 (2.569 sec)\n",
            "train - step 5663: loss = 428444.81 (1.671 sec)\n",
            "train - step 5664: loss = 442249.62 (1.661 sec)\n",
            "train - step 5665: loss = 351028.19 (1.673 sec)\n",
            "train - step 5666: loss = 419321.31 (1.681 sec)\n",
            "train - step 5667: loss = 455724.38 (1.669 sec)\n",
            "train - step 5668: loss = 424504.09 (1.663 sec)\n",
            "train - step 5669: loss = 383300.06 (1.673 sec)\n",
            "train - step 5670: loss = 316517.56 (1.652 sec)\n",
            "train - step 5671: loss = 337029.84 (1.685 sec)\n",
            "train - step 5672: loss = 502097.56 (1.662 sec)\n",
            "train - step 5673: loss = 354011.69 (1.654 sec)\n",
            "train - step 5674: loss = 330348.72 (1.654 sec)\n",
            "train - step 5675: loss = 373839.78 (1.668 sec)\n",
            "train - step 5676: loss = 461019.00 (1.681 sec)\n",
            "train - step 5677: loss = 504756.66 (1.670 sec)\n",
            "train - step 5678: loss = 358446.47 (1.668 sec)\n",
            "train - step 5679: loss = 292995.22 (1.679 sec)\n",
            "train - step 5680: loss = 393585.19 (1.663 sec)\n",
            "train - step 5681: loss = 321729.38 (1.681 sec)\n",
            "train - step 5682: loss = 441472.59 (1.659 sec)\n",
            "train - step 5683: loss = 416414.19 (1.657 sec)\n",
            "train - step 5684: loss = 417082.03 (1.684 sec)\n",
            "train - step 5685: loss = 266427.09 (1.642 sec)\n",
            "train - step 5686: loss = 326682.97 (1.637 sec)\n",
            "train - step 5687: loss = 456585.50 (1.632 sec)\n",
            "train - step 5688: loss = 325279.69 (1.632 sec)\n",
            "train - step 5689: loss = 439168.28 (1.684 sec)\n",
            "train - step 5690: loss = 363310.78 (1.672 sec)\n",
            "train - step 5691: loss = 460912.00 (1.675 sec)\n",
            "train - step 5692: loss = 426067.62 (1.667 sec)\n",
            "train - step 5693: loss = 422627.69 (1.661 sec)\n",
            "train - step 5694: loss = 383990.69 (1.698 sec)\n",
            "train - step 5695: loss = 356038.38 (1.679 sec)\n",
            "train - step 5696: loss = 293123.78 (1.667 sec)\n",
            "train - step 5697: loss = 422962.22 (1.686 sec)\n",
            "train - step 5698: loss = 347007.62 (1.674 sec)\n",
            "train - step 5699: loss = 313207.41 (2.732 sec)\n",
            "train - step 5700: loss = 365769.94 (1.662 sec)\n",
            "train - step 5701: loss = 341437.06 (1.657 sec)\n",
            "train - step 5702: loss = 407541.34 (1.676 sec)\n",
            "train - step 5703: loss = 429626.56 (1.684 sec)\n",
            "train - step 5704: loss = 467207.53 (1.679 sec)\n",
            "train - step 5705: loss = 488023.03 (1.657 sec)\n",
            "train - step 5706: loss = 374829.56 (1.683 sec)\n",
            "train - step 5707: loss = 484205.16 (1.680 sec)\n",
            "train - step 5708: loss = 390733.19 (1.674 sec)\n",
            "train - step 5709: loss = 402121.41 (1.672 sec)\n",
            "train - step 5710: loss = 431145.59 (1.669 sec)\n",
            "train - step 5711: loss = 312150.00 (1.663 sec)\n",
            "train - step 5712: loss = 419209.00 (1.665 sec)\n",
            "train - step 5713: loss = 405704.50 (1.674 sec)\n",
            "train - step 5714: loss = 540871.25 (1.661 sec)\n",
            "train - step 5715: loss = 523943.50 (1.669 sec)\n",
            "train - step 5716: loss = 297027.91 (1.677 sec)\n",
            "train - step 5717: loss = 577982.31 (1.674 sec)\n",
            "train - step 5718: loss = 415922.12 (1.683 sec)\n",
            "train - step 5719: loss = 399202.28 (1.664 sec)\n",
            "train - step 5720: loss = 371042.41 (1.647 sec)\n",
            "train - step 5721: loss = 313800.66 (1.683 sec)\n",
            "train - step 5722: loss = 329716.69 (1.697 sec)\n",
            "train - step 5723: loss = 405427.44 (1.682 sec)\n",
            "train - step 5724: loss = 283128.09 (1.683 sec)\n",
            "train - step 5725: loss = 436273.97 (1.657 sec)\n",
            "train - step 5726: loss = 327823.91 (1.659 sec)\n",
            "train - step 5727: loss = 383511.22 (1.662 sec)\n",
            "train - step 5728: loss = 485123.72 (1.665 sec)\n",
            "train - step 5729: loss = 325585.53 (1.669 sec)\n",
            "train - step 5730: loss = 325668.66 (1.654 sec)\n",
            "train - step 5731: loss = 411977.09 (1.666 sec)\n",
            "train - step 5732: loss = 514823.12 (1.653 sec)\n",
            "train - step 5733: loss = 398005.66 (1.654 sec)\n",
            "train - step 5734: loss = 488155.47 (2.601 sec)\n",
            "train - step 5735: loss = 356358.41 (1.639 sec)\n",
            "train - step 5736: loss = 668808.31 (1.648 sec)\n",
            "train - step 5737: loss = 314228.28 (1.641 sec)\n",
            "train - step 5738: loss = 447774.12 (1.641 sec)\n",
            "train - step 5739: loss = 421623.28 (1.665 sec)\n",
            "train - step 5740: loss = 426548.56 (1.666 sec)\n",
            "train - step 5741: loss = 331010.28 (1.679 sec)\n",
            "train - step 5742: loss = 539574.06 (1.676 sec)\n",
            "train - step 5743: loss = 259348.03 (1.667 sec)\n",
            "train - step 5744: loss = 386030.22 (1.668 sec)\n",
            "train - step 5745: loss = 425226.66 (1.647 sec)\n",
            "train - step 5746: loss = 571573.38 (1.671 sec)\n",
            "train - step 5747: loss = 218738.14 (1.661 sec)\n",
            "train - step 5748: loss = 539654.31 (1.663 sec)\n",
            "train - step 5749: loss = 403003.12 (1.686 sec)\n",
            "train - step 5750: loss = 385100.28 (1.686 sec)\n",
            "train - step 5751: loss = 384963.53 (1.678 sec)\n",
            "train - step 5752: loss = 267594.16 (1.680 sec)\n",
            "train - step 5753: loss = 270570.81 (1.662 sec)\n",
            "train - step 5754: loss = 272922.44 (1.674 sec)\n",
            "train - step 5755: loss = 328413.66 (1.672 sec)\n",
            "train - step 5756: loss = 433883.38 (1.666 sec)\n",
            "train - step 5757: loss = 343756.72 (1.670 sec)\n",
            "train - step 5758: loss = 353359.41 (1.673 sec)\n",
            "train - step 5759: loss = 461641.62 (1.676 sec)\n",
            "train - step 5760: loss = 332468.81 (1.680 sec)\n",
            "train - step 5761: loss = 387296.06 (1.670 sec)\n",
            "train - step 5762: loss = 372625.19 (1.675 sec)\n",
            "train - step 5763: loss = 320452.50 (1.687 sec)\n",
            "train - step 5764: loss = 402996.31 (1.660 sec)\n",
            "train - step 5765: loss = 566498.31 (1.686 sec)\n",
            "train - step 5766: loss = 367528.69 (1.663 sec)\n",
            "train - step 5767: loss = 464967.28 (1.685 sec)\n",
            "train - step 5768: loss = 394575.41 (1.663 sec)\n",
            "train - step 5769: loss = 342782.47 (1.662 sec)\n",
            "train - step 5770: loss = 376201.31 (2.661 sec)\n",
            "train - step 5771: loss = 409318.81 (1.679 sec)\n",
            "train - step 5772: loss = 558435.94 (1.678 sec)\n",
            "train - step 5773: loss = 363655.38 (1.694 sec)\n",
            "train - step 5774: loss = 427384.00 (1.684 sec)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
            "  warnings.warn(\"Attempting to use a closed FileWriter. \"\n",
            "--- 8984.92612195015 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcdzi-dfkgW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnVNO1j4EZSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9sqfWTJEbRP",
        "colab_type": "text"
      },
      "source": [
        "# 主题 第六次运行 \n",
        "模型顶点数为6000，batchsize为8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12MlkaVCHJ-7",
        "colab_type": "text"
      },
      "source": [
        "# 主题 第七次运行 \n",
        "模型顶点数为6000，batchsize为8\n",
        "然后在tensoflow1.6下运行"
      ]
    }
  ]
}