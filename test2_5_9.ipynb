{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test2_5_9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zjisuccess/mycolab2/blob/master/test2_5_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTIT8ryQyxk6",
        "colab_type": "code",
        "outputId": "8eb7e6c6-6383-4bd9-a513-b23fe926ed41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBdV-99uzy-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCGw6l_66tPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59yJzh4JLdQP",
        "colab_type": "code",
        "outputId": "cd0fdc4f-2530-4f5f-f501-eaa080181c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11758
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-13 12:56:43.653043: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-13 12:56:43.653298: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x262b020 executing computations on platform Host. Devices:\n",
            "2019-05-13 12:56:43.653407: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-13 12:56:43.764727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-13 12:56:43.765345: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x262b2e0 executing computations on platform CUDA. Devices:\n",
            "2019-05-13 12:56:43.765436: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-13 12:56:43.765811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-13 12:56:43.765846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-13 12:56:44.166246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-13 12:56:44.166383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-13 12:56:44.166440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-13 12:56:44.166821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-13 13:01:12.872524: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-13 13:01:13.309286: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x8a418e0\n",
            "train - step 10001: loss = 1687.91 (4.419 sec)\n",
            "train - step 10002: loss = 1783.60 (0.927 sec)\n",
            "train - step 10003: loss = 1968.84 (0.897 sec)\n",
            "train - step 10004: loss = 1989.82 (0.864 sec)\n",
            "train - step 10005: loss = 1909.09 (0.866 sec)\n",
            "train - step 10006: loss = 2061.97 (0.888 sec)\n",
            "train - step 10007: loss = 2097.74 (0.878 sec)\n",
            "train - step 10008: loss = 1673.33 (0.875 sec)\n",
            "train - step 10009: loss = 2024.40 (0.869 sec)\n",
            "train - step 10010: loss = 2096.03 (0.852 sec)\n",
            "train - step 10011: loss = 1780.34 (0.856 sec)\n",
            "train - step 10012: loss = 2090.34 (0.868 sec)\n",
            "train - step 10013: loss = 1965.11 (0.863 sec)\n",
            "train - step 10014: loss = 1809.07 (0.866 sec)\n",
            "train - step 10015: loss = 2258.46 (0.864 sec)\n",
            "train - step 10016: loss = 2319.02 (0.858 sec)\n",
            "train - step 10017: loss = 2068.16 (0.862 sec)\n",
            "train - step 10018: loss = 2368.23 (0.872 sec)\n",
            "train - step 10019: loss = 2419.14 (0.873 sec)\n",
            "train - step 10020: loss = 2077.75 (0.860 sec)\n",
            "train - step 10021: loss = 1942.50 (0.859 sec)\n",
            "train - step 10022: loss = 2145.12 (0.861 sec)\n",
            "train - step 10023: loss = 2339.64 (0.859 sec)\n",
            "train - step 10024: loss = 2041.98 (1.108 sec)\n",
            "train - step 10025: loss = 2454.39 (0.873 sec)\n",
            "train - step 10026: loss = 2081.66 (0.875 sec)\n",
            "train - step 10027: loss = 2186.60 (0.874 sec)\n",
            "train - step 10028: loss = 2458.79 (0.863 sec)\n",
            "train - step 10029: loss = 2427.57 (0.864 sec)\n",
            "train - step 10030: loss = 2393.25 (0.857 sec)\n",
            "train - step 10031: loss = 2315.53 (0.868 sec)\n",
            "train - step 10032: loss = 2627.14 (0.875 sec)\n",
            "train - step 10033: loss = 2779.93 (0.879 sec)\n",
            "train - step 10034: loss = 2693.67 (0.863 sec)\n",
            "train - step 10035: loss = 2616.77 (0.854 sec)\n",
            "train - step 10036: loss = 2169.86 (0.866 sec)\n",
            "train - step 10037: loss = 2426.89 (0.861 sec)\n",
            "train - step 10038: loss = 2285.94 (0.891 sec)\n",
            "train - step 10039: loss = 2724.96 (0.864 sec)\n",
            "train - step 10040: loss = 2701.05 (0.889 sec)\n",
            "train - step 10041: loss = 2649.39 (0.870 sec)\n",
            "train - step 10042: loss = 2518.09 (0.869 sec)\n",
            "train - step 10043: loss = 2387.15 (0.856 sec)\n",
            "train - step 10044: loss = 2998.52 (0.869 sec)\n",
            "train - step 10045: loss = 2483.09 (0.874 sec)\n",
            "train - step 10046: loss = 2525.31 (0.863 sec)\n",
            "train - step 10047: loss = 2433.12 (0.871 sec)\n",
            "train - step 10048: loss = 2330.33 (0.868 sec)\n",
            "train - step 10049: loss = 2706.92 (0.859 sec)\n",
            "train - step 10050: loss = 2923.58 (0.864 sec)\n",
            "train - step 10051: loss = 2649.93 (0.892 sec)\n",
            "train - step 10052: loss = 2766.38 (0.859 sec)\n",
            "train - step 10053: loss = 2097.37 (0.867 sec)\n",
            "train - step 10054: loss = 2350.05 (0.871 sec)\n",
            "train - step 10055: loss = 3123.02 (0.875 sec)\n",
            "train - step 10056: loss = 2826.82 (0.860 sec)\n",
            "train - step 10057: loss = 2365.19 (0.864 sec)\n",
            "train - step 10058: loss = 2784.62 (0.872 sec)\n",
            "train - step 10059: loss = 3002.02 (0.864 sec)\n",
            "train - step 10060: loss = 3103.05 (0.867 sec)\n",
            "train - step 10061: loss = 3287.51 (0.872 sec)\n",
            "train - step 10062: loss = 2609.09 (0.867 sec)\n",
            "train - step 10063: loss = 3245.06 (0.866 sec)\n",
            "train - step 10064: loss = 2809.57 (0.880 sec)\n",
            "train - step 10065: loss = 3124.63 (0.866 sec)\n",
            "train - step 10066: loss = 3476.32 (0.860 sec)\n",
            "train - step 10067: loss = 2942.32 (0.869 sec)\n",
            "train - step 10068: loss = 2813.07 (0.875 sec)\n",
            "train - step 10069: loss = 3064.55 (0.866 sec)\n",
            "train - step 10070: loss = 2694.34 (0.881 sec)\n",
            "train - step 10071: loss = 3115.82 (0.870 sec)\n",
            "train - step 10072: loss = 3074.74 (0.868 sec)\n",
            "train - step 10073: loss = 3306.43 (0.874 sec)\n",
            "train - step 10074: loss = 2986.74 (0.879 sec)\n",
            "train - step 10075: loss = 3272.58 (0.862 sec)\n",
            "train - step 10076: loss = 2735.67 (0.867 sec)\n",
            "train - step 10077: loss = 2805.73 (0.866 sec)\n",
            "train - step 10078: loss = 2207.97 (0.858 sec)\n",
            "train - step 10079: loss = 3324.66 (0.860 sec)\n",
            "train - step 10080: loss = 3131.67 (0.874 sec)\n",
            "train - step 10081: loss = 3608.48 (0.858 sec)\n",
            "train - step 10082: loss = 3135.20 (0.863 sec)\n",
            "train - step 10083: loss = 3231.43 (0.862 sec)\n",
            "train - step 10084: loss = 3128.65 (0.867 sec)\n",
            "train - step 10085: loss = 3336.55 (0.870 sec)\n",
            "train - step 10086: loss = 3221.87 (0.869 sec)\n",
            "train - step 10087: loss = 3480.01 (0.885 sec)\n",
            "train - step 10088: loss = 3531.43 (0.881 sec)\n",
            "train - step 10089: loss = 3109.88 (0.874 sec)\n",
            "train - step 10090: loss = 3119.72 (0.859 sec)\n",
            "train - step 10091: loss = 3556.36 (0.871 sec)\n",
            "train - step 10092: loss = 3078.41 (0.854 sec)\n",
            "train - step 10093: loss = 3037.29 (2.178 sec)\n",
            "train - step 10094: loss = 3421.08 (0.863 sec)\n",
            "train - step 10095: loss = 3631.68 (0.869 sec)\n",
            "train - step 10096: loss = 3442.48 (0.879 sec)\n",
            "train - step 10097: loss = 3697.21 (0.877 sec)\n",
            "train - step 10098: loss = 3821.70 (0.867 sec)\n",
            "train - step 10099: loss = 3878.41 (0.853 sec)\n",
            "train - step 10100: loss = 3620.15 (0.870 sec)\n",
            "train - step 10101: loss = 2830.93 (0.862 sec)\n",
            "train - step 10102: loss = 3476.02 (0.875 sec)\n",
            "train - step 10103: loss = 2756.57 (0.884 sec)\n",
            "train - step 10104: loss = 3077.31 (0.876 sec)\n",
            "train - step 10105: loss = 3755.65 (0.864 sec)\n",
            "train - step 10106: loss = 3604.68 (0.863 sec)\n",
            "train - step 10107: loss = 2949.60 (0.870 sec)\n",
            "train - step 10108: loss = 2531.11 (0.868 sec)\n",
            "train - step 10109: loss = 3263.69 (0.865 sec)\n",
            "train - step 10110: loss = 3839.90 (0.863 sec)\n",
            "train - step 10111: loss = 3634.24 (0.887 sec)\n",
            "train - step 10112: loss = 3923.94 (0.870 sec)\n",
            "train - step 10113: loss = 3807.32 (0.862 sec)\n",
            "train - step 10114: loss = 3514.26 (0.862 sec)\n",
            "train - step 10115: loss = 3274.99 (0.885 sec)\n",
            "train - step 10116: loss = 3415.74 (0.875 sec)\n",
            "train - step 10117: loss = 3681.19 (0.857 sec)\n",
            "train - step 10118: loss = 3323.29 (0.859 sec)\n",
            "train - step 10119: loss = 3609.44 (0.875 sec)\n",
            "train - step 10120: loss = 3276.84 (0.877 sec)\n",
            "train - step 10121: loss = 3500.36 (0.877 sec)\n",
            "train - step 10122: loss = 3813.46 (0.862 sec)\n",
            "train - step 10123: loss = 3439.76 (0.864 sec)\n",
            "train - step 10124: loss = 3121.40 (0.866 sec)\n",
            "train - step 10125: loss = 3474.35 (0.878 sec)\n",
            "train - step 10126: loss = 3809.86 (0.871 sec)\n",
            "train - step 10127: loss = 3125.87 (0.886 sec)\n",
            "train - step 10128: loss = 3893.60 (0.881 sec)\n",
            "train - step 10129: loss = 3348.81 (0.869 sec)\n",
            "train - step 10130: loss = 3802.31 (0.864 sec)\n",
            "train - step 10131: loss = 3600.81 (0.850 sec)\n",
            "train - step 10132: loss = 3193.63 (0.853 sec)\n",
            "train - step 10133: loss = 3507.68 (0.861 sec)\n",
            "train - step 10134: loss = 3310.61 (0.864 sec)\n",
            "train - step 10135: loss = 3638.44 (0.855 sec)\n",
            "train - step 10136: loss = 3693.78 (0.868 sec)\n",
            "train - step 10137: loss = 3386.71 (0.847 sec)\n",
            "train - step 10138: loss = 3278.13 (0.850 sec)\n",
            "train - step 10139: loss = 2688.70 (0.869 sec)\n",
            "train - step 10140: loss = 3630.99 (0.859 sec)\n",
            "train - step 10141: loss = 3185.80 (0.855 sec)\n",
            "train - step 10142: loss = 3599.29 (0.858 sec)\n",
            "train - step 10143: loss = 3947.80 (0.868 sec)\n",
            "train - step 10144: loss = 3495.72 (0.874 sec)\n",
            "train - step 10145: loss = 3623.87 (0.884 sec)\n",
            "train - step 10146: loss = 3735.06 (0.871 sec)\n",
            "train - step 10147: loss = 3163.92 (0.880 sec)\n",
            "train - step 10148: loss = 3721.01 (0.869 sec)\n",
            "train - step 10149: loss = 3038.68 (0.860 sec)\n",
            "train - step 10150: loss = 4182.94 (0.874 sec)\n",
            "train - step 10151: loss = 2935.15 (0.866 sec)\n",
            "train - step 10152: loss = 3899.95 (0.870 sec)\n",
            "train - step 10153: loss = 3614.50 (0.869 sec)\n",
            "train - step 10154: loss = 3682.81 (0.861 sec)\n",
            "train - step 10155: loss = 3287.64 (0.878 sec)\n",
            "train - step 10156: loss = 3423.46 (0.875 sec)\n",
            "train - step 10157: loss = 4052.75 (0.862 sec)\n",
            "train - step 10158: loss = 3011.83 (0.872 sec)\n",
            "train - step 10159: loss = 3550.61 (0.867 sec)\n",
            "train - step 10160: loss = 3563.94 (0.862 sec)\n",
            "train - step 10161: loss = 3408.35 (1.222 sec)\n",
            "train - step 10162: loss = 3490.06 (0.856 sec)\n",
            "train - step 10163: loss = 3142.58 (0.865 sec)\n",
            "train - step 10164: loss = 3281.92 (0.878 sec)\n",
            "train - step 10165: loss = 3750.07 (0.894 sec)\n",
            "train - step 10166: loss = 2839.01 (0.861 sec)\n",
            "train - step 10167: loss = 3901.29 (0.874 sec)\n",
            "train - step 10168: loss = 3127.39 (0.859 sec)\n",
            "train - step 10169: loss = 3656.76 (0.857 sec)\n",
            "train - step 10170: loss = 2655.01 (0.862 sec)\n",
            "train - step 10171: loss = 3423.75 (0.874 sec)\n",
            "train - step 10172: loss = 2977.61 (0.872 sec)\n",
            "train - step 10173: loss = 3481.58 (0.872 sec)\n",
            "train - step 10174: loss = 3788.94 (0.866 sec)\n",
            "train - step 10175: loss = 3032.52 (0.861 sec)\n",
            "train - step 10176: loss = 2858.93 (0.863 sec)\n",
            "train - step 10177: loss = 2808.69 (0.858 sec)\n",
            "train - step 10178: loss = 3460.81 (0.868 sec)\n",
            "train - step 10179: loss = 3761.59 (0.878 sec)\n",
            "train - step 10180: loss = 3416.95 (0.865 sec)\n",
            "train - step 10181: loss = 3465.46 (0.866 sec)\n",
            "train - step 10182: loss = 3748.01 (0.857 sec)\n",
            "train - step 10183: loss = 3604.94 (0.867 sec)\n",
            "train - step 10184: loss = 2821.42 (0.872 sec)\n",
            "train - step 10185: loss = 3260.96 (0.863 sec)\n",
            "train - step 10186: loss = 3157.53 (0.876 sec)\n",
            "train - step 10187: loss = 3305.38 (0.875 sec)\n",
            "train - step 10188: loss = 3308.18 (0.884 sec)\n",
            "train - step 10189: loss = 3571.49 (0.861 sec)\n",
            "train - step 10190: loss = 2489.14 (0.857 sec)\n",
            "train - step 10191: loss = 2989.21 (0.872 sec)\n",
            "train - step 10192: loss = 3410.56 (0.864 sec)\n",
            "train - step 10193: loss = 3353.02 (0.864 sec)\n",
            "train - step 10194: loss = 3121.31 (0.861 sec)\n",
            "train - step 10195: loss = 3240.90 (0.877 sec)\n",
            "train - step 10196: loss = 3254.09 (0.872 sec)\n",
            "train - step 10197: loss = 2721.72 (0.857 sec)\n",
            "train - step 10198: loss = 3488.53 (0.872 sec)\n",
            "train - step 10199: loss = 3191.54 (0.868 sec)\n",
            "train - step 10200: loss = 3029.59 (0.869 sec)\n",
            "train - step 10201: loss = 3535.54 (0.871 sec)\n",
            "train - step 10202: loss = 3286.78 (0.870 sec)\n",
            "train - step 10203: loss = 2392.23 (0.860 sec)\n",
            "train - step 10204: loss = 3098.03 (0.877 sec)\n",
            "train - step 10205: loss = 3084.64 (0.878 sec)\n",
            "train - step 10206: loss = 3067.91 (0.869 sec)\n",
            "train - step 10207: loss = 3205.97 (0.867 sec)\n",
            "train - step 10208: loss = 3656.97 (0.866 sec)\n",
            "train - step 10209: loss = 3669.58 (0.875 sec)\n",
            "train - step 10210: loss = 3042.10 (0.867 sec)\n",
            "train - step 10211: loss = 3574.43 (0.878 sec)\n",
            "train - step 10212: loss = 2963.15 (0.873 sec)\n",
            "train - step 10213: loss = 2551.65 (0.862 sec)\n",
            "train - step 10214: loss = 2792.74 (0.880 sec)\n",
            "train - step 10215: loss = 1948.34 (0.859 sec)\n",
            "train - step 10216: loss = 3329.04 (0.882 sec)\n",
            "train - step 10217: loss = 3251.84 (0.893 sec)\n",
            "train - step 10218: loss = 3324.64 (0.863 sec)\n",
            "train - step 10219: loss = 3541.21 (0.871 sec)\n",
            "train - step 10220: loss = 3326.22 (0.860 sec)\n",
            "train - step 10221: loss = 3466.94 (0.860 sec)\n",
            "train - step 10222: loss = 3228.43 (0.859 sec)\n",
            "train - step 10223: loss = 3279.75 (0.862 sec)\n",
            "train - step 10224: loss = 2617.54 (0.864 sec)\n",
            "train - step 10225: loss = 2970.96 (0.861 sec)\n",
            "train - step 10226: loss = 3112.22 (0.875 sec)\n",
            "train - step 10227: loss = 3148.56 (0.873 sec)\n",
            "train - step 10228: loss = 3134.52 (0.878 sec)\n",
            "train - step 10229: loss = 2834.07 (0.875 sec)\n",
            "train - step 10230: loss = 3358.59 (1.317 sec)\n",
            "train - step 10231: loss = 3057.06 (0.861 sec)\n",
            "train - step 10232: loss = 3355.78 (0.870 sec)\n",
            "train - step 10233: loss = 3439.70 (0.866 sec)\n",
            "train - step 10234: loss = 3707.51 (0.864 sec)\n",
            "train - step 10235: loss = 3220.85 (0.862 sec)\n",
            "train - step 10236: loss = 3384.29 (0.868 sec)\n",
            "train - step 10237: loss = 3167.06 (0.872 sec)\n",
            "train - step 10238: loss = 3058.34 (0.871 sec)\n",
            "train - step 10239: loss = 3186.66 (0.865 sec)\n",
            "train - step 10240: loss = 3086.78 (0.870 sec)\n",
            "train - step 10241: loss = 3155.06 (0.864 sec)\n",
            "train - step 10242: loss = 2756.07 (0.858 sec)\n",
            "train - step 10243: loss = 1943.49 (0.876 sec)\n",
            "train - step 10244: loss = 3402.44 (0.868 sec)\n",
            "train - step 10245: loss = 3251.85 (0.865 sec)\n",
            "train - step 10246: loss = 3195.15 (0.876 sec)\n",
            "train - step 10247: loss = 3287.30 (0.858 sec)\n",
            "train - step 10248: loss = 3159.94 (0.862 sec)\n",
            "train - step 10249: loss = 2741.83 (0.871 sec)\n",
            "train - step 10250: loss = 3259.92 (0.867 sec)\n",
            "train - step 10251: loss = 3036.19 (0.866 sec)\n",
            "train - step 10252: loss = 3062.22 (0.871 sec)\n",
            "train - step 10253: loss = 2836.40 (0.861 sec)\n",
            "train - step 10254: loss = 3076.22 (0.868 sec)\n",
            "train - step 10255: loss = 2653.80 (0.876 sec)\n",
            "train - step 10256: loss = 3069.78 (0.875 sec)\n",
            "train - step 10257: loss = 3060.98 (0.875 sec)\n",
            "train - step 10258: loss = 3022.26 (0.861 sec)\n",
            "train - step 10259: loss = 3256.82 (0.872 sec)\n",
            "train - step 10260: loss = 3174.32 (0.873 sec)\n",
            "train - step 10261: loss = 2818.84 (0.870 sec)\n",
            "train - step 10262: loss = 2856.96 (0.869 sec)\n",
            "train - step 10263: loss = 3082.14 (0.876 sec)\n",
            "train - step 10264: loss = 3206.88 (0.862 sec)\n",
            "train - step 10265: loss = 2926.42 (0.867 sec)\n",
            "train - step 10266: loss = 2573.03 (0.861 sec)\n",
            "train - step 10267: loss = 2944.35 (0.849 sec)\n",
            "train - step 10268: loss = 2968.08 (0.863 sec)\n",
            "train - step 10269: loss = 3166.69 (0.875 sec)\n",
            "train - step 10270: loss = 2620.26 (0.888 sec)\n",
            "train - step 10271: loss = 2689.35 (0.868 sec)\n",
            "train - step 10272: loss = 1754.31 (0.871 sec)\n",
            "train - step 10273: loss = 3155.95 (0.857 sec)\n",
            "train - step 10274: loss = 3020.88 (0.857 sec)\n",
            "train - step 10275: loss = 2964.34 (0.902 sec)\n",
            "train - step 10276: loss = 3363.59 (0.972 sec)\n",
            "train - step 10277: loss = 2647.84 (0.916 sec)\n",
            "train - step 10278: loss = 2462.46 (0.857 sec)\n",
            "train - step 10279: loss = 2784.72 (0.879 sec)\n",
            "train - step 10280: loss = 3271.29 (0.893 sec)\n",
            "train - step 10281: loss = 2414.53 (0.879 sec)\n",
            "train - step 10282: loss = 2492.20 (0.893 sec)\n",
            "train - step 10283: loss = 2400.26 (0.884 sec)\n",
            "train - step 10284: loss = 2793.86 (0.883 sec)\n",
            "train - step 10285: loss = 2706.77 (0.875 sec)\n",
            "train - step 10286: loss = 2877.25 (0.885 sec)\n",
            "train - step 10287: loss = 2748.34 (1.315 sec)\n",
            "train - step 10288: loss = 2452.08 (0.890 sec)\n",
            "train - step 10289: loss = 3255.71 (0.885 sec)\n",
            "train - step 10290: loss = 2640.22 (0.860 sec)\n",
            "train - step 10291: loss = 3070.77 (0.862 sec)\n",
            "train - step 10292: loss = 2667.56 (0.872 sec)\n",
            "train - step 10293: loss = 2778.82 (0.883 sec)\n",
            "train - step 10294: loss = 2717.07 (0.871 sec)\n",
            "train - step 10295: loss = 2505.61 (0.864 sec)\n",
            "train - step 10296: loss = 2945.23 (0.874 sec)\n",
            "train - step 10297: loss = 1974.50 (0.879 sec)\n",
            "train - step 10298: loss = 2928.86 (1.247 sec)\n",
            "train - step 10299: loss = 2652.75 (0.863 sec)\n",
            "train - step 10300: loss = 2970.61 (0.878 sec)\n",
            "train - step 10301: loss = 2709.42 (0.887 sec)\n",
            "train - step 10302: loss = 2515.46 (0.875 sec)\n",
            "train - step 10303: loss = 2436.38 (0.868 sec)\n",
            "train - step 10304: loss = 2147.80 (0.871 sec)\n",
            "train - step 10305: loss = 2609.64 (0.859 sec)\n",
            "train - step 10306: loss = 2707.01 (0.864 sec)\n",
            "train - step 10307: loss = 2740.11 (0.871 sec)\n",
            "train - step 10308: loss = 2396.01 (0.872 sec)\n",
            "train - step 10309: loss = 2703.12 (0.866 sec)\n",
            "train - step 10310: loss = 2803.41 (0.857 sec)\n",
            "train - step 10311: loss = 1339.38 (0.849 sec)\n",
            "train - step 10312: loss = 2609.20 (0.850 sec)\n",
            "train - step 10313: loss = 2524.85 (0.858 sec)\n",
            "train - step 10314: loss = 2682.53 (0.874 sec)\n",
            "train - step 10315: loss = 2767.67 (0.856 sec)\n",
            "train - step 10316: loss = 2453.67 (0.866 sec)\n",
            "train - step 10317: loss = 2388.00 (0.852 sec)\n",
            "train - step 10318: loss = 2835.40 (0.866 sec)\n",
            "train - step 10319: loss = 2207.73 (0.872 sec)\n",
            "train - step 10320: loss = 2969.02 (0.870 sec)\n",
            "train - step 10321: loss = 3016.20 (0.871 sec)\n",
            "train - step 10322: loss = 2281.60 (0.867 sec)\n",
            "train - step 10323: loss = 2826.99 (0.854 sec)\n",
            "train - step 10324: loss = 1977.30 (0.865 sec)\n",
            "train - step 10325: loss = 2895.16 (0.867 sec)\n",
            "train - step 10326: loss = 1669.61 (0.859 sec)\n",
            "train - step 10327: loss = 2949.81 (0.863 sec)\n",
            "train - step 10328: loss = 2484.94 (0.868 sec)\n",
            "train - step 10329: loss = 1963.49 (0.877 sec)\n",
            "train - step 10330: loss = 2497.04 (0.862 sec)\n",
            "train - step 10331: loss = 2375.15 (0.862 sec)\n",
            "train - step 10332: loss = 2256.31 (0.865 sec)\n",
            "train - step 10333: loss = 2639.70 (0.856 sec)\n",
            "train - step 10334: loss = 2247.39 (0.870 sec)\n",
            "train - step 10335: loss = 2859.17 (0.874 sec)\n",
            "train - step 10336: loss = 1992.32 (0.861 sec)\n",
            "train - step 10337: loss = 2487.82 (0.875 sec)\n",
            "train - step 10338: loss = 2520.33 (0.883 sec)\n",
            "train - step 10339: loss = 2526.25 (0.879 sec)\n",
            "train - step 10340: loss = 2686.26 (0.861 sec)\n",
            "train - step 10341: loss = 2513.44 (0.863 sec)\n",
            "train - step 10342: loss = 2200.21 (0.880 sec)\n",
            "train - step 10343: loss = 2343.71 (0.860 sec)\n",
            "train - step 10344: loss = 2595.95 (0.869 sec)\n",
            "train - step 10345: loss = 2250.57 (0.873 sec)\n",
            "train - step 10346: loss = 2439.84 (0.875 sec)\n",
            "train - step 10347: loss = 2431.39 (0.869 sec)\n",
            "train - step 10348: loss = 2435.31 (0.871 sec)\n",
            "train - step 10349: loss = 2500.36 (0.857 sec)\n",
            "train - step 10350: loss = 2240.96 (0.884 sec)\n",
            "train - step 10351: loss = 2497.61 (0.875 sec)\n",
            "train - step 10352: loss = 2297.70 (0.862 sec)\n",
            "train - step 10353: loss = 2702.93 (0.876 sec)\n",
            "train - step 10354: loss = 2689.81 (0.863 sec)\n",
            "train - step 10355: loss = 2726.67 (0.876 sec)\n",
            "train - step 10356: loss = 2321.94 (0.864 sec)\n",
            "train - step 10357: loss = 2427.13 (0.862 sec)\n",
            "train - step 10358: loss = 2207.71 (0.863 sec)\n",
            "train - step 10359: loss = 1863.12 (0.875 sec)\n",
            "train - step 10360: loss = 2559.80 (0.878 sec)\n",
            "train - step 10361: loss = 2910.83 (0.864 sec)\n",
            "train - step 10362: loss = 2040.07 (0.865 sec)\n",
            "train - step 10363: loss = 2444.10 (0.878 sec)\n",
            "train - step 10364: loss = 2380.97 (0.875 sec)\n",
            "train - step 10365: loss = 2246.16 (0.874 sec)\n",
            "train - step 10366: loss = 2402.32 (0.866 sec)\n",
            "train - step 10367: loss = 2781.37 (1.195 sec)\n",
            "train - step 10368: loss = 2550.23 (0.872 sec)\n",
            "train - step 10369: loss = 2834.65 (0.863 sec)\n",
            "train - step 10370: loss = 2373.38 (0.887 sec)\n",
            "train - step 10371: loss = 2629.48 (0.870 sec)\n",
            "train - step 10372: loss = 2389.00 (0.873 sec)\n",
            "train - step 10373: loss = 2175.88 (0.870 sec)\n",
            "train - step 10374: loss = 2366.62 (0.867 sec)\n",
            "train - step 10375: loss = 2209.63 (0.876 sec)\n",
            "train - step 10376: loss = 2287.77 (0.869 sec)\n",
            "train - step 10377: loss = 2426.07 (0.860 sec)\n",
            "train - step 10378: loss = 2579.24 (0.865 sec)\n",
            "train - step 10379: loss = 2666.77 (0.861 sec)\n",
            "train - step 10380: loss = 2824.59 (0.870 sec)\n",
            "train - step 10381: loss = 2441.87 (0.866 sec)\n",
            "train - step 10382: loss = 2868.71 (0.874 sec)\n",
            "train - step 10383: loss = 2781.50 (0.854 sec)\n",
            "train - step 10384: loss = 2301.97 (0.879 sec)\n",
            "train - step 10385: loss = 2636.07 (0.875 sec)\n",
            "train - step 10386: loss = 2218.79 (0.873 sec)\n",
            "train - step 10387: loss = 2459.03 (0.860 sec)\n",
            "train - step 10388: loss = 2413.25 (0.860 sec)\n",
            "train - step 10389: loss = 1093.81 (0.866 sec)\n",
            "train - step 10390: loss = 2422.33 (0.872 sec)\n",
            "train - step 10391: loss = 2429.54 (0.870 sec)\n",
            "train - step 10392: loss = 2242.62 (0.860 sec)\n",
            "train - step 10393: loss = 2480.65 (0.889 sec)\n",
            "train - step 10394: loss = 2805.02 (0.862 sec)\n",
            "train - step 10395: loss = 2600.03 (0.870 sec)\n",
            "train - step 10396: loss = 2139.97 (0.870 sec)\n",
            "train - step 10397: loss = 2603.43 (0.871 sec)\n",
            "train - step 10398: loss = 2486.49 (0.876 sec)\n",
            "train - step 10399: loss = 2683.13 (0.865 sec)\n",
            "train - step 10400: loss = 2565.93 (0.852 sec)\n",
            "train - step 10401: loss = 2056.15 (0.868 sec)\n",
            "train - step 10402: loss = 1794.13 (0.863 sec)\n",
            "train - step 10403: loss = 1688.53 (0.872 sec)\n",
            "train - step 10404: loss = 2522.54 (0.897 sec)\n",
            "train - step 10405: loss = 2567.82 (0.870 sec)\n",
            "train - step 10406: loss = 2258.76 (0.899 sec)\n",
            "train - step 10407: loss = 2627.97 (0.875 sec)\n",
            "train - step 10408: loss = 2328.85 (0.888 sec)\n",
            "train - step 10409: loss = 2833.10 (0.875 sec)\n",
            "train - step 10410: loss = 2428.94 (0.871 sec)\n",
            "train - step 10411: loss = 3061.99 (0.869 sec)\n",
            "train - step 10412: loss = 2098.97 (0.859 sec)\n",
            "train - step 10413: loss = 2588.52 (0.872 sec)\n",
            "train - step 10414: loss = 1894.63 (0.867 sec)\n",
            "train - step 10415: loss = 2539.66 (0.860 sec)\n",
            "train - step 10416: loss = 2328.38 (0.865 sec)\n",
            "train - step 10417: loss = 2066.51 (0.881 sec)\n",
            "train - step 10418: loss = 2958.15 (0.858 sec)\n",
            "train - step 10419: loss = 2017.71 (0.882 sec)\n",
            "train - step 10420: loss = 2741.43 (0.860 sec)\n",
            "train - step 10421: loss = 2631.94 (0.865 sec)\n",
            "train - step 10422: loss = 1521.17 (0.883 sec)\n",
            "train - step 10423: loss = 2328.34 (0.864 sec)\n",
            "train - step 10424: loss = 2540.07 (0.869 sec)\n",
            "train - step 10425: loss = 2570.05 (0.864 sec)\n",
            "train - step 10426: loss = 2039.51 (0.870 sec)\n",
            "train - step 10427: loss = 2456.85 (0.860 sec)\n",
            "train - step 10428: loss = 2684.98 (0.861 sec)\n",
            "train - step 10429: loss = 2530.17 (0.864 sec)\n",
            "train - step 10430: loss = 2542.78 (0.879 sec)\n",
            "train - step 10431: loss = 2377.03 (0.870 sec)\n",
            "train - step 10432: loss = 2529.92 (0.865 sec)\n",
            "train - step 10433: loss = 2345.28 (0.873 sec)\n",
            "train - step 10434: loss = 1941.66 (0.877 sec)\n",
            "train - step 10435: loss = 2722.27 (0.865 sec)\n",
            "train - step 10436: loss = 2298.20 (1.192 sec)\n",
            "train - step 10437: loss = 2652.93 (0.891 sec)\n",
            "train - step 10438: loss = 1947.99 (0.861 sec)\n",
            "train - step 10439: loss = 2751.95 (0.878 sec)\n",
            "train - step 10440: loss = 2522.56 (0.859 sec)\n",
            "train - step 10441: loss = 2245.73 (0.858 sec)\n",
            "train - step 10442: loss = 3137.70 (0.860 sec)\n",
            "train - step 10443: loss = 2485.70 (0.862 sec)\n",
            "train - step 10444: loss = 2454.84 (0.856 sec)\n",
            "train - step 10445: loss = 2159.39 (0.876 sec)\n",
            "train - step 10446: loss = 2130.06 (0.849 sec)\n",
            "train - step 10447: loss = 2406.04 (0.867 sec)\n",
            "train - step 10448: loss = 2625.10 (0.860 sec)\n",
            "train - step 10449: loss = 2481.91 (0.866 sec)\n",
            "train - step 10450: loss = 2444.59 (0.875 sec)\n",
            "train - step 10451: loss = 2544.02 (0.862 sec)\n",
            "train - step 10452: loss = 2315.78 (0.858 sec)\n",
            "train - step 10453: loss = 2635.28 (0.865 sec)\n",
            "train - step 10454: loss = 1775.49 (0.879 sec)\n",
            "train - step 10455: loss = 2312.50 (0.871 sec)\n",
            "train - step 10456: loss = 2415.41 (0.863 sec)\n",
            "train - step 10457: loss = 2572.34 (0.857 sec)\n",
            "train - step 10458: loss = 2966.23 (0.864 sec)\n",
            "train - step 10459: loss = 2284.30 (0.865 sec)\n",
            "train - step 10460: loss = 2505.46 (0.876 sec)\n",
            "train - step 10461: loss = 2189.05 (0.868 sec)\n",
            "train - step 10462: loss = 2216.57 (0.875 sec)\n",
            "train - step 10463: loss = 1978.66 (0.861 sec)\n",
            "train - step 10464: loss = 2345.10 (0.871 sec)\n",
            "train - step 10465: loss = 2397.23 (0.861 sec)\n",
            "train - step 10466: loss = 2426.51 (0.870 sec)\n",
            "train - step 10467: loss = 2328.44 (0.868 sec)\n",
            "train - step 10468: loss = 1819.74 (0.872 sec)\n",
            "train - step 10469: loss = 2132.09 (0.869 sec)\n",
            "train - step 10470: loss = 2517.84 (0.861 sec)\n",
            "train - step 10471: loss = 2328.17 (0.876 sec)\n",
            "train - step 10472: loss = 2604.16 (0.872 sec)\n",
            "train - step 10473: loss = 2236.11 (0.868 sec)\n",
            "train - step 10474: loss = 2130.85 (0.897 sec)\n",
            "train - step 10475: loss = 2460.43 (0.865 sec)\n",
            "train - step 10476: loss = 2447.48 (0.853 sec)\n",
            "train - step 10477: loss = 2337.76 (0.861 sec)\n",
            "train - step 10478: loss = 2071.00 (0.872 sec)\n",
            "train - step 10479: loss = 2309.12 (0.873 sec)\n",
            "train - step 10480: loss = 2524.58 (0.874 sec)\n",
            "train - step 10481: loss = 2266.47 (0.865 sec)\n",
            "train - step 10482: loss = 2318.58 (0.864 sec)\n",
            "train - step 10483: loss = 2386.23 (0.858 sec)\n",
            "train - step 10484: loss = 2503.19 (0.864 sec)\n",
            "train - step 10485: loss = 2274.61 (0.861 sec)\n",
            "train - step 10486: loss = 2423.45 (0.851 sec)\n",
            "train - step 10487: loss = 2130.11 (0.854 sec)\n",
            "train - step 10488: loss = 2233.92 (0.848 sec)\n",
            "train - step 10489: loss = 2653.47 (0.865 sec)\n",
            "train - step 10490: loss = 2593.83 (0.851 sec)\n",
            "train - step 10491: loss = 2476.73 (0.861 sec)\n",
            "train - step 10492: loss = 2031.56 (0.855 sec)\n",
            "train - step 10493: loss = 1953.69 (0.852 sec)\n",
            "train - step 10494: loss = 2152.96 (0.850 sec)\n",
            "train - step 10495: loss = 2744.94 (0.849 sec)\n",
            "train - step 10496: loss = 2449.69 (0.846 sec)\n",
            "train - step 10497: loss = 2080.44 (0.868 sec)\n",
            "train - step 10498: loss = 2384.97 (0.884 sec)\n",
            "train - step 10499: loss = 2184.95 (0.874 sec)\n",
            "train - step 10500: loss = 2450.28 (0.858 sec)\n",
            "train - step 10501: loss = 2489.59 (0.868 sec)\n",
            "train - step 10502: loss = 2256.70 (0.865 sec)\n",
            "train - step 10503: loss = 2433.71 (0.864 sec)\n",
            "train - step 10504: loss = 2191.17 (0.877 sec)\n",
            "train - step 10505: loss = 2251.45 (0.870 sec)\n",
            "train - step 10506: loss = 1907.52 (1.188 sec)\n",
            "train - step 10507: loss = 2066.87 (0.864 sec)\n",
            "train - step 10508: loss = 2438.55 (0.861 sec)\n",
            "train - step 10509: loss = 2503.87 (0.864 sec)\n",
            "train - step 10510: loss = 2163.38 (0.865 sec)\n",
            "train - step 10511: loss = 1745.24 (0.876 sec)\n",
            "train - step 10512: loss = 2375.66 (0.882 sec)\n",
            "train - step 10513: loss = 1945.20 (0.871 sec)\n",
            "train - step 10514: loss = 2150.08 (0.865 sec)\n",
            "train - step 10515: loss = 2313.21 (0.857 sec)\n",
            "train - step 10516: loss = 2078.54 (0.868 sec)\n",
            "train - step 10517: loss = 2374.33 (0.860 sec)\n",
            "train - step 10518: loss = 2219.16 (0.879 sec)\n",
            "train - step 10519: loss = 2659.59 (0.886 sec)\n",
            "train - step 10520: loss = 2372.85 (0.870 sec)\n",
            "train - step 10521: loss = 2531.24 (0.869 sec)\n",
            "train - step 10522: loss = 2283.55 (0.857 sec)\n",
            "train - step 10523: loss = 2364.16 (0.863 sec)\n",
            "train - step 10524: loss = 2188.51 (0.887 sec)\n",
            "train - step 10525: loss = 2618.94 (0.872 sec)\n",
            "train - step 10526: loss = 2449.01 (0.866 sec)\n",
            "train - step 10527: loss = 2127.75 (0.869 sec)\n",
            "train - step 10528: loss = 2286.74 (0.878 sec)\n",
            "train - step 10529: loss = 2390.55 (0.854 sec)\n",
            "train - step 10530: loss = 2299.94 (0.880 sec)\n",
            "train - step 10531: loss = 2303.67 (0.873 sec)\n",
            "train - step 10532: loss = 2587.20 (0.874 sec)\n",
            "train - step 10533: loss = 2415.24 (0.862 sec)\n",
            "train - step 10534: loss = 2445.13 (0.868 sec)\n",
            "train - step 10535: loss = 2499.89 (0.878 sec)\n",
            "train - step 10536: loss = 2185.93 (0.864 sec)\n",
            "train - step 10537: loss = 2132.77 (0.870 sec)\n",
            "train - step 10538: loss = 2334.39 (0.865 sec)\n",
            "train - step 10539: loss = 2179.62 (0.875 sec)\n",
            "train - step 10540: loss = 2676.48 (0.866 sec)\n",
            "train - step 10541: loss = 2450.26 (0.857 sec)\n",
            "train - step 10542: loss = 2174.25 (0.878 sec)\n",
            "train - step 10543: loss = 2140.70 (0.881 sec)\n",
            "train - step 10544: loss = 2188.39 (0.872 sec)\n",
            "train - step 10545: loss = 2343.58 (0.864 sec)\n",
            "train - step 10546: loss = 2384.05 (0.858 sec)\n",
            "train - step 10547: loss = 1869.87 (0.867 sec)\n",
            "train - step 10548: loss = 2727.86 (0.878 sec)\n",
            "train - step 10549: loss = 2264.27 (0.870 sec)\n",
            "train - step 10550: loss = 2371.77 (0.875 sec)\n",
            "train - step 10551: loss = 2068.15 (0.867 sec)\n",
            "train - step 10552: loss = 1754.78 (0.859 sec)\n",
            "train - step 10553: loss = 2281.76 (0.862 sec)\n",
            "train - step 10554: loss = 2492.74 (0.860 sec)\n",
            "train - step 10555: loss = 2342.48 (0.868 sec)\n",
            "train - step 10556: loss = 2265.02 (0.873 sec)\n",
            "train - step 10557: loss = 2196.25 (0.882 sec)\n",
            "train - step 10558: loss = 2487.48 (0.869 sec)\n",
            "train - step 10559: loss = 1800.89 (0.871 sec)\n",
            "train - step 10560: loss = 2137.98 (0.860 sec)\n",
            "train - step 10561: loss = 1961.96 (0.863 sec)\n",
            "train - step 10562: loss = 2264.25 (0.875 sec)\n",
            "train - step 10563: loss = 2296.94 (0.884 sec)\n",
            "train - step 10564: loss = 2297.79 (0.868 sec)\n",
            "train - step 10565: loss = 1934.15 (0.856 sec)\n",
            "train - step 10566: loss = 2560.16 (0.869 sec)\n",
            "train - step 10567: loss = 1953.45 (0.857 sec)\n",
            "train - step 10568: loss = 2302.44 (0.873 sec)\n",
            "train - step 10569: loss = 2305.45 (0.865 sec)\n",
            "train - step 10570: loss = 2124.30 (0.870 sec)\n",
            "train - step 10571: loss = 2454.65 (0.869 sec)\n",
            "train - step 10572: loss = 2522.71 (0.872 sec)\n",
            "train - step 10573: loss = 1952.85 (0.864 sec)\n",
            "train - step 10574: loss = 2287.95 (0.862 sec)\n",
            "train - step 10575: loss = 2023.70 (1.291 sec)\n",
            "train - step 10576: loss = 2393.96 (0.856 sec)\n",
            "train - step 10577: loss = 2161.63 (0.875 sec)\n",
            "train - step 10578: loss = 2506.29 (0.877 sec)\n",
            "train - step 10579: loss = 2334.72 (0.872 sec)\n",
            "train - step 10580: loss = 2215.05 (0.869 sec)\n",
            "train - step 10581: loss = 2462.53 (0.861 sec)\n",
            "train - step 10582: loss = 2241.18 (0.869 sec)\n",
            "train - step 10583: loss = 2298.96 (0.868 sec)\n",
            "train - step 10584: loss = 2543.84 (0.875 sec)\n",
            "train - step 10585: loss = 2356.35 (0.870 sec)\n",
            "train - step 10586: loss = 2141.67 (0.874 sec)\n",
            "train - step 10587: loss = 2651.53 (0.867 sec)\n",
            "train - step 10588: loss = 2269.35 (0.866 sec)\n",
            "train - step 10589: loss = 2269.70 (0.867 sec)\n",
            "train - step 10590: loss = 1716.92 (0.884 sec)\n",
            "train - step 10591: loss = 2300.21 (0.867 sec)\n",
            "train - step 10592: loss = 2641.10 (0.875 sec)\n",
            "train - step 10593: loss = 2419.03 (0.865 sec)\n",
            "train - step 10594: loss = 2236.28 (0.857 sec)\n",
            "train - step 10595: loss = 2186.40 (0.867 sec)\n",
            "train - step 10596: loss = 1652.49 (0.868 sec)\n",
            "train - step 10597: loss = 2257.26 (0.869 sec)\n",
            "train - step 10598: loss = 2005.64 (0.865 sec)\n",
            "train - step 10599: loss = 2003.27 (0.861 sec)\n",
            "train - step 10600: loss = 2456.94 (0.860 sec)\n",
            "train - step 10601: loss = 2091.45 (0.865 sec)\n",
            "train - step 10602: loss = 2559.47 (0.871 sec)\n",
            "train - step 10603: loss = 2800.39 (0.869 sec)\n",
            "train - step 10604: loss = 1927.97 (0.869 sec)\n",
            "train - step 10605: loss = 1780.03 (0.854 sec)\n",
            "train - step 10606: loss = 2010.53 (0.859 sec)\n",
            "train - step 10607: loss = 2107.78 (0.874 sec)\n",
            "train - step 10608: loss = 1548.78 (0.861 sec)\n",
            "train - step 10609: loss = 2130.62 (0.869 sec)\n",
            "train - step 10610: loss = 2389.48 (0.868 sec)\n",
            "train - step 10611: loss = 2143.65 (0.859 sec)\n",
            "train - step 10612: loss = 1843.08 (0.859 sec)\n",
            "train - step 10613: loss = 2355.61 (0.859 sec)\n",
            "train - step 10614: loss = 2454.77 (0.874 sec)\n",
            "train - step 10615: loss = 2564.29 (0.873 sec)\n",
            "train - step 10616: loss = 2256.43 (0.867 sec)\n",
            "train - step 10617: loss = 2363.17 (0.871 sec)\n",
            "train - step 10618: loss = 2187.57 (0.869 sec)\n",
            "train - step 10619: loss = 2463.61 (0.861 sec)\n",
            "train - step 10620: loss = 2087.11 (0.894 sec)\n",
            "train - step 10621: loss = 2484.96 (0.895 sec)\n",
            "train - step 10622: loss = 2273.29 (0.861 sec)\n",
            "train - step 10623: loss = 2132.53 (0.869 sec)\n",
            "train - step 10624: loss = 2366.64 (0.862 sec)\n",
            "train - step 10625: loss = 2014.72 (0.859 sec)\n",
            "train - step 10626: loss = 2539.66 (0.866 sec)\n",
            "train - step 10627: loss = 1895.39 (0.874 sec)\n",
            "train - step 10628: loss = 2315.66 (0.868 sec)\n",
            "train - step 10629: loss = 2064.87 (0.861 sec)\n",
            "train - step 10630: loss = 2051.91 (0.868 sec)\n",
            "train - step 10631: loss = 1943.74 (0.854 sec)\n",
            "train - step 10632: loss = 2323.12 (0.870 sec)\n",
            "train - step 10633: loss = 2524.20 (0.874 sec)\n",
            "train - step 10634: loss = 2274.79 (0.869 sec)\n",
            "train - step 10635: loss = 2273.20 (0.863 sec)\n",
            "train - step 10636: loss = 2082.69 (0.862 sec)\n",
            "train - step 10637: loss = 1860.71 (0.863 sec)\n",
            "train - step 10638: loss = 2306.04 (0.859 sec)\n",
            "train - step 10639: loss = 1837.74 (0.872 sec)\n",
            "train - step 10640: loss = 2374.42 (0.865 sec)\n",
            "train - step 10641: loss = 1208.35 (0.854 sec)\n",
            "train - step 10642: loss = 2172.89 (0.862 sec)\n",
            "train - step 10643: loss = 1844.52 (0.878 sec)\n",
            "train - step 10644: loss = 2001.31 (1.284 sec)\n",
            "train - step 10645: loss = 2282.12 (0.869 sec)\n",
            "train - step 10646: loss = 2453.80 (0.874 sec)\n",
            "train - step 10647: loss = 2204.04 (0.866 sec)\n",
            "train - step 10648: loss = 2264.59 (0.865 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEgl2_E6vP6",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第一次\n",
        "我在训练文件所在的同级目录下建立交互文件试运行相关文件，但是不行，仍提示找不到模型，所以在云端跑无论ipynb文件在哪里，模型的读取路径都必须是绝对路径，在这里就是drive/unsupervisedfmnet/Shapes/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "114BR6og0AMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0pTleRj6RX4",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第二次\n",
        "模型顶点数为3000，batch_size为16，溢出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBq1dfYN1BRW",
        "colab_type": "code",
        "outputId": "0159b0e6-6bea-40c9-dac6-58202fe3191b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14518
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 13:42:09.719663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 13:42:09.719908: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e31180 executing computations on platform Host. Devices:\n",
            "2019-05-09 13:42:09.719947: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 13:42:09.811118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 13:42:09.811704: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1e31440 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 13:42:09.811748: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 13:42:09.812151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 13:42:09.812192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 13:42:10.201818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 13:42:10.201920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 13:42:10.201940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 13:42:10.202334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 13:52:05.976061: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 13:52:07.357206: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x82038e0\n",
            "2019-05-09 13:52:19.083072: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.50GiB.  Current allocation summary follows.\n",
            "2019-05-09 13:52:19.083189: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 115, Chunks in use: 114. 28.8KiB allocated for chunks. 28.5KiB in use in bin. 464B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083240: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 3, Chunks in use: 0. 1.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083267: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 378, Chunks in use: 378. 566.8KiB allocated for chunks. 566.8KiB in use in bin. 519.4KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083288: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 2, Chunks in use: 1. 5.5KiB allocated for chunks. 2.2KiB in use in bin. 1.4KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083308: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 2, Chunks in use: 2. 15.0KiB allocated for chunks. 15.0KiB in use in bin. 15.0KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083328: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 3, Chunks in use: 2. 31.5KiB allocated for chunks. 17.5KiB in use in bin. 17.5KiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083378: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083395: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083414: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083432: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083453: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 40, Chunks in use: 40. 18.91MiB allocated for chunks. 18.91MiB in use in bin. 18.91MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083474: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 22, Chunks in use: 22. 18.88MiB allocated for chunks. 18.88MiB in use in bin. 18.12MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083494: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 3. 4.88MiB allocated for chunks. 4.88MiB in use in bin. 2.64MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083513: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 5.16MiB allocated for chunks. 5.16MiB in use in bin. 5.16MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083531: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083549: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083570: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 5, Chunks in use: 4. 110.54MiB allocated for chunks. 87.89MiB in use in bin. 87.89MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083591: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 1. 123.05MiB allocated for chunks. 61.52MiB in use in bin. 61.52MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083611: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 59, Chunks in use: 57. 3.94GiB allocated for chunks. 3.81GiB in use in bin. 3.59GiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083632: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 64.45MiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083668: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 3. 6.16GiB allocated for chunks. 4.51GiB in use in bin. 4.51GiB client-requested in use in bin.\n",
            "2019-05-09 13:52:19.083703: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 1.50GiB was 256.00MiB, Chunk State: \n",
            "2019-05-09 13:52:19.083737: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 334.28MiB | Requested Size: 12B | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083774: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 369.14MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 61.52MiB | Requested Size: 61.52MiB | in_use: 1, next:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083797: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 993.45MiB | Requested Size: 12B | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 13:52:19.083818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0000 of size 256\n",
            "2019-05-09 13:52:19.083835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0100 of size 256\n",
            "2019-05-09 13:52:19.083860: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0200 of size 495616\n",
            "2019-05-09 13:52:19.083888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039200 of size 1536\n",
            "2019-05-09 13:52:19.083906: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039800 of size 256\n",
            "2019-05-09 13:52:19.083921: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039900 of size 1536\n",
            "2019-05-09 13:52:19.083936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704039f00 of size 1536\n",
            "2019-05-09 13:52:19.083952: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403a500 of size 1536\n",
            "2019-05-09 13:52:19.083967: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403ab00 of size 1536\n",
            "2019-05-09 13:52:19.083983: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403b100 of size 1536\n",
            "2019-05-09 13:52:19.084000: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403b700 of size 1536\n",
            "2019-05-09 13:52:19.084022: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403bd00 of size 1536\n",
            "2019-05-09 13:52:19.084078: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403c300 of size 1536\n",
            "2019-05-09 13:52:19.084096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403c900 of size 1536\n",
            "2019-05-09 13:52:19.084112: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403cf00 of size 1536\n",
            "2019-05-09 13:52:19.084127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403d500 of size 1536\n",
            "2019-05-09 13:52:19.084173: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403db00 of size 256\n",
            "2019-05-09 13:52:19.084203: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403dc00 of size 1536\n",
            "2019-05-09 13:52:19.084234: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e200 of size 256\n",
            "2019-05-09 13:52:19.084250: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e300 of size 1536\n",
            "2019-05-09 13:52:19.084267: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403e900 of size 1536\n",
            "2019-05-09 13:52:19.084298: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403ef00 of size 1536\n",
            "2019-05-09 13:52:19.084313: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403f500 of size 1536\n",
            "2019-05-09 13:52:19.084329: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403fb00 of size 1536\n",
            "2019-05-09 13:52:19.084356: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040100 of size 1536\n",
            "2019-05-09 13:52:19.084372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040700 of size 1536\n",
            "2019-05-09 13:52:19.084385: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704040d00 of size 1536\n",
            "2019-05-09 13:52:19.084398: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041300 of size 1536\n",
            "2019-05-09 13:52:19.084413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041900 of size 1536\n",
            "2019-05-09 13:52:19.084428: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704041f00 of size 1536\n",
            "2019-05-09 13:52:19.084443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042500 of size 256\n",
            "2019-05-09 13:52:19.084459: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042600 of size 1536\n",
            "2019-05-09 13:52:19.084471: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042c00 of size 256\n",
            "2019-05-09 13:52:19.084489: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704042d00 of size 1536\n",
            "2019-05-09 13:52:19.084506: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043300 of size 1536\n",
            "2019-05-09 13:52:19.084521: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043900 of size 1536\n",
            "2019-05-09 13:52:19.084537: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704043f00 of size 1536\n",
            "2019-05-09 13:52:19.084552: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704044500 of size 1536\n",
            "2019-05-09 13:52:19.084570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704044b00 of size 1536\n",
            "2019-05-09 13:52:19.084587: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045100 of size 1536\n",
            "2019-05-09 13:52:19.084603: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045700 of size 1536\n",
            "2019-05-09 13:52:19.084618: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704045d00 of size 1536\n",
            "2019-05-09 13:52:19.084634: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046300 of size 1536\n",
            "2019-05-09 13:52:19.084650: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046900 of size 1536\n",
            "2019-05-09 13:52:19.084665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704046f00 of size 256\n",
            "2019-05-09 13:52:19.084680: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047000 of size 1536\n",
            "2019-05-09 13:52:19.084697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047600 of size 256\n",
            "2019-05-09 13:52:19.084712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047700 of size 1536\n",
            "2019-05-09 13:52:19.084742: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704047d00 of size 1536\n",
            "2019-05-09 13:52:19.084757: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048300 of size 1536\n",
            "2019-05-09 13:52:19.084772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048900 of size 1536\n",
            "2019-05-09 13:52:19.084788: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704048f00 of size 1536\n",
            "2019-05-09 13:52:19.084803: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704049500 of size 1536\n",
            "2019-05-09 13:52:19.084818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704049b00 of size 1536\n",
            "2019-05-09 13:52:19.084833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404a100 of size 1536\n",
            "2019-05-09 13:52:19.084848: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404a700 of size 1536\n",
            "2019-05-09 13:52:19.084863: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404ad00 of size 1536\n",
            "2019-05-09 13:52:19.084889: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404b300 of size 1536\n",
            "2019-05-09 13:52:19.084905: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404b900 of size 256\n",
            "2019-05-09 13:52:19.084921: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404ba00 of size 1536\n",
            "2019-05-09 13:52:19.084936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c000 of size 256\n",
            "2019-05-09 13:52:19.084951: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c100 of size 1536\n",
            "2019-05-09 13:52:19.084966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404c700 of size 1536\n",
            "2019-05-09 13:52:19.084981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404cd00 of size 1536\n",
            "2019-05-09 13:52:19.084997: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404d300 of size 1536\n",
            "2019-05-09 13:52:19.085012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404d900 of size 1536\n",
            "2019-05-09 13:52:19.085039: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404df00 of size 1536\n",
            "2019-05-09 13:52:19.085057: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404e500 of size 1536\n",
            "2019-05-09 13:52:19.085073: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404eb00 of size 1536\n",
            "2019-05-09 13:52:19.085088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404f100 of size 1536\n",
            "2019-05-09 13:52:19.085102: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404f700 of size 1536\n",
            "2019-05-09 13:52:19.085117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70404fd00 of size 1536\n",
            "2019-05-09 13:52:19.085132: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050300 of size 256\n",
            "2019-05-09 13:52:19.085147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050400 of size 1536\n",
            "2019-05-09 13:52:19.085161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050a00 of size 256\n",
            "2019-05-09 13:52:19.085176: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704050b00 of size 1536\n",
            "2019-05-09 13:52:19.085191: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051100 of size 1536\n",
            "2019-05-09 13:52:19.085207: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051700 of size 1536\n",
            "2019-05-09 13:52:19.085221: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704051d00 of size 1536\n",
            "2019-05-09 13:52:19.085236: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052300 of size 1536\n",
            "2019-05-09 13:52:19.085251: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052900 of size 1536\n",
            "2019-05-09 13:52:19.085266: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704052f00 of size 1536\n",
            "2019-05-09 13:52:19.085280: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704053500 of size 1536\n",
            "2019-05-09 13:52:19.085295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704053b00 of size 1536\n",
            "2019-05-09 13:52:19.085311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054100 of size 1536\n",
            "2019-05-09 13:52:19.085325: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054700 of size 1536\n",
            "2019-05-09 13:52:19.085350: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054d00 of size 256\n",
            "2019-05-09 13:52:19.085367: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704054e00 of size 1536\n",
            "2019-05-09 13:52:19.085401: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055400 of size 256\n",
            "2019-05-09 13:52:19.085417: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055500 of size 1536\n",
            "2019-05-09 13:52:19.085449: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704055b00 of size 1536\n",
            "2019-05-09 13:52:19.085464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056100 of size 1536\n",
            "2019-05-09 13:52:19.085479: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056700 of size 1536\n",
            "2019-05-09 13:52:19.085508: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704056d00 of size 1536\n",
            "2019-05-09 13:52:19.085524: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057300 of size 1536\n",
            "2019-05-09 13:52:19.085540: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057900 of size 1536\n",
            "2019-05-09 13:52:19.085556: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704057f00 of size 1536\n",
            "2019-05-09 13:52:19.085571: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704058500 of size 1536\n",
            "2019-05-09 13:52:19.085586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704058b00 of size 1536\n",
            "2019-05-09 13:52:19.085602: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059100 of size 1536\n",
            "2019-05-09 13:52:19.085617: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059700 of size 256\n",
            "2019-05-09 13:52:19.085632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059800 of size 256\n",
            "2019-05-09 13:52:19.085648: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059900 of size 1280\n",
            "2019-05-09 13:52:19.085663: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704059e00 of size 1536\n",
            "2019-05-09 13:52:19.085679: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405a400 of size 1536\n",
            "2019-05-09 13:52:19.085694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405aa00 of size 1536\n",
            "2019-05-09 13:52:19.085709: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405b000 of size 1536\n",
            "2019-05-09 13:52:19.085725: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405b600 of size 1536\n",
            "2019-05-09 13:52:19.168645: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405bc00 of size 1536\n",
            "2019-05-09 13:52:19.168718: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405c200 of size 1536\n",
            "2019-05-09 13:52:19.168739: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405c800 of size 1536\n",
            "2019-05-09 13:52:19.168756: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405ce00 of size 1536\n",
            "2019-05-09 13:52:19.168772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405d400 of size 1536\n",
            "2019-05-09 13:52:19.168788: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405da00 of size 1536\n",
            "2019-05-09 13:52:19.168805: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405e000 of size 1536\n",
            "2019-05-09 13:52:19.168824: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405e600 of size 1536\n",
            "2019-05-09 13:52:19.168841: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405ec00 of size 1536\n",
            "2019-05-09 13:52:19.168905: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405f200 of size 1536\n",
            "2019-05-09 13:52:19.168941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405f800 of size 1536\n",
            "2019-05-09 13:52:19.168961: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70405fe00 of size 1536\n",
            "2019-05-09 13:52:19.168995: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704060400 of size 1536\n",
            "2019-05-09 13:52:19.169089: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704060a00 of size 1536\n",
            "2019-05-09 13:52:19.169135: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061000 of size 1536\n",
            "2019-05-09 13:52:19.169154: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061600 of size 1536\n",
            "2019-05-09 13:52:19.169173: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704061c00 of size 1536\n",
            "2019-05-09 13:52:19.169190: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062200 of size 1536\n",
            "2019-05-09 13:52:19.169206: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062800 of size 1536\n",
            "2019-05-09 13:52:19.169224: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704062e00 of size 1536\n",
            "2019-05-09 13:52:19.169243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704063400 of size 1536\n",
            "2019-05-09 13:52:19.169262: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704063a00 of size 1536\n",
            "2019-05-09 13:52:19.169281: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064000 of size 1536\n",
            "2019-05-09 13:52:19.169298: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064600 of size 1536\n",
            "2019-05-09 13:52:19.169315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704064c00 of size 1536\n",
            "2019-05-09 13:52:19.169332: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065200 of size 1536\n",
            "2019-05-09 13:52:19.169369: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065800 of size 1536\n",
            "2019-05-09 13:52:19.169389: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704065e00 of size 1536\n",
            "2019-05-09 13:52:19.169408: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704066400 of size 1536\n",
            "2019-05-09 13:52:19.169426: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704066a00 of size 1536\n",
            "2019-05-09 13:52:19.169443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067000 of size 1536\n",
            "2019-05-09 13:52:19.169460: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067600 of size 1536\n",
            "2019-05-09 13:52:19.169477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704067c00 of size 1536\n",
            "2019-05-09 13:52:19.169493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068200 of size 1536\n",
            "2019-05-09 13:52:19.169510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068800 of size 1536\n",
            "2019-05-09 13:52:19.169528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704068e00 of size 1536\n",
            "2019-05-09 13:52:19.169712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704069400 of size 1536\n",
            "2019-05-09 13:52:19.169741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704069a00 of size 1536\n",
            "2019-05-09 13:52:19.169763: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406a000 of size 1536\n",
            "2019-05-09 13:52:19.169797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406a600 of size 1536\n",
            "2019-05-09 13:52:19.169816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ac00 of size 1536\n",
            "2019-05-09 13:52:19.169834: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406b200 of size 1536\n",
            "2019-05-09 13:52:19.169853: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406b800 of size 1536\n",
            "2019-05-09 13:52:19.169881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406be00 of size 1536\n",
            "2019-05-09 13:52:19.169903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406c400 of size 1536\n",
            "2019-05-09 13:52:19.169922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ca00 of size 1536\n",
            "2019-05-09 13:52:19.169941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406d000 of size 1536\n",
            "2019-05-09 13:52:19.169959: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406d600 of size 1536\n",
            "2019-05-09 13:52:19.169992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406dc00 of size 1536\n",
            "2019-05-09 13:52:19.170011: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406e200 of size 1536\n",
            "2019-05-09 13:52:19.170054: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406e800 of size 1536\n",
            "2019-05-09 13:52:19.170080: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406ee00 of size 1536\n",
            "2019-05-09 13:52:19.170100: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406f400 of size 1536\n",
            "2019-05-09 13:52:19.170120: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70406fa00 of size 1536\n",
            "2019-05-09 13:52:19.170139: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070000 of size 1536\n",
            "2019-05-09 13:52:19.170170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070600 of size 1536\n",
            "2019-05-09 13:52:19.170192: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704070c00 of size 1536\n",
            "2019-05-09 13:52:19.170210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071200 of size 1536\n",
            "2019-05-09 13:52:19.170228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071800 of size 1536\n",
            "2019-05-09 13:52:19.170248: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704071e00 of size 1536\n",
            "2019-05-09 13:52:19.170267: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704072400 of size 1536\n",
            "2019-05-09 13:52:19.170287: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704072a00 of size 1536\n",
            "2019-05-09 13:52:19.170306: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073000 of size 1536\n",
            "2019-05-09 13:52:19.170324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073600 of size 1536\n",
            "2019-05-09 13:52:19.170360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704073c00 of size 1536\n",
            "2019-05-09 13:52:19.170383: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074200 of size 1536\n",
            "2019-05-09 13:52:19.170403: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074800 of size 1536\n",
            "2019-05-09 13:52:19.170422: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704074e00 of size 1536\n",
            "2019-05-09 13:52:19.170441: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704075400 of size 1536\n",
            "2019-05-09 13:52:19.170459: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704075a00 of size 1536\n",
            "2019-05-09 13:52:19.170477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076000 of size 1536\n",
            "2019-05-09 13:52:19.170496: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076600 of size 1536\n",
            "2019-05-09 13:52:19.170514: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704076c00 of size 1536\n",
            "2019-05-09 13:52:19.170533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077200 of size 1536\n",
            "2019-05-09 13:52:19.170553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077800 of size 1536\n",
            "2019-05-09 13:52:19.170573: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704077e00 of size 1536\n",
            "2019-05-09 13:52:19.170591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704078400 of size 1536\n",
            "2019-05-09 13:52:19.170619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704078a00 of size 1536\n",
            "2019-05-09 13:52:19.170640: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079000 of size 1536\n",
            "2019-05-09 13:52:19.170659: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079600 of size 1536\n",
            "2019-05-09 13:52:19.170678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704079c00 of size 1536\n",
            "2019-05-09 13:52:19.170697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407a200 of size 1536\n",
            "2019-05-09 13:52:19.170715: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407a800 of size 1536\n",
            "2019-05-09 13:52:19.170734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ae00 of size 1536\n",
            "2019-05-09 13:52:19.170753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407b400 of size 1536\n",
            "2019-05-09 13:52:19.170773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ba00 of size 1536\n",
            "2019-05-09 13:52:19.170792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407c000 of size 1536\n",
            "2019-05-09 13:52:19.170812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407c600 of size 1536\n",
            "2019-05-09 13:52:19.170832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407cc00 of size 1536\n",
            "2019-05-09 13:52:19.170851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407d200 of size 1536\n",
            "2019-05-09 13:52:19.170882: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407d800 of size 1536\n",
            "2019-05-09 13:52:19.170904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407de00 of size 1536\n",
            "2019-05-09 13:52:19.170922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407e400 of size 1536\n",
            "2019-05-09 13:52:19.170941: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407ea00 of size 1536\n",
            "2019-05-09 13:52:19.170962: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407f000 of size 1536\n",
            "2019-05-09 13:52:19.170981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407f600 of size 1536\n",
            "2019-05-09 13:52:19.171000: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70407fc00 of size 1536\n",
            "2019-05-09 13:52:19.171018: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080200 of size 1536\n",
            "2019-05-09 13:52:19.171063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080800 of size 1536\n",
            "2019-05-09 13:52:19.171085: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704080e00 of size 1536\n",
            "2019-05-09 13:52:19.171103: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704081400 of size 1536\n",
            "2019-05-09 13:52:19.171122: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704081a00 of size 1536\n",
            "2019-05-09 13:52:19.171141: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082000 of size 1536\n",
            "2019-05-09 13:52:19.171160: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082600 of size 1536\n",
            "2019-05-09 13:52:19.171180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704082c00 of size 1536\n",
            "2019-05-09 13:52:19.171199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083200 of size 1536\n",
            "2019-05-09 13:52:19.171218: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083800 of size 1536\n",
            "2019-05-09 13:52:19.171237: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704083e00 of size 1536\n",
            "2019-05-09 13:52:19.171255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704084400 of size 1536\n",
            "2019-05-09 13:52:19.171289: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704084a00 of size 1536\n",
            "2019-05-09 13:52:19.171308: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085000 of size 1536\n",
            "2019-05-09 13:52:19.171326: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085600 of size 1536\n",
            "2019-05-09 13:52:19.171360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704085c00 of size 1536\n",
            "2019-05-09 13:52:19.171380: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086200 of size 1536\n",
            "2019-05-09 13:52:19.171415: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086800 of size 1536\n",
            "2019-05-09 13:52:19.171433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704086e00 of size 1536\n",
            "2019-05-09 13:52:19.171452: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704087400 of size 1536\n",
            "2019-05-09 13:52:19.171472: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704087a00 of size 1536\n",
            "2019-05-09 13:52:19.171492: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088000 of size 1536\n",
            "2019-05-09 13:52:19.171510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088600 of size 1536\n",
            "2019-05-09 13:52:19.171533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704088c00 of size 1536\n",
            "2019-05-09 13:52:19.171553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089200 of size 1536\n",
            "2019-05-09 13:52:19.171570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089800 of size 1536\n",
            "2019-05-09 13:52:19.171588: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704089e00 of size 1536\n",
            "2019-05-09 13:52:19.171607: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408a400 of size 1536\n",
            "2019-05-09 13:52:19.171625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408aa00 of size 1536\n",
            "2019-05-09 13:52:19.171644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b000 of size 1536\n",
            "2019-05-09 13:52:19.171664: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b600 of size 256\n",
            "2019-05-09 13:52:19.171683: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b700 of size 256\n",
            "2019-05-09 13:52:19.171703: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408b800 of size 1536\n",
            "2019-05-09 13:52:19.171722: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408be00 of size 1536\n",
            "2019-05-09 13:52:19.171741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408c400 of size 1536\n",
            "2019-05-09 13:52:19.171760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408ca00 of size 1536\n",
            "2019-05-09 13:52:19.171778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408d000 of size 1536\n",
            "2019-05-09 13:52:19.171796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408d600 of size 1536\n",
            "2019-05-09 13:52:19.171814: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408dc00 of size 1536\n",
            "2019-05-09 13:52:19.171832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408e200 of size 1536\n",
            "2019-05-09 13:52:19.171851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408e800 of size 1536\n",
            "2019-05-09 13:52:19.171880: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408ee00 of size 1536\n",
            "2019-05-09 13:52:19.171903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408f400 of size 1536\n",
            "2019-05-09 13:52:19.171922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70408fa00 of size 1536\n",
            "2019-05-09 13:52:19.171950: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090000 of size 1536\n",
            "2019-05-09 13:52:19.171971: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090600 of size 1536\n",
            "2019-05-09 13:52:19.171989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704090c00 of size 1536\n",
            "2019-05-09 13:52:19.172008: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091200 of size 1536\n",
            "2019-05-09 13:52:19.172027: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091800 of size 1536\n",
            "2019-05-09 13:52:19.172074: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704091e00 of size 1536\n",
            "2019-05-09 13:52:19.172095: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704092400 of size 1536\n",
            "2019-05-09 13:52:19.172115: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704092a00 of size 1536\n",
            "2019-05-09 13:52:19.172133: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093000 of size 1536\n",
            "2019-05-09 13:52:19.172152: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093600 of size 1536\n",
            "2019-05-09 13:52:19.172171: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093c00 of size 256\n",
            "2019-05-09 13:52:19.172190: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093d00 of size 256\n",
            "2019-05-09 13:52:19.172210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093e00 of size 256\n",
            "2019-05-09 13:52:19.172228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704093f00 of size 256\n",
            "2019-05-09 13:52:19.172247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094000 of size 256\n",
            "2019-05-09 13:52:19.172265: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094100 of size 256\n",
            "2019-05-09 13:52:19.172284: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094200 of size 256\n",
            "2019-05-09 13:52:19.172302: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094300 of size 256\n",
            "2019-05-09 13:52:19.172322: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094400 of size 256\n",
            "2019-05-09 13:52:19.172359: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094500 of size 256\n",
            "2019-05-09 13:52:19.172382: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094600 of size 256\n",
            "2019-05-09 13:52:19.172402: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094700 of size 1536\n",
            "2019-05-09 13:52:19.172431: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704094d00 of size 1536\n",
            "2019-05-09 13:52:19.172450: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095300 of size 1536\n",
            "2019-05-09 13:52:19.172469: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095900 of size 1536\n",
            "2019-05-09 13:52:19.172488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704095f00 of size 1536\n",
            "2019-05-09 13:52:19.172507: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704096500 of size 1536\n",
            "2019-05-09 13:52:19.172526: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704096b00 of size 1536\n",
            "2019-05-09 13:52:19.172544: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097100 of size 1536\n",
            "2019-05-09 13:52:19.172563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097700 of size 1536\n",
            "2019-05-09 13:52:19.172581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704097d00 of size 1536\n",
            "2019-05-09 13:52:19.172601: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098300 of size 1536\n",
            "2019-05-09 13:52:19.172619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098900 of size 1536\n",
            "2019-05-09 13:52:19.172639: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704098f00 of size 1536\n",
            "2019-05-09 13:52:19.172658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704099500 of size 1536\n",
            "2019-05-09 13:52:19.172676: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704099b00 of size 1536\n",
            "2019-05-09 13:52:19.172694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409a100 of size 1536\n",
            "2019-05-09 13:52:19.172712: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409a700 of size 1536\n",
            "2019-05-09 13:52:19.172731: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409ad00 of size 1536\n",
            "2019-05-09 13:52:19.172750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409b300 of size 1536\n",
            "2019-05-09 13:52:19.172769: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409b900 of size 1536\n",
            "2019-05-09 13:52:19.172804: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409bf00 of size 1536\n",
            "2019-05-09 13:52:19.172828: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409c500 of size 1536\n",
            "2019-05-09 13:52:19.172851: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409cb00 of size 1536\n",
            "2019-05-09 13:52:19.172881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409d100 of size 1536\n",
            "2019-05-09 13:52:19.172903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409d700 of size 1536\n",
            "2019-05-09 13:52:19.172922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409dd00 of size 1536\n",
            "2019-05-09 13:52:19.172942: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409e300 of size 1536\n",
            "2019-05-09 13:52:19.172962: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409e900 of size 1536\n",
            "2019-05-09 13:52:19.172980: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409ef00 of size 1536\n",
            "2019-05-09 13:52:19.172999: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409f500 of size 1536\n",
            "2019-05-09 13:52:19.173018: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70409fb00 of size 1536\n",
            "2019-05-09 13:52:19.173072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0100 of size 1536\n",
            "2019-05-09 13:52:19.173095: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0700 of size 1536\n",
            "2019-05-09 13:52:19.173115: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a0d00 of size 1536\n",
            "2019-05-09 13:52:19.173134: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1300 of size 1536\n",
            "2019-05-09 13:52:19.173152: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1900 of size 1536\n",
            "2019-05-09 13:52:19.173170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a1f00 of size 1536\n",
            "2019-05-09 13:52:19.173189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a2500 of size 1536\n",
            "2019-05-09 13:52:19.173208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a2b00 of size 1536\n",
            "2019-05-09 13:52:19.173228: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3100 of size 1536\n",
            "2019-05-09 13:52:19.173247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3700 of size 1536\n",
            "2019-05-09 13:52:19.173266: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a3d00 of size 1536\n",
            "2019-05-09 13:52:19.173297: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4300 of size 1536\n",
            "2019-05-09 13:52:19.173331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4900 of size 1536\n",
            "2019-05-09 13:52:19.173398: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a4f00 of size 1536\n",
            "2019-05-09 13:52:19.173419: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a5500 of size 1536\n",
            "2019-05-09 13:52:19.173438: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a5b00 of size 1536\n",
            "2019-05-09 13:52:19.173457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6100 of size 1536\n",
            "2019-05-09 13:52:19.173493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6700 of size 1536\n",
            "2019-05-09 13:52:19.173513: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a6d00 of size 1536\n",
            "2019-05-09 13:52:19.173532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7300 of size 1536\n",
            "2019-05-09 13:52:19.173551: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7900 of size 1536\n",
            "2019-05-09 13:52:19.173570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a7f00 of size 1536\n",
            "2019-05-09 13:52:19.173590: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a8500 of size 1536\n",
            "2019-05-09 13:52:19.173609: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a8b00 of size 1536\n",
            "2019-05-09 13:52:19.173628: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9100 of size 1536\n",
            "2019-05-09 13:52:19.173647: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9700 of size 256\n",
            "2019-05-09 13:52:19.173666: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9800 of size 256\n",
            "2019-05-09 13:52:19.173685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9900 of size 256\n",
            "2019-05-09 13:52:19.173703: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9a00 of size 256\n",
            "2019-05-09 13:52:19.173724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9b00 of size 256\n",
            "2019-05-09 13:52:19.173744: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9c00 of size 256\n",
            "2019-05-09 13:52:19.173763: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9d00 of size 256\n",
            "2019-05-09 13:52:19.173782: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9e00 of size 256\n",
            "2019-05-09 13:52:19.173801: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040a9f00 of size 256\n",
            "2019-05-09 13:52:19.173819: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa000 of size 256\n",
            "2019-05-09 13:52:19.173839: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa100 of size 256\n",
            "2019-05-09 13:52:19.173858: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa200 of size 256\n",
            "2019-05-09 13:52:19.173891: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa300 of size 256\n",
            "2019-05-09 13:52:19.173914: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa400 of size 256\n",
            "2019-05-09 13:52:19.173933: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa500 of size 256\n",
            "2019-05-09 13:52:19.173953: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa600 of size 256\n",
            "2019-05-09 13:52:19.173973: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa700 of size 256\n",
            "2019-05-09 13:52:19.173993: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa800 of size 256\n",
            "2019-05-09 13:52:19.174012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aa900 of size 256\n",
            "2019-05-09 13:52:19.174055: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aaa00 of size 256\n",
            "2019-05-09 13:52:19.174078: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aab00 of size 256\n",
            "2019-05-09 13:52:19.174098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aac00 of size 256\n",
            "2019-05-09 13:52:19.174116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aad00 of size 256\n",
            "2019-05-09 13:52:19.174136: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aae00 of size 256\n",
            "2019-05-09 13:52:19.174156: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aaf00 of size 256\n",
            "2019-05-09 13:52:19.174176: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab000 of size 256\n",
            "2019-05-09 13:52:19.174196: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab100 of size 256\n",
            "2019-05-09 13:52:19.174216: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab200 of size 256\n",
            "2019-05-09 13:52:19.174235: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab300 of size 256\n",
            "2019-05-09 13:52:19.174254: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7040ab400 of size 512\n",
            "2019-05-09 13:52:19.174276: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ab600 of size 7680\n",
            "2019-05-09 13:52:19.174295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ad400 of size 256\n",
            "2019-05-09 13:52:19.174315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ad500 of size 7680\n",
            "2019-05-09 13:52:19.174349: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040af300 of size 1536\n",
            "2019-05-09 13:52:19.174373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040af900 of size 1536\n",
            "2019-05-09 13:52:19.174393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040aff00 of size 1536\n",
            "2019-05-09 13:52:19.174413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b0500 of size 1536\n",
            "2019-05-09 13:52:19.174433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b0b00 of size 1536\n",
            "2019-05-09 13:52:19.174452: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1100 of size 256\n",
            "2019-05-09 13:52:19.174472: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1200 of size 256\n",
            "2019-05-09 13:52:19.174491: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1300 of size 256\n",
            "2019-05-09 13:52:19.174510: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1400 of size 1536\n",
            "2019-05-09 13:52:19.174528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1a00 of size 256\n",
            "2019-05-09 13:52:19.174547: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b1b00 of size 1536\n",
            "2019-05-09 13:52:19.174566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2100 of size 1536\n",
            "2019-05-09 13:52:19.174586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2700 of size 1536\n",
            "2019-05-09 13:52:19.174606: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b2d00 of size 1536\n",
            "2019-05-09 13:52:19.174625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3300 of size 256\n",
            "2019-05-09 13:52:19.174647: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3400 of size 1536\n",
            "2019-05-09 13:52:19.174666: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3a00 of size 256\n",
            "2019-05-09 13:52:19.174685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3b00 of size 256\n",
            "2019-05-09 13:52:19.174705: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3c00 of size 256\n",
            "2019-05-09 13:52:19.174724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3d00 of size 256\n",
            "2019-05-09 13:52:19.174745: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3e00 of size 256\n",
            "2019-05-09 13:52:19.174765: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b3f00 of size 256\n",
            "2019-05-09 13:52:19.174785: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4000 of size 256\n",
            "2019-05-09 13:52:19.174819: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4100 of size 256\n",
            "2019-05-09 13:52:19.174838: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4200 of size 256\n",
            "2019-05-09 13:52:19.174857: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4300 of size 256\n",
            "2019-05-09 13:52:19.174888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4400 of size 256\n",
            "2019-05-09 13:52:19.174908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4500 of size 1536\n",
            "2019-05-09 13:52:19.174928: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b4b00 of size 1536\n",
            "2019-05-09 13:52:19.174946: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5100 of size 1536\n",
            "2019-05-09 13:52:19.174965: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5700 of size 256\n",
            "2019-05-09 13:52:19.174985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5800 of size 1536\n",
            "2019-05-09 13:52:19.175005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b5e00 of size 1536\n",
            "2019-05-09 13:52:19.175024: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6400 of size 1536\n",
            "2019-05-09 13:52:19.175067: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6a00 of size 256\n",
            "2019-05-09 13:52:19.175088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6b00 of size 256\n",
            "2019-05-09 13:52:19.175123: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b6c00 of size 1536\n",
            "2019-05-09 13:52:19.175142: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7200 of size 1536\n",
            "2019-05-09 13:52:19.175161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7800 of size 1536\n",
            "2019-05-09 13:52:19.175180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b7e00 of size 1536\n",
            "2019-05-09 13:52:19.175200: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b8400 of size 1536\n",
            "2019-05-09 13:52:19.175219: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b8a00 of size 1536\n",
            "2019-05-09 13:52:19.175239: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9000 of size 1536\n",
            "2019-05-09 13:52:19.175259: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9600 of size 1536\n",
            "2019-05-09 13:52:19.175279: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040b9c00 of size 1536\n",
            "2019-05-09 13:52:19.175299: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba200 of size 1536\n",
            "2019-05-09 13:52:19.175318: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba800 of size 256\n",
            "2019-05-09 13:52:19.175352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040ba900 of size 1536\n",
            "2019-05-09 13:52:19.175374: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040baf00 of size 256\n",
            "2019-05-09 13:52:19.175394: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb000 of size 256\n",
            "2019-05-09 13:52:19.175413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb100 of size 256\n",
            "2019-05-09 13:52:19.175433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb200 of size 256\n",
            "2019-05-09 13:52:19.175453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb300 of size 1536\n",
            "2019-05-09 13:52:19.175473: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bb900 of size 256\n",
            "2019-05-09 13:52:19.175493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bba00 of size 1536\n",
            "2019-05-09 13:52:19.175512: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bc000 of size 1536\n",
            "2019-05-09 13:52:19.175531: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bc600 of size 1536\n",
            "2019-05-09 13:52:19.175549: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bcc00 of size 1536\n",
            "2019-05-09 13:52:19.175567: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bd200 of size 1536\n",
            "2019-05-09 13:52:19.175586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bd800 of size 1536\n",
            "2019-05-09 13:52:19.175605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bde00 of size 1536\n",
            "2019-05-09 13:52:19.175625: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040be400 of size 1536\n",
            "2019-05-09 13:52:19.175644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bea00 of size 1536\n",
            "2019-05-09 13:52:19.175662: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf000 of size 1536\n",
            "2019-05-09 13:52:19.175682: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf600 of size 256\n",
            "2019-05-09 13:52:19.175702: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040bf700 of size 2304\n",
            "2019-05-09 13:52:19.175724: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040c0000 of size 495616\n",
            "2019-05-09 13:52:19.175744: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139000 of size 495616\n",
            "2019-05-09 13:52:19.175777: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b2000 of size 495616\n",
            "2019-05-09 13:52:19.175795: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70422b000 of size 610304\n",
            "2019-05-09 13:52:19.175814: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0000 of size 495616\n",
            "2019-05-09 13:52:19.175833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704339000 of size 495616\n",
            "2019-05-09 13:52:19.175852: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b2000 of size 495616\n",
            "2019-05-09 13:52:19.175883: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442b000 of size 495616\n",
            "2019-05-09 13:52:19.175904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7044a4000 of size 495616\n",
            "2019-05-09 13:52:19.175929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70451d000 of size 495616\n",
            "2019-05-09 13:52:19.175947: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704596000 of size 495616\n",
            "2019-05-09 13:52:19.175966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70460f000 of size 724992\n",
            "2019-05-09 13:52:19.175986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7046c0000 of size 495616\n",
            "2019-05-09 13:52:19.176021: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739000 of size 495616\n",
            "2019-05-09 13:52:19.176066: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b2000 of size 495616\n",
            "2019-05-09 13:52:19.176087: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482b000 of size 495616\n",
            "2019-05-09 13:52:19.176107: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a4000 of size 495616\n",
            "2019-05-09 13:52:19.176127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70491d000 of size 495616\n",
            "2019-05-09 13:52:19.176147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704996000 of size 495616\n",
            "2019-05-09 13:52:19.176166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a0f000 of size 495616\n",
            "2019-05-09 13:52:19.176185: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a88000 of size 495616\n",
            "2019-05-09 13:52:19.176205: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b01000 of size 495616\n",
            "2019-05-09 13:52:19.176224: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b7a000 of size 495616\n",
            "2019-05-09 13:52:19.176243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704bf3000 of size 495616\n",
            "2019-05-09 13:52:19.176263: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c6c000 of size 495616\n",
            "2019-05-09 13:52:19.176282: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ce5000 of size 495616\n",
            "2019-05-09 13:52:19.176318: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704d5e000 of size 495616\n",
            "2019-05-09 13:52:19.176368: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704dd7000 of size 954368\n",
            "2019-05-09 13:52:19.176408: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0000 of size 495616\n",
            "2019-05-09 13:52:19.176444: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704f39000 of size 495616\n",
            "2019-05-09 13:52:19.176464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704fb2000 of size 495616\n",
            "2019-05-09 13:52:19.176482: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502b000 of size 495616\n",
            "2019-05-09 13:52:19.176501: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7050a4000 of size 495616\n",
            "2019-05-09 13:52:19.176521: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70511d000 of size 495616\n",
            "2019-05-09 13:52:19.176540: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705196000 of size 495616\n",
            "2019-05-09 13:52:19.176561: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70520f000 of size 495616\n",
            "2019-05-09 13:52:19.176580: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705288000 of size 495616\n",
            "2019-05-09 13:52:19.176599: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705301000 of size 495616\n",
            "2019-05-09 13:52:19.176619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70537a000 of size 495616\n",
            "2019-05-09 13:52:19.176638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7053f3000 of size 495616\n",
            "2019-05-09 13:52:19.176658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70546c000 of size 495616\n",
            "2019-05-09 13:52:19.176678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054e5000 of size 495616\n",
            "2019-05-09 13:52:19.176697: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555e000 of size 1536\n",
            "2019-05-09 13:52:19.176716: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555e600 of size 1536\n",
            "2019-05-09 13:52:19.176735: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555ec00 of size 1536\n",
            "2019-05-09 13:52:19.176754: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f200 of size 256\n",
            "2019-05-09 13:52:19.176773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f300 of size 1536\n",
            "2019-05-09 13:52:19.176793: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555f900 of size 256\n",
            "2019-05-09 13:52:19.176828: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555fa00 of size 256\n",
            "2019-05-09 13:52:19.176848: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70555fb00 of size 1536\n",
            "2019-05-09 13:52:19.176876: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560100 of size 256\n",
            "2019-05-09 13:52:19.176899: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560200 of size 256\n",
            "2019-05-09 13:52:19.176918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560300 of size 256\n",
            "2019-05-09 13:52:19.176937: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560400 of size 256\n",
            "2019-05-09 13:52:19.176956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560500 of size 256\n",
            "2019-05-09 13:52:19.176976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560600 of size 256\n",
            "2019-05-09 13:52:19.176995: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560700 of size 256\n",
            "2019-05-09 13:52:19.177013: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560800 of size 256\n",
            "2019-05-09 13:52:19.177051: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560900 of size 256\n",
            "2019-05-09 13:52:19.177073: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705560a00 of size 1536\n",
            "2019-05-09 13:52:19.177108: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561000 of size 1536\n",
            "2019-05-09 13:52:19.177128: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561600 of size 1536\n",
            "2019-05-09 13:52:19.177147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705561c00 of size 1536\n",
            "2019-05-09 13:52:19.177166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562200 of size 1536\n",
            "2019-05-09 13:52:19.177185: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562800 of size 1536\n",
            "2019-05-09 13:52:19.177203: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562e00 of size 256\n",
            "2019-05-09 13:52:19.177223: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705562f00 of size 256\n",
            "2019-05-09 13:52:19.177243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563000 of size 256\n",
            "2019-05-09 13:52:19.177274: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563100 of size 256\n",
            "2019-05-09 13:52:19.177294: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563200 of size 256\n",
            "2019-05-09 13:52:19.177314: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563300 of size 1536\n",
            "2019-05-09 13:52:19.177349: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563900 of size 1536\n",
            "2019-05-09 13:52:19.177373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705563f00 of size 1536\n",
            "2019-05-09 13:52:19.177393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705564500 of size 1536\n",
            "2019-05-09 13:52:19.177413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705564b00 of size 1536\n",
            "2019-05-09 13:52:19.177432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565100 of size 1536\n",
            "2019-05-09 13:52:19.177453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565700 of size 256\n",
            "2019-05-09 13:52:19.177477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565800 of size 256\n",
            "2019-05-09 13:52:19.177503: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565900 of size 256\n",
            "2019-05-09 13:52:19.177523: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565a00 of size 256\n",
            "2019-05-09 13:52:19.177546: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705565b00 of size 1536\n",
            "2019-05-09 13:52:19.177566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566100 of size 1536\n",
            "2019-05-09 13:52:19.177586: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566700 of size 1536\n",
            "2019-05-09 13:52:19.177605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705566d00 of size 1536\n",
            "2019-05-09 13:52:19.177624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567300 of size 1536\n",
            "2019-05-09 13:52:19.177643: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567900 of size 1536\n",
            "2019-05-09 13:52:19.177662: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705567f00 of size 256\n",
            "2019-05-09 13:52:19.177682: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568000 of size 256\n",
            "2019-05-09 13:52:19.177702: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568100 of size 256\n",
            "2019-05-09 13:52:19.177721: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568200 of size 1536\n",
            "2019-05-09 13:52:19.177741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568800 of size 1536\n",
            "2019-05-09 13:52:19.177760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705568e00 of size 1536\n",
            "2019-05-09 13:52:19.177778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705569400 of size 1536\n",
            "2019-05-09 13:52:19.177796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705569a00 of size 1536\n",
            "2019-05-09 13:52:19.177815: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556a000 of size 1536\n",
            "2019-05-09 13:52:19.177834: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556a600 of size 1536\n",
            "2019-05-09 13:52:19.177854: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556ac00 of size 1536\n",
            "2019-05-09 13:52:19.177886: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556b200 of size 1536\n",
            "2019-05-09 13:52:19.177909: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556b800 of size 1536\n",
            "2019-05-09 13:52:19.177929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556be00 of size 256\n",
            "2019-05-09 13:52:19.177949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556bf00 of size 1536\n",
            "2019-05-09 13:52:19.177969: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556c500 of size 512\n",
            "2019-05-09 13:52:19.177989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556c700 of size 1536\n",
            "2019-05-09 13:52:19.178007: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x70556cd00 of size 768\n",
            "2019-05-09 13:52:19.178026: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d000 of size 256\n",
            "2019-05-09 13:52:19.178068: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d100 of size 256\n",
            "2019-05-09 13:52:19.178090: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d200 of size 256\n",
            "2019-05-09 13:52:19.178110: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d300 of size 1536\n",
            "2019-05-09 13:52:19.178131: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556d900 of size 1536\n",
            "2019-05-09 13:52:19.178150: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556df00 of size 1536\n",
            "2019-05-09 13:52:19.178170: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556e500 of size 1536\n",
            "2019-05-09 13:52:19.178189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556eb00 of size 1536\n",
            "2019-05-09 13:52:19.178208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556f100 of size 1536\n",
            "2019-05-09 13:52:19.178227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70556f700 of size 8960\n",
            "2019-05-09 13:52:19.178247: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705571a00 of size 1536\n",
            "2019-05-09 13:52:19.178268: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705572000 of size 1536\n",
            "2019-05-09 13:52:19.178288: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705572600 of size 3328\n",
            "2019-05-09 13:52:19.178307: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705573300 of size 8960\n",
            "2019-05-09 13:52:19.178327: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705575600 of size 256\n",
            "2019-05-09 13:52:19.178364: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705575700 of size 14336\n",
            "2019-05-09 13:52:19.178388: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705578f00 of size 1536\n",
            "2019-05-09 13:52:19.178409: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705579500 of size 921600\n",
            "2019-05-09 13:52:19.178430: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70565a500 of size 1781760\n",
            "2019-05-09 13:52:19.178449: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70580d500 of size 2703360\n",
            "2019-05-09 13:52:19.178468: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705aa1500 of size 921600\n",
            "2019-05-09 13:52:19.178487: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705b82500 of size 921600\n",
            "2019-05-09 13:52:19.178507: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705c63500 of size 921600\n",
            "2019-05-09 13:52:19.178528: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705d44500 of size 1555200\n",
            "2019-05-09 13:52:19.178550: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7060c0000 of size 134217728\n",
            "2019-05-09 13:52:19.178577: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70e0c0000 of size 23040000\n",
            "2019-05-09 13:52:19.178597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70f6b9000 of size 23040000\n",
            "2019-05-09 13:52:19.178616: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x710cb2000 of size 23040000\n",
            "2019-05-09 13:52:19.178635: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7122ab000 of size 23040000\n",
            "2019-05-09 13:52:19.178655: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7138a4000 of size 2703360\n",
            "2019-05-09 13:52:19.178675: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713b38000 of size 921600\n",
            "2019-05-09 13:52:19.178695: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713c19000 of size 921600\n",
            "2019-05-09 13:52:19.178714: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713cfa000 of size 1781760\n",
            "2019-05-09 13:52:19.178734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713ead000 of size 921600\n",
            "2019-05-09 13:52:19.178753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x713f8e000 of size 921600\n",
            "2019-05-09 13:52:19.178773: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71406f000 of size 921600\n",
            "2019-05-09 13:52:19.178793: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714150000 of size 921600\n",
            "2019-05-09 13:52:19.178812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714231000 of size 921600\n",
            "2019-05-09 13:52:19.178831: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714312000 of size 921600\n",
            "2019-05-09 13:52:19.178904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7143f3000 of size 921600\n",
            "2019-05-09 13:52:19.178936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7144d4000 of size 921600\n",
            "2019-05-09 13:52:19.178970: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7145b5000 of size 921600\n",
            "2019-05-09 13:52:19.179019: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714696000 of size 921600\n",
            "2019-05-09 13:52:19.179094: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714777000 of size 921600\n",
            "2019-05-09 13:52:19.179116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714858000 of size 921600\n",
            "2019-05-09 13:52:19.179137: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x714939000 of size 921600\n",
            "2019-05-09 13:52:19.179156: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x714a1a000 of size 23748608\n",
            "2019-05-09 13:52:19.179177: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7160c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179197: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179217: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71e1a8000 of size 133267456\n",
            "2019-05-09 13:52:19.179238: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7260c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179258: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179277: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179296: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179315: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x736290000 of size 67584000\n",
            "2019-05-09 13:52:19.179348: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179373: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73e378000 of size 131366912\n",
            "2019-05-09 13:52:19.179396: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7460c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179415: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179434: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179454: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179474: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x756290000 of size 67584000\n",
            "2019-05-09 13:52:19.179494: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179513: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75e378000 of size 67584000\n",
            "2019-05-09 13:52:19.179532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7623ec000 of size 67584000\n",
            "2019-05-09 13:52:19.179561: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x766460000 of size 67584000\n",
            "2019-05-09 13:52:19.179581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76a4d4000 of size 67584000\n",
            "2019-05-09 13:52:19.179600: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76e548000 of size 67584000\n",
            "2019-05-09 13:52:19.179619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7725bc000 of size 67584000\n",
            "2019-05-09 13:52:19.179638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x776630000 of size 67584000\n",
            "2019-05-09 13:52:19.179657: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77a6a4000 of size 67584000\n",
            "2019-05-09 13:52:19.179678: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77e718000 of size 127565824\n",
            "2019-05-09 13:52:19.179699: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7860c0000 of size 67584000\n",
            "2019-05-09 13:52:19.179718: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78a134000 of size 67584000\n",
            "2019-05-09 13:52:19.179738: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78e1a8000 of size 67584000\n",
            "2019-05-09 13:52:19.179758: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79221c000 of size 67584000\n",
            "2019-05-09 13:52:19.179777: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x796290000 of size 67584000\n",
            "2019-05-09 13:52:19.179797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79a304000 of size 67584000\n",
            "2019-05-09 13:52:19.179816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79e378000 of size 67584000\n",
            "2019-05-09 13:52:19.179835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a23ec000 of size 67584000\n",
            "2019-05-09 13:52:19.179854: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a6460000 of size 67584000\n",
            "2019-05-09 13:52:19.179885: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7aa4d4000 of size 67584000\n",
            "2019-05-09 13:52:19.179908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ae548000 of size 67584000\n",
            "2019-05-09 13:52:19.179928: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b25bc000 of size 67584000\n",
            "2019-05-09 13:52:19.179948: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b6630000 of size 67584000\n",
            "2019-05-09 13:52:19.179967: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ba6a4000 of size 67584000\n",
            "2019-05-09 13:52:19.179986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7be718000 of size 67584000\n",
            "2019-05-09 13:52:19.180004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c278c000 of size 67584000\n",
            "2019-05-09 13:52:19.180023: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c6800000 of size 67584000\n",
            "2019-05-09 13:52:19.180088: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ca874000 of size 67584000\n",
            "2019-05-09 13:52:19.180109: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ce8e8000 of size 67584000\n",
            "2019-05-09 13:52:19.180128: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d295c000 of size 67584000\n",
            "2019-05-09 13:52:19.180147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d69d0000 of size 67584000\n",
            "2019-05-09 13:52:19.180167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7daa44000 of size 67584000\n",
            "2019-05-09 13:52:19.180201: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7deab8000 of size 67584000\n",
            "2019-05-09 13:52:19.180221: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e2b2c000 of size 67584000\n",
            "2019-05-09 13:52:19.180255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e6ba0000 of size 67584000\n",
            "2019-05-09 13:52:19.180273: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eac14000 of size 67584000\n",
            "2019-05-09 13:52:19.180291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eec88000 of size 67584000\n",
            "2019-05-09 13:52:19.180324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f2cfc000 of size 67584000\n",
            "2019-05-09 13:52:19.180360: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f6d70000 of size 67584000\n",
            "2019-05-09 13:52:19.180382: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7fade4000 of size 67584000\n",
            "2019-05-09 13:52:19.180404: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fee58000 of size 119963648\n",
            "2019-05-09 13:52:19.180425: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8060c0000 of size 64512000\n",
            "2019-05-09 13:52:19.180445: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x809e46000 of size 67584000\n",
            "2019-05-09 13:52:19.180464: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x80deba000 of size 67584000\n",
            "2019-05-09 13:52:19.180483: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x811f2e000 of size 67584000\n",
            "2019-05-09 13:52:19.180503: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x815fa2000 of size 64512000\n",
            "2019-05-09 13:52:19.180522: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x819d28000 of size 387072000\n",
            "2019-05-09 13:52:19.180541: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x830e4c000 of size 1612800000\n",
            "2019-05-09 13:52:19.180560: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x891062000 of size 1612800000\n",
            "2019-05-09 13:52:19.180581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8f1278000 of size 350519296\n",
            "2019-05-09 13:52:19.180601: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x9061c0000 of size 1612800000\n",
            "2019-05-09 13:52:19.180619: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x9663d6000 of size 1041705984\n",
            "2019-05-09 13:52:19.180639: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \n",
            "2019-05-09 13:52:19.180671: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 114 Chunks of size 256 totalling 28.5KiB\n",
            "2019-05-09 13:52:19.180696: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2019-05-09 13:52:19.180720: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 377 Chunks of size 1536 totalling 565.5KiB\n",
            "2019-05-09 13:52:19.180742: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 2304 totalling 2.2KiB\n",
            "2019-05-09 13:52:19.180763: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 7680 totalling 15.0KiB\n",
            "2019-05-09 13:52:19.180785: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 8960 totalling 17.5KiB\n",
            "2019-05-09 13:52:19.180809: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 40 Chunks of size 495616 totalling 18.91MiB\n",
            "2019-05-09 13:52:19.180832: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 610304 totalling 596.0KiB\n",
            "2019-05-09 13:52:19.180854: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 724992 totalling 708.0KiB\n",
            "2019-05-09 13:52:19.180890: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 19 Chunks of size 921600 totalling 16.70MiB\n",
            "2019-05-09 13:52:19.180915: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 954368 totalling 932.0KiB\n",
            "2019-05-09 13:52:19.180938: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1555200 totalling 1.48MiB\n",
            "2019-05-09 13:52:19.180961: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 1781760 totalling 3.40MiB\n",
            "2019-05-09 13:52:19.180983: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2703360 totalling 5.16MiB\n",
            "2019-05-09 13:52:19.181006: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 23040000 totalling 87.89MiB\n",
            "2019-05-09 13:52:19.181050: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 64512000 totalling 61.52MiB\n",
            "2019-05-09 13:52:19.181078: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 53 Chunks of size 67584000 totalling 3.34GiB\n",
            "2019-05-09 13:52:19.181102: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 119963648 totalling 114.41MiB\n",
            "2019-05-09 13:52:19.181125: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 127565824 totalling 121.66MiB\n",
            "2019-05-09 13:52:19.181147: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131366912 totalling 125.28MiB\n",
            "2019-05-09 13:52:19.181170: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 133267456 totalling 127.09MiB\n",
            "2019-05-09 13:52:19.181191: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 134217728 totalling 128.00MiB\n",
            "2019-05-09 13:52:19.181214: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 3 Chunks of size 1612800000 totalling 4.51GiB\n",
            "2019-05-09 13:52:19.181237: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 8.64GiB\n",
            "2019-05-09 13:52:19.181263: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \n",
            "Limit:                 11276946637\n",
            "InUse:                  9274200832\n",
            "MaxInUse:               9466815232\n",
            "NumAllocs:                    1294\n",
            "MaxAllocSize:           1612800000\n",
            "\n",
            "2019-05-09 13:52:19.181331: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ******************************************__******************************__***************_________\n",
            "2019-05-09 13:52:19.181551: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at batch_matmul_op_impl.h:586 : Resource exhausted: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[{{node Adam/update}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "Caused by op 'gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 211, in run_training\n",
            "    aggregation_method=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\n",
            "    grad_loss=grad_loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\n",
            "    colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\n",
            "    unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\", line 1229, in _BatchMatMul\n",
            "    grad_y = math_ops.matmul(x, grad, adjoint_a=True, adjoint_b=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "...which was originally created as op 'func_map_loss/einsum_7/MatMul', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "[elided 1 identical lines from previous traceback]\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 200, in run_training\n",
            "    target_evecs_trans, target_evals\n",
            "  File \"/content/drive/unsupervisedfmnet/DFMnet.py\", line 83, in dfmnet_model\n",
            "    F, G\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 180, in func_map_layer\n",
            "    target_evecs, target_evecs_trans) +\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 116, in penalty_desc_commutativity\n",
            "    G_diag_reduce1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 262, in einsum\n",
            "    axes_to_sum)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 394, in _einsum_reduction\n",
            "    product = math_ops.matmul(t0, t1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kV_mS9G5_rM",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第三次 \n",
        "模型顶点数为3000，batch_size=8,溢出"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R87O3veV4DmD",
        "colab_type": "code",
        "outputId": "5d42b36a-78fa-4106-cea8-4e648d5e43a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13107
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 13:55:24.073850: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 13:55:24.074117: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x19b7180 executing computations on platform Host. Devices:\n",
            "2019-05-09 13:55:24.074157: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 13:55:24.181504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 13:55:24.182102: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x19b7440 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 13:55:24.182148: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 13:55:24.182561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 13:55:24.182601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 13:55:24.584130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 13:55:24.584190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 13:55:24.584213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 13:55:24.584557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 14:02:06.927171: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 14:02:07.770591: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x7d938e0\n",
            "2019-05-09 14:02:19.197733: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.50GiB.  Current allocation summary follows.\n",
            "2019-05-09 14:02:19.197841: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 104, Chunks in use: 97. 26.0KiB allocated for chunks. 24.2KiB in use in bin. 396B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197868: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 4, Chunks in use: 0. 2.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197896: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 295, Chunks in use: 294. 442.5KiB allocated for chunks. 440.8KiB in use in bin. 403.9KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197919: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.5KiB allocated for chunks. 2.5KiB in use in bin. 1.4KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197939: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 3, Chunks in use: 2. 19.2KiB allocated for chunks. 15.0KiB in use in bin. 15.0KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197960: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 23.8KiB allocated for chunks. 23.8KiB in use in bin. 17.5KiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197979: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.197997: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198016: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198072: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198104: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 38, Chunks in use: 38. 17.96MiB allocated for chunks. 17.96MiB in use in bin. 17.96MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198124: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 23, Chunks in use: 22. 19.29MiB allocated for chunks. 18.41MiB in use in bin. 17.71MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198144: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 4.67MiB allocated for chunks. 2.97MiB in use in bin. 1.76MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198163: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 5.16MiB allocated for chunks. 5.16MiB in use in bin. 5.16MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198183: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198201: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198242: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 4, Chunks in use: 4. 87.89MiB allocated for chunks. 87.89MiB in use in bin. 87.89MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198265: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 123.05MiB allocated for chunks. 123.05MiB in use in bin. 123.05MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198285: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 60, Chunks in use: 58. 4.06GiB allocated for chunks. 3.93GiB in use in bin. 3.65GiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198305: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 64.45MiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198324: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 3. 6.07GiB allocated for chunks. 4.51GiB in use in bin. 4.51GiB client-requested in use in bin.\n",
            "2019-05-09 14:02:19.198343: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 1.50GiB was 256.00MiB, Chunk State: \n",
            "2019-05-09 14:02:19.198367: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 301.73MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198391: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 303.22MiB | Requested Size: 61.52MiB | in_use: 0, prev:   Size: 900.0KiB | Requested Size: 900.0KiB | in_use: 1, next:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198413: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 993.45MiB | Requested Size: 8.8KiB | in_use: 0, prev:   Size: 1.50GiB | Requested Size: 1.50GiB | in_use: 1\n",
            "2019-05-09 14:02:19.198433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0000 of size 1280\n",
            "2019-05-09 14:02:19.198450: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0500 of size 1536\n",
            "2019-05-09 14:02:19.198477: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0b00 of size 256\n",
            "2019-05-09 14:02:19.198519: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0c00 of size 256\n",
            "2019-05-09 14:02:19.198536: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc0d00 of size 1536\n",
            "2019-05-09 14:02:19.198551: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1300 of size 1536\n",
            "2019-05-09 14:02:19.198581: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1900 of size 1536\n",
            "2019-05-09 14:02:19.198597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc1f00 of size 1536\n",
            "2019-05-09 14:02:19.198612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc2500 of size 1536\n",
            "2019-05-09 14:02:19.198627: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc2b00 of size 1536\n",
            "2019-05-09 14:02:19.198642: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3100 of size 1536\n",
            "2019-05-09 14:02:19.198673: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3700 of size 1536\n",
            "2019-05-09 14:02:19.198689: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc3d00 of size 1536\n",
            "2019-05-09 14:02:19.198705: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4300 of size 1536\n",
            "2019-05-09 14:02:19.198720: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4900 of size 1536\n",
            "2019-05-09 14:02:19.198751: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc4f00 of size 1536\n",
            "2019-05-09 14:02:19.198781: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc5500 of size 1536\n",
            "2019-05-09 14:02:19.198812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc5b00 of size 1536\n",
            "2019-05-09 14:02:19.198827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x703fc6100 of size 495616\n",
            "2019-05-09 14:02:19.198843: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70403f100 of size 528128\n",
            "2019-05-09 14:02:19.198859: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7040c0000 of size 495616\n",
            "2019-05-09 14:02:19.198889: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139000 of size 1536\n",
            "2019-05-09 14:02:19.198904: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139600 of size 1536\n",
            "2019-05-09 14:02:19.198920: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704139c00 of size 1536\n",
            "2019-05-09 14:02:19.198936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413a200 of size 1536\n",
            "2019-05-09 14:02:19.198951: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413a800 of size 1536\n",
            "2019-05-09 14:02:19.198966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413ae00 of size 1536\n",
            "2019-05-09 14:02:19.198982: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413b400 of size 1536\n",
            "2019-05-09 14:02:19.198998: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413ba00 of size 1536\n",
            "2019-05-09 14:02:19.199014: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70413c000 of size 495616\n",
            "2019-05-09 14:02:19.199045: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b5000 of size 1536\n",
            "2019-05-09 14:02:19.199063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7041b5600 of size 495616\n",
            "2019-05-09 14:02:19.199079: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70422e600 of size 596480\n",
            "2019-05-09 14:02:19.199096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0000 of size 1536\n",
            "2019-05-09 14:02:19.199112: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0600 of size 1536\n",
            "2019-05-09 14:02:19.199127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c0c00 of size 1536\n",
            "2019-05-09 14:02:19.199143: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7042c1200 of size 495616\n",
            "2019-05-09 14:02:19.199158: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70433a200 of size 1536\n",
            "2019-05-09 14:02:19.199180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70433a800 of size 495616\n",
            "2019-05-09 14:02:19.199196: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b3800 of size 1536\n",
            "2019-05-09 14:02:19.199211: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7043b3e00 of size 495616\n",
            "2019-05-09 14:02:19.199227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442ce00 of size 1536\n",
            "2019-05-09 14:02:19.199243: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442d400 of size 1536\n",
            "2019-05-09 14:02:19.199259: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442da00 of size 1536\n",
            "2019-05-09 14:02:19.199275: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442e000 of size 1536\n",
            "2019-05-09 14:02:19.199290: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70442e600 of size 495616\n",
            "2019-05-09 14:02:19.199306: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7044a7600 of size 495616\n",
            "2019-05-09 14:02:19.199323: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704520600 of size 495616\n",
            "2019-05-09 14:02:19.199339: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704599600 of size 1536\n",
            "2019-05-09 14:02:19.199354: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704599c00 of size 1536\n",
            "2019-05-09 14:02:19.199370: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459a200 of size 1536\n",
            "2019-05-09 14:02:19.199385: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459a800 of size 1536\n",
            "2019-05-09 14:02:19.199401: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ae00 of size 1536\n",
            "2019-05-09 14:02:19.199416: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459b400 of size 1536\n",
            "2019-05-09 14:02:19.199432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ba00 of size 1536\n",
            "2019-05-09 14:02:19.199447: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459c000 of size 1536\n",
            "2019-05-09 14:02:19.199463: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459c600 of size 1536\n",
            "2019-05-09 14:02:19.199489: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459cc00 of size 1536\n",
            "2019-05-09 14:02:19.199516: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459d200 of size 1536\n",
            "2019-05-09 14:02:19.199533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459d800 of size 1536\n",
            "2019-05-09 14:02:19.199549: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459de00 of size 1536\n",
            "2019-05-09 14:02:19.199564: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459e400 of size 1536\n",
            "2019-05-09 14:02:19.199580: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459ea00 of size 1536\n",
            "2019-05-09 14:02:19.199595: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459f000 of size 1536\n",
            "2019-05-09 14:02:19.199612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459f600 of size 1536\n",
            "2019-05-09 14:02:19.199628: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70459fc00 of size 1536\n",
            "2019-05-09 14:02:19.199643: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0200 of size 1536\n",
            "2019-05-09 14:02:19.199659: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0800 of size 1536\n",
            "2019-05-09 14:02:19.199675: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a0e00 of size 1536\n",
            "2019-05-09 14:02:19.199690: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7045a1400 of size 495616\n",
            "2019-05-09 14:02:19.199706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70461a400 of size 678912\n",
            "2019-05-09 14:02:19.199722: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7046c0000 of size 495616\n",
            "2019-05-09 14:02:19.199738: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739000 of size 1536\n",
            "2019-05-09 14:02:19.199753: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739600 of size 1536\n",
            "2019-05-09 14:02:19.199770: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704739c00 of size 1536\n",
            "2019-05-09 14:02:19.199787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473a200 of size 1536\n",
            "2019-05-09 14:02:19.199802: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473a800 of size 1536\n",
            "2019-05-09 14:02:19.199818: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70473ae00 of size 495616\n",
            "2019-05-09 14:02:19.199833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b3e00 of size 1536\n",
            "2019-05-09 14:02:19.199849: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7047b4400 of size 495616\n",
            "2019-05-09 14:02:19.199864: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482d400 of size 1536\n",
            "2019-05-09 14:02:19.199880: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70482da00 of size 495616\n",
            "2019-05-09 14:02:19.199896: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a6a00 of size 1536\n",
            "2019-05-09 14:02:19.199911: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7000 of size 1536\n",
            "2019-05-09 14:02:19.199927: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7600 of size 1536\n",
            "2019-05-09 14:02:19.199942: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a7c00 of size 1536\n",
            "2019-05-09 14:02:19.199958: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8200 of size 1536\n",
            "2019-05-09 14:02:19.199974: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8800 of size 1536\n",
            "2019-05-09 14:02:19.199989: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a8e00 of size 1536\n",
            "2019-05-09 14:02:19.200005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a9400 of size 1536\n",
            "2019-05-09 14:02:19.200021: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048a9a00 of size 1536\n",
            "2019-05-09 14:02:19.200051: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aa000 of size 1536\n",
            "2019-05-09 14:02:19.200067: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aa600 of size 1536\n",
            "2019-05-09 14:02:19.200083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aac00 of size 1536\n",
            "2019-05-09 14:02:19.200099: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ab200 of size 1536\n",
            "2019-05-09 14:02:19.200116: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ab800 of size 1536\n",
            "2019-05-09 14:02:19.200132: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048abe00 of size 1536\n",
            "2019-05-09 14:02:19.200148: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ac400 of size 1536\n",
            "2019-05-09 14:02:19.200163: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aca00 of size 1536\n",
            "2019-05-09 14:02:19.200179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ad000 of size 1536\n",
            "2019-05-09 14:02:19.200195: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ad600 of size 1536\n",
            "2019-05-09 14:02:19.200210: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048adc00 of size 1536\n",
            "2019-05-09 14:02:19.200226: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ae200 of size 1536\n",
            "2019-05-09 14:02:19.200242: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048ae800 of size 1536\n",
            "2019-05-09 14:02:19.200258: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048aee00 of size 1536\n",
            "2019-05-09 14:02:19.200273: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048af400 of size 1536\n",
            "2019-05-09 14:02:19.200289: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048afa00 of size 1536\n",
            "2019-05-09 14:02:19.200305: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0000 of size 1536\n",
            "2019-05-09 14:02:19.200321: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0600 of size 1536\n",
            "2019-05-09 14:02:19.200336: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b0c00 of size 1536\n",
            "2019-05-09 14:02:19.200352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7048b1200 of size 495616\n",
            "2019-05-09 14:02:19.200368: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70492a200 of size 495616\n",
            "2019-05-09 14:02:19.203720: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7049a3200 of size 495616\n",
            "2019-05-09 14:02:19.203746: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1c200 of size 1536\n",
            "2019-05-09 14:02:19.203757: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1c800 of size 1536\n",
            "2019-05-09 14:02:19.203768: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1ce00 of size 1536\n",
            "2019-05-09 14:02:19.203778: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a1d400 of size 495616\n",
            "2019-05-09 14:02:19.203787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704a96400 of size 495616\n",
            "2019-05-09 14:02:19.203797: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b0f400 of size 1536\n",
            "2019-05-09 14:02:19.203806: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b0fa00 of size 495616\n",
            "2019-05-09 14:02:19.203815: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b88a00 of size 1536\n",
            "2019-05-09 14:02:19.203826: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89000 of size 1536\n",
            "2019-05-09 14:02:19.203835: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89600 of size 1536\n",
            "2019-05-09 14:02:19.203846: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b89c00 of size 1536\n",
            "2019-05-09 14:02:19.203856: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8a200 of size 1536\n",
            "2019-05-09 14:02:19.203867: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8a800 of size 1536\n",
            "2019-05-09 14:02:19.203877: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ae00 of size 1536\n",
            "2019-05-09 14:02:19.203887: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8b400 of size 1536\n",
            "2019-05-09 14:02:19.203897: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ba00 of size 1536\n",
            "2019-05-09 14:02:19.203907: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8c000 of size 1536\n",
            "2019-05-09 14:02:19.203918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8c600 of size 1536\n",
            "2019-05-09 14:02:19.203929: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8cc00 of size 1536\n",
            "2019-05-09 14:02:19.203939: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8d200 of size 1536\n",
            "2019-05-09 14:02:19.203949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8d800 of size 1536\n",
            "2019-05-09 14:02:19.203959: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8de00 of size 1536\n",
            "2019-05-09 14:02:19.203970: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8e400 of size 1536\n",
            "2019-05-09 14:02:19.203982: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8ea00 of size 1536\n",
            "2019-05-09 14:02:19.203992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8f000 of size 1536\n",
            "2019-05-09 14:02:19.204003: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8f600 of size 1536\n",
            "2019-05-09 14:02:19.204015: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b8fc00 of size 1536\n",
            "2019-05-09 14:02:19.204026: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90200 of size 1536\n",
            "2019-05-09 14:02:19.204059: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90800 of size 1536\n",
            "2019-05-09 14:02:19.204072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b90e00 of size 1536\n",
            "2019-05-09 14:02:19.204083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704b91400 of size 495616\n",
            "2019-05-09 14:02:19.204559: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c0a400 of size 495616\n",
            "2019-05-09 14:02:19.204591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704c83400 of size 495616\n",
            "2019-05-09 14:02:19.204612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfc400 of size 1536\n",
            "2019-05-09 14:02:19.204632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfca00 of size 1536\n",
            "2019-05-09 14:02:19.204653: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfd000 of size 1536\n",
            "2019-05-09 14:02:19.204687: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704cfd600 of size 495616\n",
            "2019-05-09 14:02:19.204706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704d76600 of size 495616\n",
            "2019-05-09 14:02:19.204727: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704def600 of size 854528\n",
            "2019-05-09 14:02:19.204747: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0000 of size 1536\n",
            "2019-05-09 14:02:19.204768: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0600 of size 1536\n",
            "2019-05-09 14:02:19.204787: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec0c00 of size 1536\n",
            "2019-05-09 14:02:19.204807: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704ec1200 of size 495616\n",
            "2019-05-09 14:02:19.204842: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704f3a200 of size 495616\n",
            "2019-05-09 14:02:19.204860: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x704fb3200 of size 495616\n",
            "2019-05-09 14:02:19.204879: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502c200 of size 1536\n",
            "2019-05-09 14:02:19.204898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502c800 of size 1536\n",
            "2019-05-09 14:02:19.204918: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502ce00 of size 1536\n",
            "2019-05-09 14:02:19.204936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502d400 of size 1536\n",
            "2019-05-09 14:02:19.204949: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70502da00 of size 495616\n",
            "2019-05-09 14:02:19.204971: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7050a6a00 of size 495616\n",
            "2019-05-09 14:02:19.204992: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70511fa00 of size 1536\n",
            "2019-05-09 14:02:19.205011: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705120000 of size 1536\n",
            "2019-05-09 14:02:19.205048: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705120600 of size 495616\n",
            "2019-05-09 14:02:19.205072: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705199600 of size 1536\n",
            "2019-05-09 14:02:19.205097: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705199c00 of size 1536\n",
            "2019-05-09 14:02:19.205119: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519a200 of size 1536\n",
            "2019-05-09 14:02:19.205139: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519a800 of size 1536\n",
            "2019-05-09 14:02:19.205160: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519ae00 of size 1536\n",
            "2019-05-09 14:02:19.205179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519b400 of size 1536\n",
            "2019-05-09 14:02:19.205199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519ba00 of size 1536\n",
            "2019-05-09 14:02:19.205218: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519c000 of size 1536\n",
            "2019-05-09 14:02:19.205255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519c600 of size 1536\n",
            "2019-05-09 14:02:19.205276: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519cc00 of size 1536\n",
            "2019-05-09 14:02:19.205296: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519d200 of size 1536\n",
            "2019-05-09 14:02:19.205317: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519d800 of size 1536\n",
            "2019-05-09 14:02:19.205337: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519de00 of size 1536\n",
            "2019-05-09 14:02:19.205358: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70519e400 of size 495616\n",
            "2019-05-09 14:02:19.205407: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705217400 of size 1536\n",
            "2019-05-09 14:02:19.205457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705217a00 of size 1536\n",
            "2019-05-09 14:02:19.205493: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218000 of size 1536\n",
            "2019-05-09 14:02:19.205524: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218600 of size 1536\n",
            "2019-05-09 14:02:19.205545: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705218c00 of size 495616\n",
            "2019-05-09 14:02:19.205566: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705291c00 of size 1536\n",
            "2019-05-09 14:02:19.205588: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705292200 of size 495616\n",
            "2019-05-09 14:02:19.205609: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b200 of size 256\n",
            "2019-05-09 14:02:19.205630: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b300 of size 256\n",
            "2019-05-09 14:02:19.205650: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b400 of size 256\n",
            "2019-05-09 14:02:19.205669: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b500 of size 256\n",
            "2019-05-09 14:02:19.205690: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b600 of size 256\n",
            "2019-05-09 14:02:19.205710: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b700 of size 256\n",
            "2019-05-09 14:02:19.205730: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530b800 of size 1536\n",
            "2019-05-09 14:02:19.205749: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530be00 of size 1536\n",
            "2019-05-09 14:02:19.205765: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530c400 of size 1536\n",
            "2019-05-09 14:02:19.205783: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530ca00 of size 1536\n",
            "2019-05-09 14:02:19.205803: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70530d000 of size 495616\n",
            "2019-05-09 14:02:19.205822: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705386000 of size 495616\n",
            "2019-05-09 14:02:19.205843: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7053ff000 of size 495616\n",
            "2019-05-09 14:02:19.205863: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478000 of size 1536\n",
            "2019-05-09 14:02:19.205883: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478600 of size 1536\n",
            "2019-05-09 14:02:19.205903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705478c00 of size 1536\n",
            "2019-05-09 14:02:19.205924: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479200 of size 1536\n",
            "2019-05-09 14:02:19.205944: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479800 of size 256\n",
            "2019-05-09 14:02:19.205964: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479900 of size 256\n",
            "2019-05-09 14:02:19.205986: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479a00 of size 256\n",
            "2019-05-09 14:02:19.206007: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479b00 of size 256\n",
            "2019-05-09 14:02:19.206042: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479c00 of size 256\n",
            "2019-05-09 14:02:19.206066: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705479d00 of size 1536\n",
            "2019-05-09 14:02:19.206087: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547a300 of size 1536\n",
            "2019-05-09 14:02:19.206107: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547a900 of size 1536\n",
            "2019-05-09 14:02:19.206126: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547af00 of size 1536\n",
            "2019-05-09 14:02:19.206147: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547b500 of size 1536\n",
            "2019-05-09 14:02:19.206167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547bb00 of size 1536\n",
            "2019-05-09 14:02:19.206187: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547c100 of size 1536\n",
            "2019-05-09 14:02:19.206208: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547c700 of size 1536\n",
            "2019-05-09 14:02:19.206229: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547cd00 of size 1536\n",
            "2019-05-09 14:02:19.206265: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547d300 of size 1536\n",
            "2019-05-09 14:02:19.206285: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547d900 of size 1536\n",
            "2019-05-09 14:02:19.206305: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547df00 of size 1536\n",
            "2019-05-09 14:02:19.206324: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547e500 of size 1536\n",
            "2019-05-09 14:02:19.206344: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547eb00 of size 1536\n",
            "2019-05-09 14:02:19.206364: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547f100 of size 1536\n",
            "2019-05-09 14:02:19.206384: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547f700 of size 1536\n",
            "2019-05-09 14:02:19.206403: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70547fd00 of size 1536\n",
            "2019-05-09 14:02:19.206423: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480300 of size 1536\n",
            "2019-05-09 14:02:19.206442: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480900 of size 1536\n",
            "2019-05-09 14:02:19.206462: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705480f00 of size 1536\n",
            "2019-05-09 14:02:19.206520: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705481500 of size 1536\n",
            "2019-05-09 14:02:19.206542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705481b00 of size 1536\n",
            "2019-05-09 14:02:19.206562: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482100 of size 1536\n",
            "2019-05-09 14:02:19.206583: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482700 of size 1536\n",
            "2019-05-09 14:02:19.206604: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705482d00 of size 1536\n",
            "2019-05-09 14:02:19.206624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483300 of size 1536\n",
            "2019-05-09 14:02:19.206645: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483900 of size 1536\n",
            "2019-05-09 14:02:19.206665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705483f00 of size 1536\n",
            "2019-05-09 14:02:19.206685: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705484500 of size 1536\n",
            "2019-05-09 14:02:19.206706: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705484b00 of size 1536\n",
            "2019-05-09 14:02:19.206741: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485100 of size 1536\n",
            "2019-05-09 14:02:19.206760: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485700 of size 1536\n",
            "2019-05-09 14:02:19.206780: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705485d00 of size 1536\n",
            "2019-05-09 14:02:19.206807: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486300 of size 1536\n",
            "2019-05-09 14:02:19.206827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486900 of size 1536\n",
            "2019-05-09 14:02:19.206847: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705486f00 of size 1536\n",
            "2019-05-09 14:02:19.206868: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705487500 of size 1536\n",
            "2019-05-09 14:02:19.206888: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705487b00 of size 1536\n",
            "2019-05-09 14:02:19.206908: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488100 of size 1536\n",
            "2019-05-09 14:02:19.206927: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488700 of size 1536\n",
            "2019-05-09 14:02:19.206946: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705488d00 of size 1536\n",
            "2019-05-09 14:02:19.206966: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489300 of size 1536\n",
            "2019-05-09 14:02:19.206985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489900 of size 1536\n",
            "2019-05-09 14:02:19.207006: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705489f00 of size 1536\n",
            "2019-05-09 14:02:19.207025: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548a500 of size 1536\n",
            "2019-05-09 14:02:19.207077: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ab00 of size 1536\n",
            "2019-05-09 14:02:19.207098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548b100 of size 1536\n",
            "2019-05-09 14:02:19.207118: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548b700 of size 1536\n",
            "2019-05-09 14:02:19.207138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548bd00 of size 1536\n",
            "2019-05-09 14:02:19.207159: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548c300 of size 1536\n",
            "2019-05-09 14:02:19.207180: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548c900 of size 1536\n",
            "2019-05-09 14:02:19.207200: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548cf00 of size 1536\n",
            "2019-05-09 14:02:19.207235: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548d500 of size 1536\n",
            "2019-05-09 14:02:19.207255: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548db00 of size 1536\n",
            "2019-05-09 14:02:19.207275: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548e100 of size 1536\n",
            "2019-05-09 14:02:19.207295: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548e700 of size 1536\n",
            "2019-05-09 14:02:19.207331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ed00 of size 256\n",
            "2019-05-09 14:02:19.207352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ee00 of size 256\n",
            "2019-05-09 14:02:19.207372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ef00 of size 256\n",
            "2019-05-09 14:02:19.207392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f000 of size 256\n",
            "2019-05-09 14:02:19.207412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f100 of size 256\n",
            "2019-05-09 14:02:19.207437: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f200 of size 256\n",
            "2019-05-09 14:02:19.207457: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f300 of size 256\n",
            "2019-05-09 14:02:19.207491: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f400 of size 256\n",
            "2019-05-09 14:02:19.207533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f500 of size 256\n",
            "2019-05-09 14:02:19.207555: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f600 of size 256\n",
            "2019-05-09 14:02:19.207576: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f700 of size 256\n",
            "2019-05-09 14:02:19.207597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f800 of size 256\n",
            "2019-05-09 14:02:19.207617: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548f900 of size 256\n",
            "2019-05-09 14:02:19.207637: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fa00 of size 256\n",
            "2019-05-09 14:02:19.207657: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fb00 of size 256\n",
            "2019-05-09 14:02:19.207677: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fc00 of size 256\n",
            "2019-05-09 14:02:19.207696: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fd00 of size 256\n",
            "2019-05-09 14:02:19.207717: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548fe00 of size 256\n",
            "2019-05-09 14:02:19.207736: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70548ff00 of size 256\n",
            "2019-05-09 14:02:19.207756: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490000 of size 256\n",
            "2019-05-09 14:02:19.207776: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490100 of size 256\n",
            "2019-05-09 14:02:19.207796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490200 of size 256\n",
            "2019-05-09 14:02:19.207816: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490300 of size 256\n",
            "2019-05-09 14:02:19.207836: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490400 of size 256\n",
            "2019-05-09 14:02:19.207857: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490500 of size 256\n",
            "2019-05-09 14:02:19.207878: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490600 of size 256\n",
            "2019-05-09 14:02:19.207898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490700 of size 256\n",
            "2019-05-09 14:02:19.207919: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490800 of size 256\n",
            "2019-05-09 14:02:19.207940: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705490900 of size 256\n",
            "2019-05-09 14:02:19.207961: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490a00 of size 256\n",
            "2019-05-09 14:02:19.207981: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490b00 of size 256\n",
            "2019-05-09 14:02:19.208002: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490c00 of size 256\n",
            "2019-05-09 14:02:19.208024: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490d00 of size 256\n",
            "2019-05-09 14:02:19.208062: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705490e00 of size 7680\n",
            "2019-05-09 14:02:19.208083: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705492c00 of size 256\n",
            "2019-05-09 14:02:19.208104: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705492d00 of size 7680\n",
            "2019-05-09 14:02:19.208125: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705494b00 of size 1536\n",
            "2019-05-09 14:02:19.208145: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495100 of size 1536\n",
            "2019-05-09 14:02:19.208167: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495700 of size 1536\n",
            "2019-05-09 14:02:19.208187: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495d00 of size 256\n",
            "2019-05-09 14:02:19.208209: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495e00 of size 256\n",
            "2019-05-09 14:02:19.208230: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705495f00 of size 256\n",
            "2019-05-09 14:02:19.208250: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496000 of size 256\n",
            "2019-05-09 14:02:19.208271: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496100 of size 256\n",
            "2019-05-09 14:02:19.208291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496200 of size 256\n",
            "2019-05-09 14:02:19.208311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496300 of size 1536\n",
            "2019-05-09 14:02:19.208331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496900 of size 1536\n",
            "2019-05-09 14:02:19.208352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705496f00 of size 256\n",
            "2019-05-09 14:02:19.208372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497000 of size 256\n",
            "2019-05-09 14:02:19.208392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497100 of size 256\n",
            "2019-05-09 14:02:19.208412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497200 of size 256\n",
            "2019-05-09 14:02:19.208432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497300 of size 256\n",
            "2019-05-09 14:02:19.208453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497400 of size 1536\n",
            "2019-05-09 14:02:19.208486: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705497a00 of size 1536\n",
            "2019-05-09 14:02:19.208515: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498000 of size 1536\n",
            "2019-05-09 14:02:19.208538: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498600 of size 1536\n",
            "2019-05-09 14:02:19.208570: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705498c00 of size 1536\n",
            "2019-05-09 14:02:19.208591: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499200 of size 1536\n",
            "2019-05-09 14:02:19.208612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499800 of size 256\n",
            "2019-05-09 14:02:19.208632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499900 of size 256\n",
            "2019-05-09 14:02:19.208653: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705499a00 of size 1536\n",
            "2019-05-09 14:02:19.208673: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549a000 of size 1536\n",
            "2019-05-09 14:02:19.208694: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549a600 of size 1536\n",
            "2019-05-09 14:02:19.208714: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549ac00 of size 1536\n",
            "2019-05-09 14:02:19.208734: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b200 of size 1536\n",
            "2019-05-09 14:02:19.208754: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b800 of size 256\n",
            "2019-05-09 14:02:19.208775: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549b900 of size 1536\n",
            "2019-05-09 14:02:19.208796: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549bf00 of size 256\n",
            "2019-05-09 14:02:19.208817: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c000 of size 256\n",
            "2019-05-09 14:02:19.208838: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c100 of size 256\n",
            "2019-05-09 14:02:19.208858: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c200 of size 256\n",
            "2019-05-09 14:02:19.208878: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c300 of size 1536\n",
            "2019-05-09 14:02:19.208898: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549c900 of size 1536\n",
            "2019-05-09 14:02:19.208917: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549cf00 of size 1536\n",
            "2019-05-09 14:02:19.208936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549d500 of size 1536\n",
            "2019-05-09 14:02:19.208956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549db00 of size 1536\n",
            "2019-05-09 14:02:19.208976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e100 of size 1536\n",
            "2019-05-09 14:02:19.208996: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e700 of size 256\n",
            "2019-05-09 14:02:19.209017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e800 of size 256\n",
            "2019-05-09 14:02:19.209057: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549e900 of size 256\n",
            "2019-05-09 14:02:19.209080: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549ea00 of size 256\n",
            "2019-05-09 14:02:19.209102: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549eb00 of size 1536\n",
            "2019-05-09 14:02:19.209123: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549f100 of size 1536\n",
            "2019-05-09 14:02:19.209145: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70549f700 of size 2560\n",
            "2019-05-09 14:02:19.209166: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0100 of size 1536\n",
            "2019-05-09 14:02:19.209186: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0700 of size 1536\n",
            "2019-05-09 14:02:19.209207: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a0d00 of size 1536\n",
            "2019-05-09 14:02:19.209227: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1300 of size 1536\n",
            "2019-05-09 14:02:19.209249: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1900 of size 1536\n",
            "2019-05-09 14:02:19.209270: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a1f00 of size 1536\n",
            "2019-05-09 14:02:19.209291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2500 of size 256\n",
            "2019-05-09 14:02:19.209311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2600 of size 1536\n",
            "2019-05-09 14:02:19.209332: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2c00 of size 256\n",
            "2019-05-09 14:02:19.209352: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a2d00 of size 1536\n",
            "2019-05-09 14:02:19.209372: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3300 of size 1536\n",
            "2019-05-09 14:02:19.209393: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3900 of size 1536\n",
            "2019-05-09 14:02:19.209413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a3f00 of size 1536\n",
            "2019-05-09 14:02:19.209434: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4500 of size 1536\n",
            "2019-05-09 14:02:19.209455: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4b00 of size 256\n",
            "2019-05-09 14:02:19.209488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4c00 of size 256\n",
            "2019-05-09 14:02:19.209517: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054a4d00 of size 256\n",
            "2019-05-09 14:02:19.209532: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4e00 of size 256\n",
            "2019-05-09 14:02:19.209553: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a4f00 of size 256\n",
            "2019-05-09 14:02:19.209572: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5000 of size 256\n",
            "2019-05-09 14:02:19.209592: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5100 of size 256\n",
            "2019-05-09 14:02:19.209612: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5200 of size 256\n",
            "2019-05-09 14:02:19.209632: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5300 of size 1536\n",
            "2019-05-09 14:02:19.209651: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5900 of size 1536\n",
            "2019-05-09 14:02:19.209671: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a5f00 of size 1536\n",
            "2019-05-09 14:02:19.209691: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a6500 of size 1536\n",
            "2019-05-09 14:02:19.209711: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a6b00 of size 1536\n",
            "2019-05-09 14:02:19.209732: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7100 of size 1536\n",
            "2019-05-09 14:02:19.209752: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7700 of size 256\n",
            "2019-05-09 14:02:19.209772: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7800 of size 256\n",
            "2019-05-09 14:02:19.209792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7900 of size 256\n",
            "2019-05-09 14:02:19.209813: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7a00 of size 256\n",
            "2019-05-09 14:02:19.209833: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a7b00 of size 1536\n",
            "2019-05-09 14:02:19.209853: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8100 of size 1536\n",
            "2019-05-09 14:02:19.209875: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8700 of size 1536\n",
            "2019-05-09 14:02:19.209895: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a8d00 of size 1536\n",
            "2019-05-09 14:02:19.209916: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9300 of size 1536\n",
            "2019-05-09 14:02:19.209936: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9900 of size 1536\n",
            "2019-05-09 14:02:19.209956: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054a9f00 of size 256\n",
            "2019-05-09 14:02:19.209976: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa000 of size 256\n",
            "2019-05-09 14:02:19.209996: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa100 of size 256\n",
            "2019-05-09 14:02:19.210017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa200 of size 256\n",
            "2019-05-09 14:02:19.210054: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa300 of size 1536\n",
            "2019-05-09 14:02:19.210077: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aa900 of size 1536\n",
            "2019-05-09 14:02:19.210098: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aaf00 of size 1536\n",
            "2019-05-09 14:02:19.210120: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ab500 of size 1536\n",
            "2019-05-09 14:02:19.210140: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054abb00 of size 1536\n",
            "2019-05-09 14:02:19.210161: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac100 of size 1536\n",
            "2019-05-09 14:02:19.210175: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac700 of size 256\n",
            "2019-05-09 14:02:19.210193: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac800 of size 256\n",
            "2019-05-09 14:02:19.210212: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ac900 of size 256\n",
            "2019-05-09 14:02:19.210233: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aca00 of size 256\n",
            "2019-05-09 14:02:19.210252: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054acb00 of size 1536\n",
            "2019-05-09 14:02:19.210269: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ad100 of size 1536\n",
            "2019-05-09 14:02:19.210288: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ad700 of size 1536\n",
            "2019-05-09 14:02:19.210308: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054add00 of size 1536\n",
            "2019-05-09 14:02:19.210327: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ae300 of size 1536\n",
            "2019-05-09 14:02:19.210348: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054ae900 of size 1536\n",
            "2019-05-09 14:02:19.210369: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054aef00 of size 1536\n",
            "2019-05-09 14:02:19.210390: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054af500 of size 256\n",
            "2019-05-09 14:02:19.210425: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054af600 of size 1536\n",
            "2019-05-09 14:02:19.210460: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054afc00 of size 1536\n",
            "2019-05-09 14:02:19.210533: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0200 of size 1536\n",
            "2019-05-09 14:02:19.210556: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b0800 of size 512\n",
            "2019-05-09 14:02:19.210577: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0a00 of size 256\n",
            "2019-05-09 14:02:19.210597: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b0b00 of size 1536\n",
            "2019-05-09 14:02:19.210618: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1100 of size 1536\n",
            "2019-05-09 14:02:19.210638: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1700 of size 1536\n",
            "2019-05-09 14:02:19.210658: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1d00 of size 256\n",
            "2019-05-09 14:02:19.210679: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b1e00 of size 256\n",
            "2019-05-09 14:02:19.210698: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b1f00 of size 1536\n",
            "2019-05-09 14:02:19.210719: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b2500 of size 1536\n",
            "2019-05-09 14:02:19.210740: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b2b00 of size 1536\n",
            "2019-05-09 14:02:19.210761: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3100 of size 1536\n",
            "2019-05-09 14:02:19.210781: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3700 of size 1536\n",
            "2019-05-09 14:02:19.210802: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b3d00 of size 1536\n",
            "2019-05-09 14:02:19.210822: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b4300 of size 1536\n",
            "2019-05-09 14:02:19.210841: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b4900 of size 1536\n",
            "2019-05-09 14:02:19.210862: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b4f00 of size 512\n",
            "2019-05-09 14:02:19.210896: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5100 of size 256\n",
            "2019-05-09 14:02:19.210916: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b5200 of size 256\n",
            "2019-05-09 14:02:19.210935: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5300 of size 1536\n",
            "2019-05-09 14:02:19.210950: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5900 of size 1536\n",
            "2019-05-09 14:02:19.210969: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b5f00 of size 1536\n",
            "2019-05-09 14:02:19.210988: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b6500 of size 256\n",
            "2019-05-09 14:02:19.211012: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b6600 of size 1536\n",
            "2019-05-09 14:02:19.211045: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b6c00 of size 1536\n",
            "2019-05-09 14:02:19.211082: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b7200 of size 1536\n",
            "2019-05-09 14:02:19.211117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b7800 of size 4352\n",
            "2019-05-09 14:02:19.211138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8900 of size 256\n",
            "2019-05-09 14:02:19.211159: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054b8a00 of size 256\n",
            "2019-05-09 14:02:19.211181: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8b00 of size 256\n",
            "2019-05-09 14:02:19.211201: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054b8c00 of size 8960\n",
            "2019-05-09 14:02:19.211222: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7054baf00 of size 512\n",
            "2019-05-09 14:02:19.211257: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bb100 of size 15360\n",
            "2019-05-09 14:02:19.211278: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bed00 of size 1536\n",
            "2019-05-09 14:02:19.211299: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7054bf300 of size 921600\n",
            "2019-05-09 14:02:19.211331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7055a0300 of size 1781760\n",
            "2019-05-09 14:02:19.211351: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705753300 of size 2703360\n",
            "2019-05-09 14:02:19.211371: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7059e7300 of size 921600\n",
            "2019-05-09 14:02:19.211392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ac8300 of size 921600\n",
            "2019-05-09 14:02:19.211413: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9300 of size 256\n",
            "2019-05-09 14:02:19.211433: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ba9400 of size 256\n",
            "2019-05-09 14:02:19.211454: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9500 of size 768\n",
            "2019-05-09 14:02:19.211488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705ba9800 of size 256\n",
            "2019-05-09 14:02:19.211518: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x705ba9900 of size 1792\n",
            "2019-05-09 14:02:19.211539: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705baa000 of size 921600\n",
            "2019-05-09 14:02:19.211562: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705c8b000 of size 979200\n",
            "2019-05-09 14:02:19.211584: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x705d7a100 of size 1335040\n",
            "2019-05-09 14:02:19.211605: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7060c0000 of size 134217728\n",
            "2019-05-09 14:02:19.211626: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70e0c0000 of size 23040000\n",
            "2019-05-09 14:02:19.211646: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x70f6b9000 of size 111177728\n",
            "2019-05-09 14:02:19.211667: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7160c0000 of size 23040000\n",
            "2019-05-09 14:02:19.211688: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7176b9000 of size 23040000\n",
            "2019-05-09 14:02:19.211708: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x718cb2000 of size 23040000\n",
            "2019-05-09 14:02:19.211729: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71a2ab000 of size 67584000\n",
            "2019-05-09 14:02:19.211750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x71e31f000 of size 131731456\n",
            "2019-05-09 14:02:19.211771: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7260c0000 of size 67584000\n",
            "2019-05-09 14:02:19.211792: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72a134000 of size 67584000\n",
            "2019-05-09 14:02:19.211812: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x72e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.211832: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73221c000 of size 67584000\n",
            "2019-05-09 14:02:19.211852: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x736290000 of size 67584000\n",
            "2019-05-09 14:02:19.211881: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73a304000 of size 67584000\n",
            "2019-05-09 14:02:19.211903: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x73e378000 of size 131366912\n",
            "2019-05-09 14:02:19.211924: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7460c0000 of size 67584000\n",
            "2019-05-09 14:02:19.211944: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74a134000 of size 67584000\n",
            "2019-05-09 14:02:19.211964: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x74e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.211985: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75221c000 of size 67584000\n",
            "2019-05-09 14:02:19.212005: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x756290000 of size 67584000\n",
            "2019-05-09 14:02:19.212025: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75a304000 of size 67584000\n",
            "2019-05-09 14:02:19.212063: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x75e378000 of size 67584000\n",
            "2019-05-09 14:02:19.212085: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7623ec000 of size 67584000\n",
            "2019-05-09 14:02:19.212106: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x766460000 of size 67584000\n",
            "2019-05-09 14:02:19.212127: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76a4d4000 of size 67584000\n",
            "2019-05-09 14:02:19.212149: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x76e548000 of size 67584000\n",
            "2019-05-09 14:02:19.212169: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7725bc000 of size 67584000\n",
            "2019-05-09 14:02:19.212189: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x776630000 of size 67584000\n",
            "2019-05-09 14:02:19.212209: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77a6a4000 of size 67584000\n",
            "2019-05-09 14:02:19.212230: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x77e718000 of size 127565824\n",
            "2019-05-09 14:02:19.212251: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7860c0000 of size 67584000\n",
            "2019-05-09 14:02:19.212271: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78a134000 of size 67584000\n",
            "2019-05-09 14:02:19.212291: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x78e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.212311: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79221c000 of size 67584000\n",
            "2019-05-09 14:02:19.212331: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x796290000 of size 67584000\n",
            "2019-05-09 14:02:19.212350: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79a304000 of size 67584000\n",
            "2019-05-09 14:02:19.212371: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x79e378000 of size 67584000\n",
            "2019-05-09 14:02:19.212392: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a23ec000 of size 67584000\n",
            "2019-05-09 14:02:19.212412: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7a6460000 of size 67584000\n",
            "2019-05-09 14:02:19.212432: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7aa4d4000 of size 67584000\n",
            "2019-05-09 14:02:19.212453: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ae548000 of size 67584000\n",
            "2019-05-09 14:02:19.212488: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b25bc000 of size 67584000\n",
            "2019-05-09 14:02:19.212519: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7b6630000 of size 67584000\n",
            "2019-05-09 14:02:19.212542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ba6a4000 of size 67584000\n",
            "2019-05-09 14:02:19.212563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7be718000 of size 67584000\n",
            "2019-05-09 14:02:19.212584: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c278c000 of size 67584000\n",
            "2019-05-09 14:02:19.212604: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7c6800000 of size 67584000\n",
            "2019-05-09 14:02:19.212624: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ca874000 of size 67584000\n",
            "2019-05-09 14:02:19.212644: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7ce8e8000 of size 67584000\n",
            "2019-05-09 14:02:19.212665: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d295c000 of size 67584000\n",
            "2019-05-09 14:02:19.212686: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7d69d0000 of size 67584000\n",
            "2019-05-09 14:02:19.212707: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7daa44000 of size 67584000\n",
            "2019-05-09 14:02:19.212728: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7deab8000 of size 67584000\n",
            "2019-05-09 14:02:19.212750: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e2b2c000 of size 67584000\n",
            "2019-05-09 14:02:19.212770: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7e6ba0000 of size 67584000\n",
            "2019-05-09 14:02:19.212791: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7eac14000 of size 67584000\n",
            "2019-05-09 14:02:19.212827: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7eec88000 of size 67584000\n",
            "2019-05-09 14:02:19.212847: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f2cfc000 of size 67584000\n",
            "2019-05-09 14:02:19.212866: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f6d70000 of size 67584000\n",
            "2019-05-09 14:02:19.212901: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fade4000 of size 67584000\n",
            "2019-05-09 14:02:19.212922: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fee58000 of size 119963648\n",
            "2019-05-09 14:02:19.212943: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8060c0000 of size 67584000\n",
            "2019-05-09 14:02:19.212963: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x80a134000 of size 67584000\n",
            "2019-05-09 14:02:19.212983: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x80e1a8000 of size 67584000\n",
            "2019-05-09 14:02:19.213004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81221c000 of size 2703360\n",
            "2019-05-09 14:02:19.213055: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8124b0000 of size 921600\n",
            "2019-05-09 14:02:19.213076: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x812591000 of size 1781760\n",
            "2019-05-09 14:02:19.213096: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812744000 of size 921600\n",
            "2019-05-09 14:02:19.213117: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812825000 of size 921600\n",
            "2019-05-09 14:02:19.213138: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812906000 of size 921600\n",
            "2019-05-09 14:02:19.213158: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8129e7000 of size 921600\n",
            "2019-05-09 14:02:19.213179: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812ac8000 of size 921600\n",
            "2019-05-09 14:02:19.213199: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812ba9000 of size 921600\n",
            "2019-05-09 14:02:19.213220: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812c8a000 of size 921600\n",
            "2019-05-09 14:02:19.213240: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812d6b000 of size 921600\n",
            "2019-05-09 14:02:19.213261: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x812e4c000 of size 85862400\n",
            "2019-05-09 14:02:19.213281: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81802e800 of size 64512000\n",
            "2019-05-09 14:02:19.213300: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81bdb4800 of size 64512000\n",
            "2019-05-09 14:02:19.213320: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x81fb3a800 of size 921600\n",
            "2019-05-09 14:02:19.213340: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fc1b800 of size 921600\n",
            "2019-05-09 14:02:19.213359: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fcfc800 of size 921600\n",
            "2019-05-09 14:02:19.213380: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81fddd800 of size 921600\n",
            "2019-05-09 14:02:19.213402: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x81febe800 of size 921600\n",
            "2019-05-09 14:02:19.213422: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x81ff9f800 of size 317952000\n",
            "2019-05-09 14:02:19.213443: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x832ed8800 of size 1612800000\n",
            "2019-05-09 14:02:19.213463: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x8930ee800 of size 1612800000\n",
            "2019-05-09 14:02:19.213496: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x8f3304800 of size 316389376\n",
            "2019-05-09 14:02:19.213542: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x9061c0000 of size 1612800000\n",
            "2019-05-09 14:02:19.213563: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x9663d6000 of size 1041705984\n",
            "2019-05-09 14:02:19.213582: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \n",
            "2019-05-09 14:02:19.213608: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 97 Chunks of size 256 totalling 24.2KiB\n",
            "2019-05-09 14:02:19.213649: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2019-05-09 14:02:19.213673: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 293 Chunks of size 1536 totalling 439.5KiB\n",
            "2019-05-09 14:02:19.213696: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 2560 totalling 2.5KiB\n",
            "2019-05-09 14:02:19.213720: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 7680 totalling 15.0KiB\n",
            "2019-05-09 14:02:19.213743: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 8960 totalling 8.8KiB\n",
            "2019-05-09 14:02:19.213767: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 15360 totalling 15.0KiB\n",
            "2019-05-09 14:02:19.213791: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 38 Chunks of size 495616 totalling 17.96MiB\n",
            "2019-05-09 14:02:19.213814: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 528128 totalling 515.8KiB\n",
            "2019-05-09 14:02:19.213838: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 596480 totalling 582.5KiB\n",
            "2019-05-09 14:02:19.213876: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 678912 totalling 663.0KiB\n",
            "2019-05-09 14:02:19.213899: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 854528 totalling 834.5KiB\n",
            "2019-05-09 14:02:19.213923: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 17 Chunks of size 921600 totalling 14.94MiB\n",
            "2019-05-09 14:02:19.213945: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 979200 totalling 956.2KiB\n",
            "2019-05-09 14:02:19.213982: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1335040 totalling 1.27MiB\n",
            "2019-05-09 14:02:19.214004: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1781760 totalling 1.70MiB\n",
            "2019-05-09 14:02:19.214027: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 2703360 totalling 5.16MiB\n",
            "2019-05-09 14:02:19.214067: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 4 Chunks of size 23040000 totalling 87.89MiB\n",
            "2019-05-09 14:02:19.214092: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 64512000 totalling 123.05MiB\n",
            "2019-05-09 14:02:19.214132: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 52 Chunks of size 67584000 totalling 3.27GiB\n",
            "2019-05-09 14:02:19.214156: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 85862400 totalling 81.88MiB\n",
            "2019-05-09 14:02:19.214179: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 111177728 totalling 106.03MiB\n",
            "2019-05-09 14:02:19.214202: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 119963648 totalling 114.41MiB\n",
            "2019-05-09 14:02:19.214225: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 127565824 totalling 121.66MiB\n",
            "2019-05-09 14:02:19.214248: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131366912 totalling 125.28MiB\n",
            "2019-05-09 14:02:19.214271: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 131731456 totalling 125.63MiB\n",
            "2019-05-09 14:02:19.214294: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 134217728 totalling 128.00MiB\n",
            "2019-05-09 14:02:19.214317: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 3 Chunks of size 1612800000 totalling 4.51GiB\n",
            "2019-05-09 14:02:19.214340: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 8.81GiB\n",
            "2019-05-09 14:02:19.214383: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \n",
            "Limit:                 11276946637\n",
            "InUse:                  9463017472\n",
            "MaxInUse:               9530602240\n",
            "NumAllocs:                    1191\n",
            "MaxAllocSize:           1612800000\n",
            "\n",
            "2019-05-09 14:02:19.214486: W tensorflow/core/common_runtime/bfc_allocator.cc:271] *******************************************__*****************************__***************_________\n",
            "2019-05-09 14:02:19.214745: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at batch_matmul_op_impl.h:586 : Resource exhausted: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[{{node Adam/update}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n",
            "Caused by op 'gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 211, in run_training\n",
            "    aggregation_method=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in minimize\n",
            "    grad_loss=grad_loss)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 512, in compute_gradients\n",
            "    colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 664, in gradients\n",
            "    unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in _GradientsHelper\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 420, in _MaybeCompile\n",
            "    return grad_fn()  # Exit early\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 965, in <lambda>\n",
            "    lambda: grad_fn(op, *out_grads))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\", line 1229, in _BatchMatMul\n",
            "    grad_y = math_ops.matmul(x, grad, adjoint_a=True, adjoint_b=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "...which was originally created as op 'func_map_loss/einsum_7/MatMul', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "[elided 1 identical lines from previous traceback]\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 200, in run_training\n",
            "    target_evecs_trans, target_evals\n",
            "  File \"/content/drive/unsupervisedfmnet/DFMnet.py\", line 83, in dfmnet_model\n",
            "    F, G\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 180, in func_map_layer\n",
            "    target_evecs, target_evecs_trans) +\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 116, in penalty_desc_commutativity\n",
            "    G_diag_reduce1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 262, in einsum\n",
            "    axes_to_sum)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 394, in _einsum_reduction\n",
            "    product = math_ops.matmul(t0, t1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 2417, in matmul\n",
            "    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1423, in batch_mat_mul\n",
            "    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,3000,8400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[node gradients/func_map_loss/einsum_7/MatMul_grad/MatMul_1 (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[node Adam/update (defined at drive/unsupervisedfmnet/train_DFMnet.py:211) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewEIKURJ7jdD",
        "colab_type": "text"
      },
      "source": [
        "#主题 运行第四次 \n",
        "模型的顶点数调整为500，batch_size调整为16，用的显卡是特斯拉K80,但是loss太大，没有训练意义，且training中的文件是会覆盖的，不用管它，覆盖完的文件自动就跑到回收站了。迭代从第一步开始的。但是上午的使用3000个顶点，batch为4时跑的是从4929步开始的，loss在1000多到2000左右"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDAnx0Pe5xfM",
        "colab_type": "code",
        "outputId": "59b1b65c-3e14-4c55-f814-dcd8a8722a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train - step 959: loss = 2819470.75 (0.859 sec)\n",
            "train - step 960: loss = 12391454.00 (0.851 sec)\n",
            "train - step 961: loss = 3207419.00 (0.836 sec)\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFgwTPpCVl_",
        "colab_type": "text"
      },
      "source": [
        "#主题   第五次运行\n",
        "模型顶点数为6000，batch_size为4，go起来吧，从700多次开始迭代\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUylORl7AqIL",
        "colab_type": "code",
        "outputId": "80049e38-4c54-47dc-b9ba-f96f707106ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84894
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-09 14:39:15.498874: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 14:39:15.499135: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3162ec0 executing computations on platform Host. Devices:\n",
            "2019-05-09 14:39:15.499175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-09 14:39:15.599356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-09 14:39:15.599867: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3163180 executing computations on platform CUDA. Devices:\n",
            "2019-05-09 14:39:15.599904: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-05-09 14:39:15.600392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-05-09 14:39:15.600427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-09 14:39:15.994116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-09 14:39:15.994214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-09 14:39:15.994233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-09 14:39:15.994529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-09 14:46:00.936236: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-09 14:46:01.468973: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x95498e0\n",
            "train - step 775: loss = 453146.94 (5.311 sec)\n",
            "train - step 776: loss = 506619.38 (1.802 sec)\n",
            "train - step 777: loss = 420729.03 (1.728 sec)\n",
            "train - step 778: loss = 369485.66 (2.564 sec)\n",
            "train - step 779: loss = 409475.81 (1.687 sec)\n",
            "train - step 780: loss = 413229.88 (1.683 sec)\n",
            "train - step 781: loss = 406036.97 (1.697 sec)\n",
            "train - step 782: loss = 514044.47 (1.708 sec)\n",
            "train - step 783: loss = 539288.94 (1.670 sec)\n",
            "train - step 784: loss = 558442.88 (1.687 sec)\n",
            "train - step 785: loss = 454052.50 (1.685 sec)\n",
            "train - step 786: loss = 367280.66 (1.717 sec)\n",
            "train - step 787: loss = 407553.44 (1.669 sec)\n",
            "train - step 788: loss = 526725.44 (1.695 sec)\n",
            "train - step 789: loss = 375453.81 (1.675 sec)\n",
            "train - step 790: loss = 388299.00 (1.676 sec)\n",
            "train - step 791: loss = 485336.94 (1.708 sec)\n",
            "train - step 792: loss = 414568.12 (1.658 sec)\n",
            "train - step 793: loss = 400970.59 (1.718 sec)\n",
            "train - step 794: loss = 458724.81 (1.685 sec)\n",
            "train - step 795: loss = 451180.12 (1.731 sec)\n",
            "train - step 796: loss = 429392.72 (1.706 sec)\n",
            "train - step 797: loss = 424181.41 (1.694 sec)\n",
            "train - step 798: loss = 289368.16 (1.691 sec)\n",
            "train - step 799: loss = 453268.31 (1.709 sec)\n",
            "train - step 800: loss = 608813.38 (1.670 sec)\n",
            "train - step 801: loss = 391998.41 (1.712 sec)\n",
            "train - step 802: loss = 305116.41 (1.684 sec)\n",
            "train - step 803: loss = 402009.81 (1.684 sec)\n",
            "train - step 804: loss = 531024.94 (1.700 sec)\n",
            "train - step 805: loss = 307740.78 (1.698 sec)\n",
            "train - step 806: loss = 522285.06 (1.700 sec)\n",
            "train - step 807: loss = 435155.28 (1.710 sec)\n",
            "train - step 808: loss = 245145.19 (1.728 sec)\n",
            "train - step 809: loss = 484228.62 (1.669 sec)\n",
            "train - step 810: loss = 517955.38 (1.697 sec)\n",
            "train - step 811: loss = 493316.88 (1.713 sec)\n",
            "train - step 812: loss = 641092.75 (1.705 sec)\n",
            "train - step 813: loss = 352564.41 (1.678 sec)\n",
            "train - step 814: loss = 444964.59 (2.820 sec)\n",
            "train - step 815: loss = 598864.25 (1.728 sec)\n",
            "train - step 816: loss = 348906.12 (1.696 sec)\n",
            "train - step 817: loss = 404744.84 (1.720 sec)\n",
            "train - step 818: loss = 462459.84 (1.672 sec)\n",
            "train - step 819: loss = 333766.19 (1.695 sec)\n",
            "train - step 820: loss = 423406.91 (1.649 sec)\n",
            "train - step 821: loss = 469373.03 (1.661 sec)\n",
            "train - step 822: loss = 333444.62 (1.644 sec)\n",
            "train - step 823: loss = 488871.91 (1.652 sec)\n",
            "train - step 824: loss = 441586.22 (1.676 sec)\n",
            "train - step 825: loss = 418657.16 (1.632 sec)\n",
            "train - step 826: loss = 578199.75 (1.681 sec)\n",
            "train - step 827: loss = 380458.88 (1.705 sec)\n",
            "train - step 828: loss = 404544.97 (1.691 sec)\n",
            "train - step 829: loss = 320112.34 (1.705 sec)\n",
            "train - step 830: loss = 437041.03 (1.688 sec)\n",
            "train - step 831: loss = 441706.41 (1.705 sec)\n",
            "train - step 832: loss = 466083.66 (1.705 sec)\n",
            "train - step 833: loss = 484033.59 (1.692 sec)\n",
            "train - step 834: loss = 498264.97 (1.697 sec)\n",
            "train - step 835: loss = 426453.72 (1.723 sec)\n",
            "train - step 836: loss = 555781.81 (1.705 sec)\n",
            "train - step 837: loss = 432876.78 (1.728 sec)\n",
            "train - step 838: loss = 376675.41 (1.704 sec)\n",
            "train - step 839: loss = 471088.00 (1.762 sec)\n",
            "train - step 840: loss = 407877.44 (1.676 sec)\n",
            "train - step 841: loss = 411700.78 (1.698 sec)\n",
            "train - step 842: loss = 509697.69 (1.734 sec)\n",
            "train - step 843: loss = 465694.06 (1.688 sec)\n",
            "train - step 844: loss = 383669.97 (1.740 sec)\n",
            "train - step 845: loss = 666123.38 (1.715 sec)\n",
            "train - step 846: loss = 519735.41 (1.711 sec)\n",
            "train - step 847: loss = 293406.56 (2.148 sec)\n",
            "train - step 848: loss = 405266.41 (1.711 sec)\n",
            "train - step 849: loss = 481961.06 (2.540 sec)\n",
            "train - step 850: loss = 671735.62 (1.709 sec)\n",
            "train - step 851: loss = 434100.81 (1.684 sec)\n",
            "train - step 852: loss = 474003.78 (1.648 sec)\n",
            "train - step 853: loss = 663206.06 (1.661 sec)\n",
            "train - step 854: loss = 425882.91 (1.664 sec)\n",
            "train - step 855: loss = 452056.06 (1.686 sec)\n",
            "train - step 856: loss = 429860.16 (1.681 sec)\n",
            "train - step 857: loss = 459202.47 (1.709 sec)\n",
            "train - step 858: loss = 518916.00 (1.686 sec)\n",
            "train - step 859: loss = 431829.41 (1.748 sec)\n",
            "train - step 860: loss = 434375.94 (1.677 sec)\n",
            "train - step 861: loss = 382375.66 (1.712 sec)\n",
            "train - step 862: loss = 322297.97 (1.704 sec)\n",
            "train - step 863: loss = 299331.06 (1.721 sec)\n",
            "train - step 864: loss = 391019.28 (1.682 sec)\n",
            "train - step 865: loss = 484541.00 (1.685 sec)\n",
            "train - step 866: loss = 361749.56 (1.695 sec)\n",
            "train - step 867: loss = 338939.00 (1.683 sec)\n",
            "train - step 868: loss = 394946.59 (1.717 sec)\n",
            "train - step 869: loss = 618932.62 (1.684 sec)\n",
            "train - step 870: loss = 305384.81 (1.686 sec)\n",
            "train - step 871: loss = 478540.88 (1.700 sec)\n",
            "train - step 872: loss = 543809.56 (1.706 sec)\n",
            "train - step 873: loss = 354112.28 (1.705 sec)\n",
            "train - step 874: loss = 363313.19 (1.683 sec)\n",
            "train - step 875: loss = 683450.12 (1.684 sec)\n",
            "train - step 876: loss = 365955.97 (1.687 sec)\n",
            "train - step 877: loss = 592501.94 (1.697 sec)\n",
            "train - step 878: loss = 449702.72 (1.684 sec)\n",
            "train - step 879: loss = 348273.31 (1.703 sec)\n",
            "train - step 880: loss = 500749.06 (1.682 sec)\n",
            "train - step 881: loss = 295672.47 (1.678 sec)\n",
            "train - step 882: loss = 607463.06 (1.682 sec)\n",
            "train - step 883: loss = 703325.44 (1.729 sec)\n",
            "train - step 884: loss = 408851.84 (1.689 sec)\n",
            "train - step 885: loss = 442423.62 (2.580 sec)\n",
            "train - step 886: loss = 309416.31 (1.717 sec)\n",
            "train - step 887: loss = 404484.03 (1.714 sec)\n",
            "train - step 888: loss = 474199.09 (1.693 sec)\n",
            "train - step 889: loss = 458743.91 (1.665 sec)\n",
            "train - step 890: loss = 443450.03 (1.700 sec)\n",
            "train - step 891: loss = 384472.41 (1.673 sec)\n",
            "train - step 892: loss = 310919.78 (1.673 sec)\n",
            "train - step 893: loss = 417029.16 (1.688 sec)\n",
            "train - step 894: loss = 455598.91 (1.679 sec)\n",
            "train - step 895: loss = 357793.03 (1.691 sec)\n",
            "train - step 896: loss = 340205.69 (1.689 sec)\n",
            "train - step 897: loss = 529065.25 (1.688 sec)\n",
            "train - step 898: loss = 374339.38 (1.693 sec)\n",
            "train - step 899: loss = 370311.72 (1.712 sec)\n",
            "train - step 900: loss = 394917.72 (1.697 sec)\n",
            "train - step 901: loss = 456552.47 (1.690 sec)\n",
            "train - step 902: loss = 651791.06 (1.701 sec)\n",
            "train - step 903: loss = 446567.84 (1.692 sec)\n",
            "train - step 904: loss = 474749.81 (1.699 sec)\n",
            "train - step 905: loss = 483795.84 (1.687 sec)\n",
            "train - step 906: loss = 401564.31 (1.676 sec)\n",
            "train - step 907: loss = 488264.41 (1.680 sec)\n",
            "train - step 908: loss = 325338.66 (1.711 sec)\n",
            "train - step 909: loss = 331574.72 (1.670 sec)\n",
            "train - step 910: loss = 359348.81 (1.690 sec)\n",
            "train - step 911: loss = 507224.00 (1.744 sec)\n",
            "train - step 912: loss = 313442.78 (1.700 sec)\n",
            "train - step 913: loss = 385773.78 (1.693 sec)\n",
            "train - step 914: loss = 563140.38 (1.691 sec)\n",
            "train - step 915: loss = 475707.53 (1.684 sec)\n",
            "train - step 916: loss = 629097.81 (1.699 sec)\n",
            "train - step 917: loss = 339531.72 (1.715 sec)\n",
            "train - step 918: loss = 533716.19 (1.688 sec)\n",
            "train - step 919: loss = 456770.72 (1.694 sec)\n",
            "train - step 920: loss = 567674.12 (1.683 sec)\n",
            "train - step 921: loss = 320178.28 (2.618 sec)\n",
            "train - step 922: loss = 343334.53 (1.709 sec)\n",
            "train - step 923: loss = 441538.19 (1.684 sec)\n",
            "train - step 924: loss = 403078.78 (1.717 sec)\n",
            "train - step 925: loss = 511383.88 (1.722 sec)\n",
            "train - step 926: loss = 476232.62 (1.688 sec)\n",
            "train - step 927: loss = 427345.84 (1.649 sec)\n",
            "train - step 928: loss = 530642.69 (1.696 sec)\n",
            "train - step 929: loss = 334743.31 (1.678 sec)\n",
            "train - step 930: loss = 426909.06 (1.700 sec)\n",
            "train - step 931: loss = 504205.97 (1.675 sec)\n",
            "train - step 932: loss = 411550.31 (1.695 sec)\n",
            "train - step 933: loss = 560156.56 (1.705 sec)\n",
            "train - step 934: loss = 466626.22 (1.705 sec)\n",
            "train - step 935: loss = 381072.78 (1.705 sec)\n",
            "train - step 936: loss = 466924.69 (1.681 sec)\n",
            "train - step 937: loss = 403586.47 (1.688 sec)\n",
            "train - step 938: loss = 519126.50 (1.677 sec)\n",
            "train - step 939: loss = 432650.41 (1.709 sec)\n",
            "train - step 940: loss = 541343.56 (1.666 sec)\n",
            "train - step 941: loss = 471664.50 (1.689 sec)\n",
            "train - step 942: loss = 558673.94 (1.695 sec)\n",
            "train - step 943: loss = 444619.88 (1.703 sec)\n",
            "train - step 944: loss = 441890.84 (1.690 sec)\n",
            "train - step 945: loss = 645164.94 (1.680 sec)\n",
            "train - step 946: loss = 395507.91 (1.696 sec)\n",
            "train - step 947: loss = 317461.62 (1.695 sec)\n",
            "train - step 948: loss = 397708.00 (1.719 sec)\n",
            "train - step 949: loss = 542614.31 (1.681 sec)\n",
            "train - step 950: loss = 369535.56 (1.671 sec)\n",
            "train - step 951: loss = 488756.34 (1.698 sec)\n",
            "train - step 952: loss = 527605.62 (1.698 sec)\n",
            "train - step 953: loss = 371316.56 (1.732 sec)\n",
            "train - step 954: loss = 384864.19 (1.689 sec)\n",
            "train - step 955: loss = 466580.56 (1.721 sec)\n",
            "train - step 956: loss = 407586.56 (1.698 sec)\n",
            "train - step 957: loss = 527495.31 (2.605 sec)\n",
            "train - step 958: loss = 493753.69 (1.727 sec)\n",
            "train - step 959: loss = 329360.66 (1.682 sec)\n",
            "train - step 960: loss = 501824.06 (1.713 sec)\n",
            "train - step 961: loss = 539477.25 (1.695 sec)\n",
            "train - step 962: loss = 399013.53 (1.685 sec)\n",
            "train - step 963: loss = 560219.69 (1.698 sec)\n",
            "train - step 964: loss = 490679.34 (1.667 sec)\n",
            "train - step 965: loss = 438868.62 (1.700 sec)\n",
            "train - step 966: loss = 595325.06 (1.679 sec)\n",
            "train - step 967: loss = 353568.00 (1.716 sec)\n",
            "train - step 968: loss = 550370.81 (1.678 sec)\n",
            "train - step 969: loss = 360542.97 (1.720 sec)\n",
            "train - step 970: loss = 373575.41 (1.693 sec)\n",
            "train - step 971: loss = 328580.97 (1.688 sec)\n",
            "train - step 972: loss = 341268.50 (1.693 sec)\n",
            "train - step 973: loss = 420320.34 (1.697 sec)\n",
            "train - step 974: loss = 490683.88 (1.680 sec)\n",
            "train - step 975: loss = 366336.19 (1.677 sec)\n",
            "train - step 976: loss = 373601.19 (1.687 sec)\n",
            "train - step 977: loss = 369259.81 (1.684 sec)\n",
            "train - step 978: loss = 376325.56 (1.686 sec)\n",
            "train - step 979: loss = 449680.03 (1.694 sec)\n",
            "train - step 980: loss = 434726.28 (1.702 sec)\n",
            "train - step 981: loss = 330161.31 (1.711 sec)\n",
            "train - step 982: loss = 481113.66 (1.685 sec)\n",
            "train - step 983: loss = 392224.69 (1.700 sec)\n",
            "train - step 984: loss = 396916.41 (1.684 sec)\n",
            "train - step 985: loss = 345830.56 (1.704 sec)\n",
            "train - step 986: loss = 538456.31 (1.691 sec)\n",
            "train - step 987: loss = 402848.88 (1.686 sec)\n",
            "train - step 988: loss = 573742.69 (1.667 sec)\n",
            "train - step 989: loss = 381874.41 (1.680 sec)\n",
            "train - step 990: loss = 445971.69 (1.703 sec)\n",
            "train - step 991: loss = 381119.34 (1.670 sec)\n",
            "train - step 992: loss = 352806.91 (1.701 sec)\n",
            "train - step 993: loss = 422001.06 (2.540 sec)\n",
            "train - step 994: loss = 603814.56 (1.698 sec)\n",
            "train - step 995: loss = 436271.72 (1.694 sec)\n",
            "train - step 996: loss = 357280.94 (1.714 sec)\n",
            "train - step 997: loss = 417522.38 (1.701 sec)\n",
            "train - step 998: loss = 440453.56 (1.672 sec)\n",
            "train - step 999: loss = 587650.38 (1.690 sec)\n",
            "train - step 1000: loss = 383447.62 (1.695 sec)\n",
            "train - step 1001: loss = 315973.41 (1.649 sec)\n",
            "train - step 1002: loss = 453755.91 (1.671 sec)\n",
            "train - step 1003: loss = 387422.12 (1.650 sec)\n",
            "train - step 1004: loss = 539067.44 (1.666 sec)\n",
            "train - step 1005: loss = 416332.72 (1.648 sec)\n",
            "train - step 1006: loss = 376390.66 (1.661 sec)\n",
            "train - step 1007: loss = 390484.22 (1.694 sec)\n",
            "train - step 1008: loss = 346912.44 (1.678 sec)\n",
            "train - step 1009: loss = 392309.69 (1.690 sec)\n",
            "train - step 1010: loss = 230596.22 (1.671 sec)\n",
            "train - step 1011: loss = 414682.03 (1.716 sec)\n",
            "train - step 1012: loss = 526813.19 (1.673 sec)\n",
            "train - step 1013: loss = 320668.03 (1.695 sec)\n",
            "train - step 1014: loss = 418977.38 (1.672 sec)\n",
            "train - step 1015: loss = 305448.47 (1.659 sec)\n",
            "train - step 1016: loss = 578776.94 (1.694 sec)\n",
            "train - step 1017: loss = 426348.09 (1.687 sec)\n",
            "train - step 1018: loss = 431934.28 (1.674 sec)\n",
            "train - step 1019: loss = 368576.91 (1.695 sec)\n",
            "train - step 1020: loss = 489402.28 (1.710 sec)\n",
            "train - step 1021: loss = 561800.06 (1.701 sec)\n",
            "train - step 1022: loss = 455883.69 (1.691 sec)\n",
            "train - step 1023: loss = 295577.34 (1.700 sec)\n",
            "train - step 1024: loss = 352627.88 (1.675 sec)\n",
            "train - step 1025: loss = 365165.84 (1.698 sec)\n",
            "train - step 1026: loss = 570622.69 (1.693 sec)\n",
            "train - step 1027: loss = 301952.62 (1.660 sec)\n",
            "train - step 1028: loss = 415014.28 (1.711 sec)\n",
            "train - step 1029: loss = 355892.66 (2.762 sec)\n",
            "train - step 1030: loss = 556081.06 (1.681 sec)\n",
            "train - step 1031: loss = 347489.50 (1.650 sec)\n",
            "train - step 1032: loss = 566983.69 (1.663 sec)\n",
            "train - step 1033: loss = 481540.97 (1.681 sec)\n",
            "train - step 1034: loss = 540400.31 (1.692 sec)\n",
            "train - step 1035: loss = 356574.72 (1.671 sec)\n",
            "train - step 1036: loss = 402765.06 (1.696 sec)\n",
            "train - step 1037: loss = 351389.38 (1.683 sec)\n",
            "train - step 1038: loss = 489789.81 (1.682 sec)\n",
            "train - step 1039: loss = 415209.56 (1.672 sec)\n",
            "train - step 1040: loss = 512672.06 (1.681 sec)\n",
            "train - step 1041: loss = 533912.25 (1.671 sec)\n",
            "train - step 1042: loss = 279709.44 (1.685 sec)\n",
            "train - step 1043: loss = 375785.44 (1.673 sec)\n",
            "train - step 1044: loss = 464548.16 (1.691 sec)\n",
            "train - step 1045: loss = 346004.81 (1.669 sec)\n",
            "train - step 1046: loss = 409171.50 (1.681 sec)\n",
            "train - step 1047: loss = 343569.56 (1.704 sec)\n",
            "train - step 1048: loss = 450964.28 (1.673 sec)\n",
            "train - step 1049: loss = 330223.94 (1.706 sec)\n",
            "train - step 1050: loss = 524627.25 (1.677 sec)\n",
            "train - step 1051: loss = 337536.06 (1.692 sec)\n",
            "train - step 1052: loss = 491544.28 (1.683 sec)\n",
            "train - step 1053: loss = 450086.72 (1.708 sec)\n",
            "train - step 1054: loss = 430916.66 (1.674 sec)\n",
            "train - step 1055: loss = 375098.56 (1.684 sec)\n",
            "train - step 1056: loss = 492405.72 (1.699 sec)\n",
            "train - step 1057: loss = 443545.03 (1.707 sec)\n",
            "train - step 1058: loss = 521929.88 (1.711 sec)\n",
            "train - step 1059: loss = 401185.38 (1.690 sec)\n",
            "train - step 1060: loss = 410726.00 (1.704 sec)\n",
            "train - step 1061: loss = 607002.94 (1.710 sec)\n",
            "train - step 1062: loss = 345541.41 (1.702 sec)\n",
            "train - step 1063: loss = 415383.53 (1.695 sec)\n",
            "train - step 1064: loss = 428625.22 (1.699 sec)\n",
            "train - step 1065: loss = 483084.78 (2.717 sec)\n",
            "train - step 1066: loss = 422542.19 (1.701 sec)\n",
            "train - step 1067: loss = 535514.75 (1.712 sec)\n",
            "train - step 1068: loss = 442585.38 (1.680 sec)\n",
            "train - step 1069: loss = 548315.06 (1.703 sec)\n",
            "train - step 1070: loss = 359684.72 (1.672 sec)\n",
            "train - step 1071: loss = 549652.69 (1.685 sec)\n",
            "train - step 1072: loss = 441435.16 (1.678 sec)\n",
            "train - step 1073: loss = 644869.81 (1.685 sec)\n",
            "train - step 1074: loss = 461583.19 (1.694 sec)\n",
            "train - step 1075: loss = 462400.06 (1.691 sec)\n",
            "train - step 1076: loss = 421014.28 (1.692 sec)\n",
            "train - step 1077: loss = 456235.84 (1.686 sec)\n",
            "train - step 1078: loss = 763452.88 (1.701 sec)\n",
            "train - step 1079: loss = 394319.44 (1.665 sec)\n",
            "train - step 1080: loss = 460044.62 (1.687 sec)\n",
            "train - step 1081: loss = 424946.44 (1.710 sec)\n",
            "train - step 1082: loss = 539922.19 (1.695 sec)\n",
            "train - step 1083: loss = 471638.41 (1.699 sec)\n",
            "train - step 1084: loss = 399951.44 (1.702 sec)\n",
            "train - step 1085: loss = 480019.06 (1.676 sec)\n",
            "train - step 1086: loss = 542287.94 (1.693 sec)\n",
            "train - step 1087: loss = 362306.97 (1.692 sec)\n",
            "train - step 1088: loss = 385413.41 (1.681 sec)\n",
            "train - step 1089: loss = 484866.81 (1.703 sec)\n",
            "train - step 1090: loss = 484700.84 (1.695 sec)\n",
            "train - step 1091: loss = 442644.88 (1.689 sec)\n",
            "train - step 1092: loss = 450297.97 (1.695 sec)\n",
            "train - step 1093: loss = 611591.00 (1.704 sec)\n",
            "train - step 1094: loss = 245958.69 (1.682 sec)\n",
            "train - step 1095: loss = 340670.00 (1.689 sec)\n",
            "train - step 1096: loss = 431602.00 (1.701 sec)\n",
            "train - step 1097: loss = 569963.69 (1.666 sec)\n",
            "train - step 1098: loss = 375568.12 (1.722 sec)\n",
            "train - step 1099: loss = 293360.03 (1.723 sec)\n",
            "train - step 1100: loss = 399630.31 (1.678 sec)\n",
            "train - step 1101: loss = 463802.59 (2.595 sec)\n",
            "train - step 1102: loss = 448937.06 (1.686 sec)\n",
            "train - step 1103: loss = 352466.09 (1.670 sec)\n",
            "train - step 1104: loss = 538428.94 (1.692 sec)\n",
            "train - step 1105: loss = 392460.16 (1.672 sec)\n",
            "train - step 1106: loss = 454082.81 (1.689 sec)\n",
            "train - step 1107: loss = 453480.22 (1.677 sec)\n",
            "train - step 1108: loss = 461457.00 (1.663 sec)\n",
            "train - step 1109: loss = 479212.62 (1.708 sec)\n",
            "train - step 1110: loss = 592019.12 (1.685 sec)\n",
            "train - step 1111: loss = 291676.69 (1.728 sec)\n",
            "train - step 1112: loss = 458112.00 (1.663 sec)\n",
            "train - step 1113: loss = 410964.88 (1.691 sec)\n",
            "train - step 1114: loss = 450192.66 (1.684 sec)\n",
            "train - step 1115: loss = 368865.19 (1.706 sec)\n",
            "train - step 1116: loss = 612897.38 (1.678 sec)\n",
            "train - step 1117: loss = 429601.34 (1.678 sec)\n",
            "train - step 1118: loss = 333169.50 (1.695 sec)\n",
            "train - step 1119: loss = 321377.34 (1.681 sec)\n",
            "train - step 1120: loss = 434379.47 (1.743 sec)\n",
            "train - step 1121: loss = 450829.78 (1.677 sec)\n",
            "train - step 1122: loss = 519117.53 (1.684 sec)\n",
            "train - step 1123: loss = 441543.53 (1.702 sec)\n",
            "train - step 1124: loss = 431085.72 (1.722 sec)\n",
            "train - step 1125: loss = 407233.59 (1.701 sec)\n",
            "train - step 1126: loss = 377767.06 (1.710 sec)\n",
            "train - step 1127: loss = 551744.19 (1.697 sec)\n",
            "train - step 1128: loss = 407641.66 (1.684 sec)\n",
            "train - step 1129: loss = 432107.78 (1.733 sec)\n",
            "train - step 1130: loss = 426087.97 (1.781 sec)\n",
            "train - step 1131: loss = 574380.81 (1.712 sec)\n",
            "train - step 1132: loss = 600538.06 (1.723 sec)\n",
            "train - step 1133: loss = 473425.12 (1.715 sec)\n",
            "train - step 1134: loss = 577815.69 (1.726 sec)\n",
            "train - step 1135: loss = 361266.47 (1.699 sec)\n",
            "train - step 1136: loss = 596591.62 (2.758 sec)\n",
            "train - step 1137: loss = 540797.88 (1.737 sec)\n",
            "train - step 1138: loss = 348718.00 (1.704 sec)\n",
            "train - step 1139: loss = 498781.44 (1.815 sec)\n",
            "train - step 1140: loss = 362942.28 (1.689 sec)\n",
            "train - step 1141: loss = 340516.97 (1.686 sec)\n",
            "train - step 1142: loss = 470879.94 (1.702 sec)\n",
            "train - step 1143: loss = 480210.47 (1.799 sec)\n",
            "train - step 1144: loss = 330129.88 (1.675 sec)\n",
            "train - step 1145: loss = 506594.66 (1.690 sec)\n",
            "train - step 1146: loss = 406707.38 (1.671 sec)\n",
            "train - step 1147: loss = 412547.41 (1.708 sec)\n",
            "train - step 1148: loss = 354343.72 (1.674 sec)\n",
            "train - step 1149: loss = 419340.34 (1.699 sec)\n",
            "train - step 1150: loss = 463711.38 (1.695 sec)\n",
            "train - step 1151: loss = 578363.00 (1.699 sec)\n",
            "train - step 1152: loss = 288011.44 (1.695 sec)\n",
            "train - step 1153: loss = 387715.78 (1.683 sec)\n",
            "train - step 1154: loss = 527845.56 (1.686 sec)\n",
            "train - step 1155: loss = 461766.09 (1.654 sec)\n",
            "train - step 1156: loss = 291144.22 (1.649 sec)\n",
            "train - step 1157: loss = 374294.28 (1.682 sec)\n",
            "train - step 1158: loss = 488170.41 (1.690 sec)\n",
            "train - step 1159: loss = 461486.03 (1.685 sec)\n",
            "train - step 1160: loss = 318411.12 (1.681 sec)\n",
            "train - step 1161: loss = 493435.06 (1.709 sec)\n",
            "train - step 1162: loss = 487990.72 (1.674 sec)\n",
            "train - step 1163: loss = 441641.53 (1.697 sec)\n",
            "train - step 1164: loss = 445847.78 (1.702 sec)\n",
            "train - step 1165: loss = 435836.50 (1.684 sec)\n",
            "train - step 1166: loss = 366837.78 (1.697 sec)\n",
            "train - step 1167: loss = 481906.12 (1.716 sec)\n",
            "train - step 1168: loss = 420114.94 (1.695 sec)\n",
            "train - step 1169: loss = 365067.50 (1.684 sec)\n",
            "train - step 1170: loss = 340750.41 (1.694 sec)\n",
            "train - step 1171: loss = 511874.91 (2.739 sec)\n",
            "train - step 1172: loss = 371629.50 (1.730 sec)\n",
            "train - step 1173: loss = 439554.97 (1.696 sec)\n",
            "train - step 1174: loss = 249651.42 (1.686 sec)\n",
            "train - step 1175: loss = 431984.28 (1.699 sec)\n",
            "train - step 1176: loss = 400052.81 (1.700 sec)\n",
            "train - step 1177: loss = 495524.38 (1.683 sec)\n",
            "train - step 1178: loss = 418421.94 (1.684 sec)\n",
            "train - step 1179: loss = 384902.00 (1.717 sec)\n",
            "train - step 1180: loss = 320134.97 (1.692 sec)\n",
            "train - step 1181: loss = 388647.28 (1.721 sec)\n",
            "train - step 1182: loss = 523571.19 (1.646 sec)\n",
            "train - step 1183: loss = 410198.69 (1.652 sec)\n",
            "train - step 1184: loss = 590039.56 (1.633 sec)\n",
            "train - step 1185: loss = 377191.12 (1.645 sec)\n",
            "train - step 1186: loss = 582317.75 (1.662 sec)\n",
            "train - step 1187: loss = 403931.94 (1.656 sec)\n",
            "train - step 1188: loss = 430097.31 (1.688 sec)\n",
            "train - step 1189: loss = 530538.75 (1.693 sec)\n",
            "train - step 1190: loss = 535007.12 (1.688 sec)\n",
            "train - step 1191: loss = 457950.28 (1.694 sec)\n",
            "train - step 1192: loss = 401828.06 (1.680 sec)\n",
            "train - step 1193: loss = 430857.69 (1.697 sec)\n",
            "train - step 1194: loss = 442999.06 (1.681 sec)\n",
            "train - step 1195: loss = 488205.62 (1.689 sec)\n",
            "train - step 1196: loss = 424122.69 (1.715 sec)\n",
            "train - step 1197: loss = 646749.69 (1.694 sec)\n",
            "train - step 1198: loss = 461682.06 (1.704 sec)\n",
            "train - step 1199: loss = 462563.22 (1.703 sec)\n",
            "train - step 1200: loss = 515672.97 (1.720 sec)\n",
            "train - step 1201: loss = 363461.97 (1.692 sec)\n",
            "train - step 1202: loss = 428638.38 (1.685 sec)\n",
            "train - step 1203: loss = 355150.47 (1.688 sec)\n",
            "train - step 1204: loss = 518376.38 (1.687 sec)\n",
            "train - step 1205: loss = 493388.22 (1.698 sec)\n",
            "train - step 1206: loss = 344018.00 (1.691 sec)\n",
            "train - step 1207: loss = 335088.31 (2.513 sec)\n",
            "train - step 1208: loss = 516737.56 (1.651 sec)\n",
            "train - step 1209: loss = 462709.09 (1.648 sec)\n",
            "train - step 1210: loss = 406092.66 (1.682 sec)\n",
            "train - step 1211: loss = 398207.62 (1.699 sec)\n",
            "train - step 1212: loss = 474026.16 (1.676 sec)\n",
            "train - step 1213: loss = 489719.41 (1.701 sec)\n",
            "train - step 1214: loss = 326093.09 (1.670 sec)\n",
            "train - step 1215: loss = 518720.94 (1.676 sec)\n",
            "train - step 1216: loss = 356304.19 (1.690 sec)\n",
            "train - step 1217: loss = 441501.31 (1.679 sec)\n",
            "train - step 1218: loss = 409193.62 (1.702 sec)\n",
            "train - step 1219: loss = 381603.84 (1.705 sec)\n",
            "train - step 1220: loss = 520435.66 (1.696 sec)\n",
            "train - step 1221: loss = 401042.97 (1.685 sec)\n",
            "train - step 1222: loss = 515774.66 (1.695 sec)\n",
            "train - step 1223: loss = 421869.34 (1.700 sec)\n",
            "train - step 1224: loss = 424747.72 (1.688 sec)\n",
            "train - step 1225: loss = 406312.12 (1.681 sec)\n",
            "train - step 1226: loss = 460813.94 (1.669 sec)\n",
            "train - step 1227: loss = 360139.97 (1.712 sec)\n",
            "train - step 1228: loss = 530217.88 (1.671 sec)\n",
            "train - step 1229: loss = 462142.81 (1.706 sec)\n",
            "train - step 1230: loss = 610202.25 (1.673 sec)\n",
            "train - step 1231: loss = 379158.72 (1.677 sec)\n",
            "train - step 1232: loss = 457826.34 (1.672 sec)\n",
            "train - step 1233: loss = 353568.72 (1.694 sec)\n",
            "train - step 1234: loss = 361636.72 (1.689 sec)\n",
            "train - step 1235: loss = 428031.41 (1.675 sec)\n",
            "train - step 1236: loss = 374525.19 (1.709 sec)\n",
            "train - step 1237: loss = 450444.72 (1.682 sec)\n",
            "train - step 1238: loss = 523442.59 (1.705 sec)\n",
            "train - step 1239: loss = 443326.00 (1.683 sec)\n",
            "train - step 1240: loss = 589305.31 (1.696 sec)\n",
            "train - step 1241: loss = 344147.41 (1.692 sec)\n",
            "train - step 1242: loss = 476320.44 (1.671 sec)\n",
            "train - step 1243: loss = 510548.88 (2.632 sec)\n",
            "train - step 1244: loss = 335749.50 (1.694 sec)\n",
            "train - step 1245: loss = 406969.59 (1.695 sec)\n",
            "train - step 1246: loss = 357414.09 (1.665 sec)\n",
            "train - step 1247: loss = 311103.97 (1.698 sec)\n",
            "train - step 1248: loss = 594808.44 (1.689 sec)\n",
            "train - step 1249: loss = 404516.59 (1.694 sec)\n",
            "train - step 1250: loss = 445179.84 (1.699 sec)\n",
            "train - step 1251: loss = 317900.34 (1.680 sec)\n",
            "train - step 1252: loss = 525045.75 (1.702 sec)\n",
            "train - step 1253: loss = 421846.12 (1.687 sec)\n",
            "train - step 1254: loss = 459011.53 (1.708 sec)\n",
            "train - step 1255: loss = 543132.81 (1.705 sec)\n",
            "train - step 1256: loss = 506070.50 (1.680 sec)\n",
            "train - step 1257: loss = 394810.84 (1.672 sec)\n",
            "train - step 1258: loss = 480619.34 (1.703 sec)\n",
            "train - step 1259: loss = 361428.94 (1.693 sec)\n",
            "train - step 1260: loss = 314989.53 (1.708 sec)\n",
            "train - step 1261: loss = 436357.81 (1.681 sec)\n",
            "train - step 1262: loss = 433141.97 (1.673 sec)\n",
            "train - step 1263: loss = 428727.72 (1.691 sec)\n",
            "train - step 1264: loss = 517693.81 (1.684 sec)\n",
            "train - step 1265: loss = 453878.22 (1.677 sec)\n",
            "train - step 1266: loss = 559258.31 (1.661 sec)\n",
            "train - step 1267: loss = 417604.66 (2.379 sec)\n",
            "train - step 1268: loss = 368065.47 (1.712 sec)\n",
            "train - step 1269: loss = 597728.06 (1.699 sec)\n",
            "train - step 1270: loss = 413049.94 (1.715 sec)\n",
            "train - step 1271: loss = 351948.94 (1.687 sec)\n",
            "train - step 1272: loss = 367428.34 (1.681 sec)\n",
            "train - step 1273: loss = 604494.56 (1.682 sec)\n",
            "train - step 1274: loss = 525847.62 (1.687 sec)\n",
            "train - step 1275: loss = 432212.41 (1.667 sec)\n",
            "train - step 1276: loss = 389299.59 (1.682 sec)\n",
            "train - step 1277: loss = 352986.88 (1.673 sec)\n",
            "train - step 1278: loss = 351434.94 (2.539 sec)\n",
            "train - step 1279: loss = 470362.62 (1.700 sec)\n",
            "train - step 1280: loss = 452884.59 (1.682 sec)\n",
            "train - step 1281: loss = 538017.12 (1.699 sec)\n",
            "train - step 1282: loss = 442739.47 (1.683 sec)\n",
            "train - step 1283: loss = 471884.84 (1.676 sec)\n",
            "train - step 1284: loss = 354731.34 (1.694 sec)\n",
            "train - step 1285: loss = 473826.81 (1.688 sec)\n",
            "train - step 1286: loss = 420004.72 (1.689 sec)\n",
            "train - step 1287: loss = 300694.41 (1.687 sec)\n",
            "train - step 1288: loss = 535150.19 (1.708 sec)\n",
            "train - step 1289: loss = 651214.00 (1.674 sec)\n",
            "train - step 1290: loss = 317839.41 (1.682 sec)\n",
            "train - step 1291: loss = 495179.03 (1.707 sec)\n",
            "train - step 1292: loss = 563479.25 (1.701 sec)\n",
            "train - step 1293: loss = 373634.59 (1.686 sec)\n",
            "train - step 1294: loss = 471321.97 (1.683 sec)\n",
            "train - step 1295: loss = 324964.28 (1.704 sec)\n",
            "train - step 1296: loss = 547445.88 (1.681 sec)\n",
            "train - step 1297: loss = 519948.94 (1.717 sec)\n",
            "train - step 1298: loss = 390901.56 (1.687 sec)\n",
            "train - step 1299: loss = 443597.38 (1.700 sec)\n",
            "train - step 1300: loss = 550466.81 (1.693 sec)\n",
            "train - step 1301: loss = 363582.66 (1.686 sec)\n",
            "train - step 1302: loss = 494965.59 (1.675 sec)\n",
            "train - step 1303: loss = 431865.47 (1.707 sec)\n",
            "train - step 1304: loss = 389082.47 (1.692 sec)\n",
            "train - step 1305: loss = 531662.94 (1.678 sec)\n",
            "train - step 1306: loss = 517605.66 (1.680 sec)\n",
            "train - step 1307: loss = 544629.25 (1.682 sec)\n",
            "train - step 1308: loss = 400404.31 (1.681 sec)\n",
            "train - step 1309: loss = 491458.06 (1.667 sec)\n",
            "train - step 1310: loss = 274397.06 (1.677 sec)\n",
            "train - step 1311: loss = 459548.06 (1.689 sec)\n",
            "train - step 1312: loss = 441491.88 (1.701 sec)\n",
            "train - step 1313: loss = 423598.72 (1.683 sec)\n",
            "train - step 1314: loss = 404079.81 (2.565 sec)\n",
            "train - step 1315: loss = 475898.16 (1.716 sec)\n",
            "train - step 1316: loss = 632892.00 (1.668 sec)\n",
            "train - step 1317: loss = 485355.72 (1.680 sec)\n",
            "train - step 1318: loss = 450978.59 (1.682 sec)\n",
            "train - step 1319: loss = 419831.97 (1.677 sec)\n",
            "train - step 1320: loss = 393749.94 (1.692 sec)\n",
            "train - step 1321: loss = 387681.47 (1.668 sec)\n",
            "train - step 1322: loss = 393149.66 (1.684 sec)\n",
            "train - step 1323: loss = 548426.06 (1.696 sec)\n",
            "train - step 1324: loss = 462974.59 (1.669 sec)\n",
            "train - step 1325: loss = 581554.31 (1.674 sec)\n",
            "train - step 1326: loss = 465394.84 (1.699 sec)\n",
            "train - step 1327: loss = 335483.62 (1.690 sec)\n",
            "train - step 1328: loss = 569015.94 (1.679 sec)\n",
            "train - step 1329: loss = 598241.38 (1.703 sec)\n",
            "train - step 1330: loss = 347274.09 (1.680 sec)\n",
            "train - step 1331: loss = 416198.53 (1.696 sec)\n",
            "train - step 1332: loss = 469603.53 (1.683 sec)\n",
            "train - step 1333: loss = 505894.81 (1.696 sec)\n",
            "train - step 1334: loss = 544749.88 (1.687 sec)\n",
            "train - step 1335: loss = 451543.19 (1.695 sec)\n",
            "train - step 1336: loss = 512289.84 (1.681 sec)\n",
            "train - step 1337: loss = 351492.81 (1.685 sec)\n",
            "train - step 1338: loss = 407429.38 (1.681 sec)\n",
            "train - step 1339: loss = 520961.62 (1.706 sec)\n",
            "train - step 1340: loss = 274838.59 (1.681 sec)\n",
            "train - step 1341: loss = 371840.72 (1.716 sec)\n",
            "train - step 1342: loss = 446852.31 (1.679 sec)\n",
            "train - step 1343: loss = 323314.22 (1.688 sec)\n",
            "train - step 1344: loss = 528288.38 (1.677 sec)\n",
            "train - step 1345: loss = 385364.72 (1.709 sec)\n",
            "train - step 1346: loss = 350057.72 (1.691 sec)\n",
            "train - step 1347: loss = 475083.47 (1.690 sec)\n",
            "train - step 1348: loss = 433920.56 (1.681 sec)\n",
            "train - step 1349: loss = 469312.72 (1.699 sec)\n",
            "train - step 1350: loss = 374735.62 (2.677 sec)\n",
            "train - step 1351: loss = 366343.28 (1.712 sec)\n",
            "train - step 1352: loss = 360761.69 (1.685 sec)\n",
            "train - step 1353: loss = 356697.59 (1.680 sec)\n",
            "train - step 1354: loss = 419506.28 (1.733 sec)\n",
            "train - step 1355: loss = 478399.00 (1.700 sec)\n",
            "train - step 1356: loss = 325131.00 (1.723 sec)\n",
            "train - step 1357: loss = 454849.94 (1.698 sec)\n",
            "train - step 1358: loss = 569959.31 (1.686 sec)\n",
            "train - step 1359: loss = 318915.28 (1.698 sec)\n",
            "train - step 1360: loss = 313689.44 (1.683 sec)\n",
            "train - step 1361: loss = 543127.38 (1.688 sec)\n",
            "train - step 1362: loss = 437382.16 (1.690 sec)\n",
            "train - step 1363: loss = 416535.94 (1.659 sec)\n",
            "train - step 1364: loss = 597074.94 (1.640 sec)\n",
            "train - step 1365: loss = 421867.81 (1.650 sec)\n",
            "train - step 1366: loss = 474993.94 (1.639 sec)\n",
            "train - step 1367: loss = 438304.28 (1.651 sec)\n",
            "train - step 1368: loss = 392367.09 (1.658 sec)\n",
            "train - step 1369: loss = 527830.06 (1.665 sec)\n",
            "train - step 1370: loss = 516240.81 (1.690 sec)\n",
            "train - step 1371: loss = 471732.53 (1.688 sec)\n",
            "train - step 1372: loss = 388593.84 (1.717 sec)\n",
            "train - step 1373: loss = 451814.81 (1.698 sec)\n",
            "train - step 1374: loss = 617659.00 (1.689 sec)\n",
            "train - step 1375: loss = 515205.88 (1.692 sec)\n",
            "train - step 1376: loss = 587576.19 (1.691 sec)\n",
            "train - step 1377: loss = 417412.66 (1.702 sec)\n",
            "train - step 1378: loss = 489953.56 (1.695 sec)\n",
            "train - step 1379: loss = 373728.53 (1.705 sec)\n",
            "train - step 1380: loss = 454836.09 (1.680 sec)\n",
            "train - step 1381: loss = 423606.72 (1.675 sec)\n",
            "train - step 1382: loss = 373443.44 (1.668 sec)\n",
            "train - step 1383: loss = 507774.72 (1.731 sec)\n",
            "train - step 1384: loss = 425674.66 (1.684 sec)\n",
            "train - step 1385: loss = 472495.50 (1.705 sec)\n",
            "train - step 1386: loss = 556157.69 (2.501 sec)\n",
            "train - step 1387: loss = 449697.34 (1.696 sec)\n",
            "train - step 1388: loss = 525615.31 (1.704 sec)\n",
            "train - step 1389: loss = 415558.84 (1.678 sec)\n",
            "train - step 1390: loss = 356324.53 (1.714 sec)\n",
            "train - step 1391: loss = 327505.47 (1.668 sec)\n",
            "train - step 1392: loss = 434525.91 (1.698 sec)\n",
            "train - step 1393: loss = 483425.34 (1.719 sec)\n",
            "train - step 1394: loss = 472232.12 (1.680 sec)\n",
            "train - step 1395: loss = 279064.06 (1.678 sec)\n",
            "train - step 1396: loss = 456258.06 (1.675 sec)\n",
            "train - step 1397: loss = 307227.19 (1.680 sec)\n",
            "train - step 1398: loss = 324331.41 (1.684 sec)\n",
            "train - step 1399: loss = 528675.19 (1.704 sec)\n",
            "train - step 1400: loss = 422670.31 (1.671 sec)\n",
            "train - step 1401: loss = 461184.16 (1.692 sec)\n",
            "train - step 1402: loss = 449936.31 (1.703 sec)\n",
            "train - step 1403: loss = 344560.31 (1.695 sec)\n",
            "train - step 1404: loss = 328881.53 (1.691 sec)\n",
            "train - step 1405: loss = 445755.00 (1.706 sec)\n",
            "train - step 1406: loss = 508752.94 (1.684 sec)\n",
            "train - step 1407: loss = 660469.81 (1.681 sec)\n",
            "train - step 1408: loss = 469068.03 (1.674 sec)\n",
            "train - step 1409: loss = 468471.38 (1.671 sec)\n",
            "train - step 1410: loss = 423025.81 (1.693 sec)\n",
            "train - step 1411: loss = 490607.78 (1.667 sec)\n",
            "train - step 1412: loss = 199697.77 (1.677 sec)\n",
            "train - step 1413: loss = 332552.91 (1.678 sec)\n",
            "train - step 1414: loss = 319918.72 (1.669 sec)\n",
            "train - step 1415: loss = 366258.97 (1.693 sec)\n",
            "train - step 1416: loss = 552879.69 (1.666 sec)\n",
            "train - step 1417: loss = 193016.80 (1.683 sec)\n",
            "train - step 1418: loss = 464233.34 (1.674 sec)\n",
            "train - step 1419: loss = 547916.25 (1.712 sec)\n",
            "train - step 1420: loss = 450011.59 (1.704 sec)\n",
            "train - step 1421: loss = 517529.66 (1.759 sec)\n",
            "train - step 1422: loss = 358908.19 (1.720 sec)\n",
            "train - step 1423: loss = 366787.72 (1.694 sec)\n",
            "train - step 1424: loss = 386275.12 (1.710 sec)\n",
            "train - step 1425: loss = 535393.88 (1.695 sec)\n",
            "train - step 1426: loss = 439911.97 (1.669 sec)\n",
            "train - step 1427: loss = 347936.94 (1.684 sec)\n",
            "train - step 1428: loss = 408103.28 (1.686 sec)\n",
            "train - step 1429: loss = 391655.78 (1.668 sec)\n",
            "train - step 1430: loss = 582305.38 (1.687 sec)\n",
            "train - step 1431: loss = 548124.44 (2.139 sec)\n",
            "train - step 1432: loss = 328992.47 (1.717 sec)\n",
            "train - step 1433: loss = 329049.59 (1.702 sec)\n",
            "train - step 1434: loss = 383847.00 (1.700 sec)\n",
            "train - step 1435: loss = 403458.47 (1.675 sec)\n",
            "train - step 1436: loss = 509997.03 (1.663 sec)\n",
            "train - step 1437: loss = 532594.19 (1.647 sec)\n",
            "train - step 1438: loss = 523897.66 (1.689 sec)\n",
            "train - step 1439: loss = 382449.56 (1.686 sec)\n",
            "train - step 1440: loss = 610762.00 (1.664 sec)\n",
            "train - step 1441: loss = 423545.41 (1.685 sec)\n",
            "train - step 1442: loss = 360963.56 (1.679 sec)\n",
            "train - step 1443: loss = 298670.09 (1.656 sec)\n",
            "train - step 1444: loss = 416577.16 (1.633 sec)\n",
            "train - step 1445: loss = 546949.75 (1.651 sec)\n",
            "train - step 1446: loss = 491796.16 (1.677 sec)\n",
            "train - step 1447: loss = 438830.66 (1.704 sec)\n",
            "train - step 1448: loss = 363772.41 (1.690 sec)\n",
            "train - step 1449: loss = 477473.06 (1.684 sec)\n",
            "train - step 1450: loss = 502121.53 (1.653 sec)\n",
            "train - step 1451: loss = 399376.66 (1.647 sec)\n",
            "train - step 1452: loss = 383471.28 (1.688 sec)\n",
            "train - step 1453: loss = 477940.94 (1.688 sec)\n",
            "train - step 1454: loss = 253917.33 (1.686 sec)\n",
            "train - step 1455: loss = 418551.56 (1.703 sec)\n",
            "train - step 1456: loss = 550854.62 (1.677 sec)\n",
            "train - step 1457: loss = 433916.91 (2.750 sec)\n",
            "train - step 1458: loss = 539466.69 (1.713 sec)\n",
            "train - step 1459: loss = 398935.09 (1.675 sec)\n",
            "train - step 1460: loss = 381415.03 (1.697 sec)\n",
            "train - step 1461: loss = 507384.56 (1.684 sec)\n",
            "train - step 1462: loss = 650614.62 (1.685 sec)\n",
            "train - step 1463: loss = 426304.47 (1.673 sec)\n",
            "train - step 1464: loss = 571061.56 (1.683 sec)\n",
            "train - step 1465: loss = 469100.50 (1.654 sec)\n",
            "train - step 1466: loss = 510158.34 (1.698 sec)\n",
            "train - step 1467: loss = 423310.22 (1.674 sec)\n",
            "train - step 1468: loss = 401930.50 (1.683 sec)\n",
            "train - step 1469: loss = 580761.12 (1.685 sec)\n",
            "train - step 1470: loss = 432290.78 (1.676 sec)\n",
            "train - step 1471: loss = 551950.31 (1.682 sec)\n",
            "train - step 1472: loss = 504195.09 (1.682 sec)\n",
            "train - step 1473: loss = 476182.16 (1.705 sec)\n",
            "train - step 1474: loss = 414296.03 (1.659 sec)\n",
            "train - step 1475: loss = 451980.34 (1.698 sec)\n",
            "train - step 1476: loss = 330296.12 (1.678 sec)\n",
            "train - step 1477: loss = 378407.62 (1.749 sec)\n",
            "train - step 1478: loss = 422916.12 (1.717 sec)\n",
            "train - step 1479: loss = 394539.97 (1.684 sec)\n",
            "train - step 1480: loss = 577221.44 (1.690 sec)\n",
            "train - step 1481: loss = 497073.47 (1.677 sec)\n",
            "train - step 1482: loss = 365242.34 (1.704 sec)\n",
            "train - step 1483: loss = 294383.66 (1.724 sec)\n",
            "train - step 1484: loss = 440863.59 (1.735 sec)\n",
            "train - step 1485: loss = 331055.28 (1.714 sec)\n",
            "train - step 1486: loss = 318303.88 (1.747 sec)\n",
            "train - step 1487: loss = 285088.59 (1.706 sec)\n",
            "train - step 1488: loss = 440137.91 (1.671 sec)\n",
            "train - step 1489: loss = 444319.69 (1.674 sec)\n",
            "train - step 1490: loss = 365187.38 (1.700 sec)\n",
            "train - step 1491: loss = 658367.88 (1.692 sec)\n",
            "train - step 1492: loss = 627580.69 (2.653 sec)\n",
            "train - step 1493: loss = 553817.06 (1.736 sec)\n",
            "train - step 1494: loss = 554807.69 (1.697 sec)\n",
            "train - step 1495: loss = 432432.12 (1.701 sec)\n",
            "train - step 1496: loss = 681608.25 (1.682 sec)\n",
            "train - step 1497: loss = 598874.88 (1.700 sec)\n",
            "train - step 1498: loss = 468123.97 (1.668 sec)\n",
            "train - step 1499: loss = 544070.38 (1.682 sec)\n",
            "train - step 1500: loss = 256486.78 (1.717 sec)\n",
            "train - step 1501: loss = 367472.28 (1.700 sec)\n",
            "train - step 1502: loss = 660601.62 (1.682 sec)\n",
            "train - step 1503: loss = 458535.22 (1.688 sec)\n",
            "train - step 1504: loss = 345323.16 (1.693 sec)\n",
            "train - step 1505: loss = 520759.66 (1.683 sec)\n",
            "train - step 1506: loss = 292941.94 (1.692 sec)\n",
            "train - step 1507: loss = 425857.41 (1.667 sec)\n",
            "train - step 1508: loss = 370380.84 (1.675 sec)\n",
            "train - step 1509: loss = 383679.44 (1.677 sec)\n",
            "train - step 1510: loss = 441846.88 (1.715 sec)\n",
            "train - step 1511: loss = 457584.12 (1.705 sec)\n",
            "train - step 1512: loss = 584273.31 (1.706 sec)\n",
            "train - step 1513: loss = 294359.41 (1.682 sec)\n",
            "train - step 1514: loss = 404250.19 (1.667 sec)\n",
            "train - step 1515: loss = 725376.62 (1.673 sec)\n",
            "train - step 1516: loss = 510918.31 (1.664 sec)\n",
            "train - step 1517: loss = 372815.81 (1.675 sec)\n",
            "train - step 1518: loss = 352391.78 (1.667 sec)\n",
            "train - step 1519: loss = 494810.28 (1.689 sec)\n",
            "train - step 1520: loss = 350244.28 (1.668 sec)\n",
            "train - step 1521: loss = 446460.28 (1.675 sec)\n",
            "train - step 1522: loss = 439916.47 (1.692 sec)\n",
            "train - step 1523: loss = 564739.00 (1.686 sec)\n",
            "train - step 1524: loss = 368678.41 (1.703 sec)\n",
            "train - step 1525: loss = 543566.19 (1.691 sec)\n",
            "train - step 1526: loss = 488834.41 (1.669 sec)\n",
            "train - step 1527: loss = 372140.16 (1.697 sec)\n",
            "train - step 1528: loss = 306684.19 (2.837 sec)\n",
            "train - step 1529: loss = 424846.81 (1.728 sec)\n",
            "train - step 1530: loss = 393544.28 (1.711 sec)\n",
            "train - step 1531: loss = 498309.19 (1.690 sec)\n",
            "train - step 1532: loss = 469615.62 (1.696 sec)\n",
            "train - step 1533: loss = 484669.47 (1.694 sec)\n",
            "train - step 1534: loss = 337346.41 (1.699 sec)\n",
            "train - step 1535: loss = 479556.03 (1.687 sec)\n",
            "train - step 1536: loss = 448947.62 (1.693 sec)\n",
            "train - step 1537: loss = 381643.53 (1.668 sec)\n",
            "train - step 1538: loss = 521859.00 (1.703 sec)\n",
            "train - step 1539: loss = 295129.34 (1.671 sec)\n",
            "train - step 1540: loss = 400464.41 (1.710 sec)\n",
            "train - step 1541: loss = 636840.75 (1.682 sec)\n",
            "train - step 1542: loss = 425940.38 (1.689 sec)\n",
            "train - step 1543: loss = 469075.66 (1.687 sec)\n",
            "train - step 1544: loss = 389939.31 (1.634 sec)\n",
            "train - step 1545: loss = 342980.59 (1.642 sec)\n",
            "train - step 1546: loss = 504160.59 (1.634 sec)\n",
            "train - step 1547: loss = 363805.59 (1.658 sec)\n",
            "train - step 1548: loss = 485715.16 (1.637 sec)\n",
            "train - step 1549: loss = 527881.81 (1.675 sec)\n",
            "train - step 1550: loss = 383293.06 (1.699 sec)\n",
            "train - step 1551: loss = 518866.09 (1.691 sec)\n",
            "train - step 1552: loss = 465540.34 (1.671 sec)\n",
            "train - step 1553: loss = 427088.22 (1.698 sec)\n",
            "train - step 1554: loss = 575748.75 (1.681 sec)\n",
            "train - step 1555: loss = 458926.44 (1.704 sec)\n",
            "train - step 1556: loss = 583190.62 (1.704 sec)\n",
            "train - step 1557: loss = 440001.34 (1.682 sec)\n",
            "train - step 1558: loss = 408673.91 (1.680 sec)\n",
            "train - step 1559: loss = 434765.12 (1.690 sec)\n",
            "train - step 1560: loss = 471280.41 (1.689 sec)\n",
            "train - step 1561: loss = 498419.72 (1.691 sec)\n",
            "train - step 1562: loss = 456799.69 (1.698 sec)\n",
            "train - step 1563: loss = 443085.78 (1.676 sec)\n",
            "train - step 1564: loss = 451876.66 (2.536 sec)\n",
            "train - step 1565: loss = 423413.56 (1.667 sec)\n",
            "train - step 1566: loss = 451492.59 (1.663 sec)\n",
            "train - step 1567: loss = 545765.44 (1.701 sec)\n",
            "train - step 1568: loss = 336814.34 (1.666 sec)\n",
            "train - step 1569: loss = 404131.19 (1.693 sec)\n",
            "train - step 1570: loss = 388046.19 (1.670 sec)\n",
            "train - step 1571: loss = 488871.41 (1.674 sec)\n",
            "train - step 1572: loss = 394670.06 (1.687 sec)\n",
            "train - step 1573: loss = 449721.50 (1.689 sec)\n",
            "train - step 1574: loss = 376034.81 (1.695 sec)\n",
            "train - step 1575: loss = 341446.22 (1.688 sec)\n",
            "train - step 1576: loss = 326405.62 (1.693 sec)\n",
            "train - step 1577: loss = 387491.66 (1.697 sec)\n",
            "train - step 1578: loss = 404806.38 (1.701 sec)\n",
            "train - step 1579: loss = 358857.53 (1.681 sec)\n",
            "train - step 1580: loss = 446730.62 (1.691 sec)\n",
            "train - step 1581: loss = 308036.44 (1.689 sec)\n",
            "train - step 1582: loss = 325121.72 (1.673 sec)\n",
            "train - step 1583: loss = 486767.41 (1.685 sec)\n",
            "train - step 1584: loss = 423028.38 (1.688 sec)\n",
            "train - step 1585: loss = 296535.28 (1.716 sec)\n",
            "train - step 1586: loss = 431013.88 (1.672 sec)\n",
            "train - step 1587: loss = 358485.84 (1.688 sec)\n",
            "train - step 1588: loss = 855481.12 (1.684 sec)\n",
            "train - step 1589: loss = 527024.81 (1.707 sec)\n",
            "train - step 1590: loss = 419251.12 (1.683 sec)\n",
            "train - step 1591: loss = 489872.84 (1.716 sec)\n",
            "train - step 1592: loss = 476516.47 (1.694 sec)\n",
            "train - step 1593: loss = 391798.16 (1.703 sec)\n",
            "train - step 1594: loss = 413120.66 (1.692 sec)\n",
            "train - step 1595: loss = 596191.88 (1.667 sec)\n",
            "train - step 1596: loss = 346672.12 (1.691 sec)\n",
            "train - step 1597: loss = 444737.31 (1.667 sec)\n",
            "train - step 1598: loss = 392118.94 (1.676 sec)\n",
            "train - step 1599: loss = 414262.34 (1.694 sec)\n",
            "train - step 1600: loss = 322526.91 (2.544 sec)\n",
            "train - step 1601: loss = 535077.62 (1.698 sec)\n",
            "train - step 1602: loss = 578638.12 (1.708 sec)\n",
            "train - step 1603: loss = 435558.06 (1.695 sec)\n",
            "train - step 1604: loss = 496504.50 (1.666 sec)\n",
            "train - step 1605: loss = 443789.00 (1.713 sec)\n",
            "train - step 1606: loss = 398759.50 (1.664 sec)\n",
            "train - step 1607: loss = 426880.03 (1.699 sec)\n",
            "train - step 1608: loss = 291785.94 (1.687 sec)\n",
            "train - step 1609: loss = 474105.50 (1.664 sec)\n",
            "train - step 1610: loss = 637714.88 (1.702 sec)\n",
            "train - step 1611: loss = 445647.69 (1.709 sec)\n",
            "train - step 1612: loss = 555455.81 (1.696 sec)\n",
            "train - step 1613: loss = 485026.41 (1.698 sec)\n",
            "train - step 1614: loss = 456533.78 (1.696 sec)\n",
            "train - step 1615: loss = 524673.25 (1.698 sec)\n",
            "train - step 1616: loss = 415486.03 (1.691 sec)\n",
            "train - step 1617: loss = 462962.41 (1.678 sec)\n",
            "train - step 1618: loss = 514039.97 (1.676 sec)\n",
            "train - step 1619: loss = 384217.84 (1.707 sec)\n",
            "train - step 1620: loss = 357158.47 (1.691 sec)\n",
            "train - step 1621: loss = 498137.03 (1.672 sec)\n",
            "train - step 1622: loss = 423256.00 (1.706 sec)\n",
            "train - step 1623: loss = 423042.34 (1.684 sec)\n",
            "train - step 1624: loss = 374415.94 (1.686 sec)\n",
            "train - step 1625: loss = 489927.19 (1.677 sec)\n",
            "train - step 1626: loss = 438978.19 (1.698 sec)\n",
            "train - step 1627: loss = 435901.81 (1.708 sec)\n",
            "train - step 1628: loss = 519447.19 (1.681 sec)\n",
            "train - step 1629: loss = 526975.12 (1.696 sec)\n",
            "train - step 1630: loss = 609532.94 (1.689 sec)\n",
            "train - step 1631: loss = 416427.03 (1.715 sec)\n",
            "train - step 1632: loss = 383681.69 (1.675 sec)\n",
            "train - step 1633: loss = 393144.56 (1.727 sec)\n",
            "train - step 1634: loss = 317351.12 (1.671 sec)\n",
            "train - step 1635: loss = 516052.16 (1.698 sec)\n",
            "train - step 1636: loss = 585478.12 (2.518 sec)\n",
            "train - step 1637: loss = 335729.06 (1.693 sec)\n",
            "train - step 1638: loss = 575756.56 (1.703 sec)\n",
            "train - step 1639: loss = 355461.97 (1.685 sec)\n",
            "train - step 1640: loss = 479977.22 (1.679 sec)\n",
            "train - step 1641: loss = 290921.09 (1.662 sec)\n",
            "train - step 1642: loss = 508092.69 (1.692 sec)\n",
            "train - step 1643: loss = 544006.31 (1.678 sec)\n",
            "train - step 1644: loss = 530043.06 (1.708 sec)\n",
            "train - step 1645: loss = 627911.88 (1.683 sec)\n",
            "train - step 1646: loss = 646631.31 (1.681 sec)\n",
            "train - step 1647: loss = 403036.34 (1.693 sec)\n",
            "train - step 1648: loss = 430056.09 (1.664 sec)\n",
            "train - step 1649: loss = 419634.53 (1.703 sec)\n",
            "train - step 1650: loss = 422123.53 (1.661 sec)\n",
            "train - step 1651: loss = 369487.47 (1.689 sec)\n",
            "train - step 1652: loss = 339321.84 (1.671 sec)\n",
            "train - step 1653: loss = 484417.50 (1.719 sec)\n",
            "train - step 1654: loss = 416435.00 (1.680 sec)\n",
            "train - step 1655: loss = 397118.44 (1.724 sec)\n",
            "train - step 1656: loss = 527133.31 (1.708 sec)\n",
            "train - step 1657: loss = 444869.94 (1.690 sec)\n",
            "train - step 1658: loss = 428339.59 (1.703 sec)\n",
            "train - step 1659: loss = 511309.47 (1.680 sec)\n",
            "train - step 1660: loss = 466520.81 (1.713 sec)\n",
            "train - step 1661: loss = 520769.50 (1.696 sec)\n",
            "train - step 1662: loss = 600857.94 (1.698 sec)\n",
            "train - step 1663: loss = 389586.59 (1.683 sec)\n",
            "train - step 1664: loss = 620492.69 (1.695 sec)\n",
            "train - step 1665: loss = 390256.56 (1.670 sec)\n",
            "train - step 1666: loss = 364473.56 (1.680 sec)\n",
            "train - step 1667: loss = 537108.62 (1.663 sec)\n",
            "train - step 1668: loss = 443699.12 (1.697 sec)\n",
            "train - step 1669: loss = 360385.78 (1.677 sec)\n",
            "train - step 1670: loss = 503132.81 (1.691 sec)\n",
            "train - step 1671: loss = 550533.62 (1.691 sec)\n",
            "train - step 1672: loss = 322768.94 (2.526 sec)\n",
            "train - step 1673: loss = 495847.88 (1.705 sec)\n",
            "train - step 1674: loss = 569519.69 (1.695 sec)\n",
            "train - step 1675: loss = 515711.34 (1.695 sec)\n",
            "train - step 1676: loss = 359530.81 (1.677 sec)\n",
            "train - step 1677: loss = 434682.16 (1.661 sec)\n",
            "train - step 1678: loss = 406375.22 (1.672 sec)\n",
            "train - step 1679: loss = 501224.56 (1.686 sec)\n",
            "train - step 1680: loss = 498919.09 (1.733 sec)\n",
            "train - step 1681: loss = 397039.12 (1.677 sec)\n",
            "train - step 1682: loss = 334882.38 (1.720 sec)\n",
            "train - step 1683: loss = 653075.81 (1.699 sec)\n",
            "train - step 1684: loss = 411481.53 (1.703 sec)\n",
            "train - step 1685: loss = 492826.72 (1.682 sec)\n",
            "train - step 1686: loss = 626166.56 (1.708 sec)\n",
            "train - step 1687: loss = 376187.03 (1.678 sec)\n",
            "train - step 1688: loss = 409015.38 (1.688 sec)\n",
            "train - step 1689: loss = 281822.16 (1.704 sec)\n",
            "train - step 1690: loss = 319276.19 (1.690 sec)\n",
            "train - step 1691: loss = 503470.53 (1.695 sec)\n",
            "train - step 1692: loss = 427437.31 (1.687 sec)\n",
            "train - step 1693: loss = 512696.78 (1.697 sec)\n",
            "train - step 1694: loss = 306399.78 (1.704 sec)\n",
            "train - step 1695: loss = 489018.50 (1.725 sec)\n",
            "train - step 1696: loss = 283706.66 (1.703 sec)\n",
            "train - step 1697: loss = 486115.16 (1.701 sec)\n",
            "train - step 1698: loss = 469746.47 (1.695 sec)\n",
            "train - step 1699: loss = 453001.78 (1.729 sec)\n",
            "train - step 1700: loss = 455439.38 (1.703 sec)\n",
            "train - step 1701: loss = 457502.34 (1.694 sec)\n",
            "train - step 1702: loss = 474392.91 (1.724 sec)\n",
            "train - step 1703: loss = 544461.94 (1.713 sec)\n",
            "train - step 1704: loss = 679292.69 (1.711 sec)\n",
            "train - step 1705: loss = 383347.84 (1.675 sec)\n",
            "train - step 1706: loss = 504094.19 (1.691 sec)\n",
            "train - step 1707: loss = 328132.44 (1.700 sec)\n",
            "train - step 1708: loss = 423959.66 (2.567 sec)\n",
            "train - step 1709: loss = 427198.91 (1.685 sec)\n",
            "train - step 1710: loss = 395175.47 (1.727 sec)\n",
            "train - step 1711: loss = 464626.72 (1.676 sec)\n",
            "train - step 1712: loss = 649124.81 (1.750 sec)\n",
            "train - step 1713: loss = 332648.94 (1.689 sec)\n",
            "train - step 1714: loss = 409766.28 (1.694 sec)\n",
            "train - step 1715: loss = 551213.56 (1.693 sec)\n",
            "train - step 1716: loss = 507335.06 (1.696 sec)\n",
            "train - step 1717: loss = 298744.72 (1.770 sec)\n",
            "train - step 1718: loss = 304825.66 (1.721 sec)\n",
            "train - step 1719: loss = 429088.91 (1.696 sec)\n",
            "train - step 1720: loss = 320790.62 (1.766 sec)\n",
            "train - step 1721: loss = 303601.38 (1.666 sec)\n",
            "train - step 1722: loss = 365173.12 (2.176 sec)\n",
            "train - step 1723: loss = 391133.88 (1.731 sec)\n",
            "train - step 1724: loss = 371868.62 (1.697 sec)\n",
            "train - step 1725: loss = 538967.12 (1.749 sec)\n",
            "train - step 1726: loss = 405513.78 (1.652 sec)\n",
            "train - step 1727: loss = 339555.06 (1.650 sec)\n",
            "train - step 1728: loss = 544033.38 (1.651 sec)\n",
            "train - step 1729: loss = 634240.94 (1.654 sec)\n",
            "train - step 1730: loss = 386407.72 (1.655 sec)\n",
            "train - step 1731: loss = 365700.53 (1.672 sec)\n",
            "train - step 1732: loss = 546749.38 (1.693 sec)\n",
            "train - step 1733: loss = 452839.78 (1.679 sec)\n",
            "train - step 1734: loss = 463362.56 (1.703 sec)\n",
            "train - step 1735: loss = 489435.50 (1.709 sec)\n",
            "train - step 1736: loss = 447874.38 (1.663 sec)\n",
            "train - step 1737: loss = 542793.25 (1.678 sec)\n",
            "train - step 1738: loss = 420736.88 (1.665 sec)\n",
            "train - step 1739: loss = 382003.19 (1.690 sec)\n",
            "train - step 1740: loss = 515585.53 (1.673 sec)\n",
            "train - step 1741: loss = 368391.28 (1.741 sec)\n",
            "train - step 1742: loss = 533352.19 (1.649 sec)\n",
            "train - step 1743: loss = 302691.72 (2.596 sec)\n",
            "train - step 1744: loss = 212040.41 (1.677 sec)\n",
            "train - step 1745: loss = 443677.44 (1.660 sec)\n",
            "train - step 1746: loss = 332458.94 (1.696 sec)\n",
            "train - step 1747: loss = 361212.69 (1.691 sec)\n",
            "train - step 1748: loss = 410697.78 (1.680 sec)\n",
            "train - step 1749: loss = 438117.78 (1.676 sec)\n",
            "train - step 1750: loss = 393074.12 (1.703 sec)\n",
            "train - step 1751: loss = 447664.81 (1.675 sec)\n",
            "train - step 1752: loss = 422772.06 (1.685 sec)\n",
            "train - step 1753: loss = 501173.78 (1.701 sec)\n",
            "train - step 1754: loss = 338246.31 (1.682 sec)\n",
            "train - step 1755: loss = 331833.72 (1.731 sec)\n",
            "train - step 1756: loss = 544873.50 (1.672 sec)\n",
            "train - step 1757: loss = 463982.50 (1.696 sec)\n",
            "train - step 1758: loss = 515358.31 (1.676 sec)\n",
            "train - step 1759: loss = 448700.09 (1.687 sec)\n",
            "train - step 1760: loss = 388114.88 (1.693 sec)\n",
            "train - step 1761: loss = 484387.88 (1.694 sec)\n",
            "train - step 1762: loss = 513406.88 (1.671 sec)\n",
            "train - step 1763: loss = 491000.69 (1.711 sec)\n",
            "train - step 1764: loss = 326196.41 (1.680 sec)\n",
            "train - step 1765: loss = 389159.34 (1.684 sec)\n",
            "train - step 1766: loss = 433538.28 (1.684 sec)\n",
            "train - step 1767: loss = 512092.94 (1.698 sec)\n",
            "train - step 1768: loss = 485266.94 (1.733 sec)\n",
            "train - step 1769: loss = 491576.28 (1.686 sec)\n",
            "train - step 1770: loss = 263115.47 (1.706 sec)\n",
            "train - step 1771: loss = 380017.16 (1.666 sec)\n",
            "train - step 1772: loss = 487653.09 (1.696 sec)\n",
            "train - step 1773: loss = 443528.72 (1.702 sec)\n",
            "train - step 1774: loss = 612695.94 (1.690 sec)\n",
            "train - step 1775: loss = 480639.12 (1.657 sec)\n",
            "train - step 1776: loss = 293477.00 (1.640 sec)\n",
            "train - step 1777: loss = 413803.78 (1.626 sec)\n",
            "train - step 1778: loss = 436897.50 (1.679 sec)\n",
            "train - step 1779: loss = 403345.06 (2.727 sec)\n",
            "train - step 1780: loss = 505862.72 (1.709 sec)\n",
            "train - step 1781: loss = 474729.91 (1.683 sec)\n",
            "train - step 1782: loss = 605170.00 (1.694 sec)\n",
            "train - step 1783: loss = 471079.81 (1.698 sec)\n",
            "train - step 1784: loss = 308242.62 (1.715 sec)\n",
            "train - step 1785: loss = 361885.88 (1.703 sec)\n",
            "train - step 1786: loss = 412813.78 (1.665 sec)\n",
            "train - step 1787: loss = 432541.62 (1.689 sec)\n",
            "train - step 1788: loss = 312973.56 (1.693 sec)\n",
            "train - step 1789: loss = 457928.12 (1.686 sec)\n",
            "train - step 1790: loss = 266767.25 (1.672 sec)\n",
            "train - step 1791: loss = 446489.53 (1.685 sec)\n",
            "train - step 1792: loss = 466895.72 (1.678 sec)\n",
            "train - step 1793: loss = 278964.41 (1.692 sec)\n",
            "train - step 1794: loss = 353968.09 (1.692 sec)\n",
            "train - step 1795: loss = 417358.81 (1.697 sec)\n",
            "train - step 1796: loss = 448158.94 (1.692 sec)\n",
            "train - step 1797: loss = 508119.31 (1.680 sec)\n",
            "train - step 1798: loss = 329103.19 (1.671 sec)\n",
            "train - step 1799: loss = 588929.06 (1.686 sec)\n",
            "train - step 1800: loss = 544803.12 (1.700 sec)\n",
            "train - step 1801: loss = 419823.88 (1.692 sec)\n",
            "train - step 1802: loss = 571800.69 (1.719 sec)\n",
            "train - step 1803: loss = 520056.69 (1.690 sec)\n",
            "train - step 1804: loss = 428811.03 (1.714 sec)\n",
            "train - step 1805: loss = 475661.06 (1.671 sec)\n",
            "train - step 1806: loss = 387493.03 (1.687 sec)\n",
            "train - step 1807: loss = 352269.41 (1.690 sec)\n",
            "train - step 1808: loss = 495421.47 (1.679 sec)\n",
            "train - step 1809: loss = 635098.81 (1.687 sec)\n",
            "train - step 1810: loss = 380035.28 (1.674 sec)\n",
            "train - step 1811: loss = 327964.84 (1.680 sec)\n",
            "train - step 1812: loss = 607097.75 (1.687 sec)\n",
            "train - step 1813: loss = 345274.94 (1.701 sec)\n",
            "train - step 1814: loss = 420116.91 (1.675 sec)\n",
            "train - step 1815: loss = 475176.66 (2.546 sec)\n",
            "train - step 1816: loss = 327904.44 (1.708 sec)\n",
            "train - step 1817: loss = 268878.06 (1.693 sec)\n",
            "train - step 1818: loss = 486122.44 (1.723 sec)\n",
            "train - step 1819: loss = 488350.81 (1.680 sec)\n",
            "train - step 1820: loss = 454387.22 (1.679 sec)\n",
            "train - step 1821: loss = 514554.88 (1.673 sec)\n",
            "train - step 1822: loss = 377974.56 (1.685 sec)\n",
            "train - step 1823: loss = 441610.47 (1.683 sec)\n",
            "train - step 1824: loss = 476581.19 (1.691 sec)\n",
            "train - step 1825: loss = 535611.00 (1.687 sec)\n",
            "train - step 1826: loss = 423905.69 (1.709 sec)\n",
            "train - step 1827: loss = 568755.25 (1.683 sec)\n",
            "train - step 1828: loss = 472175.88 (1.661 sec)\n",
            "train - step 1829: loss = 279534.34 (1.701 sec)\n",
            "train - step 1830: loss = 456966.28 (1.675 sec)\n",
            "train - step 1831: loss = 721380.00 (1.709 sec)\n",
            "train - step 1832: loss = 368602.03 (1.699 sec)\n",
            "train - step 1833: loss = 424023.84 (1.710 sec)\n",
            "train - step 1834: loss = 512941.62 (1.693 sec)\n",
            "train - step 1835: loss = 386126.22 (1.697 sec)\n",
            "train - step 1836: loss = 480646.78 (1.709 sec)\n",
            "train - step 1837: loss = 435932.56 (1.689 sec)\n",
            "train - step 1838: loss = 537310.19 (1.697 sec)\n",
            "train - step 1839: loss = 329941.47 (1.681 sec)\n",
            "train - step 1840: loss = 348066.59 (1.680 sec)\n",
            "train - step 1841: loss = 508085.09 (1.693 sec)\n",
            "train - step 1842: loss = 493667.72 (1.698 sec)\n",
            "train - step 1843: loss = 410790.88 (1.688 sec)\n",
            "train - step 1844: loss = 419084.41 (1.680 sec)\n",
            "train - step 1845: loss = 403664.44 (1.683 sec)\n",
            "train - step 1846: loss = 415242.84 (1.677 sec)\n",
            "train - step 1847: loss = 366312.19 (1.723 sec)\n",
            "train - step 1848: loss = 462464.94 (1.695 sec)\n",
            "train - step 1849: loss = 487809.03 (1.675 sec)\n",
            "train - step 1850: loss = 512980.78 (1.684 sec)\n",
            "train - step 1851: loss = 390804.38 (2.487 sec)\n",
            "train - step 1852: loss = 344141.94 (1.697 sec)\n",
            "train - step 1853: loss = 485142.38 (1.729 sec)\n",
            "train - step 1854: loss = 289120.16 (1.677 sec)\n",
            "train - step 1855: loss = 235388.25 (1.675 sec)\n",
            "train - step 1856: loss = 358656.31 (1.678 sec)\n",
            "train - step 1857: loss = 393889.22 (1.674 sec)\n",
            "train - step 1858: loss = 375060.66 (1.700 sec)\n",
            "train - step 1859: loss = 349452.09 (1.695 sec)\n",
            "train - step 1860: loss = 279833.50 (1.678 sec)\n",
            "train - step 1861: loss = 475606.72 (1.695 sec)\n",
            "train - step 1862: loss = 404008.12 (1.689 sec)\n",
            "train - step 1863: loss = 386496.41 (1.692 sec)\n",
            "train - step 1864: loss = 377693.78 (1.731 sec)\n",
            "train - step 1865: loss = 514816.31 (1.690 sec)\n",
            "train - step 1866: loss = 536709.12 (1.662 sec)\n",
            "train - step 1867: loss = 372952.81 (1.688 sec)\n",
            "train - step 1868: loss = 408974.88 (1.693 sec)\n",
            "train - step 1869: loss = 405372.19 (1.693 sec)\n",
            "train - step 1870: loss = 376239.44 (1.687 sec)\n",
            "train - step 1871: loss = 497496.00 (1.702 sec)\n",
            "train - step 1872: loss = 350173.66 (1.714 sec)\n",
            "train - step 1873: loss = 397276.62 (1.701 sec)\n",
            "train - step 1874: loss = 574101.25 (1.693 sec)\n",
            "train - step 1875: loss = 366659.53 (1.720 sec)\n",
            "train - step 1876: loss = 401376.62 (1.703 sec)\n",
            "train - step 1877: loss = 520541.00 (1.711 sec)\n",
            "train - step 1878: loss = 383367.34 (1.712 sec)\n",
            "train - step 1879: loss = 337742.03 (1.711 sec)\n",
            "train - step 1880: loss = 306220.09 (1.704 sec)\n",
            "train - step 1881: loss = 348329.66 (1.692 sec)\n",
            "train - step 1882: loss = 391825.94 (1.690 sec)\n",
            "train - step 1883: loss = 418063.69 (1.685 sec)\n",
            "train - step 1884: loss = 452743.50 (1.686 sec)\n",
            "train - step 1885: loss = 318101.22 (1.659 sec)\n",
            "train - step 1886: loss = 317234.38 (1.683 sec)\n",
            "train - step 1887: loss = 477227.19 (2.550 sec)\n",
            "train - step 1888: loss = 441119.97 (1.694 sec)\n",
            "train - step 1889: loss = 345635.69 (1.688 sec)\n",
            "train - step 1890: loss = 434924.84 (1.686 sec)\n",
            "train - step 1891: loss = 389253.91 (1.679 sec)\n",
            "train - step 1892: loss = 426665.22 (1.683 sec)\n",
            "train - step 1893: loss = 390328.34 (1.696 sec)\n",
            "train - step 1894: loss = 477094.44 (1.693 sec)\n",
            "train - step 1895: loss = 372473.12 (1.683 sec)\n",
            "train - step 1896: loss = 432543.59 (1.669 sec)\n",
            "train - step 1897: loss = 442702.50 (1.661 sec)\n",
            "train - step 1898: loss = 421533.19 (1.705 sec)\n",
            "train - step 1899: loss = 565001.69 (1.694 sec)\n",
            "train - step 1900: loss = 465782.19 (1.690 sec)\n",
            "train - step 1901: loss = 390039.41 (1.703 sec)\n",
            "train - step 1902: loss = 499791.53 (1.678 sec)\n",
            "train - step 1903: loss = 538217.31 (1.721 sec)\n",
            "train - step 1904: loss = 407833.12 (1.705 sec)\n",
            "train - step 1905: loss = 316606.91 (1.759 sec)\n",
            "train - step 1906: loss = 517816.62 (1.702 sec)\n",
            "train - step 1907: loss = 503236.56 (1.662 sec)\n",
            "train - step 1908: loss = 427154.28 (1.645 sec)\n",
            "train - step 1909: loss = 341950.97 (1.659 sec)\n",
            "train - step 1910: loss = 476654.81 (1.653 sec)\n",
            "train - step 1911: loss = 339184.38 (1.646 sec)\n",
            "train - step 1912: loss = 412068.69 (1.675 sec)\n",
            "train - step 1913: loss = 481584.84 (1.683 sec)\n",
            "train - step 1914: loss = 356304.94 (1.690 sec)\n",
            "train - step 1915: loss = 441454.47 (1.704 sec)\n",
            "train - step 1916: loss = 410734.53 (1.710 sec)\n",
            "train - step 1917: loss = 405733.69 (1.692 sec)\n",
            "train - step 1918: loss = 492458.06 (1.721 sec)\n",
            "train - step 1919: loss = 440190.41 (1.673 sec)\n",
            "train - step 1920: loss = 376135.28 (1.688 sec)\n",
            "train - step 1921: loss = 338014.31 (1.644 sec)\n",
            "train - step 1922: loss = 322254.69 (1.645 sec)\n",
            "train - step 1923: loss = 386081.28 (2.574 sec)\n",
            "train - step 1924: loss = 446248.69 (1.670 sec)\n",
            "train - step 1925: loss = 411664.69 (1.710 sec)\n",
            "train - step 1926: loss = 497127.94 (1.691 sec)\n",
            "train - step 1927: loss = 348735.03 (1.678 sec)\n",
            "train - step 1928: loss = 304397.69 (1.706 sec)\n",
            "train - step 1929: loss = 406616.91 (1.678 sec)\n",
            "train - step 1930: loss = 601366.81 (1.733 sec)\n",
            "train - step 1931: loss = 351475.50 (1.675 sec)\n",
            "train - step 1932: loss = 407939.00 (1.700 sec)\n",
            "train - step 1933: loss = 536841.00 (1.686 sec)\n",
            "train - step 1934: loss = 311265.72 (1.701 sec)\n",
            "train - step 1935: loss = 435509.28 (1.661 sec)\n",
            "train - step 1936: loss = 584623.19 (1.703 sec)\n",
            "train - step 1937: loss = 468177.56 (1.681 sec)\n",
            "train - step 1938: loss = 464260.88 (1.671 sec)\n",
            "train - step 1939: loss = 417842.78 (1.723 sec)\n",
            "train - step 1940: loss = 451602.28 (1.668 sec)\n",
            "train - step 1941: loss = 477611.53 (1.701 sec)\n",
            "train - step 1942: loss = 291878.94 (1.704 sec)\n",
            "train - step 1943: loss = 391732.97 (1.674 sec)\n",
            "train - step 1944: loss = 540537.94 (1.681 sec)\n",
            "train - step 1945: loss = 369555.59 (1.701 sec)\n",
            "train - step 1946: loss = 459562.88 (1.674 sec)\n",
            "train - step 1947: loss = 518625.88 (1.696 sec)\n",
            "train - step 1948: loss = 377262.59 (1.682 sec)\n",
            "train - step 1949: loss = 490150.59 (1.680 sec)\n",
            "train - step 1950: loss = 571367.38 (1.698 sec)\n",
            "train - step 1951: loss = 303862.31 (1.713 sec)\n",
            "train - step 1952: loss = 373653.59 (1.682 sec)\n",
            "train - step 1953: loss = 336101.00 (1.689 sec)\n",
            "train - step 1954: loss = 389173.28 (1.673 sec)\n",
            "train - step 1955: loss = 527378.12 (1.709 sec)\n",
            "train - step 1956: loss = 519145.84 (1.696 sec)\n",
            "train - step 1957: loss = 381344.41 (1.675 sec)\n",
            "train - step 1958: loss = 432466.31 (1.708 sec)\n",
            "train - step 1959: loss = 422017.78 (2.612 sec)\n",
            "train - step 1960: loss = 484054.81 (1.674 sec)\n",
            "train - step 1961: loss = 315332.41 (1.699 sec)\n",
            "train - step 1962: loss = 476753.16 (1.732 sec)\n",
            "train - step 1963: loss = 537618.31 (1.694 sec)\n",
            "train - step 1964: loss = 507142.31 (1.691 sec)\n",
            "train - step 1965: loss = 425041.91 (1.695 sec)\n",
            "train - step 1966: loss = 423842.12 (1.708 sec)\n",
            "train - step 1967: loss = 432734.72 (1.696 sec)\n",
            "train - step 1968: loss = 481217.28 (1.665 sec)\n",
            "train - step 1969: loss = 438409.38 (1.686 sec)\n",
            "train - step 1970: loss = 393946.94 (1.700 sec)\n",
            "train - step 1971: loss = 334577.41 (1.645 sec)\n",
            "train - step 1972: loss = 704131.25 (1.672 sec)\n",
            "train - step 1973: loss = 588488.31 (1.689 sec)\n",
            "train - step 1974: loss = 389782.09 (1.688 sec)\n",
            "train - step 1975: loss = 322529.03 (1.694 sec)\n",
            "train - step 1976: loss = 421338.94 (1.687 sec)\n",
            "train - step 1977: loss = 514485.47 (1.672 sec)\n",
            "train - step 1978: loss = 280545.34 (1.729 sec)\n",
            "train - step 1979: loss = 652900.56 (1.681 sec)\n",
            "train - step 1980: loss = 543760.25 (1.708 sec)\n",
            "train - step 1981: loss = 321313.88 (1.686 sec)\n",
            "train - step 1982: loss = 396502.97 (1.689 sec)\n",
            "train - step 1983: loss = 332555.56 (1.680 sec)\n",
            "train - step 1984: loss = 579752.12 (1.694 sec)\n",
            "train - step 1985: loss = 456292.41 (1.691 sec)\n",
            "train - step 1986: loss = 473985.59 (1.679 sec)\n",
            "train - step 1987: loss = 454697.41 (1.698 sec)\n",
            "train - step 1988: loss = 474456.56 (1.670 sec)\n",
            "train - step 1989: loss = 374428.38 (1.684 sec)\n",
            "train - step 1990: loss = 464628.38 (1.709 sec)\n",
            "train - step 1991: loss = 517005.72 (1.682 sec)\n",
            "train - step 1992: loss = 438008.00 (1.693 sec)\n",
            "train - step 1993: loss = 563513.38 (1.697 sec)\n",
            "train - step 1994: loss = 367813.16 (1.696 sec)\n",
            "train - step 1995: loss = 548400.50 (2.532 sec)\n",
            "train - step 1996: loss = 505516.84 (1.700 sec)\n",
            "train - step 1997: loss = 497420.94 (1.689 sec)\n",
            "train - step 1998: loss = 575429.38 (1.710 sec)\n",
            "train - step 1999: loss = 420511.72 (1.696 sec)\n",
            "train - step 2000: loss = 431735.34 (1.715 sec)\n",
            "train - step 2001: loss = 468123.91 (1.678 sec)\n",
            "train - step 2002: loss = 397196.81 (1.717 sec)\n",
            "train - step 2003: loss = 292181.56 (1.795 sec)\n",
            "train - step 2004: loss = 488407.84 (1.719 sec)\n",
            "train - step 2005: loss = 369864.88 (1.701 sec)\n",
            "train - step 2006: loss = 457872.44 (1.692 sec)\n",
            "train - step 2007: loss = 430825.06 (1.732 sec)\n",
            "train - step 2008: loss = 348855.53 (1.676 sec)\n",
            "train - step 2009: loss = 400542.69 (1.722 sec)\n",
            "train - step 2010: loss = 499766.47 (1.712 sec)\n",
            "train - step 2011: loss = 304445.84 (1.704 sec)\n",
            "train - step 2012: loss = 283732.56 (1.860 sec)\n",
            "train - step 2013: loss = 542122.75 (1.716 sec)\n",
            "train - step 2014: loss = 393318.50 (1.684 sec)\n",
            "train - step 2015: loss = 475782.00 (1.794 sec)\n",
            "train - step 2016: loss = 302575.38 (1.659 sec)\n",
            "train - step 2017: loss = 511326.03 (1.685 sec)\n",
            "train - step 2018: loss = 544843.19 (1.682 sec)\n",
            "train - step 2019: loss = 374657.88 (1.706 sec)\n",
            "train - step 2020: loss = 500453.12 (1.739 sec)\n",
            "train - step 2021: loss = 514068.59 (1.673 sec)\n",
            "train - step 2022: loss = 350846.78 (1.683 sec)\n",
            "train - step 2023: loss = 455513.34 (1.678 sec)\n",
            "train - step 2024: loss = 425177.31 (1.720 sec)\n",
            "train - step 2025: loss = 488199.41 (1.663 sec)\n",
            "train - step 2026: loss = 280335.94 (1.679 sec)\n",
            "train - step 2027: loss = 462816.81 (1.672 sec)\n",
            "train - step 2028: loss = 462592.34 (1.696 sec)\n",
            "train - step 2029: loss = 421730.09 (1.692 sec)\n",
            "train - step 2030: loss = 326795.84 (2.696 sec)\n",
            "train - step 2031: loss = 408453.19 (1.703 sec)\n",
            "train - step 2032: loss = 264852.72 (1.673 sec)\n",
            "train - step 2033: loss = 485461.53 (1.721 sec)\n",
            "train - step 2034: loss = 378948.59 (1.678 sec)\n",
            "train - step 2035: loss = 349414.06 (1.702 sec)\n",
            "train - step 2036: loss = 499606.00 (1.657 sec)\n",
            "train - step 2037: loss = 520920.59 (1.680 sec)\n",
            "train - step 2038: loss = 407040.12 (1.685 sec)\n",
            "train - step 2039: loss = 371710.31 (1.668 sec)\n",
            "train - step 2040: loss = 533108.06 (1.721 sec)\n",
            "train - step 2041: loss = 593208.62 (1.701 sec)\n",
            "train - step 2042: loss = 323003.19 (1.669 sec)\n",
            "train - step 2043: loss = 534391.88 (1.730 sec)\n",
            "train - step 2044: loss = 416207.31 (1.695 sec)\n",
            "train - step 2045: loss = 430219.12 (1.694 sec)\n",
            "train - step 2046: loss = 471634.06 (1.704 sec)\n",
            "train - step 2047: loss = 441501.47 (1.740 sec)\n",
            "train - step 2048: loss = 444250.81 (1.681 sec)\n",
            "train - step 2049: loss = 469522.81 (1.704 sec)\n",
            "train - step 2050: loss = 505298.81 (1.684 sec)\n",
            "train - step 2051: loss = 580728.62 (1.689 sec)\n",
            "train - step 2052: loss = 444044.22 (1.691 sec)\n",
            "train - step 2053: loss = 402833.84 (1.672 sec)\n",
            "train - step 2054: loss = 534298.81 (1.658 sec)\n",
            "train - step 2055: loss = 372803.22 (1.681 sec)\n",
            "train - step 2056: loss = 494060.88 (1.692 sec)\n",
            "train - step 2057: loss = 399783.94 (1.710 sec)\n",
            "train - step 2058: loss = 442563.31 (1.674 sec)\n",
            "train - step 2059: loss = 461655.91 (1.645 sec)\n",
            "train - step 2060: loss = 527262.12 (1.676 sec)\n",
            "train - step 2061: loss = 422399.81 (1.715 sec)\n",
            "train - step 2062: loss = 409301.38 (1.714 sec)\n",
            "train - step 2063: loss = 445986.12 (1.686 sec)\n",
            "train - step 2064: loss = 555003.88 (1.680 sec)\n",
            "train - step 2065: loss = 533499.94 (1.712 sec)\n",
            "train - step 2066: loss = 433735.00 (2.539 sec)\n",
            "train - step 2067: loss = 365850.22 (1.656 sec)\n",
            "train - step 2068: loss = 557355.81 (1.691 sec)\n",
            "train - step 2069: loss = 360907.97 (1.658 sec)\n",
            "train - step 2070: loss = 549212.12 (1.677 sec)\n",
            "train - step 2071: loss = 524927.75 (1.692 sec)\n",
            "train - step 2072: loss = 476624.62 (1.710 sec)\n",
            "train - step 2073: loss = 360033.12 (1.675 sec)\n",
            "train - step 2074: loss = 378691.97 (1.680 sec)\n",
            "train - step 2075: loss = 455088.59 (1.673 sec)\n",
            "train - step 2076: loss = 465584.91 (1.671 sec)\n",
            "train - step 2077: loss = 367256.81 (1.713 sec)\n",
            "train - step 2078: loss = 646335.19 (1.659 sec)\n",
            "train - step 2079: loss = 412496.84 (1.710 sec)\n",
            "train - step 2080: loss = 324538.19 (1.666 sec)\n",
            "train - step 2081: loss = 353892.03 (1.701 sec)\n",
            "train - step 2082: loss = 397495.53 (1.676 sec)\n",
            "train - step 2083: loss = 507104.84 (1.685 sec)\n",
            "train - step 2084: loss = 659059.31 (1.668 sec)\n",
            "train - step 2085: loss = 400443.50 (1.683 sec)\n",
            "train - step 2086: loss = 397381.34 (1.678 sec)\n",
            "train - step 2087: loss = 574458.75 (1.658 sec)\n",
            "train - step 2088: loss = 339082.44 (1.640 sec)\n",
            "train - step 2089: loss = 293037.81 (1.635 sec)\n",
            "train - step 2090: loss = 338705.12 (1.646 sec)\n",
            "train - step 2091: loss = 499643.88 (1.689 sec)\n",
            "train - step 2092: loss = 396218.03 (1.662 sec)\n",
            "train - step 2093: loss = 439068.47 (1.635 sec)\n",
            "train - step 2094: loss = 367628.72 (1.683 sec)\n",
            "train - step 2095: loss = 428458.59 (1.679 sec)\n",
            "train - step 2096: loss = 410465.66 (1.661 sec)\n",
            "train - step 2097: loss = 646174.31 (1.693 sec)\n",
            "train - step 2098: loss = 437038.69 (1.649 sec)\n",
            "train - step 2099: loss = 386168.72 (1.656 sec)\n",
            "train - step 2100: loss = 228656.56 (1.635 sec)\n",
            "train - step 2101: loss = 421055.62 (1.670 sec)\n",
            "train - step 2102: loss = 377935.94 (2.874 sec)\n",
            "train - step 2103: loss = 523008.38 (1.696 sec)\n",
            "train - step 2104: loss = 386125.19 (1.696 sec)\n",
            "train - step 2105: loss = 347859.94 (1.666 sec)\n",
            "train - step 2106: loss = 380710.47 (1.705 sec)\n",
            "train - step 2107: loss = 309260.00 (1.678 sec)\n",
            "train - step 2108: loss = 323379.97 (1.685 sec)\n",
            "train - step 2109: loss = 533313.88 (1.657 sec)\n",
            "train - step 2110: loss = 457819.94 (1.717 sec)\n",
            "train - step 2111: loss = 464994.22 (1.685 sec)\n",
            "train - step 2112: loss = 339404.31 (1.716 sec)\n",
            "train - step 2113: loss = 407266.50 (1.666 sec)\n",
            "train - step 2114: loss = 469210.66 (1.691 sec)\n",
            "train - step 2115: loss = 578703.69 (1.669 sec)\n",
            "train - step 2116: loss = 467574.47 (1.663 sec)\n",
            "train - step 2117: loss = 454333.28 (1.699 sec)\n",
            "train - step 2118: loss = 378339.91 (1.670 sec)\n",
            "train - step 2119: loss = 396151.28 (1.725 sec)\n",
            "train - step 2120: loss = 478851.28 (1.673 sec)\n",
            "train - step 2121: loss = 377615.03 (1.675 sec)\n",
            "train - step 2122: loss = 490085.22 (1.654 sec)\n",
            "train - step 2123: loss = 266938.56 (1.687 sec)\n",
            "train - step 2124: loss = 455678.97 (1.695 sec)\n",
            "train - step 2125: loss = 383721.16 (1.688 sec)\n",
            "train - step 2126: loss = 339117.78 (1.686 sec)\n",
            "train - step 2127: loss = 380883.94 (1.688 sec)\n",
            "train - step 2128: loss = 416289.31 (1.696 sec)\n",
            "train - step 2129: loss = 544306.62 (1.716 sec)\n",
            "train - step 2130: loss = 526907.44 (1.674 sec)\n",
            "train - step 2131: loss = 423210.34 (1.735 sec)\n",
            "train - step 2132: loss = 475976.50 (1.700 sec)\n",
            "train - step 2133: loss = 564249.94 (1.688 sec)\n",
            "train - step 2134: loss = 427499.59 (1.689 sec)\n",
            "train - step 2135: loss = 363188.03 (1.713 sec)\n",
            "train - step 2136: loss = 380417.22 (1.693 sec)\n",
            "train - step 2137: loss = 512724.62 (2.598 sec)\n",
            "train - step 2138: loss = 558859.06 (1.713 sec)\n",
            "train - step 2139: loss = 452439.69 (1.697 sec)\n",
            "train - step 2140: loss = 600668.31 (1.699 sec)\n",
            "train - step 2141: loss = 479856.97 (1.705 sec)\n",
            "train - step 2142: loss = 389256.06 (1.690 sec)\n",
            "train - step 2143: loss = 482225.91 (1.704 sec)\n",
            "train - step 2144: loss = 436477.06 (1.700 sec)\n",
            "train - step 2145: loss = 441621.72 (1.702 sec)\n",
            "train - step 2146: loss = 484286.16 (1.681 sec)\n",
            "train - step 2147: loss = 365601.41 (1.696 sec)\n",
            "train - step 2148: loss = 566994.81 (1.694 sec)\n",
            "train - step 2149: loss = 374005.16 (1.705 sec)\n",
            "train - step 2150: loss = 476708.41 (1.703 sec)\n",
            "train - step 2151: loss = 378692.53 (1.687 sec)\n",
            "train - step 2152: loss = 475499.38 (1.662 sec)\n",
            "train - step 2153: loss = 330372.34 (1.689 sec)\n",
            "train - step 2154: loss = 437354.06 (1.671 sec)\n",
            "train - step 2155: loss = 504785.88 (1.683 sec)\n",
            "train - step 2156: loss = 516152.12 (1.704 sec)\n",
            "train - step 2157: loss = 467249.00 (1.675 sec)\n",
            "train - step 2158: loss = 453320.78 (1.670 sec)\n",
            "train - step 2159: loss = 466209.44 (1.666 sec)\n",
            "train - step 2160: loss = 340515.19 (1.677 sec)\n",
            "train - step 2161: loss = 437163.94 (1.684 sec)\n",
            "train - step 2162: loss = 550983.69 (1.699 sec)\n",
            "train - step 2163: loss = 439148.66 (1.690 sec)\n",
            "train - step 2164: loss = 346069.28 (1.709 sec)\n",
            "train - step 2165: loss = 469833.53 (1.677 sec)\n",
            "train - step 2166: loss = 325819.94 (1.697 sec)\n",
            "train - step 2167: loss = 366682.88 (1.668 sec)\n",
            "train - step 2168: loss = 332661.16 (1.685 sec)\n",
            "train - step 2169: loss = 295306.78 (1.673 sec)\n",
            "train - step 2170: loss = 465759.06 (1.714 sec)\n",
            "train - step 2171: loss = 384253.06 (1.671 sec)\n",
            "train - step 2172: loss = 495323.69 (1.675 sec)\n",
            "train - step 2173: loss = 455086.81 (2.567 sec)\n",
            "train - step 2174: loss = 298312.56 (1.688 sec)\n",
            "train - step 2175: loss = 333045.22 (1.682 sec)\n",
            "train - step 2176: loss = 444892.69 (1.691 sec)\n",
            "train - step 2177: loss = 457012.56 (1.694 sec)\n",
            "train - step 2178: loss = 476539.34 (1.682 sec)\n",
            "train - step 2179: loss = 436205.78 (1.668 sec)\n",
            "train - step 2180: loss = 462047.91 (1.676 sec)\n",
            "train - step 2181: loss = 339687.69 (1.693 sec)\n",
            "train - step 2182: loss = 342166.81 (1.675 sec)\n",
            "train - step 2183: loss = 393650.16 (1.658 sec)\n",
            "train - step 2184: loss = 440507.38 (1.702 sec)\n",
            "train - step 2185: loss = 529564.06 (1.710 sec)\n",
            "train - step 2186: loss = 354228.09 (1.701 sec)\n",
            "train - step 2187: loss = 397004.91 (1.655 sec)\n",
            "train - step 2188: loss = 457715.91 (1.673 sec)\n",
            "train - step 2189: loss = 493868.16 (1.710 sec)\n",
            "train - step 2190: loss = 381813.19 (1.669 sec)\n",
            "train - step 2191: loss = 607727.19 (1.693 sec)\n",
            "train - step 2192: loss = 455121.34 (1.677 sec)\n",
            "train - step 2193: loss = 563131.62 (1.681 sec)\n",
            "train - step 2194: loss = 358189.78 (1.675 sec)\n",
            "train - step 2195: loss = 455943.44 (1.706 sec)\n",
            "train - step 2196: loss = 444294.06 (1.648 sec)\n",
            "train - step 2197: loss = 396055.66 (1.700 sec)\n",
            "train - step 2198: loss = 326711.88 (1.690 sec)\n",
            "train - step 2199: loss = 555615.94 (1.677 sec)\n",
            "train - step 2200: loss = 420288.41 (1.745 sec)\n",
            "train - step 2201: loss = 536016.75 (1.667 sec)\n",
            "train - step 2202: loss = 421184.69 (1.680 sec)\n",
            "train - step 2203: loss = 712483.25 (1.688 sec)\n",
            "train - step 2204: loss = 488171.50 (1.682 sec)\n",
            "train - step 2205: loss = 397542.00 (1.680 sec)\n",
            "train - step 2206: loss = 623940.94 (1.694 sec)\n",
            "train - step 2207: loss = 465636.81 (1.678 sec)\n",
            "train - step 2208: loss = 598855.44 (1.683 sec)\n",
            "train - step 2209: loss = 461833.34 (2.537 sec)\n",
            "train - step 2210: loss = 381986.56 (1.671 sec)\n",
            "train - step 2211: loss = 372025.03 (1.689 sec)\n",
            "train - step 2212: loss = 424556.44 (1.664 sec)\n",
            "train - step 2213: loss = 426455.50 (1.681 sec)\n",
            "train - step 2214: loss = 407641.28 (1.676 sec)\n",
            "train - step 2215: loss = 390095.66 (1.674 sec)\n",
            "train - step 2216: loss = 424400.16 (1.699 sec)\n",
            "train - step 2217: loss = 384757.66 (1.682 sec)\n",
            "train - step 2218: loss = 473349.72 (1.670 sec)\n",
            "train - step 2219: loss = 436708.84 (1.685 sec)\n",
            "train - step 2220: loss = 523618.28 (1.683 sec)\n",
            "train - step 2221: loss = 432729.94 (1.673 sec)\n",
            "train - step 2222: loss = 389831.03 (1.681 sec)\n",
            "train - step 2223: loss = 590938.62 (1.689 sec)\n",
            "train - step 2224: loss = 526103.75 (1.665 sec)\n",
            "train - step 2225: loss = 394605.59 (1.678 sec)\n",
            "train - step 2226: loss = 490619.81 (1.685 sec)\n",
            "train - step 2227: loss = 518533.41 (1.689 sec)\n",
            "train - step 2228: loss = 517384.03 (1.683 sec)\n",
            "train - step 2229: loss = 356336.06 (1.669 sec)\n",
            "train - step 2230: loss = 451469.88 (1.668 sec)\n",
            "train - step 2231: loss = 417112.88 (1.676 sec)\n",
            "train - step 2232: loss = 319901.38 (1.641 sec)\n",
            "train - step 2233: loss = 526452.81 (1.673 sec)\n",
            "train - step 2234: loss = 513408.53 (1.676 sec)\n",
            "train - step 2235: loss = 467630.12 (1.690 sec)\n",
            "train - step 2236: loss = 420428.31 (1.695 sec)\n",
            "train - step 2237: loss = 363534.91 (1.694 sec)\n",
            "train - step 2238: loss = 456911.28 (1.669 sec)\n",
            "train - step 2239: loss = 444832.84 (1.684 sec)\n",
            "train - step 2240: loss = 364345.72 (1.671 sec)\n",
            "train - step 2241: loss = 561240.88 (1.690 sec)\n",
            "train - step 2242: loss = 684758.69 (1.680 sec)\n",
            "train - step 2243: loss = 360697.06 (1.676 sec)\n",
            "train - step 2244: loss = 469996.03 (1.689 sec)\n",
            "train - step 2245: loss = 476187.06 (2.482 sec)\n",
            "train - step 2246: loss = 480107.94 (1.697 sec)\n",
            "train - step 2247: loss = 334025.94 (1.672 sec)\n",
            "train - step 2248: loss = 549768.69 (1.671 sec)\n",
            "train - step 2249: loss = 607272.38 (1.701 sec)\n",
            "train - step 2250: loss = 372980.09 (1.656 sec)\n",
            "train - step 2251: loss = 450441.00 (2.380 sec)\n",
            "train - step 2252: loss = 596012.75 (1.691 sec)\n",
            "train - step 2253: loss = 413387.94 (1.669 sec)\n",
            "train - step 2254: loss = 474623.56 (1.698 sec)\n",
            "train - step 2255: loss = 486091.03 (1.669 sec)\n",
            "train - step 2256: loss = 419994.72 (1.703 sec)\n",
            "train - step 2257: loss = 368096.69 (1.664 sec)\n",
            "train - step 2258: loss = 520681.16 (1.676 sec)\n",
            "train - step 2259: loss = 395851.38 (1.674 sec)\n",
            "train - step 2260: loss = 328539.28 (1.676 sec)\n",
            "train - step 2261: loss = 371036.94 (1.720 sec)\n",
            "train - step 2262: loss = 351239.16 (1.702 sec)\n",
            "train - step 2263: loss = 370700.81 (1.728 sec)\n",
            "train - step 2264: loss = 385150.50 (1.675 sec)\n",
            "train - step 2265: loss = 515713.97 (1.722 sec)\n",
            "train - step 2266: loss = 408008.22 (1.690 sec)\n",
            "train - step 2267: loss = 435858.94 (1.708 sec)\n",
            "train - step 2268: loss = 396031.88 (1.680 sec)\n",
            "train - step 2269: loss = 304647.28 (1.668 sec)\n",
            "train - step 2270: loss = 427526.31 (1.640 sec)\n",
            "train - step 2271: loss = 387348.88 (1.649 sec)\n",
            "train - step 2272: loss = 391080.59 (1.660 sec)\n",
            "train - step 2273: loss = 467835.97 (1.641 sec)\n",
            "train - step 2274: loss = 667629.44 (1.651 sec)\n",
            "train - step 2275: loss = 387354.31 (1.692 sec)\n",
            "train - step 2276: loss = 544455.31 (1.698 sec)\n",
            "train - step 2277: loss = 513400.66 (1.639 sec)\n",
            "train - step 2278: loss = 433528.12 (1.672 sec)\n",
            "train - step 2279: loss = 438016.03 (1.639 sec)\n",
            "train - step 2280: loss = 620122.81 (1.680 sec)\n",
            "train - step 2281: loss = 498745.81 (2.518 sec)\n",
            "train - step 2282: loss = 367738.88 (1.695 sec)\n",
            "train - step 2283: loss = 379456.34 (1.689 sec)\n",
            "train - step 2284: loss = 415852.84 (1.701 sec)\n",
            "train - step 2285: loss = 296516.88 (1.671 sec)\n",
            "train - step 2286: loss = 413930.72 (1.706 sec)\n",
            "train - step 2287: loss = 441622.47 (1.679 sec)\n",
            "train - step 2288: loss = 342130.47 (1.708 sec)\n",
            "train - step 2289: loss = 479443.97 (1.680 sec)\n",
            "train - step 2290: loss = 396733.34 (1.676 sec)\n",
            "train - step 2291: loss = 379732.69 (1.682 sec)\n",
            "train - step 2292: loss = 486342.88 (1.688 sec)\n",
            "train - step 2293: loss = 394526.38 (1.688 sec)\n",
            "train - step 2294: loss = 413049.28 (1.711 sec)\n",
            "train - step 2295: loss = 559133.75 (1.725 sec)\n",
            "train - step 2296: loss = 277280.47 (1.726 sec)\n",
            "train - step 2297: loss = 374936.28 (1.723 sec)\n",
            "train - step 2298: loss = 550663.81 (1.732 sec)\n",
            "train - step 2299: loss = 528051.31 (1.725 sec)\n",
            "train - step 2300: loss = 534316.25 (1.703 sec)\n",
            "train - step 2301: loss = 399704.72 (1.710 sec)\n",
            "train - step 2302: loss = 323892.84 (1.722 sec)\n",
            "train - step 2303: loss = 514353.41 (1.699 sec)\n",
            "train - step 2304: loss = 592066.44 (1.691 sec)\n",
            "train - step 2305: loss = 383357.00 (1.699 sec)\n",
            "train - step 2306: loss = 486987.12 (1.681 sec)\n",
            "train - step 2307: loss = 458266.66 (1.691 sec)\n",
            "train - step 2308: loss = 378105.94 (1.694 sec)\n",
            "train - step 2309: loss = 362066.66 (1.738 sec)\n",
            "train - step 2310: loss = 563095.38 (1.685 sec)\n",
            "train - step 2311: loss = 280923.47 (1.748 sec)\n",
            "train - step 2312: loss = 428066.19 (2.120 sec)\n",
            "train - step 2313: loss = 462177.69 (1.714 sec)\n",
            "train - step 2314: loss = 374793.53 (1.692 sec)\n",
            "train - step 2315: loss = 459782.91 (1.840 sec)\n",
            "train - step 2316: loss = 336446.09 (2.564 sec)\n",
            "train - step 2317: loss = 337279.31 (1.704 sec)\n",
            "train - step 2318: loss = 421635.84 (1.670 sec)\n",
            "train - step 2319: loss = 294783.38 (1.658 sec)\n",
            "train - step 2320: loss = 427913.47 (1.674 sec)\n",
            "train - step 2321: loss = 533021.69 (1.689 sec)\n",
            "train - step 2322: loss = 435278.31 (1.689 sec)\n",
            "train - step 2323: loss = 564064.19 (1.687 sec)\n",
            "train - step 2324: loss = 254545.75 (1.679 sec)\n",
            "train - step 2325: loss = 625538.38 (1.728 sec)\n",
            "train - step 2326: loss = 408191.72 (1.683 sec)\n",
            "train - step 2327: loss = 281402.31 (1.703 sec)\n",
            "train - step 2328: loss = 314103.53 (1.657 sec)\n",
            "train - step 2329: loss = 464777.00 (1.686 sec)\n",
            "train - step 2330: loss = 543856.75 (1.702 sec)\n",
            "train - step 2331: loss = 291965.78 (1.692 sec)\n",
            "train - step 2332: loss = 413358.53 (1.695 sec)\n",
            "train - step 2333: loss = 374376.41 (1.695 sec)\n",
            "train - step 2334: loss = 576728.00 (1.715 sec)\n",
            "train - step 2335: loss = 567674.00 (1.703 sec)\n",
            "train - step 2336: loss = 360168.38 (1.695 sec)\n",
            "train - step 2337: loss = 345252.00 (1.671 sec)\n",
            "train - step 2338: loss = 544790.12 (1.701 sec)\n",
            "train - step 2339: loss = 477422.69 (1.684 sec)\n",
            "train - step 2340: loss = 546865.62 (1.709 sec)\n",
            "train - step 2341: loss = 583846.62 (1.700 sec)\n",
            "train - step 2342: loss = 276943.88 (1.673 sec)\n",
            "train - step 2343: loss = 478820.19 (1.732 sec)\n",
            "train - step 2344: loss = 541900.25 (1.671 sec)\n",
            "train - step 2345: loss = 764997.56 (1.679 sec)\n",
            "train - step 2346: loss = 431544.19 (1.698 sec)\n",
            "train - step 2347: loss = 368823.28 (1.710 sec)\n",
            "train - step 2348: loss = 513324.91 (1.682 sec)\n",
            "train - step 2349: loss = 537747.81 (1.693 sec)\n",
            "train - step 2350: loss = 476244.41 (1.670 sec)\n",
            "train - step 2351: loss = 561893.62 (1.681 sec)\n",
            "train - step 2352: loss = 537857.31 (2.684 sec)\n",
            "train - step 2353: loss = 358666.47 (1.684 sec)\n",
            "train - step 2354: loss = 326285.00 (1.720 sec)\n",
            "train - step 2355: loss = 423814.56 (1.717 sec)\n",
            "train - step 2356: loss = 479090.72 (1.710 sec)\n",
            "train - step 2357: loss = 506850.56 (1.693 sec)\n",
            "train - step 2358: loss = 436568.06 (1.703 sec)\n",
            "train - step 2359: loss = 627121.56 (1.694 sec)\n",
            "train - step 2360: loss = 439180.78 (1.711 sec)\n",
            "train - step 2361: loss = 428518.34 (1.669 sec)\n",
            "train - step 2362: loss = 602528.38 (1.694 sec)\n",
            "train - step 2363: loss = 370577.62 (1.685 sec)\n",
            "train - step 2364: loss = 417147.97 (1.680 sec)\n",
            "train - step 2365: loss = 314058.28 (1.694 sec)\n",
            "train - step 2366: loss = 373073.59 (1.677 sec)\n",
            "train - step 2367: loss = 642905.94 (1.690 sec)\n",
            "train - step 2368: loss = 307833.19 (1.670 sec)\n",
            "train - step 2369: loss = 307597.09 (1.686 sec)\n",
            "train - step 2370: loss = 458176.16 (1.714 sec)\n",
            "train - step 2371: loss = 320177.47 (1.706 sec)\n",
            "train - step 2372: loss = 417372.59 (1.701 sec)\n",
            "train - step 2373: loss = 456670.94 (1.701 sec)\n",
            "train - step 2374: loss = 287848.06 (1.683 sec)\n",
            "train - step 2375: loss = 401926.41 (1.698 sec)\n",
            "train - step 2376: loss = 350737.34 (1.696 sec)\n",
            "train - step 2377: loss = 479099.84 (1.688 sec)\n",
            "train - step 2378: loss = 377677.41 (1.667 sec)\n",
            "train - step 2379: loss = 400577.41 (1.667 sec)\n",
            "train - step 2380: loss = 577173.62 (1.723 sec)\n",
            "train - step 2381: loss = 439017.34 (1.674 sec)\n",
            "train - step 2382: loss = 345761.91 (1.697 sec)\n",
            "train - step 2383: loss = 357338.78 (1.691 sec)\n",
            "train - step 2384: loss = 279830.06 (1.706 sec)\n",
            "train - step 2385: loss = 462176.72 (1.691 sec)\n",
            "train - step 2386: loss = 452618.03 (1.716 sec)\n",
            "train - step 2387: loss = 468636.50 (1.690 sec)\n",
            "train - step 2388: loss = 438775.94 (2.670 sec)\n",
            "train - step 2389: loss = 311054.50 (1.707 sec)\n",
            "train - step 2390: loss = 422824.00 (1.701 sec)\n",
            "train - step 2391: loss = 484315.50 (1.722 sec)\n",
            "train - step 2392: loss = 331674.09 (1.688 sec)\n",
            "train - step 2393: loss = 485757.59 (1.704 sec)\n",
            "train - step 2394: loss = 371440.41 (1.690 sec)\n",
            "train - step 2395: loss = 395791.59 (1.675 sec)\n",
            "train - step 2396: loss = 441404.06 (1.661 sec)\n",
            "train - step 2397: loss = 408482.97 (1.698 sec)\n",
            "train - step 2398: loss = 358964.66 (1.698 sec)\n",
            "train - step 2399: loss = 509006.19 (1.691 sec)\n",
            "train - step 2400: loss = 496628.78 (1.727 sec)\n",
            "train - step 2401: loss = 540432.75 (1.693 sec)\n",
            "train - step 2402: loss = 406984.22 (1.692 sec)\n",
            "train - step 2403: loss = 543840.56 (1.698 sec)\n",
            "train - step 2404: loss = 308470.00 (1.684 sec)\n",
            "train - step 2405: loss = 303070.69 (1.668 sec)\n",
            "train - step 2406: loss = 467932.66 (1.666 sec)\n",
            "train - step 2407: loss = 363527.53 (1.674 sec)\n",
            "train - step 2408: loss = 537518.94 (1.709 sec)\n",
            "train - step 2409: loss = 505037.06 (1.706 sec)\n",
            "train - step 2410: loss = 466708.19 (1.686 sec)\n",
            "train - step 2411: loss = 485341.06 (1.677 sec)\n",
            "train - step 2412: loss = 402342.69 (1.717 sec)\n",
            "train - step 2413: loss = 427751.50 (1.658 sec)\n",
            "train - step 2414: loss = 484211.09 (1.711 sec)\n",
            "train - step 2415: loss = 545429.38 (1.708 sec)\n",
            "train - step 2416: loss = 612107.19 (1.694 sec)\n",
            "train - step 2417: loss = 449902.72 (1.678 sec)\n",
            "train - step 2418: loss = 566449.19 (1.694 sec)\n",
            "train - step 2419: loss = 459802.81 (1.663 sec)\n",
            "train - step 2420: loss = 345926.03 (1.678 sec)\n",
            "train - step 2421: loss = 511165.28 (1.675 sec)\n",
            "train - step 2422: loss = 538628.88 (1.706 sec)\n",
            "train - step 2423: loss = 528290.94 (1.740 sec)\n",
            "train - step 2424: loss = 422483.06 (2.594 sec)\n",
            "train - step 2425: loss = 480581.81 (1.746 sec)\n",
            "train - step 2426: loss = 378869.19 (1.674 sec)\n",
            "train - step 2427: loss = 347193.53 (1.705 sec)\n",
            "train - step 2428: loss = 328175.62 (1.725 sec)\n",
            "train - step 2429: loss = 481859.59 (1.703 sec)\n",
            "train - step 2430: loss = 434359.78 (1.683 sec)\n",
            "train - step 2431: loss = 421977.03 (1.687 sec)\n",
            "train - step 2432: loss = 314694.41 (1.672 sec)\n",
            "train - step 2433: loss = 471188.50 (1.710 sec)\n",
            "train - step 2434: loss = 341653.41 (1.691 sec)\n",
            "train - step 2435: loss = 453450.38 (1.670 sec)\n",
            "train - step 2436: loss = 545144.19 (1.709 sec)\n",
            "train - step 2437: loss = 423635.59 (1.700 sec)\n",
            "train - step 2438: loss = 368299.88 (1.697 sec)\n",
            "train - step 2439: loss = 550718.88 (1.671 sec)\n",
            "train - step 2440: loss = 407222.53 (1.680 sec)\n",
            "train - step 2441: loss = 401952.72 (1.690 sec)\n",
            "train - step 2442: loss = 398710.81 (1.677 sec)\n",
            "train - step 2443: loss = 525099.31 (1.675 sec)\n",
            "train - step 2444: loss = 324720.53 (1.696 sec)\n",
            "train - step 2445: loss = 518881.88 (1.677 sec)\n",
            "train - step 2446: loss = 412805.53 (1.667 sec)\n",
            "train - step 2447: loss = 523652.28 (1.686 sec)\n",
            "train - step 2448: loss = 456233.62 (1.664 sec)\n",
            "train - step 2449: loss = 359174.44 (1.685 sec)\n",
            "train - step 2450: loss = 536454.06 (1.648 sec)\n",
            "train - step 2451: loss = 368880.91 (1.667 sec)\n",
            "train - step 2452: loss = 419385.28 (1.644 sec)\n",
            "train - step 2453: loss = 414833.72 (1.653 sec)\n",
            "train - step 2454: loss = 387772.00 (1.651 sec)\n",
            "train - step 2455: loss = 504718.00 (1.657 sec)\n",
            "train - step 2456: loss = 440985.06 (1.642 sec)\n",
            "train - step 2457: loss = 425270.97 (1.653 sec)\n",
            "train - step 2458: loss = 432825.16 (1.680 sec)\n",
            "train - step 2459: loss = 426230.00 (1.676 sec)\n",
            "train - step 2460: loss = 337805.81 (2.611 sec)\n",
            "train - step 2461: loss = 494394.66 (1.690 sec)\n",
            "train - step 2462: loss = 372159.88 (2.362 sec)\n",
            "train - step 2463: loss = 351587.38 (1.707 sec)\n",
            "train - step 2464: loss = 415522.69 (1.694 sec)\n",
            "train - step 2465: loss = 440958.16 (1.666 sec)\n",
            "train - step 2466: loss = 480583.19 (1.678 sec)\n",
            "train - step 2467: loss = 356213.50 (1.666 sec)\n",
            "train - step 2468: loss = 451963.66 (1.704 sec)\n",
            "train - step 2469: loss = 482753.16 (1.681 sec)\n",
            "train - step 2470: loss = 327525.28 (1.676 sec)\n",
            "train - step 2471: loss = 557338.75 (1.668 sec)\n",
            "train - step 2472: loss = 367589.22 (1.660 sec)\n",
            "train - step 2473: loss = 491754.16 (1.683 sec)\n",
            "train - step 2474: loss = 584009.38 (1.693 sec)\n",
            "train - step 2475: loss = 345822.47 (1.670 sec)\n",
            "train - step 2476: loss = 435480.22 (1.697 sec)\n",
            "train - step 2477: loss = 413549.34 (1.671 sec)\n",
            "train - step 2478: loss = 498691.53 (1.690 sec)\n",
            "train - step 2479: loss = 410816.41 (1.693 sec)\n",
            "train - step 2480: loss = 557817.31 (1.677 sec)\n",
            "train - step 2481: loss = 299804.03 (1.676 sec)\n",
            "train - step 2482: loss = 517245.22 (1.685 sec)\n",
            "train - step 2483: loss = 556833.31 (1.668 sec)\n",
            "train - step 2484: loss = 370020.91 (1.690 sec)\n",
            "train - step 2485: loss = 392327.69 (1.678 sec)\n",
            "train - step 2486: loss = 578875.31 (1.687 sec)\n",
            "train - step 2487: loss = 459376.03 (1.678 sec)\n",
            "train - step 2488: loss = 556362.12 (1.674 sec)\n",
            "train - step 2489: loss = 357201.53 (1.675 sec)\n",
            "train - step 2490: loss = 422566.72 (1.682 sec)\n",
            "train - step 2491: loss = 558210.38 (1.678 sec)\n",
            "train - step 2492: loss = 401869.97 (1.667 sec)\n",
            "train - step 2493: loss = 377963.88 (1.661 sec)\n",
            "train - step 2494: loss = 384692.56 (1.698 sec)\n",
            "train - step 2495: loss = 417545.84 (2.637 sec)\n",
            "train - step 2496: loss = 327778.22 (1.701 sec)\n",
            "train - step 2497: loss = 506807.31 (1.673 sec)\n",
            "train - step 2498: loss = 399051.44 (1.678 sec)\n",
            "train - step 2499: loss = 387487.31 (1.676 sec)\n",
            "train - step 2500: loss = 612828.06 (1.685 sec)\n",
            "train - step 2501: loss = 393159.88 (1.693 sec)\n",
            "train - step 2502: loss = 609640.62 (1.663 sec)\n",
            "train - step 2503: loss = 446376.44 (1.680 sec)\n",
            "train - step 2504: loss = 353532.31 (1.661 sec)\n",
            "train - step 2505: loss = 465593.12 (1.682 sec)\n",
            "train - step 2506: loss = 403184.09 (1.685 sec)\n",
            "train - step 2507: loss = 482773.06 (1.680 sec)\n",
            "train - step 2508: loss = 511175.72 (1.660 sec)\n",
            "train - step 2509: loss = 463258.28 (1.679 sec)\n",
            "train - step 2510: loss = 403605.50 (1.686 sec)\n",
            "train - step 2511: loss = 408677.53 (1.689 sec)\n",
            "train - step 2512: loss = 436181.41 (1.673 sec)\n",
            "train - step 2513: loss = 444651.28 (1.658 sec)\n",
            "train - step 2514: loss = 349841.66 (1.685 sec)\n",
            "train - step 2515: loss = 365513.12 (1.652 sec)\n",
            "train - step 2516: loss = 459745.41 (1.684 sec)\n",
            "train - step 2517: loss = 368761.59 (1.681 sec)\n",
            "train - step 2518: loss = 355843.88 (1.653 sec)\n",
            "train - step 2519: loss = 410794.03 (1.690 sec)\n",
            "train - step 2520: loss = 324379.88 (1.683 sec)\n",
            "train - step 2521: loss = 427394.09 (1.675 sec)\n",
            "train - step 2522: loss = 499712.31 (1.663 sec)\n",
            "train - step 2523: loss = 549777.25 (1.674 sec)\n",
            "train - step 2524: loss = 467907.19 (1.663 sec)\n",
            "train - step 2525: loss = 476796.72 (1.658 sec)\n",
            "train - step 2526: loss = 395395.97 (1.695 sec)\n",
            "train - step 2527: loss = 473521.06 (1.668 sec)\n",
            "train - step 2528: loss = 558093.56 (1.663 sec)\n",
            "train - step 2529: loss = 393464.97 (1.671 sec)\n",
            "train - step 2530: loss = 327305.59 (1.693 sec)\n",
            "train - step 2531: loss = 463941.97 (2.566 sec)\n",
            "train - step 2532: loss = 415693.22 (1.698 sec)\n",
            "train - step 2533: loss = 429511.50 (1.686 sec)\n",
            "train - step 2534: loss = 489635.44 (1.668 sec)\n",
            "train - step 2535: loss = 315084.66 (1.712 sec)\n",
            "train - step 2536: loss = 226417.20 (1.649 sec)\n",
            "train - step 2537: loss = 313831.84 (1.678 sec)\n",
            "train - step 2538: loss = 400887.47 (1.677 sec)\n",
            "train - step 2539: loss = 464989.94 (1.675 sec)\n",
            "train - step 2540: loss = 313813.50 (1.655 sec)\n",
            "train - step 2541: loss = 395737.06 (1.665 sec)\n",
            "train - step 2542: loss = 612998.94 (1.684 sec)\n",
            "train - step 2543: loss = 342230.81 (1.676 sec)\n",
            "train - step 2544: loss = 452524.09 (1.706 sec)\n",
            "train - step 2545: loss = 579415.25 (1.672 sec)\n",
            "train - step 2546: loss = 333998.81 (1.716 sec)\n",
            "train - step 2547: loss = 527670.94 (1.687 sec)\n",
            "train - step 2548: loss = 308245.91 (1.690 sec)\n",
            "train - step 2549: loss = 382795.12 (1.660 sec)\n",
            "train - step 2550: loss = 405577.84 (1.683 sec)\n",
            "train - step 2551: loss = 209933.59 (1.690 sec)\n",
            "train - step 2552: loss = 329497.53 (1.659 sec)\n",
            "train - step 2553: loss = 311151.38 (1.701 sec)\n",
            "train - step 2554: loss = 468964.91 (1.652 sec)\n",
            "train - step 2555: loss = 600893.56 (1.667 sec)\n",
            "train - step 2556: loss = 587530.38 (1.676 sec)\n",
            "train - step 2557: loss = 288265.16 (1.686 sec)\n",
            "train - step 2558: loss = 620569.94 (1.668 sec)\n",
            "train - step 2559: loss = 366208.53 (1.693 sec)\n",
            "train - step 2560: loss = 457344.31 (1.670 sec)\n",
            "train - step 2561: loss = 482963.47 (1.669 sec)\n",
            "train - step 2562: loss = 522446.47 (1.671 sec)\n",
            "train - step 2563: loss = 372459.19 (1.667 sec)\n",
            "train - step 2564: loss = 404779.66 (1.699 sec)\n",
            "train - step 2565: loss = 374172.72 (1.678 sec)\n",
            "train - step 2566: loss = 379025.88 (1.674 sec)\n",
            "train - step 2567: loss = 230748.45 (2.577 sec)\n",
            "train - step 2568: loss = 534541.56 (1.686 sec)\n",
            "train - step 2569: loss = 532746.06 (1.675 sec)\n",
            "train - step 2570: loss = 526165.56 (1.664 sec)\n",
            "train - step 2571: loss = 354580.47 (1.675 sec)\n",
            "train - step 2572: loss = 534481.12 (1.658 sec)\n",
            "train - step 2573: loss = 392884.66 (1.684 sec)\n",
            "train - step 2574: loss = 403052.03 (1.685 sec)\n",
            "train - step 2575: loss = 672847.31 (1.708 sec)\n",
            "train - step 2576: loss = 521781.69 (1.708 sec)\n",
            "train - step 2577: loss = 536092.88 (1.667 sec)\n",
            "train - step 2578: loss = 594844.06 (1.694 sec)\n",
            "train - step 2579: loss = 504174.88 (1.651 sec)\n",
            "train - step 2580: loss = 549249.19 (1.642 sec)\n",
            "train - step 2581: loss = 403492.91 (1.668 sec)\n",
            "train - step 2582: loss = 366247.50 (1.669 sec)\n",
            "train - step 2583: loss = 357015.59 (1.659 sec)\n",
            "train - step 2584: loss = 389482.78 (1.688 sec)\n",
            "train - step 2585: loss = 447133.34 (1.671 sec)\n",
            "train - step 2586: loss = 400831.69 (1.722 sec)\n",
            "train - step 2587: loss = 384912.59 (1.737 sec)\n",
            "train - step 2588: loss = 566904.88 (1.707 sec)\n",
            "train - step 2589: loss = 516597.69 (1.679 sec)\n",
            "train - step 2590: loss = 270851.00 (1.682 sec)\n",
            "train - step 2591: loss = 437130.28 (1.697 sec)\n",
            "train - step 2592: loss = 566044.56 (1.670 sec)\n",
            "train - step 2593: loss = 654234.25 (1.710 sec)\n",
            "train - step 2594: loss = 389021.19 (1.665 sec)\n",
            "train - step 2595: loss = 307646.09 (1.711 sec)\n",
            "train - step 2596: loss = 325739.38 (1.669 sec)\n",
            "train - step 2597: loss = 389036.88 (1.667 sec)\n",
            "train - step 2598: loss = 295410.97 (1.684 sec)\n",
            "train - step 2599: loss = 345090.19 (1.693 sec)\n",
            "train - step 2600: loss = 505146.12 (2.051 sec)\n",
            "train - step 2601: loss = 332100.28 (1.686 sec)\n",
            "train - step 2602: loss = 373405.34 (1.660 sec)\n",
            "train - step 2603: loss = 336451.62 (2.850 sec)\n",
            "train - step 2604: loss = 424466.94 (1.707 sec)\n",
            "train - step 2605: loss = 318995.19 (1.707 sec)\n",
            "train - step 2606: loss = 402715.44 (1.688 sec)\n",
            "train - step 2607: loss = 447950.00 (1.680 sec)\n",
            "train - step 2608: loss = 386897.00 (1.668 sec)\n",
            "train - step 2609: loss = 485447.97 (1.683 sec)\n",
            "train - step 2610: loss = 519302.94 (1.697 sec)\n",
            "train - step 2611: loss = 514316.97 (1.708 sec)\n",
            "train - step 2612: loss = 538869.75 (1.674 sec)\n",
            "train - step 2613: loss = 406537.50 (1.696 sec)\n",
            "train - step 2614: loss = 333140.12 (1.694 sec)\n",
            "train - step 2615: loss = 350560.09 (1.679 sec)\n",
            "train - step 2616: loss = 362128.72 (1.684 sec)\n",
            "train - step 2617: loss = 549796.25 (1.685 sec)\n",
            "train - step 2618: loss = 312184.91 (1.694 sec)\n",
            "train - step 2619: loss = 331462.22 (1.677 sec)\n",
            "train - step 2620: loss = 470857.06 (1.671 sec)\n",
            "train - step 2621: loss = 517465.50 (1.686 sec)\n",
            "train - step 2622: loss = 343341.72 (1.646 sec)\n",
            "train - step 2623: loss = 380386.34 (1.678 sec)\n",
            "train - step 2624: loss = 526365.88 (1.672 sec)\n",
            "train - step 2625: loss = 360186.06 (1.665 sec)\n",
            "train - step 2626: loss = 512722.47 (1.662 sec)\n",
            "train - step 2627: loss = 543687.94 (1.700 sec)\n",
            "train - step 2628: loss = 456648.41 (1.669 sec)\n",
            "train - step 2629: loss = 417958.09 (1.687 sec)\n",
            "train - step 2630: loss = 543825.25 (1.671 sec)\n",
            "train - step 2631: loss = 586036.75 (1.645 sec)\n",
            "train - step 2632: loss = 316750.09 (1.653 sec)\n",
            "train - step 2633: loss = 477462.66 (1.639 sec)\n",
            "train - step 2634: loss = 549729.12 (1.686 sec)\n",
            "train - step 2635: loss = 328651.81 (1.637 sec)\n",
            "train - step 2636: loss = 420597.72 (1.658 sec)\n",
            "train - step 2637: loss = 406167.91 (1.651 sec)\n",
            "train - step 2638: loss = 478569.50 (1.684 sec)\n",
            "train - step 2639: loss = 479240.59 (2.520 sec)\n",
            "train - step 2640: loss = 378632.84 (1.700 sec)\n",
            "train - step 2641: loss = 387388.03 (1.678 sec)\n",
            "train - step 2642: loss = 475800.97 (1.662 sec)\n",
            "train - step 2643: loss = 478723.81 (1.673 sec)\n",
            "train - step 2644: loss = 438754.53 (1.683 sec)\n",
            "train - step 2645: loss = 482263.56 (1.700 sec)\n",
            "train - step 2646: loss = 420633.81 (1.677 sec)\n",
            "train - step 2647: loss = 743513.69 (1.665 sec)\n",
            "train - step 2648: loss = 332632.19 (1.649 sec)\n",
            "train - step 2649: loss = 592905.56 (1.675 sec)\n",
            "train - step 2650: loss = 615930.44 (1.679 sec)\n",
            "train - step 2651: loss = 464991.97 (1.686 sec)\n",
            "train - step 2652: loss = 320970.19 (1.694 sec)\n",
            "train - step 2653: loss = 496020.06 (1.694 sec)\n",
            "train - step 2654: loss = 432836.47 (1.676 sec)\n",
            "train - step 2655: loss = 528875.00 (1.677 sec)\n",
            "train - step 2656: loss = 607311.25 (1.667 sec)\n",
            "train - step 2657: loss = 463508.94 (1.688 sec)\n",
            "train - step 2658: loss = 481763.56 (1.687 sec)\n",
            "train - step 2659: loss = 418028.78 (1.674 sec)\n",
            "train - step 2660: loss = 378425.62 (1.691 sec)\n",
            "train - step 2661: loss = 470916.16 (1.695 sec)\n",
            "train - step 2662: loss = 475367.56 (1.682 sec)\n",
            "train - step 2663: loss = 518576.47 (1.688 sec)\n",
            "train - step 2664: loss = 268885.47 (1.668 sec)\n",
            "train - step 2665: loss = 328357.69 (1.698 sec)\n",
            "train - step 2666: loss = 437895.78 (1.657 sec)\n",
            "train - step 2667: loss = 474489.66 (1.648 sec)\n",
            "train - step 2668: loss = 444622.97 (1.656 sec)\n",
            "train - step 2669: loss = 559791.38 (1.675 sec)\n",
            "train - step 2670: loss = 397020.44 (1.668 sec)\n",
            "train - step 2671: loss = 324457.38 (1.686 sec)\n",
            "train - step 2672: loss = 437992.22 (1.673 sec)\n",
            "train - step 2673: loss = 320212.34 (1.670 sec)\n",
            "train - step 2674: loss = 604828.12 (1.679 sec)\n",
            "train - step 2675: loss = 489141.44 (2.639 sec)\n",
            "train - step 2676: loss = 538994.00 (1.708 sec)\n",
            "train - step 2677: loss = 374027.28 (1.686 sec)\n",
            "train - step 2678: loss = 430835.38 (1.681 sec)\n",
            "train - step 2679: loss = 513446.53 (1.710 sec)\n",
            "train - step 2680: loss = 365485.88 (1.675 sec)\n",
            "train - step 2681: loss = 415345.38 (1.675 sec)\n",
            "train - step 2682: loss = 456615.22 (1.678 sec)\n",
            "train - step 2683: loss = 304437.59 (1.676 sec)\n",
            "train - step 2684: loss = 358550.34 (1.685 sec)\n",
            "train - step 2685: loss = 637089.00 (1.674 sec)\n",
            "train - step 2686: loss = 461027.00 (1.683 sec)\n",
            "train - step 2687: loss = 503199.12 (1.663 sec)\n",
            "train - step 2688: loss = 449593.53 (1.683 sec)\n",
            "train - step 2689: loss = 378539.47 (1.662 sec)\n",
            "train - step 2690: loss = 483265.72 (1.678 sec)\n",
            "train - step 2691: loss = 471385.38 (1.678 sec)\n",
            "train - step 2692: loss = 510969.91 (1.663 sec)\n",
            "train - step 2693: loss = 389325.91 (1.688 sec)\n",
            "train - step 2694: loss = 453656.22 (1.674 sec)\n",
            "train - step 2695: loss = 431383.84 (1.685 sec)\n",
            "train - step 2696: loss = 638980.94 (1.669 sec)\n",
            "train - step 2697: loss = 378947.41 (1.689 sec)\n",
            "train - step 2698: loss = 548276.75 (1.667 sec)\n",
            "train - step 2699: loss = 347393.72 (1.707 sec)\n",
            "train - step 2700: loss = 393549.62 (1.682 sec)\n",
            "train - step 2701: loss = 373366.66 (1.655 sec)\n",
            "train - step 2702: loss = 445591.34 (1.705 sec)\n",
            "train - step 2703: loss = 326608.97 (1.671 sec)\n",
            "train - step 2704: loss = 426508.66 (1.667 sec)\n",
            "train - step 2705: loss = 377746.00 (1.657 sec)\n",
            "train - step 2706: loss = 541938.44 (1.677 sec)\n",
            "train - step 2707: loss = 301900.91 (1.656 sec)\n",
            "train - step 2708: loss = 409648.34 (1.657 sec)\n",
            "train - step 2709: loss = 343971.50 (1.685 sec)\n",
            "train - step 2710: loss = 417385.66 (1.677 sec)\n",
            "train - step 2711: loss = 369771.72 (2.523 sec)\n",
            "train - step 2712: loss = 388286.97 (1.667 sec)\n",
            "train - step 2713: loss = 391225.59 (1.674 sec)\n",
            "train - step 2714: loss = 404881.41 (1.678 sec)\n",
            "train - step 2715: loss = 465796.69 (1.655 sec)\n",
            "train - step 2716: loss = 315501.34 (1.669 sec)\n",
            "train - step 2717: loss = 446337.47 (1.680 sec)\n",
            "train - step 2718: loss = 368011.38 (1.670 sec)\n",
            "train - step 2719: loss = 341393.84 (1.665 sec)\n",
            "train - step 2720: loss = 403393.12 (1.659 sec)\n",
            "train - step 2721: loss = 539570.19 (1.667 sec)\n",
            "train - step 2722: loss = 496584.78 (1.656 sec)\n",
            "train - step 2723: loss = 583982.75 (1.675 sec)\n",
            "train - step 2724: loss = 475740.22 (1.657 sec)\n",
            "train - step 2725: loss = 339528.41 (1.701 sec)\n",
            "train - step 2726: loss = 465691.00 (1.671 sec)\n",
            "train - step 2727: loss = 371298.78 (1.697 sec)\n",
            "train - step 2728: loss = 378291.66 (1.707 sec)\n",
            "train - step 2729: loss = 402236.88 (1.656 sec)\n",
            "train - step 2730: loss = 360761.22 (1.659 sec)\n",
            "train - step 2731: loss = 478684.00 (1.647 sec)\n",
            "train - step 2732: loss = 472561.47 (1.688 sec)\n",
            "train - step 2733: loss = 455876.66 (1.704 sec)\n",
            "train - step 2734: loss = 621026.69 (1.659 sec)\n",
            "train - step 2735: loss = 475592.12 (1.668 sec)\n",
            "train - step 2736: loss = 579207.75 (1.658 sec)\n",
            "train - step 2737: loss = 473291.16 (1.676 sec)\n",
            "train - step 2738: loss = 616558.06 (1.700 sec)\n",
            "train - step 2739: loss = 423610.03 (1.704 sec)\n",
            "train - step 2740: loss = 388489.28 (1.683 sec)\n",
            "train - step 2741: loss = 277904.19 (1.667 sec)\n",
            "train - step 2742: loss = 298114.44 (1.694 sec)\n",
            "train - step 2743: loss = 479939.53 (1.701 sec)\n",
            "train - step 2744: loss = 433032.44 (1.671 sec)\n",
            "train - step 2745: loss = 437151.12 (1.695 sec)\n",
            "train - step 2746: loss = 518673.72 (1.702 sec)\n",
            "train - step 2747: loss = 529743.69 (2.604 sec)\n",
            "train - step 2748: loss = 350854.06 (1.669 sec)\n",
            "train - step 2749: loss = 475697.12 (1.676 sec)\n",
            "train - step 2750: loss = 480280.94 (1.706 sec)\n",
            "train - step 2751: loss = 443457.59 (1.706 sec)\n",
            "train - step 2752: loss = 430689.69 (1.667 sec)\n",
            "train - step 2753: loss = 356539.59 (1.668 sec)\n",
            "train - step 2754: loss = 341735.22 (1.671 sec)\n",
            "train - step 2755: loss = 276678.53 (1.642 sec)\n",
            "train - step 2756: loss = 441395.59 (1.673 sec)\n",
            "train - step 2757: loss = 526034.94 (1.673 sec)\n",
            "train - step 2758: loss = 399889.44 (1.667 sec)\n",
            "train - step 2759: loss = 495018.31 (1.685 sec)\n",
            "train - step 2760: loss = 294586.50 (1.666 sec)\n",
            "train - step 2761: loss = 605834.12 (1.689 sec)\n",
            "train - step 2762: loss = 384117.56 (1.653 sec)\n",
            "train - step 2763: loss = 351748.38 (1.679 sec)\n",
            "train - step 2764: loss = 351952.84 (1.657 sec)\n",
            "train - step 2765: loss = 317432.53 (1.689 sec)\n",
            "train - step 2766: loss = 292516.00 (1.655 sec)\n",
            "train - step 2767: loss = 439888.00 (1.670 sec)\n",
            "train - step 2768: loss = 266019.69 (1.700 sec)\n",
            "train - step 2769: loss = 329733.53 (1.648 sec)\n",
            "train - step 2770: loss = 616416.19 (1.673 sec)\n",
            "train - step 2771: loss = 369676.56 (1.668 sec)\n",
            "train - step 2772: loss = 422909.22 (1.681 sec)\n",
            "train - step 2773: loss = 467726.41 (1.673 sec)\n",
            "train - step 2774: loss = 375706.00 (1.663 sec)\n",
            "train - step 2775: loss = 577435.75 (1.692 sec)\n",
            "train - step 2776: loss = 556151.44 (1.674 sec)\n",
            "train - step 2777: loss = 207032.80 (1.697 sec)\n",
            "train - step 2778: loss = 482923.47 (1.700 sec)\n",
            "train - step 2779: loss = 361840.09 (1.675 sec)\n",
            "train - step 2780: loss = 586541.75 (1.673 sec)\n",
            "train - step 2781: loss = 558175.06 (1.683 sec)\n",
            "train - step 2782: loss = 481754.59 (1.705 sec)\n",
            "train - step 2783: loss = 332969.16 (2.488 sec)\n",
            "train - step 2784: loss = 357460.91 (1.700 sec)\n",
            "train - step 2785: loss = 318021.94 (1.674 sec)\n",
            "train - step 2786: loss = 496601.62 (1.663 sec)\n",
            "train - step 2787: loss = 380674.59 (1.700 sec)\n",
            "train - step 2788: loss = 443918.00 (1.683 sec)\n",
            "train - step 2789: loss = 613991.00 (1.689 sec)\n",
            "train - step 2790: loss = 493458.47 (1.667 sec)\n",
            "train - step 2791: loss = 515887.78 (1.679 sec)\n",
            "train - step 2792: loss = 440712.22 (1.680 sec)\n",
            "train - step 2793: loss = 651166.12 (1.683 sec)\n",
            "train - step 2794: loss = 372257.72 (1.680 sec)\n",
            "train - step 2795: loss = 378528.62 (1.673 sec)\n",
            "train - step 2796: loss = 529016.75 (1.701 sec)\n",
            "train - step 2797: loss = 465557.16 (1.669 sec)\n",
            "train - step 2798: loss = 646695.56 (1.686 sec)\n",
            "train - step 2799: loss = 487241.66 (1.673 sec)\n",
            "train - step 2800: loss = 235748.06 (1.674 sec)\n",
            "train - step 2801: loss = 437364.19 (1.714 sec)\n",
            "train - step 2802: loss = 423850.81 (1.693 sec)\n",
            "train - step 2803: loss = 526138.19 (1.678 sec)\n",
            "train - step 2804: loss = 595837.31 (1.666 sec)\n",
            "train - step 2805: loss = 446595.44 (1.665 sec)\n",
            "train - step 2806: loss = 606784.19 (1.674 sec)\n",
            "train - step 2807: loss = 699479.69 (1.662 sec)\n",
            "train - step 2808: loss = 319490.28 (1.704 sec)\n",
            "train - step 2809: loss = 570044.06 (1.683 sec)\n",
            "train - step 2810: loss = 356652.03 (1.678 sec)\n",
            "train - step 2811: loss = 433986.38 (1.686 sec)\n",
            "train - step 2812: loss = 401650.59 (1.662 sec)\n",
            "train - step 2813: loss = 554801.50 (1.650 sec)\n",
            "train - step 2814: loss = 447198.22 (1.661 sec)\n",
            "train - step 2815: loss = 389155.66 (1.648 sec)\n",
            "train - step 2816: loss = 515115.34 (1.656 sec)\n",
            "train - step 2817: loss = 445968.56 (1.642 sec)\n",
            "train - step 2818: loss = 505514.62 (1.642 sec)\n",
            "train - step 2819: loss = 511915.97 (2.490 sec)\n",
            "train - step 2820: loss = 372475.53 (1.695 sec)\n",
            "train - step 2821: loss = 326784.66 (1.684 sec)\n",
            "train - step 2822: loss = 354499.34 (1.660 sec)\n",
            "train - step 2823: loss = 380222.19 (1.693 sec)\n",
            "train - step 2824: loss = 332154.62 (1.669 sec)\n",
            "train - step 2825: loss = 637492.38 (1.721 sec)\n",
            "train - step 2826: loss = 485762.91 (1.676 sec)\n",
            "train - step 2827: loss = 529082.81 (1.685 sec)\n",
            "train - step 2828: loss = 441505.22 (1.665 sec)\n",
            "train - step 2829: loss = 509134.22 (1.676 sec)\n",
            "train - step 2830: loss = 358068.06 (1.673 sec)\n",
            "train - step 2831: loss = 215771.47 (1.678 sec)\n",
            "train - step 2832: loss = 507922.47 (1.693 sec)\n",
            "train - step 2833: loss = 576015.00 (1.656 sec)\n",
            "train - step 2834: loss = 402984.41 (1.692 sec)\n",
            "train - step 2835: loss = 366205.66 (1.688 sec)\n",
            "train - step 2836: loss = 555514.19 (1.679 sec)\n",
            "train - step 2837: loss = 371680.19 (1.664 sec)\n",
            "train - step 2838: loss = 493348.12 (1.671 sec)\n",
            "train - step 2839: loss = 423846.19 (1.698 sec)\n",
            "train - step 2840: loss = 331737.34 (1.686 sec)\n",
            "train - step 2841: loss = 253870.08 (1.727 sec)\n",
            "train - step 2842: loss = 418015.53 (1.651 sec)\n",
            "train - step 2843: loss = 323496.91 (1.661 sec)\n",
            "train - step 2844: loss = 452379.56 (1.658 sec)\n",
            "train - step 2845: loss = 453747.78 (1.674 sec)\n",
            "train - step 2846: loss = 571386.19 (1.705 sec)\n",
            "train - step 2847: loss = 550218.19 (1.658 sec)\n",
            "train - step 2848: loss = 415365.41 (1.676 sec)\n",
            "train - step 2849: loss = 334659.06 (1.677 sec)\n",
            "train - step 2850: loss = 357522.47 (1.675 sec)\n",
            "train - step 2851: loss = 368584.84 (1.660 sec)\n",
            "train - step 2852: loss = 519447.06 (1.670 sec)\n",
            "train - step 2853: loss = 659918.19 (1.698 sec)\n",
            "train - step 2854: loss = 409066.44 (1.654 sec)\n",
            "train - step 2855: loss = 355504.91 (2.473 sec)\n",
            "train - step 2856: loss = 570995.19 (1.683 sec)\n",
            "train - step 2857: loss = 591332.38 (1.668 sec)\n",
            "train - step 2858: loss = 385853.16 (1.676 sec)\n",
            "train - step 2859: loss = 501726.16 (1.692 sec)\n",
            "train - step 2860: loss = 400326.94 (1.694 sec)\n",
            "train - step 2861: loss = 664755.75 (1.678 sec)\n",
            "train - step 2862: loss = 343684.59 (1.662 sec)\n",
            "train - step 2863: loss = 437474.97 (1.665 sec)\n",
            "train - step 2864: loss = 335435.56 (1.667 sec)\n",
            "train - step 2865: loss = 616487.12 (1.673 sec)\n",
            "train - step 2866: loss = 620966.12 (1.654 sec)\n",
            "train - step 2867: loss = 446466.06 (1.664 sec)\n",
            "train - step 2868: loss = 459086.50 (1.681 sec)\n",
            "train - step 2869: loss = 474744.78 (1.703 sec)\n",
            "train - step 2870: loss = 538963.62 (1.653 sec)\n",
            "train - step 2871: loss = 342122.72 (1.680 sec)\n",
            "train - step 2872: loss = 376917.66 (1.681 sec)\n",
            "train - step 2873: loss = 281608.81 (1.661 sec)\n",
            "train - step 2874: loss = 441516.69 (1.685 sec)\n",
            "train - step 2875: loss = 426987.38 (1.670 sec)\n",
            "train - step 2876: loss = 265915.34 (1.685 sec)\n",
            "train - step 2877: loss = 490444.00 (1.657 sec)\n",
            "train - step 2878: loss = 545684.94 (1.706 sec)\n",
            "train - step 2879: loss = 346296.38 (1.703 sec)\n",
            "train - step 2880: loss = 476342.12 (1.699 sec)\n",
            "train - step 2881: loss = 482287.97 (1.681 sec)\n",
            "train - step 2882: loss = 409234.91 (1.667 sec)\n",
            "train - step 2883: loss = 577011.44 (1.703 sec)\n",
            "train - step 2884: loss = 394879.22 (1.676 sec)\n",
            "train - step 2885: loss = 455830.44 (1.692 sec)\n",
            "train - step 2886: loss = 441881.09 (1.705 sec)\n",
            "train - step 2887: loss = 425521.94 (1.700 sec)\n",
            "train - step 2888: loss = 485054.50 (1.694 sec)\n",
            "train - step 2889: loss = 493314.31 (1.688 sec)\n",
            "train - step 2890: loss = 495057.38 (1.713 sec)\n",
            "train - step 2891: loss = 628608.88 (1.985 sec)\n",
            "train - step 2892: loss = 520799.03 (1.671 sec)\n",
            "train - step 2893: loss = 425984.12 (1.782 sec)\n",
            "train - step 2894: loss = 501933.22 (1.671 sec)\n",
            "train - step 2895: loss = 459411.00 (1.708 sec)\n",
            "train - step 2896: loss = 394695.94 (1.671 sec)\n",
            "train - step 2897: loss = 356954.69 (1.686 sec)\n",
            "train - step 2898: loss = 485378.22 (1.683 sec)\n",
            "train - step 2899: loss = 423202.69 (1.657 sec)\n",
            "train - step 2900: loss = 444399.41 (1.677 sec)\n",
            "train - step 2901: loss = 286164.09 (1.663 sec)\n",
            "train - step 2902: loss = 413630.06 (1.676 sec)\n",
            "train - step 2903: loss = 416753.41 (1.684 sec)\n",
            "train - step 2904: loss = 330756.91 (1.703 sec)\n",
            "train - step 2905: loss = 325777.53 (1.651 sec)\n",
            "train - step 2906: loss = 527690.38 (1.657 sec)\n",
            "train - step 2907: loss = 466087.00 (1.654 sec)\n",
            "train - step 2908: loss = 369917.00 (1.668 sec)\n",
            "train - step 2909: loss = 295938.47 (1.696 sec)\n",
            "train - step 2910: loss = 503914.69 (1.666 sec)\n",
            "train - step 2911: loss = 493472.84 (1.669 sec)\n",
            "train - step 2912: loss = 317285.06 (1.677 sec)\n",
            "train - step 2913: loss = 539919.06 (1.685 sec)\n",
            "train - step 2914: loss = 339228.66 (1.663 sec)\n",
            "train - step 2915: loss = 530346.31 (1.666 sec)\n",
            "train - step 2916: loss = 327185.78 (1.682 sec)\n",
            "train - step 2917: loss = 354970.78 (1.672 sec)\n",
            "train - step 2918: loss = 450340.34 (1.692 sec)\n",
            "train - step 2919: loss = 487010.12 (1.676 sec)\n",
            "train - step 2920: loss = 539743.25 (1.653 sec)\n",
            "train - step 2921: loss = 519959.00 (1.704 sec)\n",
            "train - step 2922: loss = 522224.47 (1.682 sec)\n",
            "train - step 2923: loss = 394805.56 (1.667 sec)\n",
            "train - step 2924: loss = 442209.03 (1.680 sec)\n",
            "train - step 2925: loss = 357452.97 (1.663 sec)\n",
            "train - step 2926: loss = 408633.59 (1.686 sec)\n",
            "train - step 2927: loss = 431092.59 (2.490 sec)\n",
            "train - step 2928: loss = 299946.28 (1.680 sec)\n",
            "train - step 2929: loss = 512797.91 (1.677 sec)\n",
            "train - step 2930: loss = 394370.00 (1.643 sec)\n",
            "train - step 2931: loss = 513632.53 (1.677 sec)\n",
            "train - step 2932: loss = 355386.88 (1.683 sec)\n",
            "train - step 2933: loss = 402496.66 (1.676 sec)\n",
            "train - step 2934: loss = 345637.53 (1.670 sec)\n",
            "train - step 2935: loss = 437567.12 (1.668 sec)\n",
            "train - step 2936: loss = 295396.62 (1.683 sec)\n",
            "train - step 2937: loss = 526031.75 (1.662 sec)\n",
            "train - step 2938: loss = 554212.25 (1.692 sec)\n",
            "train - step 2939: loss = 397216.19 (1.681 sec)\n",
            "train - step 2940: loss = 396702.78 (1.703 sec)\n",
            "train - step 2941: loss = 451474.28 (1.666 sec)\n",
            "train - step 2942: loss = 372058.03 (1.695 sec)\n",
            "train - step 2943: loss = 577284.12 (1.673 sec)\n",
            "train - step 2944: loss = 401400.34 (1.649 sec)\n",
            "train - step 2945: loss = 520318.94 (1.673 sec)\n",
            "train - step 2946: loss = 464356.09 (1.663 sec)\n",
            "train - step 2947: loss = 346609.47 (1.730 sec)\n",
            "train - step 2948: loss = 378644.38 (1.650 sec)\n",
            "train - step 2949: loss = 448261.38 (1.666 sec)\n",
            "train - step 2950: loss = 356222.22 (1.677 sec)\n",
            "train - step 2951: loss = 457993.28 (1.704 sec)\n",
            "train - step 2952: loss = 451693.28 (1.667 sec)\n",
            "train - step 2953: loss = 363978.62 (1.672 sec)\n",
            "train - step 2954: loss = 369848.59 (1.677 sec)\n",
            "train - step 2955: loss = 301776.28 (1.658 sec)\n",
            "train - step 2956: loss = 403652.47 (1.673 sec)\n",
            "train - step 2957: loss = 307076.69 (1.655 sec)\n",
            "train - step 2958: loss = 422360.53 (1.671 sec)\n",
            "train - step 2959: loss = 472358.41 (1.682 sec)\n",
            "train - step 2960: loss = 406909.59 (1.689 sec)\n",
            "train - step 2961: loss = 464310.28 (1.657 sec)\n",
            "train - step 2962: loss = 357189.97 (1.678 sec)\n",
            "train - step 2963: loss = 348634.72 (2.528 sec)\n",
            "train - step 2964: loss = 440429.19 (1.689 sec)\n",
            "train - step 2965: loss = 626330.19 (1.692 sec)\n",
            "train - step 2966: loss = 415646.53 (1.718 sec)\n",
            "train - step 2967: loss = 356048.12 (1.702 sec)\n",
            "train - step 2968: loss = 359622.28 (1.646 sec)\n",
            "train - step 2969: loss = 316844.09 (1.683 sec)\n",
            "train - step 2970: loss = 382084.38 (1.686 sec)\n",
            "train - step 2971: loss = 300476.47 (1.642 sec)\n",
            "train - step 2972: loss = 479961.94 (1.680 sec)\n",
            "train - step 2973: loss = 563241.75 (1.685 sec)\n",
            "train - step 2974: loss = 477410.56 (1.684 sec)\n",
            "train - step 2975: loss = 492431.91 (1.668 sec)\n",
            "train - step 2976: loss = 385300.31 (1.662 sec)\n",
            "train - step 2977: loss = 467774.06 (1.685 sec)\n",
            "train - step 2978: loss = 312466.47 (1.669 sec)\n",
            "train - step 2979: loss = 425835.81 (1.673 sec)\n",
            "train - step 2980: loss = 557358.00 (1.671 sec)\n",
            "train - step 2981: loss = 504593.28 (1.689 sec)\n",
            "train - step 2982: loss = 452294.06 (1.692 sec)\n",
            "train - step 2983: loss = 477622.03 (1.671 sec)\n",
            "train - step 2984: loss = 348287.88 (1.669 sec)\n",
            "train - step 2985: loss = 437245.34 (1.674 sec)\n",
            "train - step 2986: loss = 233616.73 (1.671 sec)\n",
            "train - step 2987: loss = 408069.72 (1.661 sec)\n",
            "train - step 2988: loss = 253248.69 (1.668 sec)\n",
            "train - step 2989: loss = 517568.41 (1.693 sec)\n",
            "train - step 2990: loss = 380426.12 (1.667 sec)\n",
            "train - step 2991: loss = 418931.72 (1.672 sec)\n",
            "train - step 2992: loss = 402755.94 (1.655 sec)\n",
            "train - step 2993: loss = 348015.44 (1.636 sec)\n",
            "train - step 2994: loss = 536228.75 (1.634 sec)\n",
            "train - step 2995: loss = 432711.94 (1.646 sec)\n",
            "train - step 2996: loss = 337557.94 (1.648 sec)\n",
            "train - step 2997: loss = 318904.94 (1.635 sec)\n",
            "train - step 2998: loss = 421202.81 (1.635 sec)\n",
            "train - step 2999: loss = 426695.69 (2.456 sec)\n",
            "train - step 3000: loss = 529501.38 (1.670 sec)\n",
            "train - step 3001: loss = 404793.69 (1.654 sec)\n",
            "train - step 3002: loss = 556218.31 (1.646 sec)\n",
            "train - step 3003: loss = 472715.56 (1.679 sec)\n",
            "train - step 3004: loss = 511597.88 (1.672 sec)\n",
            "train - step 3005: loss = 370505.47 (1.665 sec)\n",
            "train - step 3006: loss = 705096.12 (1.676 sec)\n",
            "train - step 3007: loss = 508792.69 (1.663 sec)\n",
            "train - step 3008: loss = 247344.47 (1.672 sec)\n",
            "train - step 3009: loss = 439969.19 (1.685 sec)\n",
            "train - step 3010: loss = 527993.56 (1.672 sec)\n",
            "train - step 3011: loss = 454699.66 (1.693 sec)\n",
            "train - step 3012: loss = 322461.31 (1.683 sec)\n",
            "train - step 3013: loss = 538448.25 (1.672 sec)\n",
            "train - step 3014: loss = 486682.88 (1.669 sec)\n",
            "train - step 3015: loss = 591426.88 (1.668 sec)\n",
            "train - step 3016: loss = 421855.78 (1.666 sec)\n",
            "train - step 3017: loss = 393801.16 (1.662 sec)\n",
            "train - step 3018: loss = 545943.81 (1.667 sec)\n",
            "train - step 3019: loss = 347859.94 (1.674 sec)\n",
            "train - step 3020: loss = 497040.62 (1.673 sec)\n",
            "train - step 3021: loss = 390481.03 (1.687 sec)\n",
            "train - step 3022: loss = 325031.81 (1.703 sec)\n",
            "train - step 3023: loss = 394735.50 (1.647 sec)\n",
            "train - step 3024: loss = 481794.72 (1.674 sec)\n",
            "train - step 3025: loss = 529984.81 (1.654 sec)\n",
            "train - step 3026: loss = 354722.66 (1.669 sec)\n",
            "train - step 3027: loss = 451410.28 (1.666 sec)\n",
            "train - step 3028: loss = 492037.31 (1.652 sec)\n",
            "train - step 3029: loss = 439012.69 (1.680 sec)\n",
            "train - step 3030: loss = 573328.44 (1.658 sec)\n",
            "train - step 3031: loss = 320489.78 (1.676 sec)\n",
            "train - step 3032: loss = 389450.81 (1.656 sec)\n",
            "train - step 3033: loss = 433230.12 (1.678 sec)\n",
            "train - step 3034: loss = 574098.44 (1.675 sec)\n",
            "train - step 3035: loss = 400913.53 (2.505 sec)\n",
            "train - step 3036: loss = 425357.28 (1.696 sec)\n",
            "train - step 3037: loss = 573992.94 (1.680 sec)\n",
            "train - step 3038: loss = 251896.58 (1.683 sec)\n",
            "train - step 3039: loss = 399107.34 (1.664 sec)\n",
            "train - step 3040: loss = 363670.53 (1.663 sec)\n",
            "train - step 3041: loss = 446257.34 (1.675 sec)\n",
            "train - step 3042: loss = 381260.19 (1.665 sec)\n",
            "train - step 3043: loss = 404240.03 (1.662 sec)\n",
            "train - step 3044: loss = 538828.06 (1.656 sec)\n",
            "train - step 3045: loss = 429345.47 (1.690 sec)\n",
            "train - step 3046: loss = 458713.34 (1.669 sec)\n",
            "train - step 3047: loss = 574036.00 (1.695 sec)\n",
            "train - step 3048: loss = 516384.09 (1.679 sec)\n",
            "train - step 3049: loss = 563793.75 (1.665 sec)\n",
            "train - step 3050: loss = 390069.09 (1.676 sec)\n",
            "train - step 3051: loss = 353029.56 (1.674 sec)\n",
            "train - step 3052: loss = 506050.69 (1.677 sec)\n",
            "train - step 3053: loss = 282283.28 (1.676 sec)\n",
            "train - step 3054: loss = 492819.78 (1.687 sec)\n",
            "train - step 3055: loss = 359046.31 (1.658 sec)\n",
            "train - step 3056: loss = 488279.03 (1.682 sec)\n",
            "train - step 3057: loss = 331540.34 (1.668 sec)\n",
            "train - step 3058: loss = 484487.19 (1.674 sec)\n",
            "train - step 3059: loss = 424409.56 (1.678 sec)\n",
            "train - step 3060: loss = 335882.66 (1.685 sec)\n",
            "train - step 3061: loss = 392181.28 (1.677 sec)\n",
            "train - step 3062: loss = 507986.28 (1.670 sec)\n",
            "train - step 3063: loss = 553854.62 (1.675 sec)\n",
            "train - step 3064: loss = 391664.53 (1.687 sec)\n",
            "train - step 3065: loss = 425638.59 (1.677 sec)\n",
            "train - step 3066: loss = 403259.16 (1.680 sec)\n",
            "train - step 3067: loss = 443723.44 (1.696 sec)\n",
            "train - step 3068: loss = 468742.81 (1.667 sec)\n",
            "train - step 3069: loss = 522109.94 (1.683 sec)\n",
            "train - step 3070: loss = 508011.12 (1.690 sec)\n",
            "train - step 3071: loss = 444479.94 (2.742 sec)\n",
            "train - step 3072: loss = 570520.31 (1.730 sec)\n",
            "train - step 3073: loss = 384998.69 (1.683 sec)\n",
            "train - step 3074: loss = 505388.38 (1.673 sec)\n",
            "train - step 3075: loss = 450023.88 (1.673 sec)\n",
            "train - step 3076: loss = 385784.38 (1.666 sec)\n",
            "train - step 3077: loss = 492024.06 (1.671 sec)\n",
            "train - step 3078: loss = 432996.72 (1.670 sec)\n",
            "train - step 3079: loss = 328079.41 (1.677 sec)\n",
            "train - step 3080: loss = 576564.81 (1.686 sec)\n",
            "train - step 3081: loss = 463852.19 (1.670 sec)\n",
            "train - step 3082: loss = 508989.47 (1.666 sec)\n",
            "train - step 3083: loss = 541005.25 (1.686 sec)\n",
            "train - step 3084: loss = 393595.00 (1.669 sec)\n",
            "train - step 3085: loss = 442728.34 (1.693 sec)\n",
            "train - step 3086: loss = 453662.12 (1.677 sec)\n",
            "train - step 3087: loss = 395739.81 (1.657 sec)\n",
            "train - step 3088: loss = 331924.66 (1.690 sec)\n",
            "train - step 3089: loss = 413405.78 (1.664 sec)\n",
            "train - step 3090: loss = 478047.97 (1.673 sec)\n",
            "train - step 3091: loss = 339612.44 (1.703 sec)\n",
            "train - step 3092: loss = 343139.69 (1.693 sec)\n",
            "train - step 3093: loss = 419147.47 (1.701 sec)\n",
            "train - step 3094: loss = 486421.22 (1.670 sec)\n",
            "train - step 3095: loss = 426823.78 (1.682 sec)\n",
            "train - step 3096: loss = 529020.19 (1.668 sec)\n",
            "train - step 3097: loss = 378565.84 (1.722 sec)\n",
            "train - step 3098: loss = 386267.44 (1.674 sec)\n",
            "train - step 3099: loss = 433445.12 (1.682 sec)\n",
            "train - step 3100: loss = 435726.34 (1.673 sec)\n",
            "train - step 3101: loss = 405055.72 (1.658 sec)\n",
            "train - step 3102: loss = 581902.31 (1.708 sec)\n",
            "train - step 3103: loss = 388095.78 (1.715 sec)\n",
            "train - step 3104: loss = 353897.97 (1.675 sec)\n",
            "train - step 3105: loss = 413241.22 (1.675 sec)\n",
            "train - step 3106: loss = 425255.47 (1.675 sec)\n",
            "train - step 3107: loss = 282723.91 (2.503 sec)\n",
            "train - step 3108: loss = 401007.69 (1.673 sec)\n",
            "train - step 3109: loss = 395561.38 (1.697 sec)\n",
            "train - step 3110: loss = 349200.06 (1.663 sec)\n",
            "train - step 3111: loss = 429452.88 (1.668 sec)\n",
            "train - step 3112: loss = 365557.09 (1.673 sec)\n",
            "train - step 3113: loss = 377436.34 (1.664 sec)\n",
            "train - step 3114: loss = 506990.03 (1.670 sec)\n",
            "train - step 3115: loss = 568100.00 (1.673 sec)\n",
            "train - step 3116: loss = 607038.25 (1.695 sec)\n",
            "train - step 3117: loss = 507223.06 (1.676 sec)\n",
            "train - step 3118: loss = 461063.38 (1.678 sec)\n",
            "train - step 3119: loss = 346057.62 (1.681 sec)\n",
            "train - step 3120: loss = 432612.16 (1.692 sec)\n",
            "train - step 3121: loss = 441833.94 (1.685 sec)\n",
            "train - step 3122: loss = 332412.66 (1.676 sec)\n",
            "train - step 3123: loss = 409733.97 (1.663 sec)\n",
            "train - step 3124: loss = 521986.88 (1.660 sec)\n",
            "train - step 3125: loss = 372954.53 (1.661 sec)\n",
            "train - step 3126: loss = 569164.81 (1.665 sec)\n",
            "train - step 3127: loss = 326932.19 (1.681 sec)\n",
            "train - step 3128: loss = 468414.62 (1.696 sec)\n",
            "train - step 3129: loss = 395531.09 (1.684 sec)\n",
            "train - step 3130: loss = 459641.34 (1.662 sec)\n",
            "train - step 3131: loss = 549120.75 (1.667 sec)\n",
            "train - step 3132: loss = 407482.72 (1.694 sec)\n",
            "train - step 3133: loss = 394048.47 (1.685 sec)\n",
            "train - step 3134: loss = 380257.47 (1.696 sec)\n",
            "train - step 3135: loss = 273000.28 (1.669 sec)\n",
            "train - step 3136: loss = 497217.31 (1.672 sec)\n",
            "train - step 3137: loss = 539355.44 (1.668 sec)\n",
            "train - step 3138: loss = 351695.41 (1.674 sec)\n",
            "train - step 3139: loss = 465409.50 (1.683 sec)\n",
            "train - step 3140: loss = 430055.00 (1.694 sec)\n",
            "train - step 3141: loss = 425057.28 (1.660 sec)\n",
            "train - step 3142: loss = 530838.00 (1.686 sec)\n",
            "train - step 3143: loss = 344659.53 (2.691 sec)\n",
            "train - step 3144: loss = 514346.56 (1.689 sec)\n",
            "train - step 3145: loss = 369558.28 (1.740 sec)\n",
            "train - step 3146: loss = 518153.91 (1.688 sec)\n",
            "train - step 3147: loss = 422642.41 (1.702 sec)\n",
            "train - step 3148: loss = 447629.28 (1.680 sec)\n",
            "train - step 3149: loss = 525926.75 (1.691 sec)\n",
            "train - step 3150: loss = 348405.78 (1.679 sec)\n",
            "train - step 3151: loss = 586051.69 (1.707 sec)\n",
            "train - step 3152: loss = 409741.97 (1.679 sec)\n",
            "train - step 3153: loss = 514702.12 (1.673 sec)\n",
            "train - step 3154: loss = 515812.06 (1.742 sec)\n",
            "train - step 3155: loss = 400840.06 (1.660 sec)\n",
            "train - step 3156: loss = 412414.78 (1.710 sec)\n",
            "train - step 3157: loss = 424556.62 (1.695 sec)\n",
            "train - step 3158: loss = 436723.81 (1.710 sec)\n",
            "train - step 3159: loss = 449026.53 (1.693 sec)\n",
            "train - step 3160: loss = 538621.19 (1.701 sec)\n",
            "train - step 3161: loss = 326279.12 (1.673 sec)\n",
            "train - step 3162: loss = 406868.81 (1.684 sec)\n",
            "train - step 3163: loss = 343682.91 (1.652 sec)\n",
            "train - step 3164: loss = 430440.66 (1.691 sec)\n",
            "train - step 3165: loss = 487115.78 (1.675 sec)\n",
            "train - step 3166: loss = 603560.38 (1.681 sec)\n",
            "train - step 3167: loss = 431980.50 (1.676 sec)\n",
            "train - step 3168: loss = 471658.34 (1.659 sec)\n",
            "train - step 3169: loss = 361735.81 (1.672 sec)\n",
            "train - step 3170: loss = 404182.09 (1.699 sec)\n",
            "train - step 3171: loss = 433136.81 (1.679 sec)\n",
            "train - step 3172: loss = 353649.31 (1.667 sec)\n",
            "train - step 3173: loss = 542510.38 (1.702 sec)\n",
            "train - step 3174: loss = 521944.41 (1.726 sec)\n",
            "train - step 3175: loss = 393550.03 (1.657 sec)\n",
            "train - step 3176: loss = 582230.31 (1.702 sec)\n",
            "train - step 3177: loss = 657517.19 (1.711 sec)\n",
            "train - step 3178: loss = 402621.81 (1.684 sec)\n",
            "train - step 3179: loss = 476104.59 (1.679 sec)\n",
            "train - step 3180: loss = 407698.19 (1.643 sec)\n",
            "train - step 3181: loss = 426237.97 (1.667 sec)\n",
            "train - step 3182: loss = 355455.81 (1.678 sec)\n",
            "train - step 3183: loss = 401783.72 (1.668 sec)\n",
            "train - step 3184: loss = 264332.06 (1.646 sec)\n",
            "train - step 3185: loss = 581328.81 (1.647 sec)\n",
            "train - step 3186: loss = 496422.62 (1.693 sec)\n",
            "train - step 3187: loss = 580984.81 (1.695 sec)\n",
            "train - step 3188: loss = 513020.78 (1.705 sec)\n",
            "train - step 3189: loss = 405935.34 (1.688 sec)\n",
            "train - step 3190: loss = 491699.81 (1.687 sec)\n",
            "train - step 3191: loss = 491621.59 (1.671 sec)\n",
            "train - step 3192: loss = 468724.56 (1.671 sec)\n",
            "train - step 3193: loss = 417392.78 (1.659 sec)\n",
            "train - step 3194: loss = 387762.47 (1.657 sec)\n",
            "train - step 3195: loss = 629055.31 (1.693 sec)\n",
            "train - step 3196: loss = 515325.12 (1.670 sec)\n",
            "train - step 3197: loss = 504595.88 (1.706 sec)\n",
            "train - step 3198: loss = 672505.81 (1.675 sec)\n",
            "train - step 3199: loss = 351020.00 (1.680 sec)\n",
            "train - step 3200: loss = 360981.28 (1.693 sec)\n",
            "train - step 3201: loss = 260046.92 (1.667 sec)\n",
            "train - step 3202: loss = 329163.69 (1.661 sec)\n",
            "train - step 3203: loss = 669760.56 (1.681 sec)\n",
            "train - step 3204: loss = 457317.78 (1.699 sec)\n",
            "train - step 3205: loss = 330304.00 (1.678 sec)\n",
            "train - step 3206: loss = 407095.88 (1.693 sec)\n",
            "train - step 3207: loss = 317108.53 (1.667 sec)\n",
            "train - step 3208: loss = 388434.94 (1.685 sec)\n",
            "train - step 3209: loss = 426460.94 (1.662 sec)\n",
            "train - step 3210: loss = 392413.00 (1.683 sec)\n",
            "train - step 3211: loss = 416098.88 (1.676 sec)\n",
            "train - step 3212: loss = 314656.91 (1.699 sec)\n",
            "train - step 3213: loss = 406378.78 (1.689 sec)\n",
            "train - step 3214: loss = 350908.62 (1.680 sec)\n",
            "train - step 3215: loss = 558537.00 (2.987 sec)\n",
            "train - step 3216: loss = 403071.88 (1.695 sec)\n",
            "train - step 3217: loss = 336706.47 (1.684 sec)\n",
            "train - step 3218: loss = 642935.31 (1.653 sec)\n",
            "train - step 3219: loss = 444667.06 (1.688 sec)\n",
            "train - step 3220: loss = 352577.06 (1.645 sec)\n",
            "train - step 3221: loss = 372206.47 (1.660 sec)\n",
            "train - step 3222: loss = 532252.56 (1.680 sec)\n",
            "train - step 3223: loss = 488780.34 (1.687 sec)\n",
            "train - step 3224: loss = 630501.19 (1.685 sec)\n",
            "train - step 3225: loss = 449084.22 (1.673 sec)\n",
            "train - step 3226: loss = 435258.88 (1.703 sec)\n",
            "train - step 3227: loss = 403687.16 (1.684 sec)\n",
            "train - step 3228: loss = 452606.41 (1.689 sec)\n",
            "train - step 3229: loss = 525853.69 (1.687 sec)\n",
            "train - step 3230: loss = 279814.56 (1.726 sec)\n",
            "train - step 3231: loss = 399599.72 (1.667 sec)\n",
            "train - step 3232: loss = 406415.22 (1.698 sec)\n",
            "train - step 3233: loss = 547185.19 (1.694 sec)\n",
            "train - step 3234: loss = 395357.00 (1.695 sec)\n",
            "train - step 3235: loss = 319546.81 (1.680 sec)\n",
            "train - step 3236: loss = 470097.97 (1.691 sec)\n",
            "train - step 3237: loss = 450583.34 (1.711 sec)\n",
            "train - step 3238: loss = 372304.12 (1.718 sec)\n",
            "train - step 3239: loss = 360241.19 (1.699 sec)\n",
            "train - step 3240: loss = 493610.28 (1.706 sec)\n",
            "train - step 3241: loss = 448177.91 (1.694 sec)\n",
            "train - step 3242: loss = 483289.47 (1.681 sec)\n",
            "train - step 3243: loss = 433203.06 (1.694 sec)\n",
            "train - step 3244: loss = 429427.47 (1.680 sec)\n",
            "train - step 3245: loss = 577066.75 (1.685 sec)\n",
            "train - step 3246: loss = 417839.62 (1.663 sec)\n",
            "train - step 3247: loss = 373456.09 (1.695 sec)\n",
            "train - step 3248: loss = 264452.72 (1.671 sec)\n",
            "train - step 3249: loss = 516442.44 (1.685 sec)\n",
            "train - step 3250: loss = 555207.94 (1.703 sec)\n",
            "train - step 3251: loss = 443874.34 (2.501 sec)\n",
            "train - step 3252: loss = 358493.22 (1.671 sec)\n",
            "train - step 3253: loss = 296927.84 (1.695 sec)\n",
            "train - step 3254: loss = 409427.38 (1.676 sec)\n",
            "train - step 3255: loss = 733981.88 (1.707 sec)\n",
            "train - step 3256: loss = 461313.28 (1.681 sec)\n",
            "train - step 3257: loss = 350577.16 (1.677 sec)\n",
            "train - step 3258: loss = 380216.44 (1.692 sec)\n",
            "train - step 3259: loss = 438114.06 (1.685 sec)\n",
            "train - step 3260: loss = 339443.34 (1.670 sec)\n",
            "train - step 3261: loss = 579047.00 (1.678 sec)\n",
            "train - step 3262: loss = 561092.69 (1.668 sec)\n",
            "train - step 3263: loss = 555507.38 (1.687 sec)\n",
            "train - step 3264: loss = 353865.47 (1.661 sec)\n",
            "train - step 3265: loss = 361595.97 (1.684 sec)\n",
            "train - step 3266: loss = 499493.41 (1.702 sec)\n",
            "train - step 3267: loss = 396896.47 (1.674 sec)\n",
            "train - step 3268: loss = 462931.62 (1.661 sec)\n",
            "train - step 3269: loss = 563405.38 (1.692 sec)\n",
            "train - step 3270: loss = 400334.09 (1.677 sec)\n",
            "train - step 3271: loss = 438979.12 (1.699 sec)\n",
            "train - step 3272: loss = 553704.44 (1.693 sec)\n",
            "train - step 3273: loss = 242852.84 (1.698 sec)\n",
            "train - step 3274: loss = 571886.31 (1.686 sec)\n",
            "train - step 3275: loss = 443720.44 (1.697 sec)\n",
            "train - step 3276: loss = 493311.34 (1.696 sec)\n",
            "train - step 3277: loss = 550252.12 (1.721 sec)\n",
            "train - step 3278: loss = 539443.06 (1.706 sec)\n",
            "train - step 3279: loss = 408727.06 (1.708 sec)\n",
            "train - step 3280: loss = 337068.62 (1.672 sec)\n",
            "train - step 3281: loss = 477378.31 (1.656 sec)\n",
            "train - step 3282: loss = 420878.03 (1.662 sec)\n",
            "train - step 3283: loss = 467366.66 (1.709 sec)\n",
            "train - step 3284: loss = 501379.81 (1.665 sec)\n",
            "train - step 3285: loss = 541083.69 (1.699 sec)\n",
            "train - step 3286: loss = 379260.53 (1.655 sec)\n",
            "train - step 3287: loss = 502006.00 (2.490 sec)\n",
            "train - step 3288: loss = 511123.03 (1.699 sec)\n",
            "train - step 3289: loss = 566539.31 (1.686 sec)\n",
            "train - step 3290: loss = 490492.06 (1.708 sec)\n",
            "train - step 3291: loss = 373190.69 (1.664 sec)\n",
            "train - step 3292: loss = 584608.81 (1.706 sec)\n",
            "train - step 3293: loss = 341148.84 (1.672 sec)\n",
            "train - step 3294: loss = 352968.72 (1.685 sec)\n",
            "train - step 3295: loss = 548164.12 (1.674 sec)\n",
            "train - step 3296: loss = 499316.00 (1.684 sec)\n",
            "train - step 3297: loss = 460081.72 (1.668 sec)\n",
            "train - step 3298: loss = 462353.66 (1.653 sec)\n",
            "train - step 3299: loss = 512722.06 (1.721 sec)\n",
            "train - step 3300: loss = 407507.88 (1.677 sec)\n",
            "train - step 3301: loss = 298199.66 (1.679 sec)\n",
            "train - step 3302: loss = 463028.03 (1.668 sec)\n",
            "train - step 3303: loss = 306530.66 (1.698 sec)\n",
            "train - step 3304: loss = 467820.06 (1.671 sec)\n",
            "train - step 3305: loss = 461260.59 (1.669 sec)\n",
            "train - step 3306: loss = 247936.17 (1.665 sec)\n",
            "train - step 3307: loss = 590533.69 (1.675 sec)\n",
            "train - step 3308: loss = 450435.19 (1.702 sec)\n",
            "train - step 3309: loss = 331796.59 (1.674 sec)\n",
            "train - step 3310: loss = 388229.88 (1.681 sec)\n",
            "train - step 3311: loss = 370644.22 (1.662 sec)\n",
            "train - step 3312: loss = 505143.34 (1.660 sec)\n",
            "train - step 3313: loss = 350623.19 (1.677 sec)\n",
            "train - step 3314: loss = 473972.72 (1.676 sec)\n",
            "train - step 3315: loss = 253657.70 (1.684 sec)\n",
            "train - step 3316: loss = 299673.59 (1.663 sec)\n",
            "train - step 3317: loss = 440998.34 (1.682 sec)\n",
            "train - step 3318: loss = 223291.59 (1.683 sec)\n",
            "train - step 3319: loss = 448568.69 (1.679 sec)\n",
            "train - step 3320: loss = 425197.09 (1.686 sec)\n",
            "train - step 3321: loss = 543319.38 (1.678 sec)\n",
            "train - step 3322: loss = 673931.31 (1.699 sec)\n",
            "train - step 3323: loss = 437661.59 (2.469 sec)\n",
            "train - step 3324: loss = 370058.12 (1.701 sec)\n",
            "train - step 3325: loss = 548109.06 (1.669 sec)\n",
            "train - step 3326: loss = 441341.88 (1.694 sec)\n",
            "train - step 3327: loss = 389296.59 (1.665 sec)\n",
            "train - step 3328: loss = 468808.47 (1.672 sec)\n",
            "train - step 3329: loss = 508128.34 (1.669 sec)\n",
            "train - step 3330: loss = 428925.53 (1.661 sec)\n",
            "train - step 3331: loss = 391448.22 (1.703 sec)\n",
            "train - step 3332: loss = 501802.50 (1.686 sec)\n",
            "train - step 3333: loss = 516224.72 (1.710 sec)\n",
            "train - step 3334: loss = 351647.94 (1.705 sec)\n",
            "train - step 3335: loss = 388268.88 (1.690 sec)\n",
            "train - step 3336: loss = 424600.84 (1.699 sec)\n",
            "train - step 3337: loss = 521385.72 (1.714 sec)\n",
            "train - step 3338: loss = 426750.38 (1.664 sec)\n",
            "train - step 3339: loss = 686592.00 (1.689 sec)\n",
            "train - step 3340: loss = 500994.66 (1.699 sec)\n",
            "train - step 3341: loss = 509394.44 (1.686 sec)\n",
            "train - step 3342: loss = 485708.97 (1.684 sec)\n",
            "train - step 3343: loss = 449126.28 (1.677 sec)\n",
            "train - step 3344: loss = 573676.31 (1.678 sec)\n",
            "train - step 3345: loss = 214377.67 (1.662 sec)\n",
            "train - step 3346: loss = 365827.44 (1.682 sec)\n",
            "train - step 3347: loss = 507201.59 (1.673 sec)\n",
            "train - step 3348: loss = 354216.28 (1.690 sec)\n",
            "train - step 3349: loss = 667269.56 (1.673 sec)\n",
            "train - step 3350: loss = 477650.59 (1.675 sec)\n",
            "train - step 3351: loss = 369798.72 (1.662 sec)\n",
            "train - step 3352: loss = 518689.31 (1.643 sec)\n",
            "train - step 3353: loss = 499855.88 (1.651 sec)\n",
            "train - step 3354: loss = 524256.97 (1.643 sec)\n",
            "train - step 3355: loss = 348758.66 (1.664 sec)\n",
            "train - step 3356: loss = 419697.81 (1.679 sec)\n",
            "train - step 3357: loss = 375329.81 (1.665 sec)\n",
            "train - step 3358: loss = 423543.09 (1.693 sec)\n",
            "train - step 3359: loss = 399314.97 (2.587 sec)\n",
            "train - step 3360: loss = 394214.59 (1.701 sec)\n",
            "train - step 3361: loss = 501150.94 (1.669 sec)\n",
            "train - step 3362: loss = 531332.44 (1.628 sec)\n",
            "train - step 3363: loss = 538236.94 (1.657 sec)\n",
            "train - step 3364: loss = 459869.41 (1.657 sec)\n",
            "train - step 3365: loss = 354919.62 (1.653 sec)\n",
            "train - step 3366: loss = 477266.88 (1.657 sec)\n",
            "train - step 3367: loss = 370010.88 (1.624 sec)\n",
            "train - step 3368: loss = 403628.97 (1.678 sec)\n",
            "train - step 3369: loss = 341151.28 (1.651 sec)\n",
            "train - step 3370: loss = 462444.97 (1.680 sec)\n",
            "train - step 3371: loss = 546997.12 (1.678 sec)\n",
            "train - step 3372: loss = 534418.50 (1.680 sec)\n",
            "train - step 3373: loss = 438262.91 (1.692 sec)\n",
            "train - step 3374: loss = 464991.44 (1.685 sec)\n",
            "train - step 3375: loss = 447854.03 (1.674 sec)\n",
            "train - step 3376: loss = 328311.62 (1.686 sec)\n",
            "train - step 3377: loss = 342618.16 (1.660 sec)\n",
            "train - step 3378: loss = 539461.81 (1.671 sec)\n",
            "train - step 3379: loss = 520629.09 (1.688 sec)\n",
            "train - step 3380: loss = 343645.91 (1.679 sec)\n",
            "train - step 3381: loss = 351114.19 (1.703 sec)\n",
            "train - step 3382: loss = 431998.19 (1.704 sec)\n",
            "train - step 3383: loss = 474530.72 (1.660 sec)\n",
            "train - step 3384: loss = 556439.19 (1.678 sec)\n",
            "train - step 3385: loss = 401620.34 (1.662 sec)\n",
            "train - step 3386: loss = 487150.59 (1.682 sec)\n",
            "train - step 3387: loss = 411737.44 (1.669 sec)\n",
            "train - step 3388: loss = 537485.62 (1.678 sec)\n",
            "train - step 3389: loss = 427218.81 (1.690 sec)\n",
            "train - step 3390: loss = 513100.09 (1.694 sec)\n",
            "train - step 3391: loss = 380316.47 (1.686 sec)\n",
            "train - step 3392: loss = 422805.22 (1.668 sec)\n",
            "train - step 3393: loss = 482418.00 (1.696 sec)\n",
            "train - step 3394: loss = 618122.38 (1.666 sec)\n",
            "train - step 3395: loss = 422484.41 (2.518 sec)\n",
            "train - step 3396: loss = 298613.94 (1.687 sec)\n",
            "train - step 3397: loss = 455280.38 (1.663 sec)\n",
            "train - step 3398: loss = 419870.16 (1.685 sec)\n",
            "train - step 3399: loss = 513676.38 (1.662 sec)\n",
            "train - step 3400: loss = 356855.72 (1.677 sec)\n",
            "train - step 3401: loss = 465457.19 (1.670 sec)\n",
            "train - step 3402: loss = 492696.69 (1.673 sec)\n",
            "train - step 3403: loss = 465062.81 (1.705 sec)\n",
            "train - step 3404: loss = 400645.09 (1.664 sec)\n",
            "train - step 3405: loss = 341558.78 (1.710 sec)\n",
            "train - step 3406: loss = 456107.78 (1.676 sec)\n",
            "train - step 3407: loss = 388950.28 (1.664 sec)\n",
            "train - step 3408: loss = 292911.06 (1.658 sec)\n",
            "train - step 3409: loss = 442507.47 (1.670 sec)\n",
            "train - step 3410: loss = 525676.19 (1.659 sec)\n",
            "train - step 3411: loss = 552365.75 (1.680 sec)\n",
            "train - step 3412: loss = 384416.41 (1.669 sec)\n",
            "train - step 3413: loss = 453468.62 (1.662 sec)\n",
            "train - step 3414: loss = 546138.31 (1.691 sec)\n",
            "train - step 3415: loss = 418247.72 (1.666 sec)\n",
            "train - step 3416: loss = 317811.00 (1.721 sec)\n",
            "train - step 3417: loss = 455822.53 (1.659 sec)\n",
            "train - step 3418: loss = 452396.62 (1.659 sec)\n",
            "train - step 3419: loss = 441964.66 (1.670 sec)\n",
            "train - step 3420: loss = 429467.31 (1.646 sec)\n",
            "train - step 3421: loss = 547357.62 (1.695 sec)\n",
            "train - step 3422: loss = 577701.00 (1.664 sec)\n",
            "train - step 3423: loss = 471357.16 (1.663 sec)\n",
            "train - step 3424: loss = 315829.41 (1.654 sec)\n",
            "train - step 3425: loss = 404588.72 (1.674 sec)\n",
            "train - step 3426: loss = 494663.62 (1.669 sec)\n",
            "train - step 3427: loss = 474212.72 (1.656 sec)\n",
            "train - step 3428: loss = 414660.41 (1.681 sec)\n",
            "train - step 3429: loss = 469681.47 (1.691 sec)\n",
            "train - step 3430: loss = 552968.56 (1.702 sec)\n",
            "train - step 3431: loss = 316590.47 (2.512 sec)\n",
            "train - step 3432: loss = 462358.47 (1.692 sec)\n",
            "train - step 3433: loss = 497623.53 (1.680 sec)\n",
            "train - step 3434: loss = 386568.41 (1.678 sec)\n",
            "train - step 3435: loss = 503015.03 (1.708 sec)\n",
            "train - step 3436: loss = 300790.16 (1.653 sec)\n",
            "train - step 3437: loss = 466921.12 (1.698 sec)\n",
            "train - step 3438: loss = 334241.53 (1.689 sec)\n",
            "train - step 3439: loss = 512340.31 (1.663 sec)\n",
            "train - step 3440: loss = 301372.66 (1.664 sec)\n",
            "train - step 3441: loss = 299223.97 (1.661 sec)\n",
            "train - step 3442: loss = 377276.84 (1.655 sec)\n",
            "train - step 3443: loss = 525856.25 (1.662 sec)\n",
            "train - step 3444: loss = 555515.75 (1.679 sec)\n",
            "train - step 3445: loss = 434072.62 (1.673 sec)\n",
            "train - step 3446: loss = 532816.94 (1.675 sec)\n",
            "train - step 3447: loss = 417184.72 (1.655 sec)\n",
            "train - step 3448: loss = 245043.53 (1.665 sec)\n",
            "train - step 3449: loss = 393129.56 (1.679 sec)\n",
            "train - step 3450: loss = 500326.34 (1.685 sec)\n",
            "train - step 3451: loss = 407444.59 (1.670 sec)\n",
            "train - step 3452: loss = 361250.56 (1.678 sec)\n",
            "train - step 3453: loss = 567627.31 (1.701 sec)\n",
            "train - step 3454: loss = 431229.84 (1.665 sec)\n",
            "train - step 3455: loss = 441458.47 (1.672 sec)\n",
            "train - step 3456: loss = 497269.69 (1.667 sec)\n",
            "train - step 3457: loss = 432038.31 (1.688 sec)\n",
            "train - step 3458: loss = 365764.78 (1.656 sec)\n",
            "train - step 3459: loss = 530964.88 (1.688 sec)\n",
            "train - step 3460: loss = 444898.78 (1.681 sec)\n",
            "train - step 3461: loss = 544283.06 (1.668 sec)\n",
            "train - step 3462: loss = 411249.06 (1.665 sec)\n",
            "train - step 3463: loss = 524180.22 (1.656 sec)\n",
            "train - step 3464: loss = 447153.19 (1.683 sec)\n",
            "train - step 3465: loss = 419984.91 (1.656 sec)\n",
            "train - step 3466: loss = 348799.22 (1.701 sec)\n",
            "train - step 3467: loss = 439032.59 (2.743 sec)\n",
            "train - step 3468: loss = 477010.69 (1.743 sec)\n",
            "train - step 3469: loss = 401298.81 (1.696 sec)\n",
            "train - step 3470: loss = 548868.31 (1.665 sec)\n",
            "train - step 3471: loss = 374172.62 (1.710 sec)\n",
            "train - step 3472: loss = 537806.56 (1.701 sec)\n",
            "train - step 3473: loss = 317233.91 (1.692 sec)\n",
            "train - step 3474: loss = 408946.09 (1.682 sec)\n",
            "train - step 3475: loss = 308237.88 (1.683 sec)\n",
            "train - step 3476: loss = 437742.28 (1.662 sec)\n",
            "train - step 3477: loss = 342959.19 (1.717 sec)\n",
            "train - step 3478: loss = 347667.53 (1.702 sec)\n",
            "train - step 3479: loss = 530291.94 (1.674 sec)\n",
            "train - step 3480: loss = 457798.78 (1.687 sec)\n",
            "train - step 3481: loss = 471349.28 (1.805 sec)\n",
            "train - step 3482: loss = 454560.31 (1.653 sec)\n",
            "train - step 3483: loss = 483593.44 (1.688 sec)\n",
            "train - step 3484: loss = 597659.75 (1.672 sec)\n",
            "train - step 3485: loss = 334395.50 (1.665 sec)\n",
            "train - step 3486: loss = 406619.12 (1.687 sec)\n",
            "train - step 3487: loss = 559704.25 (1.686 sec)\n",
            "train - step 3488: loss = 527609.19 (1.690 sec)\n",
            "train - step 3489: loss = 556694.94 (1.686 sec)\n",
            "train - step 3490: loss = 609439.00 (1.665 sec)\n",
            "train - step 3491: loss = 338276.34 (1.684 sec)\n",
            "train - step 3492: loss = 414954.12 (1.682 sec)\n",
            "train - step 3493: loss = 537280.88 (1.687 sec)\n",
            "train - step 3494: loss = 365859.59 (1.668 sec)\n",
            "train - step 3495: loss = 472998.88 (1.701 sec)\n",
            "train - step 3496: loss = 370306.84 (1.658 sec)\n",
            "train - step 3497: loss = 511427.47 (1.712 sec)\n",
            "train - step 3498: loss = 332249.09 (1.667 sec)\n",
            "train - step 3499: loss = 506468.44 (1.690 sec)\n",
            "train - step 3500: loss = 535444.12 (1.662 sec)\n",
            "train - step 3501: loss = 340014.88 (1.649 sec)\n",
            "train - step 3502: loss = 365991.31 (1.673 sec)\n",
            "train - step 3503: loss = 511652.28 (2.544 sec)\n",
            "train - step 3504: loss = 455485.00 (1.693 sec)\n",
            "train - step 3505: loss = 449115.03 (1.677 sec)\n",
            "train - step 3506: loss = 482869.28 (1.679 sec)\n",
            "train - step 3507: loss = 520809.66 (1.699 sec)\n",
            "train - step 3508: loss = 411396.81 (1.675 sec)\n",
            "train - step 3509: loss = 457725.47 (1.718 sec)\n",
            "train - step 3510: loss = 313255.16 (1.690 sec)\n",
            "train - step 3511: loss = 318872.09 (1.692 sec)\n",
            "train - step 3512: loss = 532907.69 (1.687 sec)\n",
            "train - step 3513: loss = 404619.94 (1.666 sec)\n",
            "train - step 3514: loss = 439329.12 (1.674 sec)\n",
            "train - step 3515: loss = 531493.69 (1.653 sec)\n",
            "train - step 3516: loss = 452011.50 (1.696 sec)\n",
            "train - step 3517: loss = 414555.53 (1.662 sec)\n",
            "train - step 3518: loss = 439301.34 (1.716 sec)\n",
            "train - step 3519: loss = 494314.84 (1.675 sec)\n",
            "train - step 3520: loss = 530442.19 (1.684 sec)\n",
            "train - step 3521: loss = 487682.44 (1.669 sec)\n",
            "train - step 3522: loss = 395542.88 (1.659 sec)\n",
            "train - step 3523: loss = 354442.47 (1.664 sec)\n",
            "train - step 3524: loss = 316311.50 (1.667 sec)\n",
            "train - step 3525: loss = 369150.66 (1.702 sec)\n",
            "train - step 3526: loss = 345896.59 (1.688 sec)\n",
            "train - step 3527: loss = 517296.34 (1.670 sec)\n",
            "train - step 3528: loss = 342097.22 (1.676 sec)\n",
            "train - step 3529: loss = 403984.53 (1.675 sec)\n",
            "train - step 3530: loss = 497543.28 (1.677 sec)\n",
            "train - step 3531: loss = 456362.06 (1.663 sec)\n",
            "train - step 3532: loss = 444116.06 (1.643 sec)\n",
            "train - step 3533: loss = 443488.97 (1.650 sec)\n",
            "train - step 3534: loss = 415867.88 (1.701 sec)\n",
            "train - step 3535: loss = 442278.00 (1.707 sec)\n",
            "train - step 3536: loss = 514813.59 (1.680 sec)\n",
            "train - step 3537: loss = 415283.97 (1.712 sec)\n",
            "train - step 3538: loss = 277622.47 (1.704 sec)\n",
            "train - step 3539: loss = 311239.28 (2.552 sec)\n",
            "train - step 3540: loss = 358198.59 (1.676 sec)\n",
            "train - step 3541: loss = 510592.56 (1.687 sec)\n",
            "train - step 3542: loss = 414540.41 (1.699 sec)\n",
            "train - step 3543: loss = 572626.12 (1.680 sec)\n",
            "train - step 3544: loss = 421240.69 (1.633 sec)\n",
            "train - step 3545: loss = 410484.97 (1.640 sec)\n",
            "train - step 3546: loss = 577877.56 (1.638 sec)\n",
            "train - step 3547: loss = 372150.31 (1.666 sec)\n",
            "train - step 3548: loss = 501431.19 (1.655 sec)\n",
            "train - step 3549: loss = 317893.81 (1.669 sec)\n",
            "train - step 3550: loss = 546044.00 (1.682 sec)\n",
            "train - step 3551: loss = 293654.38 (1.669 sec)\n",
            "train - step 3552: loss = 400866.16 (1.683 sec)\n",
            "train - step 3553: loss = 382233.06 (1.690 sec)\n",
            "train - step 3554: loss = 357281.22 (1.686 sec)\n",
            "train - step 3555: loss = 305416.78 (1.679 sec)\n",
            "train - step 3556: loss = 332113.12 (1.685 sec)\n",
            "train - step 3557: loss = 508384.59 (1.691 sec)\n",
            "train - step 3558: loss = 515492.31 (1.689 sec)\n",
            "train - step 3559: loss = 384484.06 (1.661 sec)\n",
            "train - step 3560: loss = 474007.59 (1.707 sec)\n",
            "train - step 3561: loss = 510136.47 (1.677 sec)\n",
            "train - step 3562: loss = 635118.38 (1.657 sec)\n",
            "train - step 3563: loss = 338303.19 (1.668 sec)\n",
            "train - step 3564: loss = 596079.75 (1.669 sec)\n",
            "train - step 3565: loss = 576504.56 (1.677 sec)\n",
            "train - step 3566: loss = 618641.81 (1.655 sec)\n",
            "train - step 3567: loss = 527254.62 (1.684 sec)\n",
            "train - step 3568: loss = 417272.16 (1.677 sec)\n",
            "train - step 3569: loss = 404336.84 (1.683 sec)\n",
            "train - step 3570: loss = 420972.41 (1.681 sec)\n",
            "train - step 3571: loss = 335244.47 (1.655 sec)\n",
            "train - step 3572: loss = 540238.56 (1.671 sec)\n",
            "train - step 3573: loss = 403533.31 (1.649 sec)\n",
            "train - step 3574: loss = 435554.28 (1.675 sec)\n",
            "train - step 3575: loss = 395874.22 (2.806 sec)\n",
            "train - step 3576: loss = 403009.66 (1.687 sec)\n",
            "train - step 3577: loss = 384294.72 (1.657 sec)\n",
            "train - step 3578: loss = 518208.19 (1.696 sec)\n",
            "train - step 3579: loss = 314153.69 (1.676 sec)\n",
            "train - step 3580: loss = 370442.03 (1.670 sec)\n",
            "train - step 3581: loss = 329709.84 (1.686 sec)\n",
            "train - step 3582: loss = 378750.22 (1.660 sec)\n",
            "train - step 3583: loss = 412977.59 (1.678 sec)\n",
            "train - step 3584: loss = 416377.62 (1.699 sec)\n",
            "train - step 3585: loss = 529743.25 (1.694 sec)\n",
            "train - step 3586: loss = 348314.84 (1.667 sec)\n",
            "train - step 3587: loss = 402433.53 (1.674 sec)\n",
            "train - step 3588: loss = 431924.62 (1.681 sec)\n",
            "train - step 3589: loss = 523018.22 (1.700 sec)\n",
            "train - step 3590: loss = 399774.31 (1.667 sec)\n",
            "train - step 3591: loss = 505581.19 (1.678 sec)\n",
            "train - step 3592: loss = 520957.28 (1.684 sec)\n",
            "train - step 3593: loss = 397549.09 (1.673 sec)\n",
            "train - step 3594: loss = 609949.81 (1.698 sec)\n",
            "train - step 3595: loss = 318023.94 (1.687 sec)\n",
            "train - step 3596: loss = 490126.00 (1.672 sec)\n",
            "train - step 3597: loss = 544763.75 (1.676 sec)\n",
            "train - step 3598: loss = 349139.81 (1.661 sec)\n",
            "train - step 3599: loss = 457668.50 (1.662 sec)\n",
            "train - step 3600: loss = 415234.06 (1.682 sec)\n",
            "train - step 3601: loss = 434812.06 (1.708 sec)\n",
            "train - step 3602: loss = 521883.69 (1.699 sec)\n",
            "train - step 3603: loss = 449569.66 (1.658 sec)\n",
            "train - step 3604: loss = 393269.09 (1.678 sec)\n",
            "train - step 3605: loss = 455935.34 (1.715 sec)\n",
            "train - step 3606: loss = 382367.31 (1.656 sec)\n",
            "train - step 3607: loss = 453181.53 (1.685 sec)\n",
            "train - step 3608: loss = 425191.06 (1.664 sec)\n",
            "train - step 3609: loss = 494476.41 (1.672 sec)\n",
            "train - step 3610: loss = 628197.56 (1.713 sec)\n",
            "train - step 3611: loss = 444599.09 (2.466 sec)\n",
            "train - step 3612: loss = 516650.72 (1.690 sec)\n",
            "train - step 3613: loss = 385678.53 (1.681 sec)\n",
            "train - step 3614: loss = 433724.94 (1.666 sec)\n",
            "train - step 3615: loss = 299495.03 (1.672 sec)\n",
            "train - step 3616: loss = 465825.34 (1.672 sec)\n",
            "train - step 3617: loss = 489762.00 (1.670 sec)\n",
            "train - step 3618: loss = 390991.62 (1.674 sec)\n",
            "train - step 3619: loss = 472273.09 (1.671 sec)\n",
            "train - step 3620: loss = 466771.53 (1.668 sec)\n",
            "train - step 3621: loss = 406357.12 (1.684 sec)\n",
            "train - step 3622: loss = 434886.69 (1.675 sec)\n",
            "train - step 3623: loss = 519152.47 (1.663 sec)\n",
            "train - step 3624: loss = 377592.53 (1.689 sec)\n",
            "train - step 3625: loss = 368181.66 (1.686 sec)\n",
            "train - step 3626: loss = 363153.47 (1.703 sec)\n",
            "train - step 3627: loss = 382069.78 (1.665 sec)\n",
            "train - step 3628: loss = 392832.19 (1.687 sec)\n",
            "train - step 3629: loss = 446035.22 (1.684 sec)\n",
            "train - step 3630: loss = 439658.59 (1.688 sec)\n",
            "train - step 3631: loss = 452898.34 (1.683 sec)\n",
            "train - step 3632: loss = 514605.91 (1.652 sec)\n",
            "train - step 3633: loss = 460605.28 (1.685 sec)\n",
            "train - step 3634: loss = 288338.22 (1.672 sec)\n",
            "train - step 3635: loss = 511997.66 (1.658 sec)\n",
            "train - step 3636: loss = 410672.94 (1.661 sec)\n",
            "train - step 3637: loss = 355241.50 (1.651 sec)\n",
            "train - step 3638: loss = 454718.34 (1.650 sec)\n",
            "train - step 3639: loss = 581161.00 (1.654 sec)\n",
            "train - step 3640: loss = 369037.50 (1.663 sec)\n",
            "train - step 3641: loss = 460764.72 (1.669 sec)\n",
            "train - step 3642: loss = 380337.12 (1.633 sec)\n",
            "train - step 3643: loss = 339345.00 (1.641 sec)\n",
            "train - step 3644: loss = 376650.56 (1.657 sec)\n",
            "train - step 3645: loss = 465941.09 (1.646 sec)\n",
            "train - step 3646: loss = 360254.28 (1.651 sec)\n",
            "train - step 3647: loss = 473531.50 (2.816 sec)\n",
            "train - step 3648: loss = 444897.28 (1.669 sec)\n",
            "train - step 3649: loss = 467094.62 (1.658 sec)\n",
            "train - step 3650: loss = 369329.84 (1.657 sec)\n",
            "train - step 3651: loss = 219839.80 (1.641 sec)\n",
            "train - step 3652: loss = 358987.72 (1.671 sec)\n",
            "train - step 3653: loss = 330924.09 (1.659 sec)\n",
            "train - step 3654: loss = 478052.00 (1.653 sec)\n",
            "train - step 3655: loss = 338370.03 (1.679 sec)\n",
            "train - step 3656: loss = 448663.81 (1.652 sec)\n",
            "train - step 3657: loss = 339553.84 (1.675 sec)\n",
            "train - step 3658: loss = 492282.88 (1.656 sec)\n",
            "train - step 3659: loss = 411114.81 (1.673 sec)\n",
            "train - step 3660: loss = 303181.09 (1.653 sec)\n",
            "train - step 3661: loss = 443391.81 (1.665 sec)\n",
            "train - step 3662: loss = 371273.59 (1.655 sec)\n",
            "train - step 3663: loss = 456069.16 (1.665 sec)\n",
            "train - step 3664: loss = 596013.88 (1.655 sec)\n",
            "train - step 3665: loss = 399368.69 (1.677 sec)\n",
            "train - step 3666: loss = 366863.78 (1.655 sec)\n",
            "train - step 3667: loss = 327831.50 (1.682 sec)\n",
            "train - step 3668: loss = 491061.41 (1.659 sec)\n",
            "train - step 3669: loss = 470562.69 (1.643 sec)\n",
            "train - step 3670: loss = 531803.88 (1.673 sec)\n",
            "train - step 3671: loss = 399637.03 (1.676 sec)\n",
            "train - step 3672: loss = 579350.56 (1.648 sec)\n",
            "train - step 3673: loss = 356519.88 (1.675 sec)\n",
            "train - step 3674: loss = 484042.00 (1.662 sec)\n",
            "train - step 3675: loss = 426146.81 (1.660 sec)\n",
            "train - step 3676: loss = 625807.88 (1.676 sec)\n",
            "train - step 3677: loss = 317062.22 (1.648 sec)\n",
            "train - step 3678: loss = 331832.56 (1.663 sec)\n",
            "train - step 3679: loss = 351947.06 (1.664 sec)\n",
            "train - step 3680: loss = 320058.88 (1.657 sec)\n",
            "train - step 3681: loss = 350019.22 (1.685 sec)\n",
            "train - step 3682: loss = 356918.28 (1.654 sec)\n",
            "train - step 3683: loss = 423185.22 (2.516 sec)\n",
            "train - step 3684: loss = 467813.91 (1.657 sec)\n",
            "train - step 3685: loss = 512711.81 (1.698 sec)\n",
            "train - step 3686: loss = 443308.03 (1.664 sec)\n",
            "train - step 3687: loss = 416408.88 (1.666 sec)\n",
            "train - step 3688: loss = 506916.31 (1.681 sec)\n",
            "train - step 3689: loss = 394973.97 (1.671 sec)\n",
            "train - step 3690: loss = 390430.22 (1.661 sec)\n",
            "train - step 3691: loss = 502657.41 (1.654 sec)\n",
            "train - step 3692: loss = 624330.81 (1.645 sec)\n",
            "train - step 3693: loss = 182333.61 (1.680 sec)\n",
            "train - step 3694: loss = 372543.12 (1.647 sec)\n",
            "train - step 3695: loss = 385035.47 (1.664 sec)\n",
            "train - step 3696: loss = 355483.81 (1.671 sec)\n",
            "train - step 3697: loss = 395949.88 (1.661 sec)\n",
            "train - step 3698: loss = 444163.00 (1.669 sec)\n",
            "train - step 3699: loss = 431720.09 (1.672 sec)\n",
            "train - step 3700: loss = 251777.94 (1.666 sec)\n",
            "train - step 3701: loss = 404789.91 (1.661 sec)\n",
            "train - step 3702: loss = 357312.12 (1.653 sec)\n",
            "train - step 3703: loss = 508726.47 (1.654 sec)\n",
            "train - step 3704: loss = 514971.22 (1.660 sec)\n",
            "train - step 3705: loss = 346867.16 (1.681 sec)\n",
            "train - step 3706: loss = 538458.62 (1.673 sec)\n",
            "train - step 3707: loss = 440180.41 (1.668 sec)\n",
            "train - step 3708: loss = 457933.22 (1.678 sec)\n",
            "train - step 3709: loss = 515771.59 (1.676 sec)\n",
            "train - step 3710: loss = 440093.66 (1.685 sec)\n",
            "train - step 3711: loss = 333580.84 (1.693 sec)\n",
            "train - step 3712: loss = 590164.31 (1.630 sec)\n",
            "train - step 3713: loss = 559737.75 (1.642 sec)\n",
            "train - step 3714: loss = 354031.47 (1.638 sec)\n",
            "train - step 3715: loss = 371514.06 (1.691 sec)\n",
            "train - step 3716: loss = 466561.19 (1.651 sec)\n",
            "train - step 3717: loss = 367743.00 (1.668 sec)\n",
            "train - step 3718: loss = 460612.47 (1.662 sec)\n",
            "train - step 3719: loss = 450410.97 (2.566 sec)\n",
            "train - step 3720: loss = 350696.56 (1.675 sec)\n",
            "train - step 3721: loss = 442775.06 (1.680 sec)\n",
            "train - step 3722: loss = 541441.75 (1.691 sec)\n",
            "train - step 3723: loss = 283911.16 (1.694 sec)\n",
            "train - step 3724: loss = 512926.66 (1.654 sec)\n",
            "train - step 3725: loss = 451855.72 (1.668 sec)\n",
            "train - step 3726: loss = 368172.38 (1.662 sec)\n",
            "train - step 3727: loss = 523786.41 (1.651 sec)\n",
            "train - step 3728: loss = 385177.12 (1.651 sec)\n",
            "train - step 3729: loss = 428399.44 (1.646 sec)\n",
            "train - step 3730: loss = 413922.88 (1.656 sec)\n",
            "train - step 3731: loss = 390998.78 (1.664 sec)\n",
            "train - step 3732: loss = 334768.88 (1.652 sec)\n",
            "train - step 3733: loss = 437421.28 (1.683 sec)\n",
            "train - step 3734: loss = 437472.47 (1.685 sec)\n",
            "train - step 3735: loss = 301748.06 (1.696 sec)\n",
            "train - step 3736: loss = 481871.28 (1.675 sec)\n",
            "train - step 3737: loss = 590825.69 (1.669 sec)\n",
            "train - step 3738: loss = 471310.50 (1.677 sec)\n",
            "train - step 3739: loss = 457858.59 (1.672 sec)\n",
            "train - step 3740: loss = 393677.94 (1.680 sec)\n",
            "train - step 3741: loss = 483336.47 (1.653 sec)\n",
            "train - step 3742: loss = 542888.00 (1.638 sec)\n",
            "train - step 3743: loss = 481701.44 (1.676 sec)\n",
            "train - step 3744: loss = 427567.72 (1.672 sec)\n",
            "train - step 3745: loss = 359128.16 (1.658 sec)\n",
            "train - step 3746: loss = 420799.31 (1.680 sec)\n",
            "train - step 3747: loss = 534253.38 (1.643 sec)\n",
            "train - step 3748: loss = 344426.22 (1.670 sec)\n",
            "train - step 3749: loss = 374800.84 (1.666 sec)\n",
            "train - step 3750: loss = 459077.97 (1.670 sec)\n",
            "train - step 3751: loss = 386795.12 (1.663 sec)\n",
            "train - step 3752: loss = 445219.56 (1.654 sec)\n",
            "train - step 3753: loss = 401626.19 (1.670 sec)\n",
            "train - step 3754: loss = 623245.00 (1.658 sec)\n",
            "train - step 3755: loss = 366088.59 (2.578 sec)\n",
            "train - step 3756: loss = 320336.59 (1.687 sec)\n",
            "train - step 3757: loss = 386121.59 (1.647 sec)\n",
            "train - step 3758: loss = 359482.12 (1.685 sec)\n",
            "train - step 3759: loss = 500349.31 (1.659 sec)\n",
            "train - step 3760: loss = 515144.28 (1.702 sec)\n",
            "train - step 3761: loss = 414612.88 (1.674 sec)\n",
            "train - step 3762: loss = 497081.50 (1.706 sec)\n",
            "train - step 3763: loss = 443655.34 (1.668 sec)\n",
            "train - step 3764: loss = 639503.06 (1.684 sec)\n",
            "train - step 3765: loss = 511373.28 (1.703 sec)\n",
            "train - step 3766: loss = 521073.34 (1.663 sec)\n",
            "train - step 3767: loss = 441974.72 (1.685 sec)\n",
            "train - step 3768: loss = 501607.66 (1.651 sec)\n",
            "train - step 3769: loss = 342763.44 (1.669 sec)\n",
            "train - step 3770: loss = 454116.50 (1.687 sec)\n",
            "train - step 3771: loss = 542974.81 (1.693 sec)\n",
            "train - step 3772: loss = 393522.09 (2.044 sec)\n",
            "train - step 3773: loss = 530944.44 (1.681 sec)\n",
            "train - step 3774: loss = 434044.41 (1.684 sec)\n",
            "train - step 3775: loss = 345394.94 (1.777 sec)\n",
            "train - step 3776: loss = 490079.88 (1.666 sec)\n",
            "train - step 3777: loss = 354201.41 (1.660 sec)\n",
            "train - step 3778: loss = 345397.41 (1.645 sec)\n",
            "train - step 3779: loss = 427560.66 (1.673 sec)\n",
            "train - step 3780: loss = 351129.03 (1.691 sec)\n",
            "train - step 3781: loss = 511932.69 (1.688 sec)\n",
            "train - step 3782: loss = 429230.84 (1.680 sec)\n",
            "train - step 3783: loss = 389698.22 (1.663 sec)\n",
            "train - step 3784: loss = 462686.09 (1.705 sec)\n",
            "train - step 3785: loss = 437831.22 (1.692 sec)\n",
            "train - step 3786: loss = 329247.16 (1.660 sec)\n",
            "train - step 3787: loss = 468419.12 (1.703 sec)\n",
            "train - step 3788: loss = 380832.84 (1.682 sec)\n",
            "train - step 3789: loss = 276367.38 (1.668 sec)\n",
            "train - step 3790: loss = 473880.34 (1.662 sec)\n",
            "train - step 3791: loss = 479622.31 (2.499 sec)\n",
            "train - step 3792: loss = 235684.61 (1.675 sec)\n",
            "train - step 3793: loss = 442626.81 (1.693 sec)\n",
            "train - step 3794: loss = 382782.50 (1.669 sec)\n",
            "train - step 3795: loss = 360491.06 (1.680 sec)\n",
            "train - step 3796: loss = 438540.72 (1.667 sec)\n",
            "train - step 3797: loss = 621818.75 (1.671 sec)\n",
            "train - step 3798: loss = 471217.69 (1.668 sec)\n",
            "train - step 3799: loss = 312379.00 (1.666 sec)\n",
            "train - step 3800: loss = 761441.44 (1.702 sec)\n",
            "train - step 3801: loss = 420453.41 (1.680 sec)\n",
            "train - step 3802: loss = 450128.88 (1.696 sec)\n",
            "train - step 3803: loss = 438105.59 (1.679 sec)\n",
            "train - step 3804: loss = 342599.03 (1.675 sec)\n",
            "train - step 3805: loss = 474115.28 (1.669 sec)\n",
            "train - step 3806: loss = 432959.59 (1.684 sec)\n",
            "train - step 3807: loss = 449698.34 (1.688 sec)\n",
            "train - step 3808: loss = 351427.72 (1.679 sec)\n",
            "train - step 3809: loss = 587068.62 (1.680 sec)\n",
            "train - step 3810: loss = 489395.88 (1.679 sec)\n",
            "train - step 3811: loss = 297339.22 (1.701 sec)\n",
            "train - step 3812: loss = 281868.94 (1.680 sec)\n",
            "train - step 3813: loss = 505537.53 (1.685 sec)\n",
            "train - step 3814: loss = 476628.88 (1.685 sec)\n",
            "train - step 3815: loss = 398510.84 (1.681 sec)\n",
            "train - step 3816: loss = 469850.47 (1.693 sec)\n",
            "train - step 3817: loss = 606562.69 (1.691 sec)\n",
            "train - step 3818: loss = 372970.72 (1.709 sec)\n",
            "train - step 3819: loss = 450556.72 (1.699 sec)\n",
            "train - step 3820: loss = 312544.03 (1.672 sec)\n",
            "train - step 3821: loss = 451942.28 (1.686 sec)\n",
            "train - step 3822: loss = 537111.81 (1.684 sec)\n",
            "train - step 3823: loss = 642705.38 (1.685 sec)\n",
            "train - step 3824: loss = 404926.97 (1.670 sec)\n",
            "train - step 3825: loss = 326541.53 (1.679 sec)\n",
            "train - step 3826: loss = 479548.47 (1.679 sec)\n",
            "train - step 3827: loss = 431736.28 (2.511 sec)\n",
            "train - step 3828: loss = 286801.03 (1.674 sec)\n",
            "train - step 3829: loss = 522915.59 (1.672 sec)\n",
            "train - step 3830: loss = 371429.19 (1.668 sec)\n",
            "train - step 3831: loss = 541279.81 (1.649 sec)\n",
            "train - step 3832: loss = 445678.03 (1.666 sec)\n",
            "train - step 3833: loss = 336928.88 (1.687 sec)\n",
            "train - step 3834: loss = 430453.56 (1.663 sec)\n",
            "train - step 3835: loss = 611235.44 (1.675 sec)\n",
            "train - step 3836: loss = 296130.38 (1.666 sec)\n",
            "train - step 3837: loss = 380884.19 (1.670 sec)\n",
            "train - step 3838: loss = 603926.19 (1.677 sec)\n",
            "train - step 3839: loss = 508285.66 (1.664 sec)\n",
            "train - step 3840: loss = 354843.28 (1.671 sec)\n",
            "train - step 3841: loss = 266305.00 (1.650 sec)\n",
            "train - step 3842: loss = 395749.44 (1.657 sec)\n",
            "train - step 3843: loss = 548094.62 (1.684 sec)\n",
            "train - step 3844: loss = 387530.34 (1.644 sec)\n",
            "train - step 3845: loss = 304375.44 (1.664 sec)\n",
            "train - step 3846: loss = 402187.19 (1.644 sec)\n",
            "train - step 3847: loss = 240652.11 (1.660 sec)\n",
            "train - step 3848: loss = 365527.94 (1.653 sec)\n",
            "train - step 3849: loss = 371055.78 (1.671 sec)\n",
            "train - step 3850: loss = 394345.53 (1.656 sec)\n",
            "train - step 3851: loss = 523510.59 (1.676 sec)\n",
            "train - step 3852: loss = 359084.03 (1.662 sec)\n",
            "train - step 3853: loss = 363489.69 (1.702 sec)\n",
            "train - step 3854: loss = 322138.28 (1.643 sec)\n",
            "train - step 3855: loss = 506580.66 (1.656 sec)\n",
            "train - step 3856: loss = 284890.28 (1.664 sec)\n",
            "train - step 3857: loss = 471881.12 (1.642 sec)\n",
            "train - step 3858: loss = 566811.12 (1.696 sec)\n",
            "train - step 3859: loss = 375411.81 (1.661 sec)\n",
            "train - step 3860: loss = 466555.62 (1.646 sec)\n",
            "train - step 3861: loss = 435551.97 (1.669 sec)\n",
            "train - step 3862: loss = 396413.34 (1.648 sec)\n",
            "train - step 3863: loss = 372289.56 (2.512 sec)\n",
            "train - step 3864: loss = 469911.28 (1.652 sec)\n",
            "train - step 3865: loss = 339578.94 (1.647 sec)\n",
            "train - step 3866: loss = 358687.00 (1.673 sec)\n",
            "train - step 3867: loss = 401059.88 (1.659 sec)\n",
            "train - step 3868: loss = 384231.19 (1.655 sec)\n",
            "train - step 3869: loss = 450586.72 (1.677 sec)\n",
            "train - step 3870: loss = 574284.75 (1.659 sec)\n",
            "train - step 3871: loss = 421818.19 (1.672 sec)\n",
            "train - step 3872: loss = 455754.28 (1.680 sec)\n",
            "train - step 3873: loss = 291073.56 (1.659 sec)\n",
            "train - step 3874: loss = 410440.41 (1.672 sec)\n",
            "train - step 3875: loss = 347837.09 (1.649 sec)\n",
            "train - step 3876: loss = 375468.88 (1.664 sec)\n",
            "train - step 3877: loss = 532102.31 (1.694 sec)\n",
            "train - step 3878: loss = 455018.50 (1.657 sec)\n",
            "train - step 3879: loss = 522579.94 (1.677 sec)\n",
            "train - step 3880: loss = 455818.84 (1.640 sec)\n",
            "train - step 3881: loss = 333528.12 (1.672 sec)\n",
            "train - step 3882: loss = 414477.91 (1.644 sec)\n",
            "train - step 3883: loss = 433174.41 (1.665 sec)\n",
            "train - step 3884: loss = 490796.41 (1.667 sec)\n",
            "train - step 3885: loss = 508582.06 (1.656 sec)\n",
            "train - step 3886: loss = 447603.47 (1.657 sec)\n",
            "train - step 3887: loss = 245462.58 (1.677 sec)\n",
            "train - step 3888: loss = 531522.56 (1.687 sec)\n",
            "train - step 3889: loss = 479522.66 (1.673 sec)\n",
            "train - step 3890: loss = 556031.00 (1.671 sec)\n",
            "train - step 3891: loss = 463161.66 (1.663 sec)\n",
            "train - step 3892: loss = 456561.66 (1.656 sec)\n",
            "train - step 3893: loss = 400416.72 (1.624 sec)\n",
            "train - step 3894: loss = 493332.81 (1.646 sec)\n",
            "train - step 3895: loss = 376417.69 (1.672 sec)\n",
            "train - step 3896: loss = 382742.09 (1.656 sec)\n",
            "train - step 3897: loss = 392748.59 (1.653 sec)\n",
            "train - step 3898: loss = 503012.66 (1.667 sec)\n",
            "train - step 3899: loss = 416356.44 (2.650 sec)\n",
            "train - step 3900: loss = 557460.62 (1.670 sec)\n",
            "train - step 3901: loss = 486209.22 (1.665 sec)\n",
            "train - step 3902: loss = 452691.59 (1.656 sec)\n",
            "train - step 3903: loss = 542805.38 (1.663 sec)\n",
            "train - step 3904: loss = 360003.94 (1.655 sec)\n",
            "train - step 3905: loss = 441890.09 (1.669 sec)\n",
            "train - step 3906: loss = 289671.81 (1.660 sec)\n",
            "train - step 3907: loss = 564541.31 (1.666 sec)\n",
            "train - step 3908: loss = 314938.59 (1.669 sec)\n",
            "train - step 3909: loss = 501623.09 (1.673 sec)\n",
            "train - step 3910: loss = 607785.31 (1.655 sec)\n",
            "train - step 3911: loss = 335880.31 (1.632 sec)\n",
            "train - step 3912: loss = 419384.47 (1.631 sec)\n",
            "train - step 3913: loss = 461342.50 (1.647 sec)\n",
            "train - step 3914: loss = 439951.88 (1.630 sec)\n",
            "train - step 3915: loss = 362330.22 (1.644 sec)\n",
            "train - step 3916: loss = 343006.72 (1.634 sec)\n",
            "train - step 3917: loss = 495610.34 (1.709 sec)\n",
            "train - step 3918: loss = 442361.28 (1.679 sec)\n",
            "train - step 3919: loss = 501854.88 (1.690 sec)\n",
            "train - step 3920: loss = 368359.94 (1.699 sec)\n",
            "train - step 3921: loss = 491296.44 (1.678 sec)\n",
            "train - step 3922: loss = 452080.56 (1.677 sec)\n",
            "train - step 3923: loss = 416671.56 (1.696 sec)\n",
            "train - step 3924: loss = 413539.38 (1.685 sec)\n",
            "train - step 3925: loss = 381249.41 (1.652 sec)\n",
            "train - step 3926: loss = 460799.59 (1.657 sec)\n",
            "train - step 3927: loss = 271703.78 (1.685 sec)\n",
            "train - step 3928: loss = 409154.22 (1.676 sec)\n",
            "train - step 3929: loss = 638452.69 (1.695 sec)\n",
            "train - step 3930: loss = 399416.41 (1.661 sec)\n",
            "train - step 3931: loss = 442926.03 (1.669 sec)\n",
            "train - step 3932: loss = 385272.47 (1.671 sec)\n",
            "train - step 3933: loss = 321985.19 (1.649 sec)\n",
            "train - step 3934: loss = 237289.33 (1.676 sec)\n",
            "train - step 3935: loss = 459760.53 (2.481 sec)\n",
            "train - step 3936: loss = 430661.12 (1.689 sec)\n",
            "train - step 3937: loss = 439143.00 (1.666 sec)\n",
            "train - step 3938: loss = 376467.22 (1.670 sec)\n",
            "train - step 3939: loss = 435523.94 (1.691 sec)\n",
            "train - step 3940: loss = 482564.72 (1.683 sec)\n",
            "train - step 3941: loss = 332780.16 (1.667 sec)\n",
            "train - step 3942: loss = 446421.62 (1.700 sec)\n",
            "train - step 3943: loss = 266801.62 (1.691 sec)\n",
            "train - step 3944: loss = 342728.34 (1.688 sec)\n",
            "train - step 3945: loss = 422492.41 (1.691 sec)\n",
            "train - step 3946: loss = 388493.09 (1.677 sec)\n",
            "train - step 3947: loss = 410212.19 (1.659 sec)\n",
            "train - step 3948: loss = 443273.06 (1.680 sec)\n",
            "train - step 3949: loss = 376010.81 (1.679 sec)\n",
            "train - step 3950: loss = 430490.66 (1.688 sec)\n",
            "train - step 3951: loss = 391579.59 (1.698 sec)\n",
            "train - step 3952: loss = 398012.16 (1.689 sec)\n",
            "train - step 3953: loss = 484014.97 (1.687 sec)\n",
            "train - step 3954: loss = 326487.41 (1.662 sec)\n",
            "train - step 3955: loss = 448755.53 (2.984 sec)\n",
            "train - step 3956: loss = 557722.38 (1.714 sec)\n",
            "train - step 3957: loss = 341581.59 (1.674 sec)\n",
            "train - step 3958: loss = 426907.84 (1.695 sec)\n",
            "train - step 3959: loss = 399615.81 (1.675 sec)\n",
            "train - step 3960: loss = 471796.34 (1.688 sec)\n",
            "train - step 3961: loss = 442228.47 (1.702 sec)\n",
            "train - step 3962: loss = 468820.09 (1.695 sec)\n",
            "train - step 3963: loss = 436472.28 (1.692 sec)\n",
            "train - step 3964: loss = 451839.94 (1.699 sec)\n",
            "train - step 3965: loss = 370304.47 (1.691 sec)\n",
            "train - step 3966: loss = 511027.06 (1.676 sec)\n",
            "train - step 3967: loss = 499494.50 (1.708 sec)\n",
            "train - step 3968: loss = 547055.25 (1.673 sec)\n",
            "train - step 3969: loss = 445738.88 (1.678 sec)\n",
            "train - step 3970: loss = 436410.19 (2.631 sec)\n",
            "train - step 3971: loss = 461059.22 (1.693 sec)\n",
            "train - step 3972: loss = 489860.84 (1.702 sec)\n",
            "train - step 3973: loss = 343964.22 (1.707 sec)\n",
            "train - step 3974: loss = 340764.28 (1.700 sec)\n",
            "train - step 3975: loss = 383246.44 (1.714 sec)\n",
            "train - step 3976: loss = 451316.62 (1.685 sec)\n",
            "train - step 3977: loss = 394624.84 (1.661 sec)\n",
            "train - step 3978: loss = 569498.31 (1.670 sec)\n",
            "train - step 3979: loss = 336161.12 (1.684 sec)\n",
            "train - step 3980: loss = 542782.31 (1.657 sec)\n",
            "train - step 3981: loss = 506524.62 (1.669 sec)\n",
            "train - step 3982: loss = 424379.16 (1.673 sec)\n",
            "train - step 3983: loss = 522674.56 (1.694 sec)\n",
            "train - step 3984: loss = 449950.38 (1.680 sec)\n",
            "train - step 3985: loss = 332414.44 (1.670 sec)\n",
            "train - step 3986: loss = 501317.00 (1.689 sec)\n",
            "train - step 3987: loss = 381308.72 (1.660 sec)\n",
            "train - step 3988: loss = 515525.69 (1.690 sec)\n",
            "train - step 3989: loss = 282365.47 (1.682 sec)\n",
            "train - step 3990: loss = 499879.91 (1.666 sec)\n",
            "train - step 3991: loss = 447948.84 (1.680 sec)\n",
            "train - step 3992: loss = 467902.69 (1.679 sec)\n",
            "train - step 3993: loss = 413818.56 (1.706 sec)\n",
            "train - step 3994: loss = 496530.50 (1.668 sec)\n",
            "train - step 3995: loss = 419471.12 (1.656 sec)\n",
            "train - step 3996: loss = 687665.00 (1.668 sec)\n",
            "train - step 3997: loss = 290282.09 (1.676 sec)\n",
            "train - step 3998: loss = 433517.34 (1.678 sec)\n",
            "train - step 3999: loss = 357645.00 (1.673 sec)\n",
            "train - step 4000: loss = 568892.38 (1.660 sec)\n",
            "train - step 4001: loss = 496684.06 (1.664 sec)\n",
            "train - step 4002: loss = 393469.22 (1.667 sec)\n",
            "train - step 4003: loss = 424255.56 (1.669 sec)\n",
            "train - step 4004: loss = 423131.06 (1.675 sec)\n",
            "train - step 4005: loss = 397246.88 (1.679 sec)\n",
            "train - step 4006: loss = 338797.81 (2.527 sec)\n",
            "train - step 4007: loss = 381875.47 (1.650 sec)\n",
            "train - step 4008: loss = 552737.25 (1.632 sec)\n",
            "train - step 4009: loss = 333155.53 (1.660 sec)\n",
            "train - step 4010: loss = 434405.50 (1.657 sec)\n",
            "train - step 4011: loss = 355435.62 (1.673 sec)\n",
            "train - step 4012: loss = 463187.97 (1.682 sec)\n",
            "train - step 4013: loss = 404269.69 (1.673 sec)\n",
            "train - step 4014: loss = 341643.91 (1.662 sec)\n",
            "train - step 4015: loss = 387213.09 (1.660 sec)\n",
            "train - step 4016: loss = 449845.38 (1.680 sec)\n",
            "train - step 4017: loss = 521777.12 (1.644 sec)\n",
            "train - step 4018: loss = 459993.06 (1.656 sec)\n",
            "train - step 4019: loss = 479840.84 (1.668 sec)\n",
            "train - step 4020: loss = 338545.19 (1.659 sec)\n",
            "train - step 4021: loss = 508175.22 (1.636 sec)\n",
            "train - step 4022: loss = 507911.09 (1.647 sec)\n",
            "train - step 4023: loss = 232284.17 (1.618 sec)\n",
            "train - step 4024: loss = 443403.97 (1.640 sec)\n",
            "train - step 4025: loss = 452106.97 (1.644 sec)\n",
            "train - step 4026: loss = 322708.44 (1.636 sec)\n",
            "train - step 4027: loss = 546523.56 (1.673 sec)\n",
            "train - step 4028: loss = 533519.81 (1.677 sec)\n",
            "train - step 4029: loss = 407977.59 (1.629 sec)\n",
            "train - step 4030: loss = 413080.62 (1.676 sec)\n",
            "train - step 4031: loss = 323683.53 (1.664 sec)\n",
            "train - step 4032: loss = 352976.03 (1.656 sec)\n",
            "train - step 4033: loss = 399423.78 (1.665 sec)\n",
            "train - step 4034: loss = 416104.50 (1.685 sec)\n",
            "train - step 4035: loss = 481988.72 (1.671 sec)\n",
            "train - step 4036: loss = 359806.12 (1.684 sec)\n",
            "train - step 4037: loss = 549679.94 (1.665 sec)\n",
            "train - step 4038: loss = 453694.94 (1.686 sec)\n",
            "train - step 4039: loss = 361552.81 (1.676 sec)\n",
            "train - step 4040: loss = 261376.80 (1.673 sec)\n",
            "train - step 4041: loss = 524577.06 (1.677 sec)\n",
            "train - step 4042: loss = 533986.38 (2.523 sec)\n",
            "train - step 4043: loss = 402215.19 (1.664 sec)\n",
            "train - step 4044: loss = 573218.00 (1.636 sec)\n",
            "train - step 4045: loss = 446016.38 (1.646 sec)\n",
            "train - step 4046: loss = 507134.38 (1.644 sec)\n",
            "train - step 4047: loss = 529646.19 (1.637 sec)\n",
            "train - step 4048: loss = 375977.38 (1.648 sec)\n",
            "train - step 4049: loss = 457942.41 (1.628 sec)\n",
            "train - step 4050: loss = 567526.56 (1.662 sec)\n",
            "train - step 4051: loss = 319617.12 (1.655 sec)\n",
            "train - step 4052: loss = 571537.38 (1.655 sec)\n",
            "train - step 4053: loss = 520101.47 (1.679 sec)\n",
            "train - step 4054: loss = 270828.53 (1.703 sec)\n",
            "train - step 4055: loss = 505488.31 (1.692 sec)\n",
            "train - step 4056: loss = 222523.45 (1.697 sec)\n",
            "train - step 4057: loss = 488720.59 (1.710 sec)\n",
            "train - step 4058: loss = 449213.66 (1.674 sec)\n",
            "train - step 4059: loss = 451568.12 (1.693 sec)\n",
            "train - step 4060: loss = 278103.59 (1.719 sec)\n",
            "train - step 4061: loss = 408236.28 (1.690 sec)\n",
            "train - step 4062: loss = 476031.56 (1.690 sec)\n",
            "train - step 4063: loss = 523357.22 (1.680 sec)\n",
            "train - step 4064: loss = 439609.38 (1.668 sec)\n",
            "train - step 4065: loss = 403007.84 (1.816 sec)\n",
            "train - step 4066: loss = 333295.41 (1.687 sec)\n",
            "train - step 4067: loss = 480365.97 (1.697 sec)\n",
            "train - step 4068: loss = 389683.41 (1.700 sec)\n",
            "train - step 4069: loss = 345751.09 (1.668 sec)\n",
            "train - step 4070: loss = 316355.38 (1.654 sec)\n",
            "train - step 4071: loss = 385655.94 (1.639 sec)\n",
            "train - step 4072: loss = 310124.03 (1.656 sec)\n",
            "train - step 4073: loss = 420444.97 (1.634 sec)\n",
            "train - step 4074: loss = 374623.09 (1.660 sec)\n",
            "train - step 4075: loss = 394309.88 (1.664 sec)\n",
            "train - step 4076: loss = 470009.78 (1.671 sec)\n",
            "train - step 4077: loss = 454333.72 (1.663 sec)\n",
            "train - step 4078: loss = 360675.31 (2.818 sec)\n",
            "train - step 4079: loss = 412683.00 (1.664 sec)\n",
            "train - step 4080: loss = 361272.16 (1.665 sec)\n",
            "train - step 4081: loss = 305629.78 (1.673 sec)\n",
            "train - step 4082: loss = 355385.00 (1.675 sec)\n",
            "train - step 4083: loss = 472511.38 (1.667 sec)\n",
            "train - step 4084: loss = 517998.09 (1.681 sec)\n",
            "train - step 4085: loss = 365421.88 (1.690 sec)\n",
            "train - step 4086: loss = 438817.00 (1.679 sec)\n",
            "train - step 4087: loss = 378153.53 (1.672 sec)\n",
            "train - step 4088: loss = 317362.38 (1.680 sec)\n",
            "train - step 4089: loss = 308041.81 (1.679 sec)\n",
            "train - step 4090: loss = 323264.31 (1.676 sec)\n",
            "train - step 4091: loss = 386399.66 (1.688 sec)\n",
            "train - step 4092: loss = 500412.66 (1.648 sec)\n",
            "train - step 4093: loss = 363841.56 (1.634 sec)\n",
            "train - step 4094: loss = 423176.66 (1.651 sec)\n",
            "train - step 4095: loss = 563987.12 (1.647 sec)\n",
            "train - step 4096: loss = 403110.66 (1.655 sec)\n",
            "train - step 4097: loss = 481151.53 (1.644 sec)\n",
            "train - step 4098: loss = 460302.38 (1.640 sec)\n",
            "train - step 4099: loss = 390477.22 (1.673 sec)\n",
            "train - step 4100: loss = 608215.31 (1.677 sec)\n",
            "train - step 4101: loss = 459967.03 (1.696 sec)\n",
            "train - step 4102: loss = 429985.28 (1.677 sec)\n",
            "train - step 4103: loss = 409482.44 (1.658 sec)\n",
            "train - step 4104: loss = 289275.84 (1.675 sec)\n",
            "train - step 4105: loss = 404011.41 (1.670 sec)\n",
            "train - step 4106: loss = 419825.09 (1.677 sec)\n",
            "train - step 4107: loss = 228415.30 (1.667 sec)\n",
            "train - step 4108: loss = 505031.06 (1.678 sec)\n",
            "train - step 4109: loss = 497149.47 (1.692 sec)\n",
            "train - step 4110: loss = 466247.09 (1.675 sec)\n",
            "train - step 4111: loss = 377459.94 (1.656 sec)\n",
            "train - step 4112: loss = 514921.12 (1.683 sec)\n",
            "train - step 4113: loss = 436916.59 (1.661 sec)\n",
            "train - step 4114: loss = 373298.16 (2.500 sec)\n",
            "train - step 4115: loss = 446755.16 (1.683 sec)\n",
            "train - step 4116: loss = 498769.53 (1.662 sec)\n",
            "train - step 4117: loss = 406020.59 (1.665 sec)\n",
            "train - step 4118: loss = 501783.03 (1.680 sec)\n",
            "train - step 4119: loss = 365098.56 (1.682 sec)\n",
            "train - step 4120: loss = 351671.91 (1.672 sec)\n",
            "train - step 4121: loss = 721492.19 (1.683 sec)\n",
            "train - step 4122: loss = 478472.88 (1.668 sec)\n",
            "train - step 4123: loss = 390923.47 (1.653 sec)\n",
            "train - step 4124: loss = 477187.38 (1.672 sec)\n",
            "train - step 4125: loss = 415751.78 (1.670 sec)\n",
            "train - step 4126: loss = 288956.88 (1.689 sec)\n",
            "train - step 4127: loss = 661426.56 (1.669 sec)\n",
            "train - step 4128: loss = 332837.81 (1.678 sec)\n",
            "train - step 4129: loss = 504245.94 (1.665 sec)\n",
            "train - step 4130: loss = 373108.56 (1.673 sec)\n",
            "train - step 4131: loss = 484441.84 (1.670 sec)\n",
            "train - step 4132: loss = 446894.31 (1.648 sec)\n",
            "train - step 4133: loss = 417247.16 (1.683 sec)\n",
            "train - step 4134: loss = 433550.34 (1.664 sec)\n",
            "train - step 4135: loss = 543466.94 (1.691 sec)\n",
            "train - step 4136: loss = 436904.88 (1.663 sec)\n",
            "train - step 4137: loss = 411085.38 (1.655 sec)\n",
            "train - step 4138: loss = 360676.56 (1.661 sec)\n",
            "train - step 4139: loss = 298194.72 (1.690 sec)\n",
            "train - step 4140: loss = 478375.34 (1.677 sec)\n",
            "train - step 4141: loss = 333038.41 (1.676 sec)\n",
            "train - step 4142: loss = 379433.81 (1.666 sec)\n",
            "train - step 4143: loss = 339107.06 (1.685 sec)\n",
            "train - step 4144: loss = 401622.88 (1.668 sec)\n",
            "train - step 4145: loss = 378976.38 (1.674 sec)\n",
            "train - step 4146: loss = 424133.78 (1.670 sec)\n",
            "train - step 4147: loss = 419866.78 (1.659 sec)\n",
            "train - step 4148: loss = 350080.22 (1.669 sec)\n",
            "train - step 4149: loss = 382383.41 (1.681 sec)\n",
            "train - step 4150: loss = 354072.59 (2.517 sec)\n",
            "train - step 4151: loss = 575180.44 (1.669 sec)\n",
            "train - step 4152: loss = 345564.34 (1.686 sec)\n",
            "train - step 4153: loss = 436009.62 (1.686 sec)\n",
            "train - step 4154: loss = 394648.22 (1.673 sec)\n",
            "train - step 4155: loss = 385298.09 (1.667 sec)\n",
            "train - step 4156: loss = 407784.34 (1.700 sec)\n",
            "train - step 4157: loss = 470729.22 (1.665 sec)\n",
            "train - step 4158: loss = 394301.12 (1.688 sec)\n",
            "train - step 4159: loss = 504361.94 (1.692 sec)\n",
            "train - step 4160: loss = 623770.75 (1.663 sec)\n",
            "train - step 4161: loss = 311297.91 (1.667 sec)\n",
            "train - step 4162: loss = 474547.28 (1.683 sec)\n",
            "train - step 4163: loss = 297596.22 (1.666 sec)\n",
            "train - step 4164: loss = 536536.50 (1.677 sec)\n",
            "train - step 4165: loss = 388455.31 (1.654 sec)\n",
            "train - step 4166: loss = 370563.38 (1.684 sec)\n",
            "train - step 4167: loss = 430445.19 (1.663 sec)\n",
            "train - step 4168: loss = 526440.06 (1.654 sec)\n",
            "train - step 4169: loss = 389438.31 (1.696 sec)\n",
            "train - step 4170: loss = 466674.94 (1.669 sec)\n",
            "train - step 4171: loss = 464887.56 (1.662 sec)\n",
            "train - step 4172: loss = 480942.94 (1.692 sec)\n",
            "train - step 4173: loss = 479807.06 (1.671 sec)\n",
            "train - step 4174: loss = 401323.34 (1.683 sec)\n",
            "train - step 4175: loss = 369733.91 (1.684 sec)\n",
            "train - step 4176: loss = 625285.25 (1.680 sec)\n",
            "train - step 4177: loss = 531937.00 (1.696 sec)\n",
            "train - step 4178: loss = 490000.16 (1.685 sec)\n",
            "train - step 4179: loss = 459764.19 (1.692 sec)\n",
            "train - step 4180: loss = 363647.28 (1.663 sec)\n",
            "train - step 4181: loss = 443813.09 (1.675 sec)\n",
            "train - step 4182: loss = 422199.03 (1.671 sec)\n",
            "train - step 4183: loss = 474029.09 (1.680 sec)\n",
            "train - step 4184: loss = 432104.28 (1.682 sec)\n",
            "train - step 4185: loss = 504578.44 (1.678 sec)\n",
            "train - step 4186: loss = 509293.34 (2.583 sec)\n",
            "train - step 4187: loss = 462267.50 (1.693 sec)\n",
            "train - step 4188: loss = 462973.09 (1.663 sec)\n",
            "train - step 4189: loss = 320552.44 (1.662 sec)\n",
            "train - step 4190: loss = 449325.50 (1.693 sec)\n",
            "train - step 4191: loss = 437578.94 (1.672 sec)\n",
            "train - step 4192: loss = 448435.41 (1.682 sec)\n",
            "train - step 4193: loss = 409199.66 (1.687 sec)\n",
            "train - step 4194: loss = 301080.31 (1.680 sec)\n",
            "train - step 4195: loss = 362694.56 (1.670 sec)\n",
            "train - step 4196: loss = 451267.22 (1.676 sec)\n",
            "train - step 4197: loss = 303257.19 (1.691 sec)\n",
            "train - step 4198: loss = 384259.69 (1.668 sec)\n",
            "train - step 4199: loss = 314431.72 (1.683 sec)\n",
            "train - step 4200: loss = 490922.66 (1.681 sec)\n",
            "train - step 4201: loss = 531057.31 (1.693 sec)\n",
            "train - step 4202: loss = 518475.91 (1.694 sec)\n",
            "train - step 4203: loss = 425591.59 (1.682 sec)\n",
            "train - step 4204: loss = 365368.47 (1.679 sec)\n",
            "train - step 4205: loss = 623784.88 (1.662 sec)\n",
            "train - step 4206: loss = 396882.88 (1.680 sec)\n",
            "train - step 4207: loss = 436257.41 (1.691 sec)\n",
            "train - step 4208: loss = 434754.91 (1.673 sec)\n",
            "train - step 4209: loss = 281872.19 (1.691 sec)\n",
            "train - step 4210: loss = 445382.00 (1.702 sec)\n",
            "train - step 4211: loss = 473784.94 (1.688 sec)\n",
            "train - step 4212: loss = 493642.47 (1.695 sec)\n",
            "train - step 4213: loss = 433914.66 (1.675 sec)\n",
            "train - step 4214: loss = 327066.59 (1.684 sec)\n",
            "train - step 4215: loss = 454368.09 (1.690 sec)\n",
            "train - step 4216: loss = 450967.34 (1.708 sec)\n",
            "train - step 4217: loss = 325889.91 (1.683 sec)\n",
            "train - step 4218: loss = 413005.38 (1.685 sec)\n",
            "train - step 4219: loss = 441198.00 (1.671 sec)\n",
            "train - step 4220: loss = 436399.34 (1.672 sec)\n",
            "train - step 4221: loss = 406226.66 (1.698 sec)\n",
            "train - step 4222: loss = 335572.84 (2.758 sec)\n",
            "train - step 4223: loss = 433442.44 (1.693 sec)\n",
            "train - step 4224: loss = 494235.34 (1.679 sec)\n",
            "train - step 4225: loss = 360961.38 (1.677 sec)\n",
            "train - step 4226: loss = 497087.66 (1.675 sec)\n",
            "train - step 4227: loss = 259669.16 (1.680 sec)\n",
            "train - step 4228: loss = 460513.88 (1.662 sec)\n",
            "train - step 4229: loss = 358216.88 (1.705 sec)\n",
            "train - step 4230: loss = 356089.38 (1.682 sec)\n",
            "train - step 4231: loss = 578662.12 (1.689 sec)\n",
            "train - step 4232: loss = 345834.88 (1.683 sec)\n",
            "train - step 4233: loss = 464519.56 (1.694 sec)\n",
            "train - step 4234: loss = 375463.81 (1.676 sec)\n",
            "train - step 4235: loss = 356024.88 (1.710 sec)\n",
            "train - step 4236: loss = 545480.50 (1.683 sec)\n",
            "train - step 4237: loss = 435893.88 (1.690 sec)\n",
            "train - step 4238: loss = 469563.72 (1.705 sec)\n",
            "train - step 4239: loss = 369559.56 (1.697 sec)\n",
            "train - step 4240: loss = 413799.19 (1.685 sec)\n",
            "train - step 4241: loss = 646584.25 (1.675 sec)\n",
            "train - step 4242: loss = 374429.81 (1.700 sec)\n",
            "train - step 4243: loss = 448552.72 (1.699 sec)\n",
            "train - step 4244: loss = 380687.44 (1.670 sec)\n",
            "train - step 4245: loss = 523803.16 (1.655 sec)\n",
            "train - step 4246: loss = 536680.25 (1.677 sec)\n",
            "train - step 4247: loss = 320664.78 (1.681 sec)\n",
            "train - step 4248: loss = 530499.75 (1.678 sec)\n",
            "train - step 4249: loss = 588851.88 (1.705 sec)\n",
            "train - step 4250: loss = 484916.91 (1.649 sec)\n",
            "train - step 4251: loss = 602287.94 (1.630 sec)\n",
            "train - step 4252: loss = 463028.97 (1.637 sec)\n",
            "train - step 4253: loss = 528505.38 (1.654 sec)\n",
            "train - step 4254: loss = 462466.53 (1.691 sec)\n",
            "train - step 4255: loss = 390245.41 (1.689 sec)\n",
            "train - step 4256: loss = 679021.88 (1.696 sec)\n",
            "train - step 4257: loss = 399776.38 (1.678 sec)\n",
            "train - step 4258: loss = 443531.00 (2.636 sec)\n",
            "train - step 4259: loss = 483195.78 (1.679 sec)\n",
            "train - step 4260: loss = 373648.94 (1.681 sec)\n",
            "train - step 4261: loss = 627463.44 (1.696 sec)\n",
            "train - step 4262: loss = 369328.12 (1.692 sec)\n",
            "train - step 4263: loss = 332925.47 (1.662 sec)\n",
            "train - step 4264: loss = 526547.75 (1.682 sec)\n",
            "train - step 4265: loss = 468974.03 (1.704 sec)\n",
            "train - step 4266: loss = 438829.34 (1.684 sec)\n",
            "train - step 4267: loss = 372426.00 (1.667 sec)\n",
            "train - step 4268: loss = 418524.69 (1.669 sec)\n",
            "train - step 4269: loss = 399502.72 (1.671 sec)\n",
            "train - step 4270: loss = 342373.19 (1.661 sec)\n",
            "train - step 4271: loss = 450980.28 (1.686 sec)\n",
            "train - step 4272: loss = 288634.06 (1.689 sec)\n",
            "train - step 4273: loss = 408247.00 (1.687 sec)\n",
            "train - step 4274: loss = 516748.19 (1.681 sec)\n",
            "train - step 4275: loss = 414842.72 (1.660 sec)\n",
            "train - step 4276: loss = 478332.03 (1.646 sec)\n",
            "train - step 4277: loss = 413026.81 (1.650 sec)\n",
            "train - step 4278: loss = 320455.28 (1.646 sec)\n",
            "train - step 4279: loss = 402452.12 (1.648 sec)\n",
            "train - step 4280: loss = 497706.41 (1.663 sec)\n",
            "train - step 4281: loss = 380242.53 (1.672 sec)\n",
            "train - step 4282: loss = 408815.72 (1.680 sec)\n",
            "train - step 4283: loss = 493791.41 (1.709 sec)\n",
            "train - step 4284: loss = 411425.66 (1.668 sec)\n",
            "train - step 4285: loss = 436072.81 (1.675 sec)\n",
            "train - step 4286: loss = 427011.31 (1.677 sec)\n",
            "train - step 4287: loss = 469895.28 (1.666 sec)\n",
            "train - step 4288: loss = 501787.44 (1.684 sec)\n",
            "train - step 4289: loss = 558898.69 (1.669 sec)\n",
            "train - step 4290: loss = 501431.69 (1.695 sec)\n",
            "train - step 4291: loss = 381784.78 (1.677 sec)\n",
            "train - step 4292: loss = 396820.41 (1.667 sec)\n",
            "train - step 4293: loss = 504750.31 (1.687 sec)\n",
            "train - step 4294: loss = 242918.56 (2.600 sec)\n",
            "train - step 4295: loss = 440308.88 (1.682 sec)\n",
            "train - step 4296: loss = 360712.00 (1.691 sec)\n",
            "train - step 4297: loss = 434039.47 (1.680 sec)\n",
            "train - step 4298: loss = 419304.59 (1.710 sec)\n",
            "train - step 4299: loss = 332398.72 (1.673 sec)\n",
            "train - step 4300: loss = 498540.56 (1.658 sec)\n",
            "train - step 4301: loss = 451475.88 (1.674 sec)\n",
            "train - step 4302: loss = 485452.62 (1.663 sec)\n",
            "train - step 4303: loss = 509259.16 (1.666 sec)\n",
            "train - step 4304: loss = 500138.41 (1.683 sec)\n",
            "train - step 4305: loss = 408115.94 (1.682 sec)\n",
            "train - step 4306: loss = 439817.78 (1.697 sec)\n",
            "train - step 4307: loss = 484786.41 (1.677 sec)\n",
            "train - step 4308: loss = 372044.47 (1.671 sec)\n",
            "train - step 4309: loss = 388210.19 (1.668 sec)\n",
            "train - step 4310: loss = 405394.84 (1.673 sec)\n",
            "train - step 4311: loss = 462132.38 (1.675 sec)\n",
            "train - step 4312: loss = 457015.97 (1.678 sec)\n",
            "train - step 4313: loss = 390049.53 (1.688 sec)\n",
            "train - step 4314: loss = 382862.81 (1.683 sec)\n",
            "train - step 4315: loss = 359022.47 (1.688 sec)\n",
            "train - step 4316: loss = 489789.38 (1.662 sec)\n",
            "train - step 4317: loss = 454155.62 (1.695 sec)\n",
            "train - step 4318: loss = 401607.62 (1.691 sec)\n",
            "train - step 4319: loss = 471986.09 (1.679 sec)\n",
            "train - step 4320: loss = 422385.44 (1.680 sec)\n",
            "train - step 4321: loss = 508062.09 (1.690 sec)\n",
            "train - step 4322: loss = 409729.09 (1.686 sec)\n",
            "train - step 4323: loss = 304587.59 (1.679 sec)\n",
            "train - step 4324: loss = 406400.19 (1.673 sec)\n",
            "train - step 4325: loss = 436985.47 (1.686 sec)\n",
            "train - step 4326: loss = 409071.41 (1.667 sec)\n",
            "train - step 4327: loss = 570665.25 (1.668 sec)\n",
            "train - step 4328: loss = 434160.97 (1.674 sec)\n",
            "train - step 4329: loss = 478365.06 (1.653 sec)\n",
            "train - step 4330: loss = 451791.16 (2.565 sec)\n",
            "train - step 4331: loss = 492660.88 (1.682 sec)\n",
            "train - step 4332: loss = 380839.78 (1.674 sec)\n",
            "train - step 4333: loss = 500027.88 (1.683 sec)\n",
            "train - step 4334: loss = 320996.97 (1.680 sec)\n",
            "train - step 4335: loss = 507287.59 (1.675 sec)\n",
            "train - step 4336: loss = 408065.81 (1.676 sec)\n",
            "train - step 4337: loss = 415196.88 (1.676 sec)\n",
            "train - step 4338: loss = 354379.59 (1.665 sec)\n",
            "train - step 4339: loss = 322418.91 (1.670 sec)\n",
            "train - step 4340: loss = 415713.97 (1.679 sec)\n",
            "train - step 4341: loss = 422773.91 (1.673 sec)\n",
            "train - step 4342: loss = 369532.84 (1.665 sec)\n",
            "train - step 4343: loss = 402116.88 (1.667 sec)\n",
            "train - step 4344: loss = 493502.88 (1.665 sec)\n",
            "train - step 4345: loss = 459270.41 (1.662 sec)\n",
            "train - step 4346: loss = 467404.62 (1.682 sec)\n",
            "train - step 4347: loss = 308083.31 (1.698 sec)\n",
            "train - step 4348: loss = 486338.12 (1.706 sec)\n",
            "train - step 4349: loss = 376914.91 (1.714 sec)\n",
            "train - step 4350: loss = 369491.66 (1.689 sec)\n",
            "train - step 4351: loss = 498063.34 (1.678 sec)\n",
            "train - step 4352: loss = 424123.69 (1.709 sec)\n",
            "train - step 4353: loss = 517748.72 (1.703 sec)\n",
            "train - step 4354: loss = 344603.06 (1.694 sec)\n",
            "train - step 4355: loss = 301272.59 (1.666 sec)\n",
            "train - step 4356: loss = 437990.84 (1.669 sec)\n",
            "train - step 4357: loss = 555559.31 (1.671 sec)\n",
            "train - step 4358: loss = 393345.34 (1.659 sec)\n",
            "train - step 4359: loss = 335502.94 (1.672 sec)\n",
            "train - step 4360: loss = 440969.91 (1.682 sec)\n",
            "train - step 4361: loss = 345454.41 (1.834 sec)\n",
            "train - step 4362: loss = 318462.97 (1.669 sec)\n",
            "train - step 4363: loss = 303850.34 (1.677 sec)\n",
            "train - step 4364: loss = 375153.31 (1.690 sec)\n",
            "train - step 4365: loss = 531144.12 (1.691 sec)\n",
            "train - step 4366: loss = 458578.16 (2.502 sec)\n",
            "train - step 4367: loss = 408630.31 (1.667 sec)\n",
            "train - step 4368: loss = 482152.00 (1.705 sec)\n",
            "train - step 4369: loss = 443077.66 (1.677 sec)\n",
            "train - step 4370: loss = 370136.88 (1.676 sec)\n",
            "train - step 4371: loss = 295473.12 (1.679 sec)\n",
            "train - step 4372: loss = 394298.31 (1.680 sec)\n",
            "train - step 4373: loss = 399677.53 (1.706 sec)\n",
            "train - step 4374: loss = 425631.19 (1.667 sec)\n",
            "train - step 4375: loss = 341652.81 (1.711 sec)\n",
            "train - step 4376: loss = 445032.19 (1.708 sec)\n",
            "train - step 4377: loss = 393043.69 (1.681 sec)\n",
            "train - step 4378: loss = 498810.88 (1.672 sec)\n",
            "train - step 4379: loss = 367150.56 (1.688 sec)\n",
            "train - step 4380: loss = 374587.50 (1.665 sec)\n",
            "train - step 4381: loss = 489089.06 (1.675 sec)\n",
            "train - step 4382: loss = 538646.94 (1.674 sec)\n",
            "train - step 4383: loss = 542983.81 (1.699 sec)\n",
            "train - step 4384: loss = 342291.44 (1.684 sec)\n",
            "train - step 4385: loss = 453400.28 (1.674 sec)\n",
            "train - step 4386: loss = 480916.81 (1.668 sec)\n",
            "train - step 4387: loss = 434583.66 (1.694 sec)\n",
            "train - step 4388: loss = 450422.94 (1.672 sec)\n",
            "train - step 4389: loss = 281447.53 (1.675 sec)\n",
            "train - step 4390: loss = 313170.62 (1.686 sec)\n",
            "train - step 4391: loss = 390407.53 (1.665 sec)\n",
            "train - step 4392: loss = 312723.78 (1.690 sec)\n",
            "train - step 4393: loss = 374958.59 (1.692 sec)\n",
            "train - step 4394: loss = 460494.06 (1.667 sec)\n",
            "train - step 4395: loss = 434666.28 (1.700 sec)\n",
            "train - step 4396: loss = 347198.44 (1.655 sec)\n",
            "train - step 4397: loss = 368546.94 (1.674 sec)\n",
            "train - step 4398: loss = 527942.00 (1.693 sec)\n",
            "train - step 4399: loss = 357253.59 (1.667 sec)\n",
            "train - step 4400: loss = 332734.28 (1.676 sec)\n",
            "train - step 4401: loss = 387313.53 (1.687 sec)\n",
            "train - step 4402: loss = 355595.09 (2.550 sec)\n",
            "train - step 4403: loss = 430626.91 (1.685 sec)\n",
            "train - step 4404: loss = 547937.88 (1.699 sec)\n",
            "train - step 4405: loss = 442013.34 (1.672 sec)\n",
            "train - step 4406: loss = 684803.31 (1.677 sec)\n",
            "train - step 4407: loss = 487574.47 (1.694 sec)\n",
            "train - step 4408: loss = 400752.22 (1.717 sec)\n",
            "train - step 4409: loss = 545734.75 (1.677 sec)\n",
            "train - step 4410: loss = 449245.53 (1.676 sec)\n",
            "train - step 4411: loss = 308167.94 (1.683 sec)\n",
            "train - step 4412: loss = 422290.16 (1.685 sec)\n",
            "train - step 4413: loss = 445392.38 (1.683 sec)\n",
            "train - step 4414: loss = 364938.88 (1.691 sec)\n",
            "train - step 4415: loss = 434994.56 (1.682 sec)\n",
            "train - step 4416: loss = 440715.16 (1.714 sec)\n",
            "train - step 4417: loss = 320728.84 (1.694 sec)\n",
            "train - step 4418: loss = 375241.12 (1.679 sec)\n",
            "train - step 4419: loss = 548212.56 (1.692 sec)\n",
            "train - step 4420: loss = 453548.72 (1.688 sec)\n",
            "train - step 4421: loss = 400826.28 (1.691 sec)\n",
            "train - step 4422: loss = 356512.31 (1.677 sec)\n",
            "train - step 4423: loss = 403906.88 (1.684 sec)\n",
            "train - step 4424: loss = 375543.78 (1.673 sec)\n",
            "train - step 4425: loss = 503415.62 (1.690 sec)\n",
            "train - step 4426: loss = 338537.59 (1.678 sec)\n",
            "train - step 4427: loss = 476647.12 (1.688 sec)\n",
            "train - step 4428: loss = 433139.91 (1.679 sec)\n",
            "train - step 4429: loss = 468724.84 (1.682 sec)\n",
            "train - step 4430: loss = 373680.38 (1.643 sec)\n",
            "train - step 4431: loss = 374410.91 (1.650 sec)\n",
            "train - step 4432: loss = 247721.02 (1.636 sec)\n",
            "train - step 4433: loss = 375222.09 (1.656 sec)\n",
            "train - step 4434: loss = 573942.81 (1.697 sec)\n",
            "train - step 4435: loss = 380176.94 (1.695 sec)\n",
            "train - step 4436: loss = 531512.38 (1.713 sec)\n",
            "train - step 4437: loss = 522041.53 (1.683 sec)\n",
            "train - step 4438: loss = 537184.00 (2.199 sec)\n",
            "train - step 4439: loss = 383730.66 (1.687 sec)\n",
            "train - step 4440: loss = 398914.72 (1.666 sec)\n",
            "train - step 4441: loss = 650090.75 (1.666 sec)\n",
            "train - step 4442: loss = 444226.88 (1.689 sec)\n",
            "train - step 4443: loss = 241591.16 (1.684 sec)\n",
            "train - step 4444: loss = 428629.44 (1.667 sec)\n",
            "train - step 4445: loss = 426683.00 (1.683 sec)\n",
            "train - step 4446: loss = 423008.19 (1.653 sec)\n",
            "train - step 4447: loss = 472793.06 (1.693 sec)\n",
            "train - step 4448: loss = 390821.69 (1.661 sec)\n",
            "train - step 4449: loss = 524020.97 (1.663 sec)\n",
            "train - step 4450: loss = 369522.97 (1.670 sec)\n",
            "train - step 4451: loss = 352353.00 (1.687 sec)\n",
            "train - step 4452: loss = 463100.41 (1.667 sec)\n",
            "train - step 4453: loss = 365327.62 (1.664 sec)\n",
            "train - step 4454: loss = 280192.28 (1.685 sec)\n",
            "train - step 4455: loss = 495905.41 (1.677 sec)\n",
            "train - step 4456: loss = 381772.69 (1.691 sec)\n",
            "train - step 4457: loss = 190671.59 (1.660 sec)\n",
            "train - step 4458: loss = 405606.78 (1.652 sec)\n",
            "train - step 4459: loss = 406178.47 (1.649 sec)\n",
            "train - step 4460: loss = 366061.22 (1.647 sec)\n",
            "train - step 4461: loss = 302313.06 (1.655 sec)\n",
            "train - step 4462: loss = 445070.94 (1.635 sec)\n",
            "train - step 4463: loss = 625025.00 (1.660 sec)\n",
            "train - step 4464: loss = 528180.75 (1.685 sec)\n",
            "train - step 4465: loss = 436553.31 (1.670 sec)\n",
            "train - step 4466: loss = 510449.34 (1.669 sec)\n",
            "train - step 4467: loss = 423409.22 (1.694 sec)\n",
            "train - step 4468: loss = 455517.84 (1.681 sec)\n",
            "train - step 4469: loss = 243726.70 (1.673 sec)\n",
            "train - step 4470: loss = 623970.25 (1.672 sec)\n",
            "train - step 4471: loss = 557385.31 (1.689 sec)\n",
            "train - step 4472: loss = 317300.88 (1.672 sec)\n",
            "train - step 4473: loss = 568824.69 (1.689 sec)\n",
            "train - step 4474: loss = 502695.19 (2.569 sec)\n",
            "train - step 4475: loss = 396702.03 (1.686 sec)\n",
            "train - step 4476: loss = 403557.59 (1.679 sec)\n",
            "train - step 4477: loss = 628178.94 (1.698 sec)\n",
            "train - step 4478: loss = 340385.06 (1.679 sec)\n",
            "train - step 4479: loss = 427504.03 (1.664 sec)\n",
            "train - step 4480: loss = 379708.28 (1.697 sec)\n",
            "train - step 4481: loss = 366148.78 (1.688 sec)\n",
            "train - step 4482: loss = 565021.12 (1.681 sec)\n",
            "train - step 4483: loss = 399768.19 (1.664 sec)\n",
            "train - step 4484: loss = 461209.81 (1.683 sec)\n",
            "train - step 4485: loss = 379924.78 (1.671 sec)\n",
            "train - step 4486: loss = 394015.56 (1.666 sec)\n",
            "train - step 4487: loss = 424646.94 (1.673 sec)\n",
            "train - step 4488: loss = 462115.66 (1.687 sec)\n",
            "train - step 4489: loss = 531753.75 (1.687 sec)\n",
            "train - step 4490: loss = 282053.84 (1.688 sec)\n",
            "train - step 4491: loss = 553027.12 (1.678 sec)\n",
            "train - step 4492: loss = 398606.94 (1.675 sec)\n",
            "train - step 4493: loss = 321978.78 (1.672 sec)\n",
            "train - step 4494: loss = 351971.09 (1.671 sec)\n",
            "train - step 4495: loss = 378094.62 (1.694 sec)\n",
            "train - step 4496: loss = 338379.19 (1.670 sec)\n",
            "train - step 4497: loss = 454547.78 (1.682 sec)\n",
            "train - step 4498: loss = 164375.44 (1.675 sec)\n",
            "train - step 4499: loss = 340769.41 (1.687 sec)\n",
            "train - step 4500: loss = 592674.75 (1.677 sec)\n",
            "train - step 4501: loss = 523751.97 (1.663 sec)\n",
            "train - step 4502: loss = 673878.00 (1.681 sec)\n",
            "train - step 4503: loss = 439043.56 (1.675 sec)\n",
            "train - step 4504: loss = 473497.53 (1.686 sec)\n",
            "train - step 4505: loss = 268899.84 (1.672 sec)\n",
            "train - step 4506: loss = 380676.91 (1.671 sec)\n",
            "train - step 4507: loss = 454679.53 (1.681 sec)\n",
            "train - step 4508: loss = 289966.91 (1.693 sec)\n",
            "train - step 4509: loss = 501823.47 (1.685 sec)\n",
            "train - step 4510: loss = 307620.19 (2.645 sec)\n",
            "train - step 4511: loss = 479198.94 (1.707 sec)\n",
            "train - step 4512: loss = 358551.03 (1.683 sec)\n",
            "train - step 4513: loss = 388030.81 (1.688 sec)\n",
            "train - step 4514: loss = 494740.22 (1.685 sec)\n",
            "train - step 4515: loss = 459498.88 (1.687 sec)\n",
            "train - step 4516: loss = 438080.31 (1.698 sec)\n",
            "train - step 4517: loss = 583675.06 (1.675 sec)\n",
            "train - step 4518: loss = 495621.34 (1.678 sec)\n",
            "train - step 4519: loss = 492780.62 (1.690 sec)\n",
            "train - step 4520: loss = 420730.16 (1.702 sec)\n",
            "train - step 4521: loss = 474939.47 (1.666 sec)\n",
            "train - step 4522: loss = 625472.00 (1.704 sec)\n",
            "train - step 4523: loss = 324751.41 (1.674 sec)\n",
            "train - step 4524: loss = 398504.19 (1.670 sec)\n",
            "train - step 4525: loss = 366127.56 (1.668 sec)\n",
            "train - step 4526: loss = 358934.88 (1.670 sec)\n",
            "train - step 4527: loss = 451157.38 (1.682 sec)\n",
            "train - step 4528: loss = 480795.53 (1.672 sec)\n",
            "train - step 4529: loss = 522402.19 (1.676 sec)\n",
            "train - step 4530: loss = 401951.72 (1.686 sec)\n",
            "train - step 4531: loss = 375318.91 (1.684 sec)\n",
            "train - step 4532: loss = 377392.97 (1.666 sec)\n",
            "train - step 4533: loss = 369857.22 (1.674 sec)\n",
            "train - step 4534: loss = 428768.44 (1.667 sec)\n",
            "train - step 4535: loss = 473868.41 (1.685 sec)\n",
            "train - step 4536: loss = 380440.91 (1.707 sec)\n",
            "train - step 4537: loss = 478278.44 (1.677 sec)\n",
            "train - step 4538: loss = 479234.06 (1.671 sec)\n",
            "train - step 4539: loss = 614012.75 (1.656 sec)\n",
            "train - step 4540: loss = 515280.38 (1.664 sec)\n",
            "train - step 4541: loss = 468891.03 (1.679 sec)\n",
            "train - step 4542: loss = 549530.62 (1.677 sec)\n",
            "train - step 4543: loss = 362484.59 (1.686 sec)\n",
            "train - step 4544: loss = 439224.44 (1.695 sec)\n",
            "train - step 4545: loss = 421822.62 (1.723 sec)\n",
            "train - step 4546: loss = 308142.59 (2.501 sec)\n",
            "train - step 4547: loss = 411273.12 (1.687 sec)\n",
            "train - step 4548: loss = 296885.16 (1.675 sec)\n",
            "train - step 4549: loss = 304679.81 (1.691 sec)\n",
            "train - step 4550: loss = 454524.91 (1.666 sec)\n",
            "train - step 4551: loss = 315517.16 (1.678 sec)\n",
            "train - step 4552: loss = 564822.81 (1.678 sec)\n",
            "train - step 4553: loss = 342574.19 (1.701 sec)\n",
            "train - step 4554: loss = 353786.47 (1.686 sec)\n",
            "train - step 4555: loss = 242806.39 (1.705 sec)\n",
            "train - step 4556: loss = 340982.06 (1.687 sec)\n",
            "train - step 4557: loss = 353652.03 (1.679 sec)\n",
            "train - step 4558: loss = 444626.44 (1.671 sec)\n",
            "train - step 4559: loss = 494333.00 (1.696 sec)\n",
            "train - step 4560: loss = 534493.25 (1.699 sec)\n",
            "train - step 4561: loss = 312005.81 (1.678 sec)\n",
            "train - step 4562: loss = 481273.72 (1.685 sec)\n",
            "train - step 4563: loss = 468530.34 (1.698 sec)\n",
            "train - step 4564: loss = 331391.34 (1.694 sec)\n",
            "train - step 4565: loss = 362484.34 (1.694 sec)\n",
            "train - step 4566: loss = 298983.12 (1.676 sec)\n",
            "train - step 4567: loss = 434309.53 (1.678 sec)\n",
            "train - step 4568: loss = 456870.31 (1.728 sec)\n",
            "train - step 4569: loss = 440102.94 (1.681 sec)\n",
            "train - step 4570: loss = 428364.38 (1.697 sec)\n",
            "train - step 4571: loss = 391425.91 (1.678 sec)\n",
            "train - step 4572: loss = 412647.59 (1.692 sec)\n",
            "train - step 4573: loss = 292023.41 (1.685 sec)\n",
            "train - step 4574: loss = 495631.91 (1.681 sec)\n",
            "train - step 4575: loss = 510607.19 (1.729 sec)\n",
            "train - step 4576: loss = 448863.56 (1.691 sec)\n",
            "train - step 4577: loss = 295759.94 (1.706 sec)\n",
            "train - step 4578: loss = 330120.41 (1.687 sec)\n",
            "train - step 4579: loss = 431576.34 (1.722 sec)\n",
            "train - step 4580: loss = 414713.88 (1.692 sec)\n",
            "train - step 4581: loss = 404585.78 (1.658 sec)\n",
            "train - step 4582: loss = 494547.59 (3.084 sec)\n",
            "train - step 4583: loss = 413380.06 (1.685 sec)\n",
            "train - step 4584: loss = 433366.31 (1.671 sec)\n",
            "train - step 4585: loss = 435904.72 (1.665 sec)\n",
            "train - step 4586: loss = 396538.16 (1.680 sec)\n",
            "train - step 4587: loss = 259375.66 (1.664 sec)\n",
            "train - step 4588: loss = 522494.00 (1.676 sec)\n",
            "train - step 4589: loss = 409249.16 (1.677 sec)\n",
            "train - step 4590: loss = 453448.72 (1.693 sec)\n",
            "train - step 4591: loss = 441378.22 (1.679 sec)\n",
            "train - step 4592: loss = 586184.31 (1.695 sec)\n",
            "train - step 4593: loss = 318434.00 (1.674 sec)\n",
            "train - step 4594: loss = 380262.09 (1.669 sec)\n",
            "train - step 4595: loss = 485159.97 (1.665 sec)\n",
            "train - step 4596: loss = 474327.22 (1.683 sec)\n",
            "train - step 4597: loss = 321587.47 (1.670 sec)\n",
            "train - step 4598: loss = 346049.22 (1.655 sec)\n",
            "train - step 4599: loss = 398360.38 (1.675 sec)\n",
            "train - step 4600: loss = 519875.44 (1.669 sec)\n",
            "train - step 4601: loss = 433609.06 (1.688 sec)\n",
            "train - step 4602: loss = 428720.78 (1.680 sec)\n",
            "train - step 4603: loss = 259715.86 (1.668 sec)\n",
            "train - step 4604: loss = 407745.19 (1.673 sec)\n",
            "train - step 4605: loss = 286373.91 (1.705 sec)\n",
            "train - step 4606: loss = 459469.19 (1.663 sec)\n",
            "train - step 4607: loss = 539033.38 (1.675 sec)\n",
            "train - step 4608: loss = 546368.25 (1.664 sec)\n",
            "train - step 4609: loss = 328551.22 (1.677 sec)\n",
            "train - step 4610: loss = 327384.53 (1.647 sec)\n",
            "train - step 4611: loss = 319078.34 (1.658 sec)\n",
            "train - step 4612: loss = 589742.94 (1.651 sec)\n",
            "train - step 4613: loss = 494786.34 (1.685 sec)\n",
            "train - step 4614: loss = 364785.34 (1.675 sec)\n",
            "train - step 4615: loss = 436694.19 (1.711 sec)\n",
            "train - step 4616: loss = 519046.88 (1.685 sec)\n",
            "train - step 4617: loss = 600003.06 (1.667 sec)\n",
            "train - step 4618: loss = 456408.53 (2.756 sec)\n",
            "train - step 4619: loss = 429687.22 (1.687 sec)\n",
            "train - step 4620: loss = 500662.34 (1.668 sec)\n",
            "train - step 4621: loss = 264823.09 (1.666 sec)\n",
            "train - step 4622: loss = 455063.19 (1.669 sec)\n",
            "train - step 4623: loss = 542313.12 (1.671 sec)\n",
            "train - step 4624: loss = 303659.41 (1.675 sec)\n",
            "train - step 4625: loss = 450990.19 (1.673 sec)\n",
            "train - step 4626: loss = 455821.31 (1.683 sec)\n",
            "train - step 4627: loss = 401954.91 (1.662 sec)\n",
            "train - step 4628: loss = 299060.53 (1.667 sec)\n",
            "train - step 4629: loss = 468979.53 (1.674 sec)\n",
            "train - step 4630: loss = 439548.00 (1.670 sec)\n",
            "train - step 4631: loss = 463165.81 (1.689 sec)\n",
            "train - step 4632: loss = 331677.41 (1.691 sec)\n",
            "train - step 4633: loss = 509054.03 (1.666 sec)\n",
            "train - step 4634: loss = 380072.09 (1.683 sec)\n",
            "train - step 4635: loss = 469387.69 (1.686 sec)\n",
            "train - step 4636: loss = 438501.06 (1.699 sec)\n",
            "train - step 4637: loss = 426324.19 (1.710 sec)\n",
            "train - step 4638: loss = 393608.78 (1.679 sec)\n",
            "train - step 4639: loss = 303960.88 (1.696 sec)\n",
            "train - step 4640: loss = 397465.31 (1.629 sec)\n",
            "train - step 4641: loss = 423229.59 (1.674 sec)\n",
            "train - step 4642: loss = 496377.62 (1.652 sec)\n",
            "train - step 4643: loss = 352017.34 (1.739 sec)\n",
            "train - step 4644: loss = 300703.91 (1.671 sec)\n",
            "train - step 4645: loss = 451141.78 (1.661 sec)\n",
            "train - step 4646: loss = 362402.38 (1.686 sec)\n",
            "train - step 4647: loss = 438024.09 (1.742 sec)\n",
            "train - step 4648: loss = 552030.94 (1.744 sec)\n",
            "train - step 4649: loss = 558751.50 (1.700 sec)\n",
            "train - step 4650: loss = 464184.12 (1.697 sec)\n",
            "train - step 4651: loss = 335454.12 (1.713 sec)\n",
            "train - step 4652: loss = 328025.78 (1.674 sec)\n",
            "train - step 4653: loss = 464592.44 (1.767 sec)\n",
            "train - step 4654: loss = 363096.84 (1.691 sec)\n",
            "train - step 4655: loss = 398096.34 (1.691 sec)\n",
            "train - step 4656: loss = 430940.16 (1.697 sec)\n",
            "train - step 4657: loss = 384920.12 (1.671 sec)\n",
            "train - step 4658: loss = 380904.28 (1.670 sec)\n",
            "train - step 4659: loss = 422946.06 (1.712 sec)\n",
            "train - step 4660: loss = 594030.88 (1.684 sec)\n",
            "train - step 4661: loss = 272356.59 (1.676 sec)\n",
            "train - step 4662: loss = 345142.59 (1.677 sec)\n",
            "train - step 4663: loss = 483681.28 (1.675 sec)\n",
            "train - step 4664: loss = 530048.69 (1.690 sec)\n",
            "train - step 4665: loss = 363570.56 (1.692 sec)\n",
            "train - step 4666: loss = 343176.53 (1.659 sec)\n",
            "train - step 4667: loss = 414367.44 (1.700 sec)\n",
            "train - step 4668: loss = 487649.34 (1.707 sec)\n",
            "train - step 4669: loss = 502222.94 (1.682 sec)\n",
            "train - step 4670: loss = 707609.75 (1.681 sec)\n",
            "train - step 4671: loss = 437994.31 (1.684 sec)\n",
            "train - step 4672: loss = 369128.09 (1.696 sec)\n",
            "train - step 4673: loss = 443048.34 (1.676 sec)\n",
            "train - step 4674: loss = 421928.84 (1.669 sec)\n",
            "train - step 4675: loss = 498537.88 (1.670 sec)\n",
            "train - step 4676: loss = 471816.72 (1.671 sec)\n",
            "train - step 4677: loss = 373218.62 (1.680 sec)\n",
            "train - step 4678: loss = 478581.34 (1.686 sec)\n",
            "train - step 4679: loss = 440474.88 (1.716 sec)\n",
            "train - step 4680: loss = 260277.23 (1.652 sec)\n",
            "train - step 4681: loss = 438463.53 (1.697 sec)\n",
            "train - step 4682: loss = 416774.84 (1.680 sec)\n",
            "train - step 4683: loss = 395612.78 (1.670 sec)\n",
            "train - step 4684: loss = 565247.44 (1.692 sec)\n",
            "train - step 4685: loss = 371444.41 (1.689 sec)\n",
            "train - step 4686: loss = 502399.34 (1.686 sec)\n",
            "train - step 4687: loss = 341352.19 (1.662 sec)\n",
            "train - step 4688: loss = 365831.59 (1.671 sec)\n",
            "train - step 4689: loss = 528269.94 (1.686 sec)\n",
            "train - step 4690: loss = 510321.53 (2.561 sec)\n",
            "train - step 4691: loss = 514521.88 (1.697 sec)\n",
            "train - step 4692: loss = 419291.00 (1.680 sec)\n",
            "train - step 4693: loss = 320653.09 (1.676 sec)\n",
            "train - step 4694: loss = 465514.88 (1.690 sec)\n",
            "train - step 4695: loss = 420324.94 (1.691 sec)\n",
            "train - step 4696: loss = 436552.44 (1.666 sec)\n",
            "train - step 4697: loss = 356456.91 (1.688 sec)\n",
            "train - step 4698: loss = 435284.78 (1.679 sec)\n",
            "train - step 4699: loss = 461566.66 (1.657 sec)\n",
            "train - step 4700: loss = 375646.22 (1.685 sec)\n",
            "train - step 4701: loss = 466555.00 (1.666 sec)\n",
            "train - step 4702: loss = 604239.88 (1.665 sec)\n",
            "train - step 4703: loss = 409456.41 (1.656 sec)\n",
            "train - step 4704: loss = 356268.59 (1.684 sec)\n",
            "train - step 4705: loss = 485940.16 (1.679 sec)\n",
            "train - step 4706: loss = 360620.34 (1.659 sec)\n",
            "train - step 4707: loss = 315158.19 (1.693 sec)\n",
            "train - step 4708: loss = 420303.19 (1.701 sec)\n",
            "train - step 4709: loss = 466328.28 (1.662 sec)\n",
            "train - step 4710: loss = 359212.41 (1.678 sec)\n",
            "train - step 4711: loss = 486768.34 (1.669 sec)\n",
            "train - step 4712: loss = 451370.78 (1.662 sec)\n",
            "train - step 4713: loss = 310090.53 (1.676 sec)\n",
            "train - step 4714: loss = 505670.88 (1.672 sec)\n",
            "train - step 4715: loss = 449303.88 (1.679 sec)\n",
            "train - step 4716: loss = 493122.12 (1.672 sec)\n",
            "train - step 4717: loss = 429034.41 (1.692 sec)\n",
            "train - step 4718: loss = 438190.88 (1.689 sec)\n",
            "train - step 4719: loss = 434528.31 (1.704 sec)\n",
            "train - step 4720: loss = 408153.66 (1.684 sec)\n",
            "train - step 4721: loss = 426210.16 (1.681 sec)\n",
            "train - step 4722: loss = 367823.22 (1.682 sec)\n",
            "train - step 4723: loss = 443076.53 (1.683 sec)\n",
            "train - step 4724: loss = 423258.22 (1.679 sec)\n",
            "train - step 4725: loss = 419375.34 (1.685 sec)\n",
            "train - step 4726: loss = 414281.38 (2.609 sec)\n",
            "train - step 4727: loss = 465433.62 (1.682 sec)\n",
            "train - step 4728: loss = 530418.38 (1.705 sec)\n",
            "train - step 4729: loss = 607265.06 (1.703 sec)\n",
            "train - step 4730: loss = 328205.19 (1.690 sec)\n",
            "train - step 4731: loss = 372018.03 (1.688 sec)\n",
            "train - step 4732: loss = 331459.81 (1.684 sec)\n",
            "train - step 4733: loss = 544437.69 (1.669 sec)\n",
            "train - step 4734: loss = 311642.53 (1.684 sec)\n",
            "train - step 4735: loss = 453520.94 (1.702 sec)\n",
            "train - step 4736: loss = 320299.00 (1.690 sec)\n",
            "train - step 4737: loss = 329370.50 (1.679 sec)\n",
            "train - step 4738: loss = 406463.59 (1.662 sec)\n",
            "train - step 4739: loss = 410683.19 (1.700 sec)\n",
            "train - step 4740: loss = 485201.56 (1.680 sec)\n",
            "train - step 4741: loss = 581678.38 (1.678 sec)\n",
            "train - step 4742: loss = 361071.56 (1.688 sec)\n",
            "train - step 4743: loss = 324172.19 (1.681 sec)\n",
            "train - step 4744: loss = 322588.81 (1.677 sec)\n",
            "train - step 4745: loss = 371361.78 (1.677 sec)\n",
            "train - step 4746: loss = 373863.53 (1.683 sec)\n",
            "train - step 4747: loss = 411922.81 (1.676 sec)\n",
            "train - step 4748: loss = 403223.81 (1.671 sec)\n",
            "train - step 4749: loss = 541909.88 (1.684 sec)\n",
            "train - step 4750: loss = 459847.62 (1.676 sec)\n",
            "train - step 4751: loss = 415167.78 (1.681 sec)\n",
            "train - step 4752: loss = 479017.16 (1.660 sec)\n",
            "train - step 4753: loss = 398492.94 (1.703 sec)\n",
            "train - step 4754: loss = 342812.84 (1.705 sec)\n",
            "train - step 4755: loss = 536070.06 (1.690 sec)\n",
            "train - step 4756: loss = 430584.06 (1.695 sec)\n",
            "train - step 4757: loss = 268382.03 (1.672 sec)\n",
            "train - step 4758: loss = 370038.06 (1.676 sec)\n",
            "train - step 4759: loss = 373158.34 (1.695 sec)\n",
            "train - step 4760: loss = 486649.56 (1.696 sec)\n",
            "train - step 4761: loss = 574098.62 (1.690 sec)\n",
            "train - step 4762: loss = 474409.09 (2.636 sec)\n",
            "train - step 4763: loss = 355435.66 (1.715 sec)\n",
            "train - step 4764: loss = 386637.94 (1.669 sec)\n",
            "train - step 4765: loss = 440518.22 (1.687 sec)\n",
            "train - step 4766: loss = 268689.03 (1.696 sec)\n",
            "train - step 4767: loss = 359588.12 (1.687 sec)\n",
            "train - step 4768: loss = 385984.97 (1.676 sec)\n",
            "train - step 4769: loss = 400035.50 (1.674 sec)\n",
            "train - step 4770: loss = 393794.38 (1.695 sec)\n",
            "train - step 4771: loss = 364810.59 (1.667 sec)\n",
            "train - step 4772: loss = 371227.84 (1.687 sec)\n",
            "train - step 4773: loss = 421523.12 (1.680 sec)\n",
            "train - step 4774: loss = 492044.19 (1.686 sec)\n",
            "train - step 4775: loss = 423739.88 (1.708 sec)\n",
            "train - step 4776: loss = 363701.00 (1.675 sec)\n",
            "train - step 4777: loss = 357911.53 (1.665 sec)\n",
            "train - step 4778: loss = 560829.31 (1.678 sec)\n",
            "train - step 4779: loss = 407180.59 (1.687 sec)\n",
            "train - step 4780: loss = 460780.97 (1.681 sec)\n",
            "train - step 4781: loss = 415595.66 (1.672 sec)\n",
            "train - step 4782: loss = 418273.62 (1.683 sec)\n",
            "train - step 4783: loss = 342153.41 (1.675 sec)\n",
            "train - step 4784: loss = 289544.16 (1.672 sec)\n",
            "train - step 4785: loss = 571070.56 (1.661 sec)\n",
            "train - step 4786: loss = 420355.69 (1.670 sec)\n",
            "train - step 4787: loss = 332875.84 (1.674 sec)\n",
            "train - step 4788: loss = 348966.66 (1.680 sec)\n",
            "train - step 4789: loss = 357047.91 (1.655 sec)\n",
            "train - step 4790: loss = 393340.28 (1.646 sec)\n",
            "train - step 4791: loss = 423670.19 (1.651 sec)\n",
            "train - step 4792: loss = 412664.38 (1.682 sec)\n",
            "train - step 4793: loss = 366661.44 (1.682 sec)\n",
            "train - step 4794: loss = 501177.19 (1.666 sec)\n",
            "train - step 4795: loss = 500709.56 (1.686 sec)\n",
            "train - step 4796: loss = 392594.19 (1.696 sec)\n",
            "train - step 4797: loss = 312957.06 (1.695 sec)\n",
            "train - step 4798: loss = 377810.34 (2.501 sec)\n",
            "train - step 4799: loss = 526544.38 (1.681 sec)\n",
            "train - step 4800: loss = 360100.94 (1.675 sec)\n",
            "train - step 4801: loss = 443816.50 (1.692 sec)\n",
            "train - step 4802: loss = 379254.94 (1.672 sec)\n",
            "train - step 4803: loss = 447376.00 (1.670 sec)\n",
            "train - step 4804: loss = 425284.56 (1.661 sec)\n",
            "train - step 4805: loss = 342907.69 (1.688 sec)\n",
            "train - step 4806: loss = 358248.72 (1.681 sec)\n",
            "train - step 4807: loss = 400424.88 (1.689 sec)\n",
            "train - step 4808: loss = 624487.44 (1.692 sec)\n",
            "train - step 4809: loss = 432886.09 (1.659 sec)\n",
            "train - step 4810: loss = 448763.12 (1.676 sec)\n",
            "train - step 4811: loss = 337733.47 (1.669 sec)\n",
            "train - step 4812: loss = 519755.47 (1.692 sec)\n",
            "train - step 4813: loss = 435759.94 (1.675 sec)\n",
            "train - step 4814: loss = 420961.69 (1.666 sec)\n",
            "train - step 4815: loss = 380487.94 (1.679 sec)\n",
            "train - step 4816: loss = 427972.22 (1.675 sec)\n",
            "train - step 4817: loss = 313544.94 (1.677 sec)\n",
            "train - step 4818: loss = 341562.53 (1.685 sec)\n",
            "train - step 4819: loss = 558953.56 (1.691 sec)\n",
            "train - step 4820: loss = 478423.72 (1.698 sec)\n",
            "train - step 4821: loss = 400528.38 (1.653 sec)\n",
            "train - step 4822: loss = 246580.27 (1.643 sec)\n",
            "train - step 4823: loss = 328831.69 (1.639 sec)\n",
            "train - step 4824: loss = 321084.22 (1.654 sec)\n",
            "train - step 4825: loss = 391314.47 (1.631 sec)\n",
            "train - step 4826: loss = 339647.78 (1.665 sec)\n",
            "train - step 4827: loss = 385546.72 (1.642 sec)\n",
            "train - step 4828: loss = 343631.41 (1.648 sec)\n",
            "train - step 4829: loss = 288481.09 (1.689 sec)\n",
            "train - step 4830: loss = 267091.59 (1.672 sec)\n",
            "train - step 4831: loss = 362776.88 (1.714 sec)\n",
            "train - step 4832: loss = 348305.91 (1.674 sec)\n",
            "train - step 4833: loss = 349977.81 (1.669 sec)\n",
            "train - step 4834: loss = 536024.44 (2.814 sec)\n",
            "train - step 4835: loss = 297557.00 (1.690 sec)\n",
            "train - step 4836: loss = 305021.78 (1.696 sec)\n",
            "train - step 4837: loss = 340045.34 (1.688 sec)\n",
            "train - step 4838: loss = 477981.47 (1.693 sec)\n",
            "train - step 4839: loss = 445751.28 (1.663 sec)\n",
            "train - step 4840: loss = 495764.12 (1.674 sec)\n",
            "train - step 4841: loss = 344545.66 (1.657 sec)\n",
            "train - step 4842: loss = 485708.84 (1.672 sec)\n",
            "train - step 4843: loss = 535066.69 (1.678 sec)\n",
            "train - step 4844: loss = 363173.59 (1.662 sec)\n",
            "train - step 4845: loss = 307283.97 (1.678 sec)\n",
            "train - step 4846: loss = 542276.31 (1.679 sec)\n",
            "train - step 4847: loss = 286424.16 (1.678 sec)\n",
            "train - step 4848: loss = 404994.06 (1.659 sec)\n",
            "train - step 4849: loss = 342312.69 (1.664 sec)\n",
            "train - step 4850: loss = 511027.34 (1.700 sec)\n",
            "train - step 4851: loss = 353834.81 (1.684 sec)\n",
            "train - step 4852: loss = 350968.19 (1.666 sec)\n",
            "train - step 4853: loss = 405933.66 (1.679 sec)\n",
            "train - step 4854: loss = 522045.53 (1.650 sec)\n",
            "train - step 4855: loss = 578069.06 (1.686 sec)\n",
            "train - step 4856: loss = 423519.47 (1.672 sec)\n",
            "train - step 4857: loss = 416567.34 (1.670 sec)\n",
            "train - step 4858: loss = 603161.00 (1.665 sec)\n",
            "train - step 4859: loss = 515770.06 (1.704 sec)\n",
            "train - step 4860: loss = 464539.03 (1.672 sec)\n",
            "train - step 4861: loss = 343214.81 (1.672 sec)\n",
            "train - step 4862: loss = 329055.97 (1.693 sec)\n",
            "train - step 4863: loss = 316641.56 (1.662 sec)\n",
            "train - step 4864: loss = 436624.16 (1.703 sec)\n",
            "train - step 4865: loss = 496251.47 (1.672 sec)\n",
            "train - step 4866: loss = 279789.78 (1.677 sec)\n",
            "train - step 4867: loss = 413031.59 (1.686 sec)\n",
            "train - step 4868: loss = 386783.69 (1.667 sec)\n",
            "train - step 4869: loss = 432382.00 (1.684 sec)\n",
            "train - step 4870: loss = 372991.00 (2.801 sec)\n",
            "train - step 4871: loss = 299171.16 (1.696 sec)\n",
            "train - step 4872: loss = 466131.97 (1.677 sec)\n",
            "train - step 4873: loss = 372519.28 (1.677 sec)\n",
            "train - step 4874: loss = 554333.56 (1.680 sec)\n",
            "train - step 4875: loss = 496754.91 (1.675 sec)\n",
            "train - step 4876: loss = 415292.03 (1.680 sec)\n",
            "train - step 4877: loss = 503529.47 (1.690 sec)\n",
            "train - step 4878: loss = 363427.81 (1.673 sec)\n",
            "train - step 4879: loss = 401952.69 (1.664 sec)\n",
            "train - step 4880: loss = 347668.47 (1.699 sec)\n",
            "train - step 4881: loss = 362021.47 (1.679 sec)\n",
            "train - step 4882: loss = 368872.09 (1.687 sec)\n",
            "train - step 4883: loss = 605987.12 (1.660 sec)\n",
            "train - step 4884: loss = 270933.47 (1.696 sec)\n",
            "train - step 4885: loss = 388621.72 (1.694 sec)\n",
            "train - step 4886: loss = 534317.81 (1.662 sec)\n",
            "train - step 4887: loss = 462755.97 (1.666 sec)\n",
            "train - step 4888: loss = 378207.78 (1.679 sec)\n",
            "train - step 4889: loss = 267475.84 (1.668 sec)\n",
            "train - step 4890: loss = 381901.41 (1.672 sec)\n",
            "train - step 4891: loss = 441084.81 (1.681 sec)\n",
            "train - step 4892: loss = 499419.22 (1.659 sec)\n",
            "train - step 4893: loss = 478351.16 (1.660 sec)\n",
            "train - step 4894: loss = 399018.19 (1.666 sec)\n",
            "train - step 4895: loss = 322961.91 (1.668 sec)\n",
            "train - step 4896: loss = 393257.69 (1.676 sec)\n",
            "train - step 4897: loss = 466214.09 (1.659 sec)\n",
            "train - step 4898: loss = 394930.00 (1.675 sec)\n",
            "train - step 4899: loss = 385089.66 (1.666 sec)\n",
            "train - step 4900: loss = 444311.97 (1.679 sec)\n",
            "train - step 4901: loss = 365306.22 (1.675 sec)\n",
            "train - step 4902: loss = 444811.22 (1.695 sec)\n",
            "train - step 4903: loss = 424082.84 (1.658 sec)\n",
            "train - step 4904: loss = 394101.50 (1.675 sec)\n",
            "train - step 4905: loss = 416449.69 (1.681 sec)\n",
            "train - step 4906: loss = 350485.50 (2.643 sec)\n",
            "train - step 4907: loss = 471162.47 (1.696 sec)\n",
            "train - step 4908: loss = 364366.81 (1.666 sec)\n",
            "train - step 4909: loss = 396936.12 (1.669 sec)\n",
            "train - step 4910: loss = 476272.38 (1.687 sec)\n",
            "train - step 4911: loss = 456804.94 (1.673 sec)\n",
            "train - step 4912: loss = 473302.00 (1.693 sec)\n",
            "train - step 4913: loss = 385663.12 (1.669 sec)\n",
            "train - step 4914: loss = 513984.16 (1.702 sec)\n",
            "train - step 4915: loss = 355400.62 (1.674 sec)\n",
            "train - step 4916: loss = 549352.81 (1.680 sec)\n",
            "train - step 4917: loss = 333482.84 (1.696 sec)\n",
            "train - step 4918: loss = 339923.66 (1.674 sec)\n",
            "train - step 4919: loss = 370014.88 (1.706 sec)\n",
            "train - step 4920: loss = 521055.91 (1.707 sec)\n",
            "train - step 4921: loss = 398906.09 (1.672 sec)\n",
            "train - step 4922: loss = 377120.16 (1.683 sec)\n",
            "train - step 4923: loss = 325326.59 (1.668 sec)\n",
            "train - step 4924: loss = 288994.09 (1.677 sec)\n",
            "train - step 4925: loss = 377705.41 (1.689 sec)\n",
            "train - step 4926: loss = 354265.88 (1.688 sec)\n",
            "train - step 4927: loss = 332287.59 (1.673 sec)\n",
            "train - step 4928: loss = 225593.28 (1.680 sec)\n",
            "train - step 4929: loss = 423007.62 (1.676 sec)\n",
            "train - step 4930: loss = 364064.66 (1.671 sec)\n",
            "train - step 4931: loss = 247230.36 (1.693 sec)\n",
            "train - step 4932: loss = 394778.38 (1.695 sec)\n",
            "train - step 4933: loss = 364668.00 (1.699 sec)\n",
            "train - step 4934: loss = 359682.41 (1.681 sec)\n",
            "train - step 4935: loss = 473107.69 (1.723 sec)\n",
            "train - step 4936: loss = 341608.66 (1.692 sec)\n",
            "train - step 4937: loss = 385455.62 (1.693 sec)\n",
            "train - step 4938: loss = 483531.44 (1.687 sec)\n",
            "train - step 4939: loss = 418546.81 (1.711 sec)\n",
            "train - step 4940: loss = 365948.06 (1.702 sec)\n",
            "train - step 4941: loss = 407245.78 (1.686 sec)\n",
            "train - step 4942: loss = 475315.03 (1.697 sec)\n",
            "train - step 4943: loss = 299230.59 (1.679 sec)\n",
            "train - step 4944: loss = 451721.62 (1.668 sec)\n",
            "train - step 4945: loss = 392086.19 (1.672 sec)\n",
            "train - step 4946: loss = 391741.34 (1.682 sec)\n",
            "train - step 4947: loss = 442934.78 (1.699 sec)\n",
            "train - step 4948: loss = 494370.19 (1.678 sec)\n",
            "train - step 4949: loss = 694143.19 (1.714 sec)\n",
            "train - step 4950: loss = 477136.56 (1.806 sec)\n",
            "train - step 4951: loss = 449727.88 (1.689 sec)\n",
            "train - step 4952: loss = 387097.59 (1.660 sec)\n",
            "train - step 4953: loss = 416813.94 (1.701 sec)\n",
            "train - step 4954: loss = 436233.09 (1.680 sec)\n",
            "train - step 4955: loss = 344654.41 (1.659 sec)\n",
            "train - step 4956: loss = 354481.53 (1.682 sec)\n",
            "train - step 4957: loss = 388897.81 (1.680 sec)\n",
            "train - step 4958: loss = 427053.50 (1.698 sec)\n",
            "train - step 4959: loss = 343049.31 (1.689 sec)\n",
            "train - step 4960: loss = 395182.28 (1.679 sec)\n",
            "train - step 4961: loss = 347941.31 (1.676 sec)\n",
            "train - step 4962: loss = 377561.00 (1.680 sec)\n",
            "train - step 4963: loss = 325594.09 (1.667 sec)\n",
            "train - step 4964: loss = 414138.22 (1.677 sec)\n",
            "train - step 4965: loss = 455766.03 (1.683 sec)\n",
            "train - step 4966: loss = 522371.03 (1.680 sec)\n",
            "train - step 4967: loss = 356269.28 (1.708 sec)\n",
            "train - step 4968: loss = 550878.19 (1.664 sec)\n",
            "train - step 4969: loss = 346586.00 (1.659 sec)\n",
            "train - step 4970: loss = 251253.41 (1.639 sec)\n",
            "train - step 4971: loss = 537146.94 (1.670 sec)\n",
            "train - step 4972: loss = 306257.97 (1.671 sec)\n",
            "train - step 4973: loss = 422801.53 (1.665 sec)\n",
            "train - step 4974: loss = 435140.28 (1.682 sec)\n",
            "train - step 4975: loss = 332363.59 (1.675 sec)\n",
            "train - step 4976: loss = 420675.84 (1.669 sec)\n",
            "train - step 4977: loss = 289636.97 (1.671 sec)\n",
            "train - step 4978: loss = 377781.28 (3.002 sec)\n",
            "train - step 4979: loss = 401004.62 (1.699 sec)\n",
            "train - step 4980: loss = 351969.28 (1.693 sec)\n",
            "train - step 4981: loss = 504822.81 (1.682 sec)\n",
            "train - step 4982: loss = 468990.91 (1.688 sec)\n",
            "train - step 4983: loss = 427043.47 (1.692 sec)\n",
            "train - step 4984: loss = 354646.53 (1.686 sec)\n",
            "train - step 4985: loss = 430223.12 (1.672 sec)\n",
            "train - step 4986: loss = 641047.62 (1.676 sec)\n",
            "train - step 4987: loss = 358671.50 (1.667 sec)\n",
            "train - step 4988: loss = 472901.69 (1.665 sec)\n",
            "train - step 4989: loss = 496249.03 (1.680 sec)\n",
            "train - step 4990: loss = 512028.06 (1.673 sec)\n",
            "train - step 4991: loss = 441014.28 (1.680 sec)\n",
            "train - step 4992: loss = 284521.66 (1.678 sec)\n",
            "train - step 4993: loss = 305063.12 (1.667 sec)\n",
            "train - step 4994: loss = 457690.84 (1.703 sec)\n",
            "train - step 4995: loss = 502017.88 (1.664 sec)\n",
            "train - step 4996: loss = 497561.88 (1.689 sec)\n",
            "train - step 4997: loss = 298370.91 (1.689 sec)\n",
            "train - step 4998: loss = 407225.53 (1.683 sec)\n",
            "train - step 4999: loss = 388284.31 (1.695 sec)\n",
            "train - step 5000: loss = 425852.50 (1.702 sec)\n",
            "train - step 5001: loss = 359271.06 (1.680 sec)\n",
            "train - step 5002: loss = 509353.19 (1.690 sec)\n",
            "train - step 5003: loss = 399380.62 (1.666 sec)\n",
            "train - step 5004: loss = 347671.03 (1.667 sec)\n",
            "train - step 5005: loss = 513552.47 (1.658 sec)\n",
            "train - step 5006: loss = 395937.66 (1.622 sec)\n",
            "train - step 5007: loss = 364410.19 (1.644 sec)\n",
            "train - step 5008: loss = 353878.78 (1.642 sec)\n",
            "train - step 5009: loss = 401533.66 (1.638 sec)\n",
            "train - step 5010: loss = 418802.31 (1.655 sec)\n",
            "train - step 5011: loss = 420181.00 (1.664 sec)\n",
            "train - step 5012: loss = 685356.12 (1.690 sec)\n",
            "train - step 5013: loss = 456747.72 (1.673 sec)\n",
            "train - step 5014: loss = 293266.56 (2.221 sec)\n",
            "train - step 5015: loss = 303168.94 (1.669 sec)\n",
            "train - step 5016: loss = 335417.47 (1.668 sec)\n",
            "train - step 5017: loss = 483548.50 (1.686 sec)\n",
            "train - step 5018: loss = 425252.56 (1.676 sec)\n",
            "train - step 5019: loss = 416865.66 (1.684 sec)\n",
            "train - step 5020: loss = 337801.91 (1.672 sec)\n",
            "train - step 5021: loss = 533062.56 (1.665 sec)\n",
            "train - step 5022: loss = 315206.53 (1.684 sec)\n",
            "train - step 5023: loss = 280271.50 (1.675 sec)\n",
            "train - step 5024: loss = 398841.22 (1.664 sec)\n",
            "train - step 5025: loss = 435688.41 (1.686 sec)\n",
            "train - step 5026: loss = 352170.22 (1.672 sec)\n",
            "train - step 5027: loss = 347729.16 (1.714 sec)\n",
            "train - step 5028: loss = 407274.88 (1.681 sec)\n",
            "train - step 5029: loss = 510348.97 (1.665 sec)\n",
            "train - step 5030: loss = 405244.44 (1.696 sec)\n",
            "train - step 5031: loss = 450105.62 (1.666 sec)\n",
            "train - step 5032: loss = 311716.00 (1.673 sec)\n",
            "train - step 5033: loss = 417819.72 (1.685 sec)\n",
            "train - step 5034: loss = 393769.56 (1.687 sec)\n",
            "train - step 5035: loss = 468491.19 (1.682 sec)\n",
            "train - step 5036: loss = 313972.66 (1.677 sec)\n",
            "train - step 5037: loss = 298622.06 (1.676 sec)\n",
            "train - step 5038: loss = 432263.72 (1.679 sec)\n",
            "train - step 5039: loss = 408680.50 (1.673 sec)\n",
            "train - step 5040: loss = 459650.62 (1.676 sec)\n",
            "train - step 5041: loss = 385850.38 (1.678 sec)\n",
            "train - step 5042: loss = 287582.03 (1.667 sec)\n",
            "train - step 5043: loss = 415007.22 (1.673 sec)\n",
            "train - step 5044: loss = 290318.28 (1.686 sec)\n",
            "train - step 5045: loss = 408237.34 (1.675 sec)\n",
            "train - step 5046: loss = 359962.47 (1.678 sec)\n",
            "train - step 5047: loss = 344999.06 (1.684 sec)\n",
            "train - step 5048: loss = 512880.62 (1.670 sec)\n",
            "train - step 5049: loss = 494271.12 (1.682 sec)\n",
            "train - step 5050: loss = 267257.28 (2.528 sec)\n",
            "train - step 5051: loss = 358163.59 (1.708 sec)\n",
            "train - step 5052: loss = 182846.14 (1.663 sec)\n",
            "train - step 5053: loss = 578753.62 (1.695 sec)\n",
            "train - step 5054: loss = 352005.00 (1.677 sec)\n",
            "train - step 5055: loss = 345340.94 (1.688 sec)\n",
            "train - step 5056: loss = 376159.03 (1.669 sec)\n",
            "train - step 5057: loss = 323046.00 (1.682 sec)\n",
            "train - step 5058: loss = 548566.44 (1.688 sec)\n",
            "train - step 5059: loss = 458521.44 (1.677 sec)\n",
            "train - step 5060: loss = 408852.84 (1.678 sec)\n",
            "train - step 5061: loss = 606303.00 (1.676 sec)\n",
            "train - step 5062: loss = 392823.22 (1.668 sec)\n",
            "train - step 5063: loss = 422421.62 (1.680 sec)\n",
            "train - step 5064: loss = 461762.38 (1.671 sec)\n",
            "train - step 5065: loss = 317294.50 (1.684 sec)\n",
            "train - step 5066: loss = 481115.97 (1.700 sec)\n",
            "train - step 5067: loss = 318725.41 (1.694 sec)\n",
            "train - step 5068: loss = 570122.38 (1.666 sec)\n",
            "train - step 5069: loss = 357899.66 (1.685 sec)\n",
            "train - step 5070: loss = 485852.72 (1.686 sec)\n",
            "train - step 5071: loss = 405468.91 (1.675 sec)\n",
            "train - step 5072: loss = 354057.66 (1.698 sec)\n",
            "train - step 5073: loss = 422233.44 (1.682 sec)\n",
            "train - step 5074: loss = 511117.91 (1.680 sec)\n",
            "train - step 5075: loss = 559524.06 (1.689 sec)\n",
            "train - step 5076: loss = 517308.16 (1.676 sec)\n",
            "train - step 5077: loss = 439199.66 (1.681 sec)\n",
            "train - step 5078: loss = 422736.06 (1.697 sec)\n",
            "train - step 5079: loss = 386779.69 (1.668 sec)\n",
            "train - step 5080: loss = 426648.09 (1.684 sec)\n",
            "train - step 5081: loss = 409137.44 (1.676 sec)\n",
            "train - step 5082: loss = 449516.91 (1.675 sec)\n",
            "train - step 5083: loss = 346219.09 (1.698 sec)\n",
            "train - step 5084: loss = 374829.28 (1.708 sec)\n",
            "train - step 5085: loss = 656573.62 (1.673 sec)\n",
            "train - step 5086: loss = 447412.97 (2.525 sec)\n",
            "train - step 5087: loss = 468703.34 (1.677 sec)\n",
            "train - step 5088: loss = 540907.31 (1.668 sec)\n",
            "train - step 5089: loss = 548504.06 (1.681 sec)\n",
            "train - step 5090: loss = 404120.47 (1.695 sec)\n",
            "train - step 5091: loss = 464284.69 (1.697 sec)\n",
            "train - step 5092: loss = 456691.56 (1.673 sec)\n",
            "train - step 5093: loss = 480400.62 (1.684 sec)\n",
            "train - step 5094: loss = 404017.41 (1.679 sec)\n",
            "train - step 5095: loss = 432869.53 (1.679 sec)\n",
            "train - step 5096: loss = 330824.03 (1.673 sec)\n",
            "train - step 5097: loss = 359673.97 (1.669 sec)\n",
            "train - step 5098: loss = 299764.59 (1.668 sec)\n",
            "train - step 5099: loss = 604045.56 (1.675 sec)\n",
            "train - step 5100: loss = 341339.72 (1.679 sec)\n",
            "train - step 5101: loss = 371162.19 (1.692 sec)\n",
            "train - step 5102: loss = 394505.47 (1.680 sec)\n",
            "train - step 5103: loss = 351349.44 (1.703 sec)\n",
            "train - step 5104: loss = 456990.53 (1.689 sec)\n",
            "train - step 5105: loss = 335947.81 (1.674 sec)\n",
            "train - step 5106: loss = 388843.50 (1.676 sec)\n",
            "train - step 5107: loss = 361285.81 (1.690 sec)\n",
            "train - step 5108: loss = 390354.91 (1.681 sec)\n",
            "train - step 5109: loss = 469473.06 (1.664 sec)\n",
            "train - step 5110: loss = 388087.34 (1.686 sec)\n",
            "train - step 5111: loss = 431918.66 (1.672 sec)\n",
            "train - step 5112: loss = 394904.56 (1.672 sec)\n",
            "train - step 5113: loss = 449133.38 (1.683 sec)\n",
            "train - step 5114: loss = 340406.41 (1.664 sec)\n",
            "train - step 5115: loss = 439157.41 (1.683 sec)\n",
            "train - step 5116: loss = 431566.47 (1.684 sec)\n",
            "train - step 5117: loss = 440506.16 (1.686 sec)\n",
            "train - step 5118: loss = 494733.97 (1.696 sec)\n",
            "train - step 5119: loss = 424653.47 (1.681 sec)\n",
            "train - step 5120: loss = 427007.72 (1.681 sec)\n",
            "train - step 5121: loss = 591223.06 (1.686 sec)\n",
            "train - step 5122: loss = 403989.22 (2.604 sec)\n",
            "train - step 5123: loss = 385462.28 (1.690 sec)\n",
            "train - step 5124: loss = 360199.91 (1.707 sec)\n",
            "train - step 5125: loss = 441032.53 (1.678 sec)\n",
            "train - step 5126: loss = 364757.97 (1.679 sec)\n",
            "train - step 5127: loss = 420753.53 (1.674 sec)\n",
            "train - step 5128: loss = 535242.19 (1.690 sec)\n",
            "train - step 5129: loss = 494323.50 (1.672 sec)\n",
            "train - step 5130: loss = 396675.88 (1.671 sec)\n",
            "train - step 5131: loss = 467909.19 (1.682 sec)\n",
            "train - step 5132: loss = 538918.56 (1.672 sec)\n",
            "train - step 5133: loss = 417996.88 (1.676 sec)\n",
            "train - step 5134: loss = 633218.75 (1.684 sec)\n",
            "train - step 5135: loss = 371455.50 (1.685 sec)\n",
            "train - step 5136: loss = 402988.53 (1.664 sec)\n",
            "train - step 5137: loss = 458609.00 (1.679 sec)\n",
            "train - step 5138: loss = 250193.53 (1.655 sec)\n",
            "train - step 5139: loss = 673690.00 (1.690 sec)\n",
            "train - step 5140: loss = 540231.12 (1.671 sec)\n",
            "train - step 5141: loss = 427903.31 (1.699 sec)\n",
            "train - step 5142: loss = 495579.28 (1.687 sec)\n",
            "train - step 5143: loss = 448571.84 (1.668 sec)\n",
            "train - step 5144: loss = 479145.78 (1.672 sec)\n",
            "train - step 5145: loss = 343796.38 (1.670 sec)\n",
            "train - step 5146: loss = 395624.97 (1.679 sec)\n",
            "train - step 5147: loss = 357854.94 (1.677 sec)\n",
            "train - step 5148: loss = 354520.44 (1.640 sec)\n",
            "train - step 5149: loss = 394910.97 (1.632 sec)\n",
            "train - step 5150: loss = 408074.91 (1.632 sec)\n",
            "train - step 5151: loss = 335982.28 (1.666 sec)\n",
            "train - step 5152: loss = 474112.06 (1.679 sec)\n",
            "train - step 5153: loss = 562896.31 (1.705 sec)\n",
            "train - step 5154: loss = 378689.69 (1.683 sec)\n",
            "train - step 5155: loss = 414013.44 (1.668 sec)\n",
            "train - step 5156: loss = 501557.19 (1.676 sec)\n",
            "train - step 5157: loss = 374974.91 (1.691 sec)\n",
            "train - step 5158: loss = 445786.88 (3.041 sec)\n",
            "train - step 5159: loss = 412947.22 (1.685 sec)\n",
            "train - step 5160: loss = 288374.72 (1.682 sec)\n",
            "train - step 5161: loss = 450078.16 (1.683 sec)\n",
            "train - step 5162: loss = 486176.78 (1.684 sec)\n",
            "train - step 5163: loss = 493662.47 (1.679 sec)\n",
            "train - step 5164: loss = 349859.28 (1.686 sec)\n",
            "train - step 5165: loss = 393160.22 (1.665 sec)\n",
            "train - step 5166: loss = 399846.09 (1.698 sec)\n",
            "train - step 5167: loss = 332109.97 (1.690 sec)\n",
            "train - step 5168: loss = 250545.58 (1.679 sec)\n",
            "train - step 5169: loss = 327932.22 (1.661 sec)\n",
            "train - step 5170: loss = 454215.97 (1.681 sec)\n",
            "train - step 5171: loss = 352075.03 (1.681 sec)\n",
            "train - step 5172: loss = 256881.75 (1.677 sec)\n",
            "train - step 5173: loss = 497501.53 (1.666 sec)\n",
            "train - step 5174: loss = 406251.03 (1.677 sec)\n",
            "train - step 5175: loss = 408323.69 (1.675 sec)\n",
            "train - step 5176: loss = 454711.72 (1.679 sec)\n",
            "train - step 5177: loss = 459628.91 (1.670 sec)\n",
            "train - step 5178: loss = 438572.81 (1.688 sec)\n",
            "train - step 5179: loss = 374744.00 (1.683 sec)\n",
            "train - step 5180: loss = 511678.38 (1.675 sec)\n",
            "train - step 5181: loss = 508315.44 (1.685 sec)\n",
            "train - step 5182: loss = 279418.19 (1.673 sec)\n",
            "train - step 5183: loss = 332194.88 (1.680 sec)\n",
            "train - step 5184: loss = 445838.06 (1.667 sec)\n",
            "train - step 5185: loss = 377619.00 (1.684 sec)\n",
            "train - step 5186: loss = 407541.38 (1.672 sec)\n",
            "train - step 5187: loss = 303513.38 (1.648 sec)\n",
            "train - step 5188: loss = 387776.28 (1.647 sec)\n",
            "train - step 5189: loss = 480852.12 (1.637 sec)\n",
            "train - step 5190: loss = 532515.94 (1.637 sec)\n",
            "train - step 5191: loss = 399454.00 (1.653 sec)\n",
            "train - step 5192: loss = 310456.22 (1.644 sec)\n",
            "train - step 5193: loss = 366333.03 (1.669 sec)\n",
            "train - step 5194: loss = 388846.47 (2.578 sec)\n",
            "train - step 5195: loss = 427099.94 (1.695 sec)\n",
            "train - step 5196: loss = 545226.25 (1.672 sec)\n",
            "train - step 5197: loss = 414912.50 (1.669 sec)\n",
            "train - step 5198: loss = 357574.94 (1.683 sec)\n",
            "train - step 5199: loss = 377102.00 (1.683 sec)\n",
            "train - step 5200: loss = 268427.53 (1.682 sec)\n",
            "train - step 5201: loss = 355121.19 (1.708 sec)\n",
            "train - step 5202: loss = 562213.12 (1.674 sec)\n",
            "train - step 5203: loss = 293010.00 (1.664 sec)\n",
            "train - step 5204: loss = 440312.00 (1.691 sec)\n",
            "train - step 5205: loss = 393749.00 (1.669 sec)\n",
            "train - step 5206: loss = 491230.56 (1.683 sec)\n",
            "train - step 5207: loss = 414480.66 (1.664 sec)\n",
            "train - step 5208: loss = 320549.06 (1.672 sec)\n",
            "train - step 5209: loss = 367901.81 (1.685 sec)\n",
            "train - step 5210: loss = 501835.03 (1.673 sec)\n",
            "train - step 5211: loss = 294528.88 (1.693 sec)\n",
            "train - step 5212: loss = 293183.44 (1.680 sec)\n",
            "train - step 5213: loss = 398516.34 (1.680 sec)\n",
            "train - step 5214: loss = 558478.38 (1.686 sec)\n",
            "train - step 5215: loss = 492882.59 (1.703 sec)\n",
            "train - step 5216: loss = 380841.03 (1.673 sec)\n",
            "train - step 5217: loss = 384144.38 (1.675 sec)\n",
            "train - step 5218: loss = 440193.38 (1.671 sec)\n",
            "train - step 5219: loss = 284542.72 (1.681 sec)\n",
            "train - step 5220: loss = 260226.02 (1.679 sec)\n",
            "train - step 5221: loss = 341429.91 (1.699 sec)\n",
            "train - step 5222: loss = 429935.62 (1.692 sec)\n",
            "train - step 5223: loss = 490479.56 (1.689 sec)\n",
            "train - step 5224: loss = 340136.44 (1.694 sec)\n",
            "train - step 5225: loss = 269312.50 (1.670 sec)\n",
            "train - step 5226: loss = 261834.23 (1.668 sec)\n",
            "train - step 5227: loss = 346608.41 (1.658 sec)\n",
            "train - step 5228: loss = 513154.97 (1.688 sec)\n",
            "train - step 5229: loss = 467339.09 (1.694 sec)\n",
            "train - step 5230: loss = 445706.44 (2.657 sec)\n",
            "train - step 5231: loss = 399211.91 (1.741 sec)\n",
            "train - step 5232: loss = 361254.00 (1.683 sec)\n",
            "train - step 5233: loss = 353830.97 (1.709 sec)\n",
            "train - step 5234: loss = 486117.16 (1.694 sec)\n",
            "train - step 5235: loss = 332975.16 (1.722 sec)\n",
            "train - step 5236: loss = 382807.28 (1.703 sec)\n",
            "train - step 5237: loss = 448581.69 (1.695 sec)\n",
            "train - step 5238: loss = 315802.28 (1.684 sec)\n",
            "train - step 5239: loss = 453411.78 (1.681 sec)\n",
            "train - step 5240: loss = 317699.22 (1.874 sec)\n",
            "train - step 5241: loss = 465879.94 (1.682 sec)\n",
            "train - step 5242: loss = 445619.19 (1.693 sec)\n",
            "train - step 5243: loss = 467498.88 (1.666 sec)\n",
            "train - step 5244: loss = 252055.47 (1.671 sec)\n",
            "train - step 5245: loss = 608883.00 (1.681 sec)\n",
            "train - step 5246: loss = 424165.97 (1.678 sec)\n",
            "train - step 5247: loss = 525925.62 (1.669 sec)\n",
            "train - step 5248: loss = 419525.38 (1.687 sec)\n",
            "train - step 5249: loss = 434546.72 (1.660 sec)\n",
            "train - step 5250: loss = 507956.50 (1.675 sec)\n",
            "train - step 5251: loss = 424884.09 (1.669 sec)\n",
            "train - step 5252: loss = 297994.69 (1.675 sec)\n",
            "train - step 5253: loss = 415824.69 (1.671 sec)\n",
            "train - step 5254: loss = 228329.80 (1.672 sec)\n",
            "train - step 5255: loss = 445975.59 (1.696 sec)\n",
            "train - step 5256: loss = 408563.09 (1.669 sec)\n",
            "train - step 5257: loss = 416598.41 (1.692 sec)\n",
            "train - step 5258: loss = 377251.78 (1.692 sec)\n",
            "train - step 5259: loss = 352964.59 (1.685 sec)\n",
            "train - step 5260: loss = 349651.59 (1.676 sec)\n",
            "train - step 5261: loss = 290005.44 (1.683 sec)\n",
            "train - step 5262: loss = 428860.78 (1.668 sec)\n",
            "train - step 5263: loss = 435900.12 (1.675 sec)\n",
            "train - step 5264: loss = 379660.72 (1.700 sec)\n",
            "train - step 5265: loss = 428296.44 (1.680 sec)\n",
            "train - step 5266: loss = 494934.28 (2.252 sec)\n",
            "train - step 5267: loss = 474057.59 (1.691 sec)\n",
            "train - step 5268: loss = 342836.78 (1.693 sec)\n",
            "train - step 5269: loss = 459524.12 (1.659 sec)\n",
            "train - step 5270: loss = 409227.81 (1.676 sec)\n",
            "train - step 5271: loss = 412863.34 (1.671 sec)\n",
            "train - step 5272: loss = 422353.06 (1.673 sec)\n",
            "train - step 5273: loss = 282087.03 (1.695 sec)\n",
            "train - step 5274: loss = 504541.97 (1.681 sec)\n",
            "train - step 5275: loss = 384054.97 (1.683 sec)\n",
            "train - step 5276: loss = 462231.91 (1.684 sec)\n",
            "train - step 5277: loss = 359869.94 (1.667 sec)\n",
            "train - step 5278: loss = 421777.50 (1.666 sec)\n",
            "train - step 5279: loss = 424968.12 (1.658 sec)\n",
            "train - step 5280: loss = 361888.41 (1.689 sec)\n",
            "train - step 5281: loss = 332128.72 (1.672 sec)\n",
            "train - step 5282: loss = 441663.31 (1.692 sec)\n",
            "train - step 5283: loss = 495314.81 (1.689 sec)\n",
            "train - step 5284: loss = 393962.56 (1.687 sec)\n",
            "train - step 5285: loss = 351236.66 (1.718 sec)\n",
            "train - step 5286: loss = 414312.59 (1.666 sec)\n",
            "train - step 5287: loss = 507626.38 (1.707 sec)\n",
            "train - step 5288: loss = 281447.22 (1.672 sec)\n",
            "train - step 5289: loss = 375833.72 (1.684 sec)\n",
            "train - step 5290: loss = 541606.31 (1.688 sec)\n",
            "train - step 5291: loss = 383200.47 (1.703 sec)\n",
            "train - step 5292: loss = 436361.91 (1.705 sec)\n",
            "train - step 5293: loss = 231563.27 (1.680 sec)\n",
            "train - step 5294: loss = 370945.88 (1.692 sec)\n",
            "train - step 5295: loss = 420541.56 (1.686 sec)\n",
            "train - step 5296: loss = 536887.81 (1.705 sec)\n",
            "train - step 5297: loss = 401910.78 (1.664 sec)\n",
            "train - step 5298: loss = 384338.03 (1.678 sec)\n",
            "train - step 5299: loss = 309240.94 (1.702 sec)\n",
            "train - step 5300: loss = 405523.00 (1.674 sec)\n",
            "train - step 5301: loss = 379097.91 (1.681 sec)\n",
            "train - step 5302: loss = 455007.88 (2.777 sec)\n",
            "train - step 5303: loss = 461202.38 (1.677 sec)\n",
            "train - step 5304: loss = 378501.56 (1.682 sec)\n",
            "train - step 5305: loss = 398823.34 (1.658 sec)\n",
            "train - step 5306: loss = 359551.44 (1.666 sec)\n",
            "train - step 5307: loss = 384200.03 (1.689 sec)\n",
            "train - step 5308: loss = 521824.59 (1.682 sec)\n",
            "train - step 5309: loss = 443570.09 (1.687 sec)\n",
            "train - step 5310: loss = 403644.34 (1.673 sec)\n",
            "train - step 5311: loss = 526566.62 (1.693 sec)\n",
            "train - step 5312: loss = 379392.41 (1.658 sec)\n",
            "train - step 5313: loss = 394650.19 (1.668 sec)\n",
            "train - step 5314: loss = 423554.28 (1.691 sec)\n",
            "train - step 5315: loss = 474936.00 (1.671 sec)\n",
            "train - step 5316: loss = 305957.62 (1.662 sec)\n",
            "train - step 5317: loss = 381437.69 (1.686 sec)\n",
            "train - step 5318: loss = 544328.00 (1.681 sec)\n",
            "train - step 5319: loss = 429303.81 (1.681 sec)\n",
            "train - step 5320: loss = 381735.12 (1.676 sec)\n",
            "train - step 5321: loss = 399524.00 (1.679 sec)\n",
            "train - step 5322: loss = 401599.53 (1.681 sec)\n",
            "train - step 5323: loss = 370614.31 (1.669 sec)\n",
            "train - step 5324: loss = 318086.91 (1.650 sec)\n",
            "train - step 5325: loss = 572163.56 (1.670 sec)\n",
            "train - step 5326: loss = 608481.75 (1.665 sec)\n",
            "train - step 5327: loss = 392816.38 (1.650 sec)\n",
            "train - step 5328: loss = 483176.81 (1.624 sec)\n",
            "train - step 5329: loss = 451405.09 (1.636 sec)\n",
            "train - step 5330: loss = 457144.00 (1.656 sec)\n",
            "train - step 5331: loss = 365870.16 (1.675 sec)\n",
            "train - step 5332: loss = 362587.19 (1.686 sec)\n",
            "train - step 5333: loss = 390556.66 (1.676 sec)\n",
            "train - step 5334: loss = 475318.78 (1.672 sec)\n",
            "train - step 5335: loss = 366357.53 (1.704 sec)\n",
            "train - step 5336: loss = 507686.66 (1.678 sec)\n",
            "train - step 5337: loss = 244881.83 (1.653 sec)\n",
            "train - step 5338: loss = 399265.00 (2.535 sec)\n",
            "train - step 5339: loss = 417977.19 (1.665 sec)\n",
            "train - step 5340: loss = 417400.94 (1.682 sec)\n",
            "train - step 5341: loss = 250163.98 (1.686 sec)\n",
            "train - step 5342: loss = 445479.19 (1.688 sec)\n",
            "train - step 5343: loss = 256360.30 (1.689 sec)\n",
            "train - step 5344: loss = 399174.34 (1.676 sec)\n",
            "train - step 5345: loss = 475748.78 (1.691 sec)\n",
            "train - step 5346: loss = 357401.28 (1.695 sec)\n",
            "train - step 5347: loss = 544787.38 (1.668 sec)\n",
            "train - step 5348: loss = 418668.69 (1.676 sec)\n",
            "train - step 5349: loss = 441369.78 (1.665 sec)\n",
            "train - step 5350: loss = 412082.12 (1.682 sec)\n",
            "train - step 5351: loss = 346625.06 (1.680 sec)\n",
            "train - step 5352: loss = 600953.69 (1.685 sec)\n",
            "train - step 5353: loss = 365950.84 (1.678 sec)\n",
            "train - step 5354: loss = 450394.06 (1.667 sec)\n",
            "train - step 5355: loss = 426188.44 (1.674 sec)\n",
            "train - step 5356: loss = 283536.19 (1.670 sec)\n",
            "train - step 5357: loss = 399621.09 (1.668 sec)\n",
            "train - step 5358: loss = 466280.50 (1.685 sec)\n",
            "train - step 5359: loss = 389210.81 (1.709 sec)\n",
            "train - step 5360: loss = 248905.14 (1.673 sec)\n",
            "train - step 5361: loss = 490644.56 (1.686 sec)\n",
            "train - step 5362: loss = 458839.47 (1.670 sec)\n",
            "train - step 5363: loss = 315386.00 (1.698 sec)\n",
            "train - step 5364: loss = 307930.16 (1.677 sec)\n",
            "train - step 5365: loss = 564386.12 (1.691 sec)\n",
            "train - step 5366: loss = 285328.72 (1.681 sec)\n",
            "train - step 5367: loss = 347409.38 (1.667 sec)\n",
            "train - step 5368: loss = 439666.00 (1.680 sec)\n",
            "train - step 5369: loss = 471443.41 (1.656 sec)\n",
            "train - step 5370: loss = 358245.50 (1.655 sec)\n",
            "train - step 5371: loss = 380079.47 (1.650 sec)\n",
            "train - step 5372: loss = 563502.25 (1.678 sec)\n",
            "train - step 5373: loss = 397706.06 (1.651 sec)\n",
            "train - step 5374: loss = 399466.34 (2.537 sec)\n",
            "train - step 5375: loss = 468998.88 (1.691 sec)\n",
            "train - step 5376: loss = 413137.69 (1.657 sec)\n",
            "train - step 5377: loss = 431810.94 (1.711 sec)\n",
            "train - step 5378: loss = 485879.69 (1.678 sec)\n",
            "train - step 5379: loss = 391975.00 (1.658 sec)\n",
            "train - step 5380: loss = 330702.94 (1.675 sec)\n",
            "train - step 5381: loss = 422066.31 (1.686 sec)\n",
            "train - step 5382: loss = 515253.09 (1.667 sec)\n",
            "train - step 5383: loss = 427940.62 (1.691 sec)\n",
            "train - step 5384: loss = 380999.03 (1.683 sec)\n",
            "train - step 5385: loss = 458610.19 (1.702 sec)\n",
            "train - step 5386: loss = 518769.19 (1.655 sec)\n",
            "train - step 5387: loss = 437785.59 (1.670 sec)\n",
            "train - step 5388: loss = 397334.72 (1.712 sec)\n",
            "train - step 5389: loss = 439312.94 (1.674 sec)\n",
            "train - step 5390: loss = 361900.69 (1.677 sec)\n",
            "train - step 5391: loss = 421016.69 (1.690 sec)\n",
            "train - step 5392: loss = 427561.31 (1.667 sec)\n",
            "train - step 5393: loss = 383604.72 (1.679 sec)\n",
            "train - step 5394: loss = 488053.91 (1.674 sec)\n",
            "train - step 5395: loss = 386895.22 (1.679 sec)\n",
            "train - step 5396: loss = 452470.94 (1.689 sec)\n",
            "train - step 5397: loss = 346805.16 (1.686 sec)\n",
            "train - step 5398: loss = 303751.12 (1.690 sec)\n",
            "train - step 5399: loss = 411991.66 (1.675 sec)\n",
            "train - step 5400: loss = 532357.69 (1.674 sec)\n",
            "train - step 5401: loss = 464915.88 (1.689 sec)\n",
            "train - step 5402: loss = 359299.34 (1.670 sec)\n",
            "train - step 5403: loss = 219887.48 (1.662 sec)\n",
            "train - step 5404: loss = 405253.06 (1.687 sec)\n",
            "train - step 5405: loss = 545395.50 (1.671 sec)\n",
            "train - step 5406: loss = 451186.66 (1.687 sec)\n",
            "train - step 5407: loss = 469334.94 (1.691 sec)\n",
            "train - step 5408: loss = 427492.00 (1.687 sec)\n",
            "train - step 5409: loss = 485944.22 (1.686 sec)\n",
            "train - step 5410: loss = 406750.59 (2.532 sec)\n",
            "train - step 5411: loss = 555709.81 (1.710 sec)\n",
            "train - step 5412: loss = 252876.00 (1.670 sec)\n",
            "train - step 5413: loss = 304215.66 (1.690 sec)\n",
            "train - step 5414: loss = 520475.84 (1.681 sec)\n",
            "train - step 5415: loss = 420770.72 (1.670 sec)\n",
            "train - step 5416: loss = 441305.88 (1.690 sec)\n",
            "train - step 5417: loss = 359336.38 (1.689 sec)\n",
            "train - step 5418: loss = 534775.75 (1.669 sec)\n",
            "train - step 5419: loss = 542674.81 (1.668 sec)\n",
            "train - step 5420: loss = 467736.06 (1.692 sec)\n",
            "train - step 5421: loss = 452293.72 (1.666 sec)\n",
            "train - step 5422: loss = 435226.31 (1.679 sec)\n",
            "train - step 5423: loss = 467481.62 (1.702 sec)\n",
            "train - step 5424: loss = 322755.47 (1.696 sec)\n",
            "train - step 5425: loss = 533517.44 (1.702 sec)\n",
            "train - step 5426: loss = 359217.19 (1.675 sec)\n",
            "train - step 5427: loss = 387880.41 (1.684 sec)\n",
            "train - step 5428: loss = 346904.59 (1.666 sec)\n",
            "train - step 5429: loss = 467048.69 (1.665 sec)\n",
            "train - step 5430: loss = 449697.41 (1.685 sec)\n",
            "train - step 5431: loss = 418918.78 (1.684 sec)\n",
            "train - step 5432: loss = 383509.28 (1.701 sec)\n",
            "train - step 5433: loss = 414975.44 (1.695 sec)\n",
            "train - step 5434: loss = 392425.59 (1.687 sec)\n",
            "train - step 5435: loss = 453821.22 (1.703 sec)\n",
            "train - step 5436: loss = 502408.12 (1.683 sec)\n",
            "train - step 5437: loss = 353383.44 (1.708 sec)\n",
            "train - step 5438: loss = 512436.41 (1.688 sec)\n",
            "train - step 5439: loss = 352061.81 (1.708 sec)\n",
            "train - step 5440: loss = 315504.12 (1.676 sec)\n",
            "train - step 5441: loss = 320199.88 (1.685 sec)\n",
            "train - step 5442: loss = 330523.97 (1.709 sec)\n",
            "train - step 5443: loss = 449973.84 (1.674 sec)\n",
            "train - step 5444: loss = 438494.81 (1.682 sec)\n",
            "train - step 5445: loss = 394257.06 (1.713 sec)\n",
            "train - step 5446: loss = 359149.62 (3.104 sec)\n",
            "train - step 5447: loss = 429412.94 (1.665 sec)\n",
            "train - step 5448: loss = 287477.78 (1.679 sec)\n",
            "train - step 5449: loss = 411855.62 (1.692 sec)\n",
            "train - step 5450: loss = 452758.31 (1.662 sec)\n",
            "train - step 5451: loss = 376291.47 (1.697 sec)\n",
            "train - step 5452: loss = 549214.75 (1.672 sec)\n",
            "train - step 5453: loss = 399647.72 (1.673 sec)\n",
            "train - step 5454: loss = 374538.47 (1.692 sec)\n",
            "train - step 5455: loss = 420958.53 (1.672 sec)\n",
            "train - step 5456: loss = 470606.19 (1.689 sec)\n",
            "train - step 5457: loss = 385695.53 (1.696 sec)\n",
            "train - step 5458: loss = 474402.84 (1.679 sec)\n",
            "train - step 5459: loss = 284357.59 (1.673 sec)\n",
            "train - step 5460: loss = 468947.41 (1.672 sec)\n",
            "train - step 5461: loss = 369910.28 (1.706 sec)\n",
            "train - step 5462: loss = 520263.97 (1.668 sec)\n",
            "train - step 5463: loss = 453835.22 (1.682 sec)\n",
            "train - step 5464: loss = 516822.38 (1.693 sec)\n",
            "train - step 5465: loss = 302199.00 (1.694 sec)\n",
            "train - step 5466: loss = 232398.48 (1.676 sec)\n",
            "train - step 5467: loss = 553184.44 (1.689 sec)\n",
            "train - step 5468: loss = 383853.78 (1.683 sec)\n",
            "train - step 5469: loss = 509778.16 (1.675 sec)\n",
            "train - step 5470: loss = 422023.81 (1.689 sec)\n",
            "train - step 5471: loss = 443236.19 (1.698 sec)\n",
            "train - step 5472: loss = 430461.94 (1.674 sec)\n",
            "train - step 5473: loss = 415931.16 (1.705 sec)\n",
            "train - step 5474: loss = 521100.03 (1.666 sec)\n",
            "train - step 5475: loss = 456076.16 (1.667 sec)\n",
            "train - step 5476: loss = 418967.34 (1.697 sec)\n",
            "train - step 5477: loss = 357426.88 (1.683 sec)\n",
            "train - step 5478: loss = 384756.66 (1.667 sec)\n",
            "train - step 5479: loss = 320519.62 (1.679 sec)\n",
            "train - step 5480: loss = 354839.81 (1.668 sec)\n",
            "train - step 5481: loss = 577259.81 (1.680 sec)\n",
            "train - step 5482: loss = 501663.69 (2.553 sec)\n",
            "train - step 5483: loss = 431195.69 (1.684 sec)\n",
            "train - step 5484: loss = 446465.28 (1.678 sec)\n",
            "train - step 5485: loss = 453386.06 (1.694 sec)\n",
            "train - step 5486: loss = 342380.38 (1.684 sec)\n",
            "train - step 5487: loss = 482032.94 (1.687 sec)\n",
            "train - step 5488: loss = 442358.88 (1.681 sec)\n",
            "train - step 5489: loss = 460508.62 (1.696 sec)\n",
            "train - step 5490: loss = 406751.38 (1.672 sec)\n",
            "train - step 5491: loss = 360168.47 (1.680 sec)\n",
            "train - step 5492: loss = 336100.19 (1.679 sec)\n",
            "train - step 5493: loss = 296665.78 (1.681 sec)\n",
            "train - step 5494: loss = 402943.72 (1.677 sec)\n",
            "train - step 5495: loss = 507036.47 (1.673 sec)\n",
            "train - step 5496: loss = 403125.19 (1.663 sec)\n",
            "train - step 5497: loss = 487001.00 (1.700 sec)\n",
            "train - step 5498: loss = 568481.00 (1.671 sec)\n",
            "train - step 5499: loss = 333644.81 (1.674 sec)\n",
            "train - step 5500: loss = 410085.22 (1.677 sec)\n",
            "train - step 5501: loss = 403780.81 (1.683 sec)\n",
            "train - step 5502: loss = 410219.06 (1.679 sec)\n",
            "train - step 5503: loss = 440430.44 (1.710 sec)\n",
            "train - step 5504: loss = 339028.91 (1.670 sec)\n",
            "train - step 5505: loss = 327739.66 (1.683 sec)\n",
            "train - step 5506: loss = 416260.41 (1.639 sec)\n",
            "train - step 5507: loss = 248926.03 (1.654 sec)\n",
            "train - step 5508: loss = 291283.88 (1.635 sec)\n",
            "train - step 5509: loss = 446984.78 (1.674 sec)\n",
            "train - step 5510: loss = 497721.22 (1.678 sec)\n",
            "train - step 5511: loss = 365223.19 (1.681 sec)\n",
            "train - step 5512: loss = 527068.19 (1.680 sec)\n",
            "train - step 5513: loss = 366874.69 (1.692 sec)\n",
            "train - step 5514: loss = 421717.59 (1.684 sec)\n",
            "train - step 5515: loss = 381004.78 (1.680 sec)\n",
            "train - step 5516: loss = 427972.41 (1.690 sec)\n",
            "train - step 5517: loss = 504557.91 (1.673 sec)\n",
            "train - step 5518: loss = 450452.84 (2.568 sec)\n",
            "train - step 5519: loss = 300913.00 (1.671 sec)\n",
            "train - step 5520: loss = 541132.69 (1.691 sec)\n",
            "train - step 5521: loss = 278331.59 (1.742 sec)\n",
            "train - step 5522: loss = 472366.66 (1.699 sec)\n",
            "train - step 5523: loss = 446676.47 (1.698 sec)\n",
            "train - step 5524: loss = 376684.91 (1.696 sec)\n",
            "train - step 5525: loss = 433850.00 (1.676 sec)\n",
            "train - step 5526: loss = 572081.69 (1.676 sec)\n",
            "train - step 5527: loss = 346284.91 (1.720 sec)\n",
            "train - step 5528: loss = 296519.69 (1.696 sec)\n",
            "train - step 5529: loss = 347229.12 (1.685 sec)\n",
            "train - step 5530: loss = 346530.69 (1.683 sec)\n",
            "train - step 5531: loss = 462810.97 (1.689 sec)\n",
            "train - step 5532: loss = 338819.53 (1.678 sec)\n",
            "train - step 5533: loss = 426190.62 (1.719 sec)\n",
            "train - step 5534: loss = 358372.28 (1.690 sec)\n",
            "train - step 5535: loss = 436993.69 (1.681 sec)\n",
            "train - step 5536: loss = 282481.09 (1.669 sec)\n",
            "train - step 5537: loss = 413555.41 (1.693 sec)\n",
            "train - step 5538: loss = 403660.50 (1.679 sec)\n",
            "train - step 5539: loss = 414595.28 (1.681 sec)\n",
            "train - step 5540: loss = 392542.84 (1.676 sec)\n",
            "train - step 5541: loss = 371114.38 (1.655 sec)\n",
            "train - step 5542: loss = 442316.66 (1.681 sec)\n",
            "train - step 5543: loss = 385947.28 (1.684 sec)\n",
            "train - step 5544: loss = 348771.19 (1.677 sec)\n",
            "train - step 5545: loss = 303146.84 (1.679 sec)\n",
            "train - step 5546: loss = 292976.06 (1.693 sec)\n",
            "train - step 5547: loss = 477817.28 (1.696 sec)\n",
            "train - step 5548: loss = 438674.06 (1.701 sec)\n",
            "train - step 5549: loss = 286730.94 (1.668 sec)\n",
            "train - step 5550: loss = 292450.56 (1.690 sec)\n",
            "train - step 5551: loss = 322265.50 (1.655 sec)\n",
            "train - step 5552: loss = 294029.91 (1.630 sec)\n",
            "train - step 5553: loss = 443596.88 (1.676 sec)\n",
            "train - step 5554: loss = 462844.31 (2.606 sec)\n",
            "train - step 5555: loss = 312708.41 (1.642 sec)\n",
            "train - step 5556: loss = 255693.52 (1.645 sec)\n",
            "train - step 5557: loss = 308575.12 (1.668 sec)\n",
            "train - step 5558: loss = 261559.50 (1.682 sec)\n",
            "train - step 5559: loss = 327675.81 (1.671 sec)\n",
            "train - step 5560: loss = 457692.34 (1.677 sec)\n",
            "train - step 5561: loss = 189098.09 (1.686 sec)\n",
            "train - step 5562: loss = 377201.81 (1.668 sec)\n",
            "train - step 5563: loss = 304786.41 (1.679 sec)\n",
            "train - step 5564: loss = 268525.06 (1.679 sec)\n",
            "train - step 5565: loss = 282569.84 (1.688 sec)\n",
            "train - step 5566: loss = 280294.97 (1.678 sec)\n",
            "train - step 5567: loss = 288326.03 (1.658 sec)\n",
            "train - step 5568: loss = 506381.34 (1.684 sec)\n",
            "train - step 5569: loss = 478161.56 (1.692 sec)\n",
            "train - step 5570: loss = 492710.88 (1.668 sec)\n",
            "train - step 5571: loss = 342866.78 (1.683 sec)\n",
            "train - step 5572: loss = 374054.97 (1.668 sec)\n",
            "train - step 5573: loss = 469542.59 (1.677 sec)\n",
            "train - step 5574: loss = 405710.81 (1.677 sec)\n",
            "train - step 5575: loss = 468543.94 (1.699 sec)\n",
            "train - step 5576: loss = 600584.31 (1.678 sec)\n",
            "train - step 5577: loss = 363060.16 (1.687 sec)\n",
            "train - step 5578: loss = 457770.62 (1.674 sec)\n",
            "train - step 5579: loss = 348096.41 (1.693 sec)\n",
            "train - step 5580: loss = 354208.28 (1.668 sec)\n",
            "train - step 5581: loss = 368336.91 (1.689 sec)\n",
            "train - step 5582: loss = 357692.00 (1.675 sec)\n",
            "train - step 5583: loss = 391719.84 (1.672 sec)\n",
            "train - step 5584: loss = 517765.91 (1.686 sec)\n",
            "train - step 5585: loss = 351397.97 (1.689 sec)\n",
            "train - step 5586: loss = 348912.97 (1.672 sec)\n",
            "train - step 5587: loss = 405972.94 (1.669 sec)\n",
            "train - step 5588: loss = 462124.16 (1.666 sec)\n",
            "train - step 5589: loss = 357003.97 (1.669 sec)\n",
            "train - step 5590: loss = 346688.56 (2.540 sec)\n",
            "train - step 5591: loss = 364252.78 (1.680 sec)\n",
            "train - step 5592: loss = 488553.94 (1.669 sec)\n",
            "train - step 5593: loss = 388314.34 (1.692 sec)\n",
            "train - step 5594: loss = 532327.88 (1.669 sec)\n",
            "train - step 5595: loss = 317201.31 (1.716 sec)\n",
            "train - step 5596: loss = 269341.94 (1.712 sec)\n",
            "train - step 5597: loss = 475082.00 (1.677 sec)\n",
            "train - step 5598: loss = 314130.59 (1.676 sec)\n",
            "train - step 5599: loss = 455688.50 (1.675 sec)\n",
            "train - step 5600: loss = 364642.22 (1.675 sec)\n",
            "train - step 5601: loss = 326297.12 (1.682 sec)\n",
            "train - step 5602: loss = 372494.53 (1.684 sec)\n",
            "train - step 5603: loss = 436853.34 (1.671 sec)\n",
            "train - step 5604: loss = 339452.59 (1.681 sec)\n",
            "train - step 5605: loss = 544895.94 (1.664 sec)\n",
            "train - step 5606: loss = 292773.62 (1.681 sec)\n",
            "train - step 5607: loss = 297155.81 (1.679 sec)\n",
            "train - step 5608: loss = 443180.47 (1.668 sec)\n",
            "train - step 5609: loss = 383494.41 (1.686 sec)\n",
            "train - step 5610: loss = 238443.70 (1.689 sec)\n",
            "train - step 5611: loss = 376258.31 (1.688 sec)\n",
            "train - step 5612: loss = 396999.41 (1.689 sec)\n",
            "train - step 5613: loss = 520210.22 (1.685 sec)\n",
            "train - step 5614: loss = 531466.31 (1.678 sec)\n",
            "train - step 5615: loss = 302538.31 (1.703 sec)\n",
            "train - step 5616: loss = 391419.12 (1.668 sec)\n",
            "train - step 5617: loss = 353050.50 (1.683 sec)\n",
            "train - step 5618: loss = 468860.56 (1.676 sec)\n",
            "train - step 5619: loss = 376518.69 (1.688 sec)\n",
            "train - step 5620: loss = 517219.28 (1.680 sec)\n",
            "train - step 5621: loss = 398888.19 (1.660 sec)\n",
            "train - step 5622: loss = 439295.59 (1.678 sec)\n",
            "train - step 5623: loss = 376123.09 (1.680 sec)\n",
            "train - step 5624: loss = 333245.69 (1.673 sec)\n",
            "train - step 5625: loss = 449453.34 (1.677 sec)\n",
            "train - step 5626: loss = 390162.22 (2.697 sec)\n",
            "train - step 5627: loss = 497095.44 (1.698 sec)\n",
            "train - step 5628: loss = 364769.03 (1.669 sec)\n",
            "train - step 5629: loss = 451278.06 (1.677 sec)\n",
            "train - step 5630: loss = 327693.72 (1.675 sec)\n",
            "train - step 5631: loss = 468645.94 (1.700 sec)\n",
            "train - step 5632: loss = 283949.97 (1.666 sec)\n",
            "train - step 5633: loss = 380183.97 (1.679 sec)\n",
            "train - step 5634: loss = 296837.50 (1.682 sec)\n",
            "train - step 5635: loss = 308774.94 (1.679 sec)\n",
            "train - step 5636: loss = 361187.22 (1.690 sec)\n",
            "train - step 5637: loss = 378766.22 (1.683 sec)\n",
            "train - step 5638: loss = 394410.22 (1.689 sec)\n",
            "train - step 5639: loss = 426448.12 (1.683 sec)\n",
            "train - step 5640: loss = 295832.59 (1.681 sec)\n",
            "train - step 5641: loss = 337044.69 (1.689 sec)\n",
            "train - step 5642: loss = 345117.00 (1.672 sec)\n",
            "train - step 5643: loss = 303470.81 (1.673 sec)\n",
            "train - step 5644: loss = 363405.47 (1.698 sec)\n",
            "train - step 5645: loss = 397449.44 (1.703 sec)\n",
            "train - step 5646: loss = 426218.44 (1.682 sec)\n",
            "train - step 5647: loss = 382441.31 (1.680 sec)\n",
            "train - step 5648: loss = 318089.72 (1.685 sec)\n",
            "train - step 5649: loss = 376779.53 (1.686 sec)\n",
            "train - step 5650: loss = 421857.94 (1.682 sec)\n",
            "train - step 5651: loss = 362331.59 (1.683 sec)\n",
            "train - step 5652: loss = 559473.75 (1.653 sec)\n",
            "train - step 5653: loss = 434159.03 (1.667 sec)\n",
            "train - step 5654: loss = 488487.19 (1.657 sec)\n",
            "train - step 5655: loss = 368525.69 (1.685 sec)\n",
            "train - step 5656: loss = 345498.16 (1.674 sec)\n",
            "train - step 5657: loss = 414140.09 (1.670 sec)\n",
            "train - step 5658: loss = 445776.69 (1.678 sec)\n",
            "train - step 5659: loss = 446864.69 (1.673 sec)\n",
            "train - step 5660: loss = 520779.69 (1.666 sec)\n",
            "train - step 5661: loss = 266550.88 (1.667 sec)\n",
            "train - step 5662: loss = 247774.91 (2.569 sec)\n",
            "train - step 5663: loss = 428444.81 (1.671 sec)\n",
            "train - step 5664: loss = 442249.62 (1.661 sec)\n",
            "train - step 5665: loss = 351028.19 (1.673 sec)\n",
            "train - step 5666: loss = 419321.31 (1.681 sec)\n",
            "train - step 5667: loss = 455724.38 (1.669 sec)\n",
            "train - step 5668: loss = 424504.09 (1.663 sec)\n",
            "train - step 5669: loss = 383300.06 (1.673 sec)\n",
            "train - step 5670: loss = 316517.56 (1.652 sec)\n",
            "train - step 5671: loss = 337029.84 (1.685 sec)\n",
            "train - step 5672: loss = 502097.56 (1.662 sec)\n",
            "train - step 5673: loss = 354011.69 (1.654 sec)\n",
            "train - step 5674: loss = 330348.72 (1.654 sec)\n",
            "train - step 5675: loss = 373839.78 (1.668 sec)\n",
            "train - step 5676: loss = 461019.00 (1.681 sec)\n",
            "train - step 5677: loss = 504756.66 (1.670 sec)\n",
            "train - step 5678: loss = 358446.47 (1.668 sec)\n",
            "train - step 5679: loss = 292995.22 (1.679 sec)\n",
            "train - step 5680: loss = 393585.19 (1.663 sec)\n",
            "train - step 5681: loss = 321729.38 (1.681 sec)\n",
            "train - step 5682: loss = 441472.59 (1.659 sec)\n",
            "train - step 5683: loss = 416414.19 (1.657 sec)\n",
            "train - step 5684: loss = 417082.03 (1.684 sec)\n",
            "train - step 5685: loss = 266427.09 (1.642 sec)\n",
            "train - step 5686: loss = 326682.97 (1.637 sec)\n",
            "train - step 5687: loss = 456585.50 (1.632 sec)\n",
            "train - step 5688: loss = 325279.69 (1.632 sec)\n",
            "train - step 5689: loss = 439168.28 (1.684 sec)\n",
            "train - step 5690: loss = 363310.78 (1.672 sec)\n",
            "train - step 5691: loss = 460912.00 (1.675 sec)\n",
            "train - step 5692: loss = 426067.62 (1.667 sec)\n",
            "train - step 5693: loss = 422627.69 (1.661 sec)\n",
            "train - step 5694: loss = 383990.69 (1.698 sec)\n",
            "train - step 5695: loss = 356038.38 (1.679 sec)\n",
            "train - step 5696: loss = 293123.78 (1.667 sec)\n",
            "train - step 5697: loss = 422962.22 (1.686 sec)\n",
            "train - step 5698: loss = 347007.62 (1.674 sec)\n",
            "train - step 5699: loss = 313207.41 (2.732 sec)\n",
            "train - step 5700: loss = 365769.94 (1.662 sec)\n",
            "train - step 5701: loss = 341437.06 (1.657 sec)\n",
            "train - step 5702: loss = 407541.34 (1.676 sec)\n",
            "train - step 5703: loss = 429626.56 (1.684 sec)\n",
            "train - step 5704: loss = 467207.53 (1.679 sec)\n",
            "train - step 5705: loss = 488023.03 (1.657 sec)\n",
            "train - step 5706: loss = 374829.56 (1.683 sec)\n",
            "train - step 5707: loss = 484205.16 (1.680 sec)\n",
            "train - step 5708: loss = 390733.19 (1.674 sec)\n",
            "train - step 5709: loss = 402121.41 (1.672 sec)\n",
            "train - step 5710: loss = 431145.59 (1.669 sec)\n",
            "train - step 5711: loss = 312150.00 (1.663 sec)\n",
            "train - step 5712: loss = 419209.00 (1.665 sec)\n",
            "train - step 5713: loss = 405704.50 (1.674 sec)\n",
            "train - step 5714: loss = 540871.25 (1.661 sec)\n",
            "train - step 5715: loss = 523943.50 (1.669 sec)\n",
            "train - step 5716: loss = 297027.91 (1.677 sec)\n",
            "train - step 5717: loss = 577982.31 (1.674 sec)\n",
            "train - step 5718: loss = 415922.12 (1.683 sec)\n",
            "train - step 5719: loss = 399202.28 (1.664 sec)\n",
            "train - step 5720: loss = 371042.41 (1.647 sec)\n",
            "train - step 5721: loss = 313800.66 (1.683 sec)\n",
            "train - step 5722: loss = 329716.69 (1.697 sec)\n",
            "train - step 5723: loss = 405427.44 (1.682 sec)\n",
            "train - step 5724: loss = 283128.09 (1.683 sec)\n",
            "train - step 5725: loss = 436273.97 (1.657 sec)\n",
            "train - step 5726: loss = 327823.91 (1.659 sec)\n",
            "train - step 5727: loss = 383511.22 (1.662 sec)\n",
            "train - step 5728: loss = 485123.72 (1.665 sec)\n",
            "train - step 5729: loss = 325585.53 (1.669 sec)\n",
            "train - step 5730: loss = 325668.66 (1.654 sec)\n",
            "train - step 5731: loss = 411977.09 (1.666 sec)\n",
            "train - step 5732: loss = 514823.12 (1.653 sec)\n",
            "train - step 5733: loss = 398005.66 (1.654 sec)\n",
            "train - step 5734: loss = 488155.47 (2.601 sec)\n",
            "train - step 5735: loss = 356358.41 (1.639 sec)\n",
            "train - step 5736: loss = 668808.31 (1.648 sec)\n",
            "train - step 5737: loss = 314228.28 (1.641 sec)\n",
            "train - step 5738: loss = 447774.12 (1.641 sec)\n",
            "train - step 5739: loss = 421623.28 (1.665 sec)\n",
            "train - step 5740: loss = 426548.56 (1.666 sec)\n",
            "train - step 5741: loss = 331010.28 (1.679 sec)\n",
            "train - step 5742: loss = 539574.06 (1.676 sec)\n",
            "train - step 5743: loss = 259348.03 (1.667 sec)\n",
            "train - step 5744: loss = 386030.22 (1.668 sec)\n",
            "train - step 5745: loss = 425226.66 (1.647 sec)\n",
            "train - step 5746: loss = 571573.38 (1.671 sec)\n",
            "train - step 5747: loss = 218738.14 (1.661 sec)\n",
            "train - step 5748: loss = 539654.31 (1.663 sec)\n",
            "train - step 5749: loss = 403003.12 (1.686 sec)\n",
            "train - step 5750: loss = 385100.28 (1.686 sec)\n",
            "train - step 5751: loss = 384963.53 (1.678 sec)\n",
            "train - step 5752: loss = 267594.16 (1.680 sec)\n",
            "train - step 5753: loss = 270570.81 (1.662 sec)\n",
            "train - step 5754: loss = 272922.44 (1.674 sec)\n",
            "train - step 5755: loss = 328413.66 (1.672 sec)\n",
            "train - step 5756: loss = 433883.38 (1.666 sec)\n",
            "train - step 5757: loss = 343756.72 (1.670 sec)\n",
            "train - step 5758: loss = 353359.41 (1.673 sec)\n",
            "train - step 5759: loss = 461641.62 (1.676 sec)\n",
            "train - step 5760: loss = 332468.81 (1.680 sec)\n",
            "train - step 5761: loss = 387296.06 (1.670 sec)\n",
            "train - step 5762: loss = 372625.19 (1.675 sec)\n",
            "train - step 5763: loss = 320452.50 (1.687 sec)\n",
            "train - step 5764: loss = 402996.31 (1.660 sec)\n",
            "train - step 5765: loss = 566498.31 (1.686 sec)\n",
            "train - step 5766: loss = 367528.69 (1.663 sec)\n",
            "train - step 5767: loss = 464967.28 (1.685 sec)\n",
            "train - step 5768: loss = 394575.41 (1.663 sec)\n",
            "train - step 5769: loss = 342782.47 (1.662 sec)\n",
            "train - step 5770: loss = 376201.31 (2.661 sec)\n",
            "train - step 5771: loss = 409318.81 (1.679 sec)\n",
            "train - step 5772: loss = 558435.94 (1.678 sec)\n",
            "train - step 5773: loss = 363655.38 (1.694 sec)\n",
            "train - step 5774: loss = 427384.00 (1.684 sec)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
            "  warnings.warn(\"Attempting to use a closed FileWriter. \"\n",
            "--- 8984.92612195015 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcdzi-dfkgW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnVNO1j4EZSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9sqfWTJEbRP",
        "colab_type": "text"
      },
      "source": [
        "# 主题 第六次运行 \n",
        "模型顶点数为6000，batchsize依然为4，从5000多次开始迭代"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idlBWNxPSnxH",
        "colab_type": "code",
        "outputId": "ed53dc6b-598f-4e29-9f4b-75a7a26d17bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train - step 6008: loss = 256274.61 (1.594 sec)\n",
            "train - step 6009: loss = 361558.34 (0.955 sec)\n",
            "train - step 6010: loss = 361152.94 (0.974 sec)\n",
            "train - step 6011: loss = 396839.50 (0.957 sec)\n",
            "train - step 6012: loss = 306242.12 (0.958 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12MlkaVCHJ-7",
        "colab_type": "text"
      },
      "source": [
        "# 主题 第七次运行 \n",
        "模型顶点数为6000，batchsize为8\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VZjdVosWfKm",
        "colab_type": "code",
        "outputId": "38cb5546-e5c4-418c-fe47-ea5a271542c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7160
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-10 01:27:34.556435: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-10 01:27:34.556650: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1d4eec0 executing computations on platform Host. Devices:\n",
            "2019-05-10 01:27:34.556680: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 01:27:34.717676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 01:27:34.718178: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1d4f020 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 01:27:34.718208: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 01:27:34.718604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-10 01:27:34.718630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 01:27:35.153717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 01:27:35.153784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 01:27:35.153797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 01:27:35.154056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-10 01:31:58.291482: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-10 01:31:58.890449: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x89ef4d0\n",
            "train - step 5985: loss = 485599.34 (4.428 sec)\n",
            "train - step 5986: loss = 270887.44 (1.007 sec)\n",
            "train - step 5987: loss = 372146.12 (0.947 sec)\n",
            "train - step 5988: loss = 329170.38 (0.944 sec)\n",
            "train - step 5989: loss = 395375.81 (0.940 sec)\n",
            "train - step 5990: loss = 409026.72 (0.948 sec)\n",
            "train - step 5991: loss = 355052.59 (0.943 sec)\n",
            "train - step 5992: loss = 417370.88 (0.943 sec)\n",
            "train - step 5993: loss = 440740.09 (0.946 sec)\n",
            "train - step 5994: loss = 410222.62 (0.946 sec)\n",
            "train - step 5995: loss = 416794.19 (0.940 sec)\n",
            "train - step 5996: loss = 415084.00 (0.955 sec)\n",
            "train - step 5997: loss = 298896.88 (0.935 sec)\n",
            "train - step 5998: loss = 412761.69 (0.965 sec)\n",
            "train - step 5999: loss = 591076.88 (0.954 sec)\n",
            "train - step 6000: loss = 314255.94 (0.957 sec)\n",
            "train - step 6001: loss = 453899.88 (0.944 sec)\n",
            "train - step 6002: loss = 468285.50 (0.935 sec)\n",
            "train - step 6003: loss = 351168.50 (0.951 sec)\n",
            "train - step 6004: loss = 283923.28 (0.967 sec)\n",
            "train - step 6005: loss = 263373.38 (0.997 sec)\n",
            "train - step 6006: loss = 453113.59 (0.996 sec)\n",
            "train - step 6007: loss = 431426.66 (1.000 sec)\n",
            "train - step 6008: loss = 360651.44 (0.996 sec)\n",
            "train - step 6009: loss = 402521.03 (0.987 sec)\n",
            "train - step 6010: loss = 391581.06 (0.986 sec)\n",
            "train - step 6011: loss = 519669.06 (1.011 sec)\n",
            "train - step 6012: loss = 442692.97 (0.999 sec)\n",
            "train - step 6013: loss = 428491.38 (1.454 sec)\n",
            "train - step 6014: loss = 282323.31 (1.002 sec)\n",
            "train - step 6015: loss = 340958.41 (0.982 sec)\n",
            "train - step 6016: loss = 463632.69 (1.001 sec)\n",
            "train - step 6017: loss = 308212.69 (0.982 sec)\n",
            "train - step 6018: loss = 365803.00 (0.979 sec)\n",
            "train - step 6019: loss = 366310.59 (0.984 sec)\n",
            "train - step 6020: loss = 357300.12 (1.002 sec)\n",
            "train - step 6021: loss = 403918.03 (0.985 sec)\n",
            "train - step 6022: loss = 423731.53 (0.992 sec)\n",
            "train - step 6023: loss = 357947.41 (0.995 sec)\n",
            "train - step 6024: loss = 387679.06 (0.951 sec)\n",
            "train - step 6025: loss = 314269.84 (0.952 sec)\n",
            "train - step 6026: loss = 465123.62 (0.960 sec)\n",
            "train - step 6027: loss = 377581.91 (0.951 sec)\n",
            "train - step 6028: loss = 394032.19 (0.966 sec)\n",
            "train - step 6029: loss = 395064.84 (0.958 sec)\n",
            "train - step 6030: loss = 437253.56 (0.958 sec)\n",
            "train - step 6031: loss = 383331.84 (0.949 sec)\n",
            "train - step 6032: loss = 450941.06 (0.951 sec)\n",
            "train - step 6033: loss = 384823.56 (0.949 sec)\n",
            "train - step 6034: loss = 368809.53 (0.967 sec)\n",
            "train - step 6035: loss = 349003.81 (0.951 sec)\n",
            "train - step 6036: loss = 431565.19 (0.966 sec)\n",
            "train - step 6037: loss = 406468.81 (0.948 sec)\n",
            "train - step 6038: loss = 304132.88 (0.977 sec)\n",
            "train - step 6039: loss = 563963.75 (0.965 sec)\n",
            "train - step 6040: loss = 417936.19 (0.953 sec)\n",
            "train - step 6041: loss = 303173.12 (0.947 sec)\n",
            "train - step 6042: loss = 381316.78 (0.961 sec)\n",
            "train - step 6043: loss = 284961.69 (0.960 sec)\n",
            "train - step 6044: loss = 421454.44 (0.974 sec)\n",
            "train - step 6045: loss = 416885.19 (0.960 sec)\n",
            "train - step 6046: loss = 361451.19 (0.971 sec)\n",
            "train - step 6047: loss = 285209.97 (0.948 sec)\n",
            "train - step 6048: loss = 372857.28 (0.956 sec)\n",
            "train - step 6049: loss = 383375.78 (0.946 sec)\n",
            "train - step 6050: loss = 389942.22 (0.975 sec)\n",
            "train - step 6051: loss = 518360.38 (0.966 sec)\n",
            "train - step 6052: loss = 496997.84 (0.968 sec)\n",
            "train - step 6053: loss = 396455.44 (0.967 sec)\n",
            "train - step 6054: loss = 476123.03 (0.962 sec)\n",
            "train - step 6055: loss = 410751.91 (0.978 sec)\n",
            "train - step 6056: loss = 391676.88 (0.966 sec)\n",
            "train - step 6057: loss = 336314.12 (0.961 sec)\n",
            "train - step 6058: loss = 403056.19 (0.985 sec)\n",
            "train - step 6059: loss = 418901.09 (0.966 sec)\n",
            "train - step 6060: loss = 434373.34 (0.974 sec)\n",
            "train - step 6061: loss = 466745.22 (0.964 sec)\n",
            "train - step 6062: loss = 432779.62 (0.959 sec)\n",
            "train - step 6063: loss = 314041.41 (0.962 sec)\n",
            "train - step 6064: loss = 300900.84 (0.973 sec)\n",
            "train - step 6065: loss = 351404.00 (0.972 sec)\n",
            "train - step 6066: loss = 298004.22 (0.953 sec)\n",
            "train - step 6067: loss = 399700.03 (0.964 sec)\n",
            "train - step 6068: loss = 245909.64 (0.959 sec)\n",
            "train - step 6069: loss = 355134.56 (0.968 sec)\n",
            "train - step 6070: loss = 298331.91 (0.973 sec)\n",
            "train - step 6071: loss = 295734.47 (0.971 sec)\n",
            "train - step 6072: loss = 453495.69 (0.973 sec)\n",
            "train - step 6073: loss = 440064.69 (0.963 sec)\n",
            "train - step 6074: loss = 469486.66 (0.974 sec)\n",
            "train - step 6075: loss = 502309.41 (1.428 sec)\n",
            "train - step 6076: loss = 386254.16 (0.971 sec)\n",
            "train - step 6077: loss = 384645.62 (0.974 sec)\n",
            "train - step 6078: loss = 402017.19 (0.966 sec)\n",
            "train - step 6079: loss = 382497.94 (0.945 sec)\n",
            "train - step 6080: loss = 576503.31 (0.993 sec)\n",
            "train - step 6081: loss = 378386.88 (0.968 sec)\n",
            "train - step 6082: loss = 386602.03 (0.980 sec)\n",
            "train - step 6083: loss = 384624.62 (0.976 sec)\n",
            "train - step 6084: loss = 324615.59 (0.984 sec)\n",
            "train - step 6085: loss = 342935.47 (0.952 sec)\n",
            "train - step 6086: loss = 411204.72 (0.988 sec)\n",
            "train - step 6087: loss = 309014.50 (0.970 sec)\n",
            "train - step 6088: loss = 395596.06 (0.984 sec)\n",
            "train - step 6089: loss = 381483.19 (0.974 sec)\n",
            "train - step 6090: loss = 367871.50 (0.965 sec)\n",
            "train - step 6091: loss = 438710.41 (0.962 sec)\n",
            "train - step 6092: loss = 437551.00 (0.964 sec)\n",
            "train - step 6093: loss = 360339.94 (0.962 sec)\n",
            "train - step 6094: loss = 335583.50 (0.985 sec)\n",
            "train - step 6095: loss = 393349.47 (0.969 sec)\n",
            "train - step 6096: loss = 300937.72 (1.012 sec)\n",
            "train - step 6097: loss = 543509.81 (0.998 sec)\n",
            "train - step 6098: loss = 412563.12 (1.002 sec)\n",
            "train - step 6099: loss = 327721.00 (1.000 sec)\n",
            "train - step 6100: loss = 371453.41 (0.995 sec)\n",
            "train - step 6101: loss = 459575.72 (1.003 sec)\n",
            "train - step 6102: loss = 368004.16 (1.013 sec)\n",
            "train - step 6103: loss = 392471.47 (1.010 sec)\n",
            "train - step 6104: loss = 305173.38 (1.022 sec)\n",
            "train - step 6105: loss = 572877.44 (0.998 sec)\n",
            "train - step 6106: loss = 336884.38 (0.975 sec)\n",
            "train - step 6107: loss = 442693.88 (0.982 sec)\n",
            "train - step 6108: loss = 276916.62 (0.997 sec)\n",
            "train - step 6109: loss = 423398.00 (0.985 sec)\n",
            "train - step 6110: loss = 361045.94 (0.977 sec)\n",
            "train - step 6111: loss = 457184.34 (0.969 sec)\n",
            "train - step 6112: loss = 335211.56 (0.983 sec)\n",
            "train - step 6113: loss = 420054.47 (0.986 sec)\n",
            "train - step 6114: loss = 312563.44 (0.969 sec)\n",
            "train - step 6115: loss = 423701.19 (1.004 sec)\n",
            "train - step 6116: loss = 456173.03 (1.014 sec)\n",
            "train - step 6117: loss = 357553.78 (1.012 sec)\n",
            "train - step 6118: loss = 445998.66 (1.014 sec)\n",
            "train - step 6119: loss = 366702.59 (1.010 sec)\n",
            "train - step 6120: loss = 473418.31 (0.978 sec)\n",
            "train - step 6121: loss = 275207.19 (0.975 sec)\n",
            "train - step 6122: loss = 273893.16 (0.970 sec)\n",
            "train - step 6123: loss = 588341.94 (0.973 sec)\n",
            "train - step 6124: loss = 294144.22 (0.971 sec)\n",
            "train - step 6125: loss = 508370.66 (0.966 sec)\n",
            "train - step 6126: loss = 341229.81 (0.989 sec)\n",
            "train - step 6127: loss = 277419.41 (0.979 sec)\n",
            "train - step 6128: loss = 364704.28 (0.970 sec)\n",
            "train - step 6129: loss = 352848.56 (0.977 sec)\n",
            "train - step 6130: loss = 376626.16 (0.972 sec)\n",
            "train - step 6131: loss = 353984.28 (0.957 sec)\n",
            "train - step 6132: loss = 494760.78 (0.964 sec)\n",
            "train - step 6133: loss = 370559.69 (0.977 sec)\n",
            "train - step 6134: loss = 349143.56 (0.966 sec)\n",
            "train - step 6135: loss = 457532.66 (0.958 sec)\n",
            "train - step 6136: loss = 508070.34 (1.640 sec)\n",
            "train - step 6137: loss = 448816.22 (0.961 sec)\n",
            "train - step 6138: loss = 359233.31 (0.976 sec)\n",
            "train - step 6139: loss = 322642.50 (0.966 sec)\n",
            "train - step 6140: loss = 566271.38 (0.969 sec)\n",
            "train - step 6141: loss = 283820.56 (0.957 sec)\n",
            "train - step 6142: loss = 465959.62 (0.977 sec)\n",
            "train - step 6143: loss = 410567.31 (0.967 sec)\n",
            "train - step 6144: loss = 488312.78 (0.974 sec)\n",
            "train - step 6145: loss = 371106.38 (0.991 sec)\n",
            "train - step 6146: loss = 370704.16 (0.963 sec)\n",
            "train - step 6147: loss = 384662.19 (0.964 sec)\n",
            "train - step 6148: loss = 513987.91 (0.981 sec)\n",
            "train - step 6149: loss = 288747.62 (0.968 sec)\n",
            "train - step 6150: loss = 337281.91 (0.967 sec)\n",
            "train - step 6151: loss = 293317.88 (0.977 sec)\n",
            "train - step 6152: loss = 362183.19 (0.979 sec)\n",
            "train - step 6153: loss = 447221.22 (0.961 sec)\n",
            "train - step 6154: loss = 378500.41 (0.980 sec)\n",
            "train - step 6155: loss = 495633.34 (0.977 sec)\n",
            "train - step 6156: loss = 381531.66 (0.981 sec)\n",
            "train - step 6157: loss = 422526.34 (0.964 sec)\n",
            "train - step 6158: loss = 463685.88 (0.958 sec)\n",
            "train - step 6159: loss = 464357.78 (0.957 sec)\n",
            "train - step 6160: loss = 305546.06 (0.957 sec)\n",
            "train - step 6161: loss = 439075.72 (0.970 sec)\n",
            "train - step 6162: loss = 421977.94 (0.972 sec)\n",
            "train - step 6163: loss = 318167.78 (0.967 sec)\n",
            "train - step 6164: loss = 412719.38 (0.965 sec)\n",
            "train - step 6165: loss = 452415.94 (0.966 sec)\n",
            "train - step 6166: loss = 495726.66 (0.974 sec)\n",
            "train - step 6167: loss = 510595.41 (0.984 sec)\n",
            "train - step 6168: loss = 363247.06 (0.976 sec)\n",
            "train - step 6169: loss = 482860.97 (0.951 sec)\n",
            "train - step 6170: loss = 484070.94 (0.981 sec)\n",
            "train - step 6171: loss = 405877.88 (0.973 sec)\n",
            "train - step 6172: loss = 369996.81 (0.964 sec)\n",
            "train - step 6173: loss = 243132.42 (0.970 sec)\n",
            "train - step 6174: loss = 323245.41 (0.975 sec)\n",
            "train - step 6175: loss = 473068.66 (0.969 sec)\n",
            "train - step 6176: loss = 424003.47 (0.980 sec)\n",
            "train - step 6177: loss = 361042.56 (0.965 sec)\n",
            "train - step 6178: loss = 460510.19 (1.001 sec)\n",
            "train - step 6179: loss = 502958.88 (0.996 sec)\n",
            "train - step 6180: loss = 412196.12 (1.014 sec)\n",
            "train - step 6181: loss = 389224.06 (0.994 sec)\n",
            "train - step 6182: loss = 364220.44 (1.002 sec)\n",
            "train - step 6183: loss = 363896.91 (0.991 sec)\n",
            "train - step 6184: loss = 379381.56 (1.011 sec)\n",
            "train - step 6185: loss = 382238.28 (0.991 sec)\n",
            "train - step 6186: loss = 474074.72 (1.021 sec)\n",
            "train - step 6187: loss = 270058.31 (0.984 sec)\n",
            "train - step 6188: loss = 231961.00 (0.966 sec)\n",
            "train - step 6189: loss = 440711.19 (0.976 sec)\n",
            "train - step 6190: loss = 340919.72 (0.983 sec)\n",
            "train - step 6191: loss = 330963.84 (0.965 sec)\n",
            "train - step 6192: loss = 325029.97 (0.967 sec)\n",
            "train - step 6193: loss = 400894.59 (0.956 sec)\n",
            "train - step 6194: loss = 422875.09 (0.968 sec)\n",
            "train - step 6195: loss = 353198.56 (0.969 sec)\n",
            "train - step 6196: loss = 433761.88 (0.980 sec)\n",
            "train - step 6197: loss = 444448.28 (0.956 sec)\n",
            "train - step 6198: loss = 378331.84 (1.573 sec)\n",
            "train - step 6199: loss = 368722.19 (0.969 sec)\n",
            "train - step 6200: loss = 385615.28 (0.974 sec)\n",
            "train - step 6201: loss = 422233.78 (0.965 sec)\n",
            "train - step 6202: loss = 306854.00 (0.981 sec)\n",
            "train - step 6203: loss = 411268.62 (0.959 sec)\n",
            "train - step 6204: loss = 565975.19 (0.984 sec)\n",
            "train - step 6205: loss = 463077.38 (0.974 sec)\n",
            "train - step 6206: loss = 317510.00 (0.982 sec)\n",
            "train - step 6207: loss = 364636.00 (0.974 sec)\n",
            "train - step 6208: loss = 337532.72 (0.967 sec)\n",
            "train - step 6209: loss = 382311.06 (0.966 sec)\n",
            "train - step 6210: loss = 303975.53 (0.981 sec)\n",
            "train - step 6211: loss = 382568.34 (0.961 sec)\n",
            "train - step 6212: loss = 314235.34 (0.976 sec)\n",
            "train - step 6213: loss = 316199.78 (0.965 sec)\n",
            "train - step 6214: loss = 324672.97 (0.972 sec)\n",
            "train - step 6215: loss = 362840.50 (0.959 sec)\n",
            "train - step 6216: loss = 355483.16 (0.969 sec)\n",
            "train - step 6217: loss = 364202.16 (0.955 sec)\n",
            "train - step 6218: loss = 539331.12 (0.980 sec)\n",
            "train - step 6219: loss = 399166.06 (0.976 sec)\n",
            "train - step 6220: loss = 318211.81 (0.966 sec)\n",
            "train - step 6221: loss = 515736.06 (0.955 sec)\n",
            "train - step 6222: loss = 297907.28 (0.978 sec)\n",
            "train - step 6223: loss = 381238.91 (0.966 sec)\n",
            "train - step 6224: loss = 374589.34 (0.975 sec)\n",
            "train - step 6225: loss = 428672.88 (0.974 sec)\n",
            "train - step 6226: loss = 351008.44 (0.978 sec)\n",
            "train - step 6227: loss = 339284.00 (0.961 sec)\n",
            "train - step 6228: loss = 330423.81 (0.972 sec)\n",
            "train - step 6229: loss = 341926.62 (0.962 sec)\n",
            "train - step 6230: loss = 414502.06 (0.975 sec)\n",
            "train - step 6231: loss = 326635.03 (0.964 sec)\n",
            "train - step 6232: loss = 437482.97 (0.977 sec)\n",
            "train - step 6233: loss = 432590.50 (0.957 sec)\n",
            "train - step 6234: loss = 294859.22 (0.968 sec)\n",
            "train - step 6235: loss = 342219.88 (0.956 sec)\n",
            "train - step 6236: loss = 478201.81 (0.971 sec)\n",
            "train - step 6237: loss = 385271.22 (0.957 sec)\n",
            "train - step 6238: loss = 297904.53 (0.970 sec)\n",
            "train - step 6239: loss = 384928.53 (0.993 sec)\n",
            "train - step 6240: loss = 354027.50 (0.985 sec)\n",
            "train - step 6241: loss = 431143.97 (0.974 sec)\n",
            "train - step 6242: loss = 389169.06 (0.970 sec)\n",
            "train - step 6243: loss = 507670.19 (0.980 sec)\n",
            "train - step 6244: loss = 411803.62 (0.990 sec)\n",
            "train - step 6245: loss = 427488.00 (0.954 sec)\n",
            "train - step 6246: loss = 378391.31 (0.959 sec)\n",
            "train - step 6247: loss = 343265.91 (0.977 sec)\n",
            "train - step 6248: loss = 385484.12 (0.983 sec)\n",
            "train - step 6249: loss = 480417.34 (0.963 sec)\n",
            "train - step 6250: loss = 362872.41 (0.968 sec)\n",
            "train - step 6251: loss = 453043.06 (1.015 sec)\n",
            "train - step 6252: loss = 534324.88 (1.001 sec)\n",
            "train - step 6253: loss = 374997.28 (1.299 sec)\n",
            "train - step 6254: loss = 502301.56 (0.974 sec)\n",
            "train - step 6255: loss = 364232.97 (0.973 sec)\n",
            "train - step 6256: loss = 368989.28 (0.957 sec)\n",
            "train - step 6257: loss = 468792.84 (0.947 sec)\n",
            "train - step 6258: loss = 477816.66 (0.967 sec)\n",
            "train - step 6259: loss = 305083.06 (1.477 sec)\n",
            "train - step 6260: loss = 485874.00 (1.001 sec)\n",
            "train - step 6261: loss = 340116.00 (1.003 sec)\n",
            "train - step 6262: loss = 391056.41 (1.002 sec)\n",
            "train - step 6263: loss = 399899.97 (0.991 sec)\n",
            "train - step 6264: loss = 374020.38 (1.012 sec)\n",
            "train - step 6265: loss = 429292.88 (1.002 sec)\n",
            "train - step 6266: loss = 341371.62 (1.007 sec)\n",
            "train - step 6267: loss = 378126.94 (0.993 sec)\n",
            "train - step 6268: loss = 328634.56 (0.999 sec)\n",
            "train - step 6269: loss = 383310.84 (0.963 sec)\n",
            "train - step 6270: loss = 274068.41 (0.966 sec)\n",
            "train - step 6271: loss = 242578.78 (0.948 sec)\n",
            "train - step 6272: loss = 333515.28 (0.967 sec)\n",
            "train - step 6273: loss = 412118.06 (0.960 sec)\n",
            "train - step 6274: loss = 431372.81 (0.979 sec)\n",
            "train - step 6275: loss = 444380.44 (0.968 sec)\n",
            "train - step 6276: loss = 392406.94 (0.974 sec)\n",
            "train - step 6277: loss = 357766.06 (0.961 sec)\n",
            "train - step 6278: loss = 324496.97 (0.967 sec)\n",
            "train - step 6279: loss = 483919.78 (0.955 sec)\n",
            "train - step 6280: loss = 356688.47 (0.973 sec)\n",
            "train - step 6281: loss = 498658.81 (0.980 sec)\n",
            "train - step 6282: loss = 356241.59 (0.971 sec)\n",
            "train - step 6283: loss = 414139.19 (0.969 sec)\n",
            "train - step 6284: loss = 388472.22 (0.979 sec)\n",
            "train - step 6285: loss = 426393.28 (0.959 sec)\n",
            "train - step 6286: loss = 187069.95 (0.958 sec)\n",
            "train - step 6287: loss = 384358.72 (0.960 sec)\n",
            "train - step 6288: loss = 328050.28 (0.991 sec)\n",
            "train - step 6289: loss = 398332.72 (0.951 sec)\n",
            "train - step 6290: loss = 274090.22 (0.968 sec)\n",
            "train - step 6291: loss = 414591.44 (0.971 sec)\n",
            "train - step 6292: loss = 344556.59 (0.965 sec)\n",
            "train - step 6293: loss = 251895.84 (0.946 sec)\n",
            "train - step 6294: loss = 418425.22 (0.964 sec)\n",
            "train - step 6295: loss = 325486.44 (0.959 sec)\n",
            "train - step 6296: loss = 431551.31 (0.976 sec)\n",
            "train - step 6297: loss = 303757.88 (0.963 sec)\n",
            "train - step 6298: loss = 395466.72 (0.976 sec)\n",
            "train - step 6299: loss = 294922.72 (0.967 sec)\n",
            "train - step 6300: loss = 353816.97 (0.952 sec)\n",
            "train - step 6301: loss = 504345.81 (0.976 sec)\n",
            "train - step 6302: loss = 553273.81 (0.979 sec)\n",
            "train - step 6303: loss = 357963.03 (0.976 sec)\n",
            "train - step 6304: loss = 437868.00 (0.970 sec)\n",
            "train - step 6305: loss = 365670.31 (0.956 sec)\n",
            "train - step 6306: loss = 359004.91 (0.970 sec)\n",
            "train - step 6307: loss = 432330.00 (0.961 sec)\n",
            "train - step 6308: loss = 464581.84 (0.959 sec)\n",
            "train - step 6309: loss = 264570.22 (0.974 sec)\n",
            "train - step 6310: loss = 423759.69 (0.954 sec)\n",
            "train - step 6311: loss = 275207.16 (0.951 sec)\n",
            "train - step 6312: loss = 399397.00 (0.978 sec)\n",
            "train - step 6313: loss = 394147.53 (0.964 sec)\n",
            "train - step 6314: loss = 278793.12 (0.970 sec)\n",
            "train - step 6315: loss = 477118.56 (0.965 sec)\n",
            "train - step 6316: loss = 423861.44 (0.973 sec)\n",
            "train - step 6317: loss = 391974.47 (0.955 sec)\n",
            "train - step 6318: loss = 297902.59 (0.968 sec)\n",
            "train - step 6319: loss = 226680.52 (0.963 sec)\n",
            "train - step 6320: loss = 411358.66 (0.977 sec)\n",
            "train - step 6321: loss = 346406.78 (1.615 sec)\n",
            "train - step 6322: loss = 269248.16 (1.006 sec)\n",
            "train - step 6323: loss = 320440.31 (1.017 sec)\n",
            "train - step 6324: loss = 418465.00 (1.024 sec)\n",
            "train - step 6325: loss = 472520.53 (1.007 sec)\n",
            "train - step 6326: loss = 353892.62 (1.014 sec)\n",
            "train - step 6327: loss = 446556.66 (0.989 sec)\n",
            "train - step 6328: loss = 565182.25 (1.007 sec)\n",
            "train - step 6329: loss = 474828.81 (1.008 sec)\n",
            "train - step 6330: loss = 320625.56 (0.991 sec)\n",
            "train - step 6331: loss = 515523.00 (0.944 sec)\n",
            "train - step 6332: loss = 410752.91 (0.982 sec)\n",
            "train - step 6333: loss = 329476.84 (0.962 sec)\n",
            "train - step 6334: loss = 598141.12 (0.988 sec)\n",
            "train - step 6335: loss = 386781.06 (0.974 sec)\n",
            "train - step 6336: loss = 542435.94 (0.980 sec)\n",
            "train - step 6337: loss = 402637.19 (0.948 sec)\n",
            "train - step 6338: loss = 463013.00 (0.965 sec)\n",
            "train - step 6339: loss = 446851.19 (0.970 sec)\n",
            "train - step 6340: loss = 469135.31 (0.973 sec)\n",
            "train - step 6341: loss = 412874.06 (0.996 sec)\n",
            "train - step 6342: loss = 314182.06 (1.001 sec)\n",
            "train - step 6343: loss = 438539.47 (0.998 sec)\n",
            "train - step 6344: loss = 471836.62 (1.005 sec)\n",
            "train - step 6345: loss = 374868.94 (1.005 sec)\n",
            "train - step 6346: loss = 436816.81 (1.014 sec)\n",
            "train - step 6347: loss = 358053.66 (0.990 sec)\n",
            "train - step 6348: loss = 305765.03 (0.986 sec)\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 242, in run_training\n",
            "    FLAGS.num_vertices)\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 94, in get_input_pair\n",
            "    evecs = batch_input_['source_evecs'][ind_source, :]\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W6ftKOXZOmS",
        "colab_type": "text"
      },
      "source": [
        "# 第八次运行 模型顶点数为6000，batchsize为16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXbb1l8wZYyq",
        "colab_type": "code",
        "outputId": "ac9a07a3-bbdb-4d6d-d8eb-46ad26b3e8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1952
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-10 01:41:14.374984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-10 01:41:14.375189: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2690ec0 executing computations on platform Host. Devices:\n",
            "2019-05-10 01:41:14.375220: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-10 01:41:14.538859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-10 01:41:14.539363: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2691020 executing computations on platform CUDA. Devices:\n",
            "2019-05-10 01:41:14.539394: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-10 01:41:14.539895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-10 01:41:14.539936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-10 01:41:14.980792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-10 01:41:14.980871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-10 01:41:14.980883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-10 01:41:14.981157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-10 01:45:31.147091: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-10 01:45:31.836641: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x93314d0\n",
            "train - step 6239: loss = 454023.56 (5.577 sec)\n",
            "train - step 6240: loss = 520261.34 (2.015 sec)\n",
            "train - step 6241: loss = 323300.38 (1.953 sec)\n",
            "train - step 6242: loss = 387040.84 (1.909 sec)\n",
            "train - step 6243: loss = 337827.00 (1.929 sec)\n",
            "train - step 6244: loss = 484603.91 (1.914 sec)\n",
            "train - step 6245: loss = 506860.59 (1.935 sec)\n",
            "train - step 6246: loss = 419543.03 (1.903 sec)\n",
            "train - step 6247: loss = 326255.34 (1.953 sec)\n",
            "train - step 6248: loss = 374639.00 (1.907 sec)\n",
            "train - step 6249: loss = 361029.31 (1.980 sec)\n",
            "train - step 6250: loss = 336845.78 (1.979 sec)\n",
            "train - step 6251: loss = 373585.59 (2.017 sec)\n",
            "train - step 6252: loss = 411940.03 (1.985 sec)\n",
            "train - step 6253: loss = 391775.81 (2.011 sec)\n",
            "train - step 6254: loss = 415970.66 (1.925 sec)\n",
            "train - step 6255: loss = 333404.06 (1.939 sec)\n",
            "train - step 6256: loss = 425962.81 (1.910 sec)\n",
            "train - step 6257: loss = 418681.09 (2.487 sec)\n",
            "train - step 6258: loss = 484733.62 (1.911 sec)\n",
            "train - step 6259: loss = 374645.38 (1.956 sec)\n",
            "train - step 6260: loss = 363780.12 (1.903 sec)\n",
            "train - step 6261: loss = 382893.91 (1.968 sec)\n",
            "train - step 6262: loss = 442307.91 (1.918 sec)\n",
            "train - step 6263: loss = 377297.22 (1.963 sec)\n",
            "train - step 6264: loss = 456299.09 (1.936 sec)\n",
            "train - step 6265: loss = 270940.16 (1.959 sec)\n",
            "train - step 6266: loss = 413129.62 (1.932 sec)\n",
            "train - step 6267: loss = 307927.91 (1.951 sec)\n",
            "train - step 6268: loss = 425235.00 (1.935 sec)\n",
            "train - step 6269: loss = 340008.78 (1.960 sec)\n",
            "train - step 6270: loss = 449124.03 (1.923 sec)\n",
            "train - step 6271: loss = 416455.94 (1.958 sec)\n",
            "train - step 6272: loss = 330595.19 (1.916 sec)\n",
            "train - step 6273: loss = 491521.59 (1.992 sec)\n",
            "train - step 6274: loss = 424854.97 (1.936 sec)\n",
            "train - step 6275: loss = 452591.81 (1.984 sec)\n",
            "train - step 6276: loss = 404582.59 (1.950 sec)\n",
            "train - step 6277: loss = 391689.72 (1.971 sec)\n",
            "train - step 6278: loss = 468892.09 (1.934 sec)\n",
            "train - step 6279: loss = 439590.56 (1.992 sec)\n",
            "train - step 6280: loss = 397876.50 (1.968 sec)\n",
            "train - step 6281: loss = 401925.16 (2.003 sec)\n",
            "train - step 6282: loss = 400468.06 (1.950 sec)\n",
            "train - step 6283: loss = 284542.31 (1.987 sec)\n",
            "train - step 6284: loss = 428105.50 (1.976 sec)\n",
            "train - step 6285: loss = 327094.19 (1.992 sec)\n",
            "train - step 6286: loss = 368430.00 (1.931 sec)\n",
            "train - step 6287: loss = 397052.09 (1.982 sec)\n",
            "train - step 6288: loss = 413061.78 (2.582 sec)\n",
            "train - step 6289: loss = 413209.09 (1.964 sec)\n",
            "train - step 6290: loss = 436303.34 (2.007 sec)\n",
            "train - step 6291: loss = 457978.88 (2.060 sec)\n",
            "train - step 6292: loss = 429360.22 (2.015 sec)\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 242, in run_training\n",
            "    FLAGS.num_vertices)\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 106, in get_input_pair\n",
            "    shot = batch_input_['target_shot'][ind_target, :]\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T2sBfnuZ-l2",
        "colab_type": "text"
      },
      "source": [
        "# 在TF1.6下，使用batch)size=32,顶点数为1500，来跑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbptO1bl9nF",
        "colab_type": "code",
        "outputId": "8839f0e3-3e6a-488b-8ef1-18b1c610ecbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "!pip install tensorflow==1.6\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/0f/fbd8bb92459c75db93040f80702ebe4ba83a52cdb6ad930654c31dc0b711/tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.7.1)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0 (from tensorflow==1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/67/a8c91665987d359211dcdca5c8b2a7c1e0876eb0702a4383c1e4ff76228d/tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.16.3)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.33.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (0.15.2)\n",
            "Collecting html5lib==0.9999999 (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.6MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6)\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.6) (41.0.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.13.1\n",
            "    Uninstalling tensorboard-1.13.1:\n",
            "      Successfully uninstalled tensorboard-1.13.1\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZ-OeBOmiQI",
        "colab_type": "code",
        "outputId": "144f8245-1205-4f2b-a142-550fcd8e4505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1263
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-10 03:05:43.675370: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-10 03:09:26.360329: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at gather_nd_op.cc:50 : Invalid argument: flat indices[2239, :] = [31, 491] does not index into param (shape: [32,352,1500]).\n",
            "2019-05-10 03:09:26.363518: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at gather_nd_op.cc:50 : Invalid argument: flat indices[2239, :] = [31, 1364] does not index into param (shape: [32,352,1500]).\n",
            "2019-05-10 03:09:26.964225: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at gather_nd_op.cc:50 : Invalid argument: flat indices[2239, :] = [31, 491] does not index into param (shape: [32,352,1500]).\n",
            "2019-05-10 03:09:26.969264: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at gather_nd_op.cc:50 : Invalid argument: flat indices[2239, :] = [31, 1364] does not index into param (shape: [32,352,1500]).\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\n",
            "    target_list, status, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[2239, :] = [31, 491] does not index into param (shape: [32,352,1500]).\n",
            "\t [[Node: func_map_loss/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](func_map_loss/transpose_2, func_map_loss/concat)]]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 258, in run_training\n",
            "    feed_dict=feed_dict\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 905, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\n",
            "    options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[2239, :] = [31, 491] does not index into param (shape: [32,352,1500]).\n",
            "\t [[Node: func_map_loss/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](func_map_loss/transpose_2, func_map_loss/concat)]]\n",
            "\n",
            "Caused by op 'func_map_loss/GatherNd', defined at:\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 283, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 278, in main\n",
            "    run_training()\n",
            "  File \"drive/unsupervisedfmnet/train_DFMnet.py\", line 200, in run_training\n",
            "    target_evecs_trans, target_evals\n",
            "  File \"/content/drive/unsupervisedfmnet/DFMnet.py\", line 83, in dfmnet_model\n",
            "    F, G\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 180, in func_map_layer\n",
            "    target_evecs, target_evecs_trans) +\n",
            "  File \"/content/drive/unsupervisedfmnet/loss_DFMnet.py\", line 97, in penalty_desc_commutativity\n",
            "    F_ = tf.gather_nd(F_trans, indices)  # percent% of descriptors chosen\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1915, in gather_nd\n",
            "    \"GatherNd\", params=params, indices=indices, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n",
            "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
            "\n",
            "InvalidArgumentError (see above for traceback): flat indices[2239, :] = [31, 491] does not index into param (shape: [32,352,1500]).\n",
            "\t [[Node: func_map_loss/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](func_map_loss/transpose_2, func_map_loss/concat)]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJquCt9auaOq",
        "colab_type": "code",
        "outputId": "8f42126b-2881-4678-e148-cd6e8ce3c118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "!import tensorflow as tf\n",
        "tf.test.gpu_device_name() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: import: command not found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b7858e24b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'import tensorflow as tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThMSqKNJvnL_",
        "colab_type": "code",
        "outputId": "aebef926-775b-48c7-fab5-23143ba65241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 13382261559692838197]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJfXjyzjwKS2",
        "colab_type": "code",
        "outputId": "410af1cf-2e8a-4a5a-e436-fe61ea8395fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.6.0\n",
            "Summary: TensorFlow helps the tensors flow\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: numpy, tensorboard, protobuf, termcolor, astor, grpcio, gast, wheel, six, absl-py\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k8tCJ5-wPjF",
        "colab_type": "code",
        "outputId": "868b459d-ff16-46b8-dbf9-9eb38e15520b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.2.0)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 1.6.0\n",
            "    Uninstalling tensorboard-1.6.0:\n",
            "      Successfully uninstalled tensorboard-1.6.0\n",
            "  Found existing installation: tensorflow 1.6.0\n",
            "    Uninstalling tensorflow-1.6.0:\n",
            "      Successfully uninstalled tensorflow-1.6.0\n",
            "Successfully installed tensorboard-1.13.1 tensorflow-1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HQpJbUQwXNk",
        "colab_type": "code",
        "outputId": "6f3e1c62-36e6-4d42-85dc-a67fd69b6646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.13.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: tensorboard, wheel, absl-py, grpcio, numpy, keras-preprocessing, tensorflow-estimator, protobuf, keras-applications, gast, six, termcolor, astor\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWs8uVOfwc1u",
        "colab_type": "code",
        "outputId": "542865de-4411-49d9-e44f-476eda346d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6832658391531614442]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE_OwSApwoFG",
        "colab_type": "code",
        "outputId": "ad0208d6-4807-47bb-cb04-d8cd49e09d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2406
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/test_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 12:53:00.615732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-24 12:53:00.616088: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ebf340 executing computations on platform Host. Devices:\n",
            "2019-05-24 12:53:00.616137: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 12:53:00.839989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 12:53:00.840566: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ebedc0 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 12:53:00.840617: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 12:53:00.841028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 12:53:00.841058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 12:53:02.216550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 12:53:02.216621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 12:53:02.216633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 12:53:02.216944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "restoring graph...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-24 12:53:19.458438: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-24 12:53:20.337755: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x18ecba38\n",
            "Computed correspondences for pair: 80, 81. Took 9.526773 seconds\n",
            "Computed correspondences for pair: 80, 82. Took 10.911813 seconds\n",
            "Computed correspondences for pair: 80, 83. Took 8.694066 seconds\n",
            "Computed correspondences for pair: 80, 84. Took 6.261260 seconds\n",
            "Computed correspondences for pair: 80, 85. Took 6.948559 seconds\n",
            "Computed correspondences for pair: 80, 86. Took 7.743754 seconds\n",
            "Computed correspondences for pair: 80, 87. Took 8.557280 seconds\n",
            "Computed correspondences for pair: 80, 88. Took 8.270167 seconds\n",
            "Computed correspondences for pair: 80, 89. Took 7.609592 seconds\n",
            "Computed correspondences for pair: 80, 90. Took 9.111737 seconds\n",
            "Computed correspondences for pair: 80, 91. Took 9.107447 seconds\n",
            "Computed correspondences for pair: 80, 92. Took 7.852061 seconds\n",
            "Computed correspondences for pair: 80, 93. Took 9.137715 seconds\n",
            "Computed correspondences for pair: 80, 94. Took 8.316390 seconds\n",
            "Computed correspondences for pair: 80, 95. Took 7.728957 seconds\n",
            "Computed correspondences for pair: 80, 96. Took 12.423061 seconds\n",
            "Computed correspondences for pair: 80, 97. Took 8.530000 seconds\n",
            "Computed correspondences for pair: 80, 98. Took 8.045705 seconds\n",
            "Computed correspondences for pair: 80, 99. Took 11.032603 seconds\n",
            "Computed correspondences for pair: 81, 82. Took 3.477942 seconds\n",
            "Computed correspondences for pair: 81, 83. Took 3.191246 seconds\n",
            "Computed correspondences for pair: 81, 84. Took 3.437362 seconds\n",
            "Computed correspondences for pair: 81, 85. Took 6.559477 seconds\n",
            "Computed correspondences for pair: 81, 86. Took 3.478906 seconds\n",
            "Computed correspondences for pair: 81, 87. Took 7.259806 seconds\n",
            "Computed correspondences for pair: 81, 88. Took 3.513033 seconds\n",
            "Computed correspondences for pair: 81, 89. Took 3.469144 seconds\n",
            "Computed correspondences for pair: 81, 90. Took 7.552850 seconds\n",
            "Computed correspondences for pair: 81, 91. Took 4.315548 seconds\n",
            "Computed correspondences for pair: 81, 92. Took 4.594684 seconds\n",
            "Computed correspondences for pair: 81, 93. Took 7.655083 seconds\n",
            "Computed correspondences for pair: 81, 94. Took 4.324219 seconds\n",
            "Computed correspondences for pair: 81, 95. Took 4.562128 seconds\n",
            "Computed correspondences for pair: 81, 96. Took 4.526575 seconds\n",
            "Computed correspondences for pair: 81, 97. Took 4.742277 seconds\n",
            "Computed correspondences for pair: 81, 98. Took 5.353215 seconds\n",
            "Computed correspondences for pair: 81, 99. Took 4.540072 seconds\n",
            "Computed correspondences for pair: 82, 83. Took 3.417833 seconds\n",
            "Computed correspondences for pair: 82, 84. Took 2.932148 seconds\n",
            "Computed correspondences for pair: 82, 85. Took 4.302758 seconds\n",
            "Computed correspondences for pair: 82, 86. Took 3.778525 seconds\n",
            "Computed correspondences for pair: 82, 87. Took 5.007032 seconds\n",
            "Computed correspondences for pair: 82, 88. Took 3.255369 seconds\n",
            "Computed correspondences for pair: 82, 89. Took 3.153232 seconds\n",
            "Computed correspondences for pair: 82, 90. Took 7.269907 seconds\n",
            "Computed correspondences for pair: 82, 91. Took 4.293763 seconds\n",
            "Computed correspondences for pair: 82, 92. Took 4.966799 seconds\n",
            "Computed correspondences for pair: 82, 93. Took 8.103796 seconds\n",
            "Computed correspondences for pair: 82, 94. Took 4.417486 seconds\n",
            "Computed correspondences for pair: 82, 95. Took 4.509446 seconds\n",
            "Computed correspondences for pair: 82, 96. Took 4.410897 seconds\n",
            "Computed correspondences for pair: 82, 97. Took 4.358075 seconds\n",
            "Computed correspondences for pair: 82, 98. Took 4.379443 seconds\n",
            "Computed correspondences for pair: 82, 99. Took 4.520950 seconds\n",
            "Computed correspondences for pair: 83, 84. Took 3.789865 seconds\n",
            "Computed correspondences for pair: 83, 85. Took 4.921783 seconds\n",
            "Computed correspondences for pair: 83, 86. Took 2.876091 seconds\n",
            "Computed correspondences for pair: 83, 87. Took 5.908296 seconds\n",
            "Computed correspondences for pair: 83, 88. Took 3.577442 seconds\n",
            "Computed correspondences for pair: 83, 89. Took 3.355551 seconds\n",
            "Computed correspondences for pair: 83, 90. Took 5.497230 seconds\n",
            "Computed correspondences for pair: 83, 91. Took 4.305110 seconds\n",
            "Computed correspondences for pair: 83, 92. Took 4.414963 seconds\n",
            "Computed correspondences for pair: 83, 93. Took 5.466632 seconds\n",
            "Computed correspondences for pair: 83, 94. Took 4.408313 seconds\n",
            "Computed correspondences for pair: 83, 95. Took 4.516165 seconds\n",
            "Computed correspondences for pair: 83, 96. Took 4.283286 seconds\n",
            "Computed correspondences for pair: 83, 97. Took 4.321809 seconds\n",
            "Computed correspondences for pair: 83, 98. Took 4.992788 seconds\n",
            "Computed correspondences for pair: 83, 99. Took 5.240998 seconds\n",
            "Computed correspondences for pair: 84, 85. Took 4.431358 seconds\n",
            "Computed correspondences for pair: 84, 86. Took 3.989119 seconds\n",
            "Computed correspondences for pair: 84, 87. Took 5.914047 seconds\n",
            "Computed correspondences for pair: 84, 88. Took 3.577174 seconds\n",
            "Computed correspondences for pair: 84, 89. Took 3.579795 seconds\n",
            "Computed correspondences for pair: 84, 90. Took 6.004693 seconds\n",
            "Computed correspondences for pair: 84, 91. Took 4.420863 seconds\n",
            "Computed correspondences for pair: 84, 92. Took 4.357395 seconds\n",
            "Computed correspondences for pair: 84, 93. Took 6.527468 seconds\n",
            "Computed correspondences for pair: 84, 94. Took 4.543957 seconds\n",
            "Computed correspondences for pair: 84, 95. Took 5.005235 seconds\n",
            "Computed correspondences for pair: 84, 96. Took 4.523797 seconds\n",
            "Computed correspondences for pair: 84, 97. Took 5.216222 seconds\n",
            "Computed correspondences for pair: 84, 98. Took 4.851132 seconds\n",
            "Computed correspondences for pair: 84, 99. Took 4.593782 seconds\n",
            "Computed correspondences for pair: 85, 86. Took 3.580353 seconds\n",
            "Computed correspondences for pair: 85, 87. Took 6.471253 seconds\n",
            "Computed correspondences for pair: 85, 88. Took 3.291399 seconds\n",
            "Computed correspondences for pair: 85, 89. Took 3.336768 seconds\n",
            "Computed correspondences for pair: 85, 90. Took 7.143383 seconds\n",
            "Computed correspondences for pair: 85, 91. Took 5.118040 seconds\n",
            "Computed correspondences for pair: 85, 92. Took 4.540053 seconds\n",
            "Computed correspondences for pair: 85, 93. Took 6.623691 seconds\n",
            "Computed correspondences for pair: 85, 94. Took 4.380295 seconds\n",
            "Computed correspondences for pair: 85, 95. Took 4.361548 seconds\n",
            "Computed correspondences for pair: 85, 96. Took 4.448638 seconds\n",
            "Computed correspondences for pair: 85, 97. Took 4.407250 seconds\n",
            "Computed correspondences for pair: 85, 98. Took 5.156006 seconds\n",
            "Computed correspondences for pair: 85, 99. Took 4.552052 seconds\n",
            "Computed correspondences for pair: 86, 87. Took 5.137130 seconds\n",
            "Computed correspondences for pair: 86, 88. Took 3.446437 seconds\n",
            "Computed correspondences for pair: 86, 89. Took 4.219309 seconds\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 123, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 119, in main\n",
            "    run_test()\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 80, in run_test\n",
            "    '%.3d' % j)\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 40, in get_test_pair_target\n",
            "    input_data.update(sio.loadmat(target_file))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\", line 218, in loadmat\n",
            "    matfile_dict = MR.get_variables(variable_names)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio5.py\", line 292, in get_variables\n",
            "    res = self.read_var_array(hdr, process)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio5.py\", line 252, in read_var_array\n",
            "    return self._matrix_reader.array_from_header(header, process)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeLC6Xz_9N0c",
        "colab_type": "code",
        "outputId": "9c6c3213-c0d2-421b-8efb-d631b2d9f065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3598
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/test_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 13:08:26.990725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-24 13:08:26.990995: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x31a3340 executing computations on platform Host. Devices:\n",
            "2019-05-24 13:08:26.991048: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 13:08:27.185031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 13:08:27.185590: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x31a2dc0 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 13:08:27.185643: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 13:08:27.186073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 13:08:27.186101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 13:08:27.625714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 13:08:27.625791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 13:08:27.625801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 13:08:27.626076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "restoring graph...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-24 13:08:31.980800: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-24 13:08:32.342440: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x17defa38\n",
            "Computed correspondences for pair: 80, 81. Took 6.021699 seconds\n",
            "Computed correspondences for pair: 80, 82. Took 2.944687 seconds\n",
            "Computed correspondences for pair: 80, 83. Took 3.329654 seconds\n",
            "Computed correspondences for pair: 80, 84. Took 2.840489 seconds\n",
            "Computed correspondences for pair: 80, 85. Took 4.268275 seconds\n",
            "Computed correspondences for pair: 80, 86. Took 4.064808 seconds\n",
            "Computed correspondences for pair: 80, 87. Took 5.192406 seconds\n",
            "Computed correspondences for pair: 80, 88. Took 3.280622 seconds\n",
            "Computed correspondences for pair: 80, 89. Took 3.238242 seconds\n",
            "Computed correspondences for pair: 80, 90. Took 5.870911 seconds\n",
            "Computed correspondences for pair: 80, 91. Took 4.910852 seconds\n",
            "Computed correspondences for pair: 80, 92. Took 5.009575 seconds\n",
            "Computed correspondences for pair: 80, 93. Took 6.710870 seconds\n",
            "Computed correspondences for pair: 80, 94. Took 4.340869 seconds\n",
            "Computed correspondences for pair: 80, 95. Took 4.533532 seconds\n",
            "Computed correspondences for pair: 80, 96. Took 4.428755 seconds\n",
            "Computed correspondences for pair: 80, 97. Took 4.339134 seconds\n",
            "Computed correspondences for pair: 80, 98. Took 4.353235 seconds\n",
            "Computed correspondences for pair: 80, 99. Took 4.488492 seconds\n",
            "Computed correspondences for pair: 81, 82. Took 3.470725 seconds\n",
            "Computed correspondences for pair: 81, 83. Took 3.189237 seconds\n",
            "Computed correspondences for pair: 81, 84. Took 3.441160 seconds\n",
            "Computed correspondences for pair: 81, 85. Took 6.680862 seconds\n",
            "Computed correspondences for pair: 81, 86. Took 4.031542 seconds\n",
            "Computed correspondences for pair: 81, 87. Took 6.077288 seconds\n",
            "Computed correspondences for pair: 81, 88. Took 3.499722 seconds\n",
            "Computed correspondences for pair: 81, 89. Took 3.450591 seconds\n",
            "Computed correspondences for pair: 81, 90. Took 6.681418 seconds\n",
            "Computed correspondences for pair: 81, 91. Took 4.312011 seconds\n",
            "Computed correspondences for pair: 81, 92. Took 4.569018 seconds\n",
            "Computed correspondences for pair: 81, 93. Took 5.638085 seconds\n",
            "Computed correspondences for pair: 81, 94. Took 4.357306 seconds\n",
            "Computed correspondences for pair: 81, 95. Took 4.579148 seconds\n",
            "Computed correspondences for pair: 81, 96. Took 4.528480 seconds\n",
            "Computed correspondences for pair: 81, 97. Took 4.320707 seconds\n",
            "Computed correspondences for pair: 81, 98. Took 4.602357 seconds\n",
            "Computed correspondences for pair: 81, 99. Took 5.356956 seconds\n",
            "Computed correspondences for pair: 82, 83. Took 3.398903 seconds\n",
            "Computed correspondences for pair: 82, 84. Took 2.912369 seconds\n",
            "Computed correspondences for pair: 82, 85. Took 4.152193 seconds\n",
            "Computed correspondences for pair: 82, 86. Took 3.588422 seconds\n",
            "Computed correspondences for pair: 82, 87. Took 5.040274 seconds\n",
            "Computed correspondences for pair: 82, 88. Took 3.279434 seconds\n",
            "Computed correspondences for pair: 82, 89. Took 3.126570 seconds\n",
            "Computed correspondences for pair: 82, 90. Took 6.453467 seconds\n",
            "Computed correspondences for pair: 82, 91. Took 4.299500 seconds\n",
            "Computed correspondences for pair: 82, 92. Took 4.227959 seconds\n",
            "Computed correspondences for pair: 82, 93. Took 5.724039 seconds\n",
            "Computed correspondences for pair: 82, 94. Took 4.426380 seconds\n",
            "Computed correspondences for pair: 82, 95. Took 4.819207 seconds\n",
            "Computed correspondences for pair: 82, 96. Took 5.218499 seconds\n",
            "Computed correspondences for pair: 82, 97. Took 4.291466 seconds\n",
            "Computed correspondences for pair: 82, 98. Took 4.428304 seconds\n",
            "Computed correspondences for pair: 82, 99. Took 4.488462 seconds\n",
            "Computed correspondences for pair: 83, 84. Took 3.819885 seconds\n",
            "Computed correspondences for pair: 83, 85. Took 5.911982 seconds\n",
            "Computed correspondences for pair: 83, 86. Took 2.855961 seconds\n",
            "Computed correspondences for pair: 83, 87. Took 4.914984 seconds\n",
            "Computed correspondences for pair: 83, 88. Took 3.530758 seconds\n",
            "Computed correspondences for pair: 83, 89. Took 3.336538 seconds\n",
            "Computed correspondences for pair: 83, 90. Took 6.974315 seconds\n",
            "Computed correspondences for pair: 83, 91. Took 4.303042 seconds\n",
            "Computed correspondences for pair: 83, 92. Took 4.725350 seconds\n",
            "Computed correspondences for pair: 83, 93. Took 6.936905 seconds\n",
            "Computed correspondences for pair: 83, 94. Took 4.519414 seconds\n",
            "Computed correspondences for pair: 83, 95. Took 4.508396 seconds\n",
            "Computed correspondences for pair: 83, 96. Took 4.285050 seconds\n",
            "Computed correspondences for pair: 83, 97. Took 4.319085 seconds\n",
            "Computed correspondences for pair: 83, 98. Took 4.492892 seconds\n",
            "Computed correspondences for pair: 83, 99. Took 4.550009 seconds\n",
            "Computed correspondences for pair: 84, 85. Took 4.062483 seconds\n",
            "Computed correspondences for pair: 84, 86. Took 3.995160 seconds\n",
            "Computed correspondences for pair: 84, 87. Took 5.396851 seconds\n",
            "Computed correspondences for pair: 84, 88. Took 3.588568 seconds\n",
            "Computed correspondences for pair: 84, 89. Took 3.554504 seconds\n",
            "Computed correspondences for pair: 84, 90. Took 6.013403 seconds\n",
            "Computed correspondences for pair: 84, 91. Took 5.209000 seconds\n",
            "Computed correspondences for pair: 84, 92. Took 4.777396 seconds\n",
            "Computed correspondences for pair: 84, 93. Took 5.859581 seconds\n",
            "Computed correspondences for pair: 84, 94. Took 4.257135 seconds\n",
            "Computed correspondences for pair: 84, 95. Took 4.411512 seconds\n",
            "Computed correspondences for pair: 84, 96. Took 4.506701 seconds\n",
            "Computed correspondences for pair: 84, 97. Took 4.328148 seconds\n",
            "Computed correspondences for pair: 84, 98. Took 4.618568 seconds\n",
            "Computed correspondences for pair: 84, 99. Took 4.580301 seconds\n",
            "Computed correspondences for pair: 85, 86. Took 3.553721 seconds\n",
            "Computed correspondences for pair: 85, 87. Took 4.970437 seconds\n",
            "Computed correspondences for pair: 85, 88. Took 3.284700 seconds\n",
            "Computed correspondences for pair: 85, 89. Took 3.318959 seconds\n",
            "Computed correspondences for pair: 85, 90. Took 6.212769 seconds\n",
            "Computed correspondences for pair: 85, 91. Took 6.043319 seconds\n",
            "Computed correspondences for pair: 85, 92. Took 4.231968 seconds\n",
            "Computed correspondences for pair: 85, 93. Took 6.986003 seconds\n",
            "Computed correspondences for pair: 85, 94. Took 4.392617 seconds\n",
            "Computed correspondences for pair: 85, 95. Took 4.359966 seconds\n",
            "Computed correspondences for pair: 85, 96. Took 4.436226 seconds\n",
            "Computed correspondences for pair: 85, 97. Took 4.392183 seconds\n",
            "Computed correspondences for pair: 85, 98. Took 4.331264 seconds\n",
            "Computed correspondences for pair: 85, 99. Took 4.520433 seconds\n",
            "Computed correspondences for pair: 86, 87. Took 5.713581 seconds\n",
            "Computed correspondences for pair: 86, 88. Took 3.465634 seconds\n",
            "Computed correspondences for pair: 86, 89. Took 3.578216 seconds\n",
            "Computed correspondences for pair: 86, 90. Took 6.407033 seconds\n",
            "Computed correspondences for pair: 86, 91. Took 4.457397 seconds\n",
            "Computed correspondences for pair: 86, 92. Took 4.874629 seconds\n",
            "Computed correspondences for pair: 86, 93. Took 5.577024 seconds\n",
            "Computed correspondences for pair: 86, 94. Took 4.530928 seconds\n",
            "Computed correspondences for pair: 86, 95. Took 4.630844 seconds\n",
            "Computed correspondences for pair: 86, 96. Took 4.246480 seconds\n",
            "Computed correspondences for pair: 86, 97. Took 4.477285 seconds\n",
            "Computed correspondences for pair: 86, 98. Took 4.405477 seconds\n",
            "Computed correspondences for pair: 86, 99. Took 5.527225 seconds\n",
            "Computed correspondences for pair: 87, 88. Took 4.132070 seconds\n",
            "Computed correspondences for pair: 87, 89. Took 3.579390 seconds\n",
            "Computed correspondences for pair: 87, 90. Took 6.172445 seconds\n",
            "Computed correspondences for pair: 87, 91. Took 4.583628 seconds\n",
            "Computed correspondences for pair: 87, 92. Took 4.479226 seconds\n",
            "Computed correspondences for pair: 87, 93. Took 6.770747 seconds\n",
            "Computed correspondences for pair: 87, 94. Took 4.518239 seconds\n",
            "Computed correspondences for pair: 87, 95. Took 4.801070 seconds\n",
            "Computed correspondences for pair: 87, 96. Took 4.679549 seconds\n",
            "Computed correspondences for pair: 87, 97. Took 4.261558 seconds\n",
            "Computed correspondences for pair: 87, 98. Took 4.576847 seconds\n",
            "Computed correspondences for pair: 87, 99. Took 5.082268 seconds\n",
            "Computed correspondences for pair: 88, 89. Took 4.021407 seconds\n",
            "Computed correspondences for pair: 88, 90. Took 5.949921 seconds\n",
            "Computed correspondences for pair: 88, 91. Took 4.388439 seconds\n",
            "Computed correspondences for pair: 88, 92. Took 4.391662 seconds\n",
            "Computed correspondences for pair: 88, 93. Took 6.162983 seconds\n",
            "Computed correspondences for pair: 88, 94. Took 4.367958 seconds\n",
            "Computed correspondences for pair: 88, 95. Took 4.537041 seconds\n",
            "Computed correspondences for pair: 88, 96. Took 4.529052 seconds\n",
            "Computed correspondences for pair: 88, 97. Took 4.426089 seconds\n",
            "Computed correspondences for pair: 88, 98. Took 4.440654 seconds\n",
            "Computed correspondences for pair: 88, 99. Took 5.308296 seconds\n",
            "Computed correspondences for pair: 89, 90. Took 5.751879 seconds\n",
            "Computed correspondences for pair: 89, 91. Took 4.580487 seconds\n",
            "Computed correspondences for pair: 89, 92. Took 4.498301 seconds\n",
            "Computed correspondences for pair: 89, 93. Took 6.629044 seconds\n",
            "Computed correspondences for pair: 89, 94. Took 4.507364 seconds\n",
            "Computed correspondences for pair: 89, 95. Took 4.701037 seconds\n",
            "Computed correspondences for pair: 89, 96. Took 4.642272 seconds\n",
            "Computed correspondences for pair: 89, 97. Took 4.326118 seconds\n",
            "Computed correspondences for pair: 89, 98. Took 4.613911 seconds\n",
            "Computed correspondences for pair: 89, 99. Took 4.826596 seconds\n",
            "Computed correspondences for pair: 90, 91. Took 3.546055 seconds\n",
            "Computed correspondences for pair: 90, 92. Took 3.468720 seconds\n",
            "Computed correspondences for pair: 90, 93. Took 4.491355 seconds\n",
            "Computed correspondences for pair: 90, 94. Took 2.614102 seconds\n",
            "Computed correspondences for pair: 90, 95. Took 2.556485 seconds\n",
            "Computed correspondences for pair: 90, 96. Took 3.391883 seconds\n",
            "Computed correspondences for pair: 90, 97. Took 3.828058 seconds\n",
            "Computed correspondences for pair: 90, 98. Took 3.134230 seconds\n",
            "Computed correspondences for pair: 90, 99. Took 4.429209 seconds\n",
            "Computed correspondences for pair: 91, 92. Took 4.408939 seconds\n",
            "Computed correspondences for pair: 91, 93. Took 3.249158 seconds\n",
            "Computed correspondences for pair: 91, 94. Took 3.478385 seconds\n",
            "Computed correspondences for pair: 91, 95. Took 4.030875 seconds\n",
            "Computed correspondences for pair: 91, 96. Took 4.092938 seconds\n",
            "Computed correspondences for pair: 91, 97. Took 4.543238 seconds\n",
            "Computed correspondences for pair: 91, 98. Took 3.294262 seconds\n",
            "Computed correspondences for pair: 91, 99. Took 3.826815 seconds\n",
            "Computed correspondences for pair: 92, 93. Took 3.773813 seconds\n",
            "Computed correspondences for pair: 92, 94. Took 3.884563 seconds\n",
            "Computed correspondences for pair: 92, 95. Took 3.481373 seconds\n",
            "Computed correspondences for pair: 92, 96. Took 3.523712 seconds\n",
            "Computed correspondences for pair: 92, 97. Took 3.869961 seconds\n",
            "Computed correspondences for pair: 92, 98. Took 3.378812 seconds\n",
            "Computed correspondences for pair: 92, 99. Took 3.700640 seconds\n",
            "Computed correspondences for pair: 93, 94. Took 3.704465 seconds\n",
            "Computed correspondences for pair: 93, 95. Took 4.215587 seconds\n",
            "Computed correspondences for pair: 93, 96. Took 3.182772 seconds\n",
            "Computed correspondences for pair: 93, 97. Took 3.915625 seconds\n",
            "Computed correspondences for pair: 93, 98. Took 3.367440 seconds\n",
            "Computed correspondences for pair: 93, 99. Took 3.883868 seconds\n",
            "Computed correspondences for pair: 94, 95. Took 3.109233 seconds\n",
            "Computed correspondences for pair: 94, 96. Took 3.870501 seconds\n",
            "Computed correspondences for pair: 94, 97. Took 3.990668 seconds\n",
            "Computed correspondences for pair: 94, 98. Took 3.550716 seconds\n",
            "Computed correspondences for pair: 94, 99. Took 6.590336 seconds\n",
            "Computed correspondences for pair: 95, 96. Took 5.709578 seconds\n",
            "Computed correspondences for pair: 95, 97. Took 7.106138 seconds\n",
            "Computed correspondences for pair: 95, 98. Took 5.816207 seconds\n",
            "Computed correspondences for pair: 95, 99. Took 3.862166 seconds\n",
            "Computed correspondences for pair: 96, 97. Took 3.925342 seconds\n",
            "Computed correspondences for pair: 96, 98. Took 3.578503 seconds\n",
            "Computed correspondences for pair: 96, 99. Took 3.866221 seconds\n",
            "Computed correspondences for pair: 97, 98. Took 6.203461 seconds\n",
            "Computed correspondences for pair: 97, 99. Took 5.624287 seconds\n",
            "Computed correspondences for pair: 98, 99. Took 4.199834 seconds\n",
            "--- 1257.5107955932617 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuFqzj7ANRa-",
        "colab_type": "code",
        "outputId": "17e00a76-a5c9-4c94-ab2e-441217e62f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!ls drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " DeepFunctionalMaps-master\n",
            " Or_Litany\n",
            " test_train_DFMnet_code_2019_5_9.ipynb\n",
            " unsupervisedfmnet\n",
            " Untitled0.ipynb\n",
            "'如何将Colaboratory与google drive关联.odt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZntTEXIEN3bA",
        "colab_type": "code",
        "outputId": "7997ce94-c881-4749-cda8-7e1fddd54d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84944
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/train_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_dir=drive/unsupervisedfmnet/Training/\n",
            "num_evecs=120\n",
            "building graph...\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/unsupervisedfmnet/DFMnet.py:168: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From drive/unsupervisedfmnet/train_DFMnet.py:222: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "starting session...\n",
            "2019-05-24 14:23:14.744999: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-24 14:23:14.745253: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x258f2e0 executing computations on platform Host. Devices:\n",
            "2019-05-24 14:23:14.745291: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 14:23:14.926935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 14:23:14.927556: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x258f020 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 14:23:14.927593: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 14:23:14.927970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 14:23:14.927999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 14:23:15.423521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 14:23:15.423592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 14:23:15.423606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 14:23:15.423941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "loading data to ram...\n",
            "starting training loop...\n",
            "2019-05-24 14:30:32.761538: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-24 14:30:33.285165: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x89279a0\n",
            "train - step 15001: loss = 1424.28 (3.705 sec)\n",
            "train - step 15002: loss = 2268.09 (0.340 sec)\n",
            "train - step 15003: loss = 1629.83 (0.302 sec)\n",
            "train - step 15004: loss = 1448.62 (0.265 sec)\n",
            "train - step 15005: loss = 1890.14 (0.266 sec)\n",
            "train - step 15006: loss = 1745.98 (0.266 sec)\n",
            "train - step 15007: loss = 2034.53 (0.259 sec)\n",
            "train - step 15008: loss = 1681.35 (0.259 sec)\n",
            "train - step 15009: loss = 1838.84 (0.268 sec)\n",
            "train - step 15010: loss = 1733.72 (0.263 sec)\n",
            "train - step 15011: loss = 1850.62 (0.261 sec)\n",
            "train - step 15012: loss = 1729.74 (0.259 sec)\n",
            "train - step 15013: loss = 2159.35 (0.267 sec)\n",
            "train - step 15014: loss = 2353.40 (0.262 sec)\n",
            "train - step 15015: loss = 1171.09 (0.264 sec)\n",
            "train - step 15016: loss = 1979.83 (0.268 sec)\n",
            "train - step 15017: loss = 1681.23 (0.264 sec)\n",
            "train - step 15018: loss = 1998.52 (0.260 sec)\n",
            "train - step 15019: loss = 1776.48 (0.266 sec)\n",
            "train - step 15020: loss = 2200.41 (0.264 sec)\n",
            "train - step 15021: loss = 2043.91 (0.270 sec)\n",
            "train - step 15022: loss = 1948.54 (0.263 sec)\n",
            "train - step 15023: loss = 2219.17 (0.263 sec)\n",
            "train - step 15024: loss = 1778.35 (0.261 sec)\n",
            "train - step 15025: loss = 1525.34 (0.263 sec)\n",
            "train - step 15026: loss = 1409.93 (0.259 sec)\n",
            "train - step 15027: loss = 1974.26 (0.262 sec)\n",
            "train - step 15028: loss = 1553.29 (0.266 sec)\n",
            "train - step 15029: loss = 1953.94 (0.272 sec)\n",
            "train - step 15030: loss = 2403.34 (0.271 sec)\n",
            "train - step 15031: loss = 1740.91 (0.273 sec)\n",
            "train - step 15032: loss = 2268.68 (0.269 sec)\n",
            "train - step 15033: loss = 1870.88 (0.267 sec)\n",
            "train - step 15034: loss = 2639.28 (0.256 sec)\n",
            "train - step 15035: loss = 2039.23 (0.271 sec)\n",
            "train - step 15036: loss = 1923.79 (0.269 sec)\n",
            "train - step 15037: loss = 1975.91 (0.273 sec)\n",
            "train - step 15038: loss = 2063.50 (0.263 sec)\n",
            "train - step 15039: loss = 1915.22 (0.270 sec)\n",
            "train - step 15040: loss = 2821.56 (0.271 sec)\n",
            "train - step 15041: loss = 2071.70 (0.268 sec)\n",
            "train - step 15042: loss = 1900.29 (0.269 sec)\n",
            "train - step 15043: loss = 1307.99 (0.274 sec)\n",
            "train - step 15044: loss = 2377.66 (0.271 sec)\n",
            "train - step 15045: loss = 2156.37 (0.266 sec)\n",
            "train - step 15046: loss = 2383.36 (0.265 sec)\n",
            "train - step 15047: loss = 2646.57 (0.268 sec)\n",
            "train - step 15048: loss = 2464.07 (0.260 sec)\n",
            "train - step 15049: loss = 2034.92 (0.264 sec)\n",
            "train - step 15050: loss = 2567.37 (0.264 sec)\n",
            "train - step 15051: loss = 2338.25 (0.261 sec)\n",
            "train - step 15052: loss = 2456.47 (0.261 sec)\n",
            "train - step 15053: loss = 2194.09 (0.264 sec)\n",
            "train - step 15054: loss = 2346.62 (0.270 sec)\n",
            "train - step 15055: loss = 2379.64 (0.278 sec)\n",
            "train - step 15056: loss = 3313.63 (0.254 sec)\n",
            "train - step 15057: loss = 2437.63 (0.268 sec)\n",
            "train - step 15058: loss = 2798.72 (0.264 sec)\n",
            "train - step 15059: loss = 2767.38 (0.272 sec)\n",
            "train - step 15060: loss = 2686.58 (0.266 sec)\n",
            "train - step 15061: loss = 3396.21 (0.269 sec)\n",
            "train - step 15062: loss = 2311.71 (0.262 sec)\n",
            "train - step 15063: loss = 3118.81 (0.275 sec)\n",
            "train - step 15064: loss = 2931.40 (0.269 sec)\n",
            "train - step 15065: loss = 2522.69 (0.270 sec)\n",
            "train - step 15066: loss = 3138.16 (0.265 sec)\n",
            "train - step 15067: loss = 2615.59 (0.276 sec)\n",
            "train - step 15068: loss = 2732.22 (0.261 sec)\n",
            "train - step 15069: loss = 2970.10 (0.267 sec)\n",
            "train - step 15070: loss = 3316.10 (0.261 sec)\n",
            "train - step 15071: loss = 2665.04 (0.266 sec)\n",
            "train - step 15072: loss = 2977.92 (0.255 sec)\n",
            "train - step 15073: loss = 3531.70 (0.270 sec)\n",
            "train - step 15074: loss = 2235.24 (0.264 sec)\n",
            "train - step 15075: loss = 3441.63 (0.267 sec)\n",
            "train - step 15076: loss = 2644.09 (0.265 sec)\n",
            "train - step 15077: loss = 2923.52 (0.271 sec)\n",
            "train - step 15078: loss = 2282.64 (0.261 sec)\n",
            "train - step 15079: loss = 3126.97 (0.265 sec)\n",
            "train - step 15080: loss = 2631.13 (0.266 sec)\n",
            "train - step 15081: loss = 3163.57 (0.267 sec)\n",
            "train - step 15082: loss = 2923.59 (0.269 sec)\n",
            "train - step 15083: loss = 3098.41 (0.277 sec)\n",
            "train - step 15084: loss = 3281.50 (0.274 sec)\n",
            "train - step 15085: loss = 2052.66 (0.272 sec)\n",
            "train - step 15086: loss = 3101.81 (0.278 sec)\n",
            "train - step 15087: loss = 4027.04 (0.270 sec)\n",
            "train - step 15088: loss = 3039.60 (0.262 sec)\n",
            "train - step 15089: loss = 3191.38 (0.281 sec)\n",
            "train - step 15090: loss = 3487.48 (0.272 sec)\n",
            "train - step 15091: loss = 2870.69 (0.266 sec)\n",
            "train - step 15092: loss = 1656.47 (0.262 sec)\n",
            "train - step 15093: loss = 3454.04 (0.275 sec)\n",
            "train - step 15094: loss = 3567.76 (0.264 sec)\n",
            "train - step 15095: loss = 3770.40 (0.266 sec)\n",
            "train - step 15096: loss = 3327.53 (0.271 sec)\n",
            "train - step 15097: loss = 4478.29 (0.270 sec)\n",
            "train - step 15098: loss = 3950.12 (0.265 sec)\n",
            "train - step 15099: loss = 3317.78 (0.267 sec)\n",
            "train - step 15100: loss = 3437.75 (0.268 sec)\n",
            "train - step 15101: loss = 2223.41 (0.269 sec)\n",
            "train - step 15102: loss = 2360.86 (0.275 sec)\n",
            "train - step 15103: loss = 3968.52 (0.264 sec)\n",
            "train - step 15104: loss = 3912.07 (0.260 sec)\n",
            "train - step 15105: loss = 3904.68 (0.279 sec)\n",
            "train - step 15106: loss = 4213.18 (0.265 sec)\n",
            "train - step 15107: loss = 3336.51 (0.268 sec)\n",
            "train - step 15108: loss = 3674.87 (0.263 sec)\n",
            "train - step 15109: loss = 3769.08 (0.271 sec)\n",
            "train - step 15110: loss = 3650.63 (0.262 sec)\n",
            "train - step 15111: loss = 3244.46 (0.262 sec)\n",
            "train - step 15112: loss = 5166.98 (0.267 sec)\n",
            "train - step 15113: loss = 3441.79 (0.276 sec)\n",
            "train - step 15114: loss = 3815.76 (0.270 sec)\n",
            "train - step 15115: loss = 4229.01 (0.268 sec)\n",
            "train - step 15116: loss = 3903.48 (0.260 sec)\n",
            "train - step 15117: loss = 4279.50 (0.266 sec)\n",
            "train - step 15118: loss = 3731.90 (0.264 sec)\n",
            "train - step 15119: loss = 4237.29 (0.268 sec)\n",
            "train - step 15120: loss = 3659.36 (0.263 sec)\n",
            "train - step 15121: loss = 2712.89 (0.269 sec)\n",
            "train - step 15122: loss = 4168.43 (0.259 sec)\n",
            "train - step 15123: loss = 3248.05 (0.270 sec)\n",
            "train - step 15124: loss = 3539.01 (0.265 sec)\n",
            "train - step 15125: loss = 3975.92 (0.289 sec)\n",
            "train - step 15126: loss = 3311.47 (0.265 sec)\n",
            "train - step 15127: loss = 3640.74 (0.270 sec)\n",
            "train - step 15128: loss = 4293.75 (0.259 sec)\n",
            "train - step 15129: loss = 3348.23 (0.273 sec)\n",
            "train - step 15130: loss = 4101.09 (0.269 sec)\n",
            "train - step 15131: loss = 4283.54 (0.268 sec)\n",
            "train - step 15132: loss = 3908.57 (0.268 sec)\n",
            "train - step 15133: loss = 4211.50 (0.265 sec)\n",
            "train - step 15134: loss = 3596.76 (0.264 sec)\n",
            "train - step 15135: loss = 3961.86 (0.270 sec)\n",
            "train - step 15136: loss = 3860.74 (0.269 sec)\n",
            "train - step 15137: loss = 4547.43 (0.259 sec)\n",
            "train - step 15138: loss = 3868.38 (0.264 sec)\n",
            "train - step 15139: loss = 4041.62 (0.265 sec)\n",
            "train - step 15140: loss = 3991.42 (0.272 sec)\n",
            "train - step 15141: loss = 3198.08 (0.267 sec)\n",
            "train - step 15142: loss = 3100.02 (0.261 sec)\n",
            "train - step 15143: loss = 2962.47 (0.269 sec)\n",
            "train - step 15144: loss = 3415.72 (0.949 sec)\n",
            "train - step 15145: loss = 3507.22 (0.268 sec)\n",
            "train - step 15146: loss = 4184.10 (0.256 sec)\n",
            "train - step 15147: loss = 4276.09 (0.267 sec)\n",
            "train - step 15148: loss = 3857.18 (0.265 sec)\n",
            "train - step 15149: loss = 3050.64 (0.272 sec)\n",
            "train - step 15150: loss = 4976.88 (0.267 sec)\n",
            "train - step 15151: loss = 3258.11 (0.267 sec)\n",
            "train - step 15152: loss = 4342.85 (0.269 sec)\n",
            "train - step 15153: loss = 3956.15 (0.276 sec)\n",
            "train - step 15154: loss = 4351.29 (0.274 sec)\n",
            "train - step 15155: loss = 3831.04 (0.271 sec)\n",
            "train - step 15156: loss = 4186.59 (0.281 sec)\n",
            "train - step 15157: loss = 2370.03 (0.267 sec)\n",
            "train - step 15158: loss = 3363.98 (0.274 sec)\n",
            "train - step 15159: loss = 4155.00 (0.274 sec)\n",
            "train - step 15160: loss = 3755.66 (0.287 sec)\n",
            "train - step 15161: loss = 5574.66 (0.269 sec)\n",
            "train - step 15162: loss = 5707.22 (0.283 sec)\n",
            "train - step 15163: loss = 3633.34 (0.275 sec)\n",
            "train - step 15164: loss = 4524.51 (0.292 sec)\n",
            "train - step 15165: loss = 4419.09 (0.274 sec)\n",
            "train - step 15166: loss = 4283.96 (0.274 sec)\n",
            "train - step 15167: loss = 3861.11 (0.273 sec)\n",
            "train - step 15168: loss = 3469.13 (0.278 sec)\n",
            "train - step 15169: loss = 572.36 (0.267 sec)\n",
            "train - step 15170: loss = 3243.18 (0.275 sec)\n",
            "train - step 15171: loss = 4645.78 (0.290 sec)\n",
            "train - step 15172: loss = 4215.78 (0.272 sec)\n",
            "train - step 15173: loss = 3711.36 (0.277 sec)\n",
            "train - step 15174: loss = 3201.00 (0.267 sec)\n",
            "train - step 15175: loss = 4364.49 (0.282 sec)\n",
            "train - step 15176: loss = 3993.04 (0.277 sec)\n",
            "train - step 15177: loss = 4649.14 (0.278 sec)\n",
            "train - step 15178: loss = 4282.75 (0.281 sec)\n",
            "train - step 15179: loss = 4544.08 (0.281 sec)\n",
            "train - step 15180: loss = 4375.67 (0.273 sec)\n",
            "train - step 15181: loss = 4054.17 (0.285 sec)\n",
            "train - step 15182: loss = 4151.23 (0.284 sec)\n",
            "train - step 15183: loss = 3264.56 (0.276 sec)\n",
            "train - step 15184: loss = 4799.60 (0.274 sec)\n",
            "train - step 15185: loss = 3924.74 (0.260 sec)\n",
            "train - step 15186: loss = 3944.00 (0.289 sec)\n",
            "train - step 15187: loss = 4360.15 (0.276 sec)\n",
            "train - step 15188: loss = 4204.08 (0.267 sec)\n",
            "train - step 15189: loss = 4221.92 (0.264 sec)\n",
            "train - step 15190: loss = 4497.34 (0.275 sec)\n",
            "train - step 15191: loss = 2942.48 (0.265 sec)\n",
            "train - step 15192: loss = 3112.93 (0.260 sec)\n",
            "train - step 15193: loss = 3453.86 (0.271 sec)\n",
            "train - step 15194: loss = 5041.43 (0.258 sec)\n",
            "train - step 15195: loss = 3522.16 (0.266 sec)\n",
            "train - step 15196: loss = 4246.86 (0.268 sec)\n",
            "train - step 15197: loss = 3718.50 (0.266 sec)\n",
            "train - step 15198: loss = 3596.39 (0.277 sec)\n",
            "train - step 15199: loss = 3999.02 (0.262 sec)\n",
            "train - step 15200: loss = 3291.03 (0.281 sec)\n",
            "train - step 15201: loss = 3371.91 (0.260 sec)\n",
            "train - step 15202: loss = 3404.74 (0.265 sec)\n",
            "train - step 15203: loss = 2629.02 (0.260 sec)\n",
            "train - step 15204: loss = 4663.06 (0.261 sec)\n",
            "train - step 15205: loss = 2791.39 (0.283 sec)\n",
            "train - step 15206: loss = 5109.68 (0.270 sec)\n",
            "train - step 15207: loss = 2903.93 (0.268 sec)\n",
            "train - step 15208: loss = 3537.59 (0.268 sec)\n",
            "train - step 15209: loss = 3357.06 (0.270 sec)\n",
            "train - step 15210: loss = 3888.97 (0.274 sec)\n",
            "train - step 15211: loss = 4070.15 (0.270 sec)\n",
            "train - step 15212: loss = 4070.23 (0.270 sec)\n",
            "train - step 15213: loss = 4056.42 (0.289 sec)\n",
            "train - step 15214: loss = 3288.53 (0.267 sec)\n",
            "train - step 15215: loss = 3303.99 (0.265 sec)\n",
            "train - step 15216: loss = 3798.08 (0.280 sec)\n",
            "train - step 15217: loss = 3358.84 (0.273 sec)\n",
            "train - step 15218: loss = 4036.99 (0.265 sec)\n",
            "train - step 15219: loss = 3331.62 (0.271 sec)\n",
            "train - step 15220: loss = 3167.01 (0.266 sec)\n",
            "train - step 15221: loss = 3411.80 (0.268 sec)\n",
            "train - step 15222: loss = 3471.68 (0.264 sec)\n",
            "train - step 15223: loss = 3194.23 (0.272 sec)\n",
            "train - step 15224: loss = 3110.65 (0.269 sec)\n",
            "train - step 15225: loss = 4109.71 (0.267 sec)\n",
            "train - step 15226: loss = 3332.40 (0.265 sec)\n",
            "train - step 15227: loss = 3617.34 (0.263 sec)\n",
            "train - step 15228: loss = 3718.41 (0.267 sec)\n",
            "train - step 15229: loss = 3316.11 (0.269 sec)\n",
            "train - step 15230: loss = 4191.86 (0.274 sec)\n",
            "train - step 15231: loss = 3626.10 (0.265 sec)\n",
            "train - step 15232: loss = 3917.26 (0.278 sec)\n",
            "train - step 15233: loss = 3657.48 (0.266 sec)\n",
            "train - step 15234: loss = 3724.02 (0.273 sec)\n",
            "train - step 15235: loss = 3335.05 (0.275 sec)\n",
            "train - step 15236: loss = 3351.04 (0.271 sec)\n",
            "train - step 15237: loss = 3389.88 (0.273 sec)\n",
            "train - step 15238: loss = 3549.84 (0.269 sec)\n",
            "train - step 15239: loss = 3212.75 (0.269 sec)\n",
            "train - step 15240: loss = 2277.11 (0.272 sec)\n",
            "train - step 15241: loss = 3184.34 (0.262 sec)\n",
            "train - step 15242: loss = 2887.74 (0.280 sec)\n",
            "train - step 15243: loss = 2792.97 (0.272 sec)\n",
            "train - step 15244: loss = 3355.26 (0.260 sec)\n",
            "train - step 15245: loss = 3077.55 (0.265 sec)\n",
            "train - step 15246: loss = 3041.34 (0.266 sec)\n",
            "train - step 15247: loss = 2868.60 (0.270 sec)\n",
            "train - step 15248: loss = 4054.79 (0.266 sec)\n",
            "train - step 15249: loss = 2727.01 (0.265 sec)\n",
            "train - step 15250: loss = 3585.34 (0.270 sec)\n",
            "train - step 15251: loss = 2961.09 (0.274 sec)\n",
            "train - step 15252: loss = 3681.41 (0.265 sec)\n",
            "train - step 15253: loss = 3735.06 (0.266 sec)\n",
            "train - step 15254: loss = 2923.75 (0.264 sec)\n",
            "train - step 15255: loss = 2751.44 (0.276 sec)\n",
            "train - step 15256: loss = 2859.21 (0.267 sec)\n",
            "train - step 15257: loss = 2789.85 (0.276 sec)\n",
            "train - step 15258: loss = 2800.60 (0.263 sec)\n",
            "train - step 15259: loss = 3406.20 (0.259 sec)\n",
            "train - step 15260: loss = 3992.99 (0.266 sec)\n",
            "train - step 15261: loss = 3482.66 (0.264 sec)\n",
            "train - step 15262: loss = 3148.49 (0.267 sec)\n",
            "train - step 15263: loss = 3133.06 (0.265 sec)\n",
            "train - step 15264: loss = 3241.70 (0.263 sec)\n",
            "train - step 15265: loss = 3437.91 (0.267 sec)\n",
            "train - step 15266: loss = 2933.32 (0.280 sec)\n",
            "train - step 15267: loss = 3194.60 (0.264 sec)\n",
            "train - step 15268: loss = 3218.70 (0.273 sec)\n",
            "train - step 15269: loss = 3635.14 (0.270 sec)\n",
            "train - step 15270: loss = 2642.77 (0.270 sec)\n",
            "train - step 15271: loss = 3317.59 (0.267 sec)\n",
            "train - step 15272: loss = 3228.05 (0.264 sec)\n",
            "train - step 15273: loss = 2619.30 (0.270 sec)\n",
            "train - step 15274: loss = 2484.41 (0.283 sec)\n",
            "train - step 15275: loss = 3864.15 (0.271 sec)\n",
            "train - step 15276: loss = 3035.37 (0.271 sec)\n",
            "train - step 15277: loss = 3570.42 (0.264 sec)\n",
            "train - step 15278: loss = 3641.41 (0.265 sec)\n",
            "train - step 15279: loss = 3019.55 (0.266 sec)\n",
            "train - step 15280: loss = 3225.80 (0.265 sec)\n",
            "train - step 15281: loss = 3539.96 (0.287 sec)\n",
            "train - step 15282: loss = 2334.61 (0.269 sec)\n",
            "train - step 15283: loss = 3083.07 (0.274 sec)\n",
            "train - step 15284: loss = 2972.50 (0.276 sec)\n",
            "train - step 15285: loss = 3084.47 (0.284 sec)\n",
            "train - step 15286: loss = 3418.76 (0.302 sec)\n",
            "train - step 15287: loss = 3551.83 (0.278 sec)\n",
            "train - step 15288: loss = 3282.96 (0.302 sec)\n",
            "train - step 15289: loss = 3884.61 (0.268 sec)\n",
            "train - step 15290: loss = 2688.55 (0.268 sec)\n",
            "train - step 15291: loss = 2386.34 (0.271 sec)\n",
            "train - step 15292: loss = 2262.11 (0.279 sec)\n",
            "train - step 15293: loss = 3041.34 (0.271 sec)\n",
            "train - step 15294: loss = 3662.43 (0.267 sec)\n",
            "train - step 15295: loss = 2927.39 (0.267 sec)\n",
            "train - step 15296: loss = 3121.37 (0.274 sec)\n",
            "train - step 15297: loss = 3427.46 (0.281 sec)\n",
            "train - step 15298: loss = 3733.32 (0.266 sec)\n",
            "train - step 15299: loss = 2414.49 (0.263 sec)\n",
            "train - step 15300: loss = 2865.06 (0.280 sec)\n",
            "train - step 15301: loss = 3413.93 (0.272 sec)\n",
            "train - step 15302: loss = 2968.98 (0.266 sec)\n",
            "train - step 15303: loss = 1979.25 (0.277 sec)\n",
            "train - step 15304: loss = 2694.51 (0.274 sec)\n",
            "train - step 15305: loss = 2964.31 (0.279 sec)\n",
            "train - step 15306: loss = 2909.02 (0.274 sec)\n",
            "train - step 15307: loss = 567.20 (0.265 sec)\n",
            "train - step 15308: loss = 3290.26 (0.272 sec)\n",
            "train - step 15309: loss = 2682.59 (0.271 sec)\n",
            "train - step 15310: loss = 2540.90 (0.264 sec)\n",
            "train - step 15311: loss = 696.62 (0.258 sec)\n",
            "train - step 15312: loss = 2981.06 (0.279 sec)\n",
            "train - step 15313: loss = 2872.97 (0.268 sec)\n",
            "train - step 15314: loss = 2697.29 (0.270 sec)\n",
            "train - step 15315: loss = 2510.82 (0.285 sec)\n",
            "train - step 15316: loss = 2673.24 (0.267 sec)\n",
            "train - step 15317: loss = 2506.09 (0.277 sec)\n",
            "train - step 15318: loss = 3482.80 (0.274 sec)\n",
            "train - step 15319: loss = 2219.15 (0.286 sec)\n",
            "train - step 15320: loss = 2147.71 (0.284 sec)\n",
            "train - step 15321: loss = 2272.37 (0.265 sec)\n",
            "train - step 15322: loss = 2486.99 (0.270 sec)\n",
            "train - step 15323: loss = 2681.24 (0.267 sec)\n",
            "train - step 15324: loss = 2835.21 (0.277 sec)\n",
            "train - step 15325: loss = 3299.10 (0.264 sec)\n",
            "train - step 15326: loss = 3102.11 (0.272 sec)\n",
            "train - step 15327: loss = 2837.85 (0.270 sec)\n",
            "train - step 15328: loss = 3164.12 (0.272 sec)\n",
            "train - step 15329: loss = 2418.12 (0.265 sec)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "train - step 15330: loss = 2768.80 (0.273 sec)\n",
            "train - step 15331: loss = 2445.26 (0.262 sec)\n",
            "train - step 15332: loss = 2243.04 (0.258 sec)\n",
            "train - step 15333: loss = 2184.49 (0.272 sec)\n",
            "train - step 15334: loss = 1951.90 (0.273 sec)\n",
            "train - step 15335: loss = 583.94 (0.271 sec)\n",
            "train - step 15336: loss = 2120.06 (0.283 sec)\n",
            "train - step 15337: loss = 2047.55 (0.265 sec)\n",
            "train - step 15338: loss = 3370.56 (0.273 sec)\n",
            "train - step 15339: loss = 2974.86 (0.264 sec)\n",
            "train - step 15340: loss = 2459.13 (0.264 sec)\n",
            "train - step 15341: loss = 2257.60 (0.270 sec)\n",
            "train - step 15342: loss = 3319.82 (0.276 sec)\n",
            "train - step 15343: loss = 2493.45 (0.264 sec)\n",
            "train - step 15344: loss = 2455.22 (0.268 sec)\n",
            "train - step 15345: loss = 2253.87 (0.272 sec)\n",
            "train - step 15346: loss = 2756.53 (0.260 sec)\n",
            "train - step 15347: loss = 2488.63 (0.272 sec)\n",
            "train - step 15348: loss = 2893.34 (0.545 sec)\n",
            "train - step 15349: loss = 2334.80 (0.278 sec)\n",
            "train - step 15350: loss = 2499.64 (0.262 sec)\n",
            "train - step 15351: loss = 2336.95 (0.272 sec)\n",
            "train - step 15352: loss = 2445.48 (0.285 sec)\n",
            "train - step 15353: loss = 2406.93 (0.269 sec)\n",
            "train - step 15354: loss = 2831.93 (0.270 sec)\n",
            "train - step 15355: loss = 1513.27 (0.272 sec)\n",
            "train - step 15356: loss = 2276.81 (0.270 sec)\n",
            "train - step 15357: loss = 2524.62 (0.273 sec)\n",
            "train - step 15358: loss = 3183.24 (0.279 sec)\n",
            "train - step 15359: loss = 2528.74 (0.270 sec)\n",
            "train - step 15360: loss = 2943.54 (0.274 sec)\n",
            "train - step 15361: loss = 2397.51 (0.266 sec)\n",
            "train - step 15362: loss = 2207.96 (0.262 sec)\n",
            "train - step 15363: loss = 2470.82 (1.061 sec)\n",
            "train - step 15364: loss = 2378.85 (0.263 sec)\n",
            "train - step 15365: loss = 2318.56 (0.271 sec)\n",
            "train - step 15366: loss = 2576.26 (0.269 sec)\n",
            "train - step 15367: loss = 2667.20 (0.262 sec)\n",
            "train - step 15368: loss = 2138.50 (0.268 sec)\n",
            "train - step 15369: loss = 2354.08 (0.277 sec)\n",
            "train - step 15370: loss = 2557.49 (0.270 sec)\n",
            "train - step 15371: loss = 2367.97 (0.268 sec)\n",
            "train - step 15372: loss = 2460.51 (0.269 sec)\n",
            "train - step 15373: loss = 2854.77 (0.273 sec)\n",
            "train - step 15374: loss = 2581.02 (0.273 sec)\n",
            "train - step 15375: loss = 2582.80 (0.266 sec)\n",
            "train - step 15376: loss = 2745.53 (0.275 sec)\n",
            "train - step 15377: loss = 2562.21 (0.260 sec)\n",
            "train - step 15378: loss = 2393.69 (0.265 sec)\n",
            "train - step 15379: loss = 2236.12 (0.265 sec)\n",
            "train - step 15380: loss = 2707.08 (0.276 sec)\n",
            "train - step 15381: loss = 2404.40 (0.269 sec)\n",
            "train - step 15382: loss = 2205.47 (0.275 sec)\n",
            "train - step 15383: loss = 2353.92 (0.261 sec)\n",
            "train - step 15384: loss = 3022.65 (0.257 sec)\n",
            "train - step 15385: loss = 2429.19 (0.270 sec)\n",
            "train - step 15386: loss = 2320.24 (0.269 sec)\n",
            "train - step 15387: loss = 2621.94 (0.273 sec)\n",
            "train - step 15388: loss = 2634.92 (0.258 sec)\n",
            "train - step 15389: loss = 1735.27 (0.269 sec)\n",
            "train - step 15390: loss = 2219.81 (0.266 sec)\n",
            "train - step 15391: loss = 1957.31 (0.275 sec)\n",
            "train - step 15392: loss = 2627.66 (0.269 sec)\n",
            "train - step 15393: loss = 2374.30 (0.272 sec)\n",
            "train - step 15394: loss = 1756.96 (0.275 sec)\n",
            "train - step 15395: loss = 2091.75 (0.272 sec)\n",
            "train - step 15396: loss = 2621.73 (0.270 sec)\n",
            "train - step 15397: loss = 2477.00 (0.261 sec)\n",
            "train - step 15398: loss = 2899.09 (0.261 sec)\n",
            "train - step 15399: loss = 2448.27 (0.269 sec)\n",
            "train - step 15400: loss = 2633.41 (0.267 sec)\n",
            "train - step 15401: loss = 2220.86 (0.278 sec)\n",
            "train - step 15402: loss = 2026.90 (0.261 sec)\n",
            "train - step 15403: loss = 2441.06 (0.274 sec)\n",
            "train - step 15404: loss = 2157.03 (0.272 sec)\n",
            "train - step 15405: loss = 2322.41 (0.267 sec)\n",
            "train - step 15406: loss = 2594.50 (0.277 sec)\n",
            "train - step 15407: loss = 1709.09 (0.272 sec)\n",
            "train - step 15408: loss = 1799.95 (0.288 sec)\n",
            "train - step 15409: loss = 1931.03 (0.278 sec)\n",
            "train - step 15410: loss = 2173.08 (0.281 sec)\n",
            "train - step 15411: loss = 2375.52 (0.277 sec)\n",
            "train - step 15412: loss = 2213.79 (0.287 sec)\n",
            "train - step 15413: loss = 3019.53 (0.275 sec)\n",
            "train - step 15414: loss = 2190.53 (0.280 sec)\n",
            "train - step 15415: loss = 2576.89 (0.277 sec)\n",
            "train - step 15416: loss = 2022.72 (0.279 sec)\n",
            "train - step 15417: loss = 2587.55 (0.271 sec)\n",
            "train - step 15418: loss = 2662.89 (0.274 sec)\n",
            "train - step 15419: loss = 1936.21 (0.280 sec)\n",
            "train - step 15420: loss = 2263.82 (0.278 sec)\n",
            "train - step 15421: loss = 2566.23 (0.272 sec)\n",
            "train - step 15422: loss = 1845.78 (0.275 sec)\n",
            "train - step 15423: loss = 2096.99 (0.282 sec)\n",
            "train - step 15424: loss = 1775.82 (0.276 sec)\n",
            "train - step 15425: loss = 736.09 (0.269 sec)\n",
            "train - step 15426: loss = 1950.61 (0.267 sec)\n",
            "train - step 15427: loss = 628.76 (0.278 sec)\n",
            "train - step 15428: loss = 2068.72 (0.274 sec)\n",
            "train - step 15429: loss = 1829.64 (0.287 sec)\n",
            "train - step 15430: loss = 2224.22 (0.273 sec)\n",
            "train - step 15431: loss = 2486.36 (0.280 sec)\n",
            "train - step 15432: loss = 2264.25 (0.280 sec)\n",
            "train - step 15433: loss = 2200.35 (0.290 sec)\n",
            "train - step 15434: loss = 2305.54 (0.271 sec)\n",
            "train - step 15435: loss = 2580.72 (0.284 sec)\n",
            "train - step 15436: loss = 2327.08 (0.280 sec)\n",
            "train - step 15437: loss = 3010.06 (0.267 sec)\n",
            "train - step 15438: loss = 2120.55 (0.277 sec)\n",
            "train - step 15439: loss = 1843.98 (0.286 sec)\n",
            "train - step 15440: loss = 2067.06 (0.283 sec)\n",
            "train - step 15441: loss = 2208.70 (0.281 sec)\n",
            "train - step 15442: loss = 2196.65 (0.283 sec)\n",
            "train - step 15443: loss = 1899.36 (0.270 sec)\n",
            "train - step 15444: loss = 2895.66 (0.270 sec)\n",
            "train - step 15445: loss = 2005.93 (0.273 sec)\n",
            "train - step 15446: loss = 2341.00 (0.269 sec)\n",
            "train - step 15447: loss = 2096.15 (0.270 sec)\n",
            "train - step 15448: loss = 2363.38 (0.277 sec)\n",
            "train - step 15449: loss = 2488.61 (0.269 sec)\n",
            "train - step 15450: loss = 2475.27 (0.283 sec)\n",
            "train - step 15451: loss = 2280.61 (0.278 sec)\n",
            "train - step 15452: loss = 2835.14 (0.270 sec)\n",
            "train - step 15453: loss = 2158.99 (0.275 sec)\n",
            "train - step 15454: loss = 2522.43 (0.273 sec)\n",
            "train - step 15455: loss = 1529.76 (0.269 sec)\n",
            "train - step 15456: loss = 1792.42 (0.264 sec)\n",
            "train - step 15457: loss = 2420.64 (0.283 sec)\n",
            "train - step 15458: loss = 2258.48 (0.271 sec)\n",
            "train - step 15459: loss = 2553.95 (0.285 sec)\n",
            "train - step 15460: loss = 2254.38 (0.272 sec)\n",
            "train - step 15461: loss = 2215.26 (0.268 sec)\n",
            "train - step 15462: loss = 2313.10 (0.278 sec)\n",
            "train - step 15463: loss = 2744.95 (0.269 sec)\n",
            "train - step 15464: loss = 2261.01 (0.273 sec)\n",
            "train - step 15465: loss = 2190.76 (0.271 sec)\n",
            "train - step 15466: loss = 2262.82 (0.281 sec)\n",
            "train - step 15467: loss = 2425.08 (0.274 sec)\n",
            "train - step 15468: loss = 2063.08 (0.274 sec)\n",
            "train - step 15469: loss = 2001.21 (0.278 sec)\n",
            "train - step 15470: loss = 2049.77 (0.277 sec)\n",
            "train - step 15471: loss = 2617.33 (0.286 sec)\n",
            "train - step 15472: loss = 1949.49 (0.278 sec)\n",
            "train - step 15473: loss = 2265.63 (0.273 sec)\n",
            "train - step 15474: loss = 2238.18 (0.268 sec)\n",
            "train - step 15475: loss = 2231.21 (0.272 sec)\n",
            "train - step 15476: loss = 1988.80 (0.272 sec)\n",
            "train - step 15477: loss = 2156.52 (0.277 sec)\n",
            "train - step 15478: loss = 2846.64 (0.275 sec)\n",
            "train - step 15479: loss = 2111.23 (0.273 sec)\n",
            "train - step 15480: loss = 2122.78 (0.277 sec)\n",
            "train - step 15481: loss = 2290.95 (0.273 sec)\n",
            "train - step 15482: loss = 2168.36 (0.270 sec)\n",
            "train - step 15483: loss = 1760.29 (0.271 sec)\n",
            "train - step 15484: loss = 2134.22 (0.267 sec)\n",
            "train - step 15485: loss = 2072.74 (0.275 sec)\n",
            "train - step 15486: loss = 567.10 (0.253 sec)\n",
            "train - step 15487: loss = 2069.67 (0.264 sec)\n",
            "train - step 15488: loss = 1759.39 (0.270 sec)\n",
            "train - step 15489: loss = 2006.99 (0.264 sec)\n",
            "train - step 15490: loss = 2307.66 (0.276 sec)\n",
            "train - step 15491: loss = 2074.51 (0.262 sec)\n",
            "train - step 15492: loss = 2107.79 (0.267 sec)\n",
            "train - step 15493: loss = 2478.15 (0.273 sec)\n",
            "train - step 15494: loss = 2004.20 (0.265 sec)\n",
            "train - step 15495: loss = 2213.95 (0.261 sec)\n",
            "train - step 15496: loss = 2167.92 (0.266 sec)\n",
            "train - step 15497: loss = 2373.10 (0.267 sec)\n",
            "train - step 15498: loss = 2907.13 (0.265 sec)\n",
            "train - step 15499: loss = 2038.68 (0.264 sec)\n",
            "train - step 15500: loss = 2482.93 (0.263 sec)\n",
            "train - step 15501: loss = 2225.20 (0.265 sec)\n",
            "train - step 15502: loss = 2183.39 (0.269 sec)\n",
            "train - step 15503: loss = 2442.71 (0.274 sec)\n",
            "train - step 15504: loss = 1712.94 (0.264 sec)\n",
            "train - step 15505: loss = 2032.05 (0.267 sec)\n",
            "train - step 15506: loss = 2157.82 (0.273 sec)\n",
            "train - step 15507: loss = 1969.78 (0.273 sec)\n",
            "train - step 15508: loss = 2427.64 (0.269 sec)\n",
            "train - step 15509: loss = 2321.56 (0.267 sec)\n",
            "train - step 15510: loss = 2121.24 (0.271 sec)\n",
            "train - step 15511: loss = 1709.21 (0.275 sec)\n",
            "train - step 15512: loss = 2501.69 (0.267 sec)\n",
            "train - step 15513: loss = 2042.81 (0.271 sec)\n",
            "train - step 15514: loss = 2634.85 (0.257 sec)\n",
            "train - step 15515: loss = 2853.50 (0.265 sec)\n",
            "train - step 15516: loss = 2435.22 (0.276 sec)\n",
            "train - step 15517: loss = 1920.13 (0.263 sec)\n",
            "train - step 15518: loss = 2455.93 (0.269 sec)\n",
            "train - step 15519: loss = 2547.62 (0.265 sec)\n",
            "train - step 15520: loss = 2123.13 (0.271 sec)\n",
            "train - step 15521: loss = 1894.28 (0.277 sec)\n",
            "train - step 15522: loss = 2445.31 (0.261 sec)\n",
            "train - step 15523: loss = 2175.09 (0.257 sec)\n",
            "train - step 15524: loss = 2231.54 (0.264 sec)\n",
            "train - step 15525: loss = 2447.98 (0.263 sec)\n",
            "train - step 15526: loss = 2065.84 (0.271 sec)\n",
            "train - step 15527: loss = 2281.55 (0.273 sec)\n",
            "train - step 15528: loss = 2429.85 (0.272 sec)\n",
            "train - step 15529: loss = 2155.59 (0.261 sec)\n",
            "train - step 15530: loss = 2575.27 (0.258 sec)\n",
            "train - step 15531: loss = 2111.19 (0.264 sec)\n",
            "train - step 15532: loss = 2118.69 (0.269 sec)\n",
            "train - step 15533: loss = 1889.51 (0.268 sec)\n",
            "train - step 15534: loss = 1985.86 (0.262 sec)\n",
            "train - step 15535: loss = 1902.97 (0.272 sec)\n",
            "train - step 15536: loss = 1757.29 (0.269 sec)\n",
            "train - step 15537: loss = 2167.63 (0.266 sec)\n",
            "train - step 15538: loss = 2257.74 (0.269 sec)\n",
            "train - step 15539: loss = 2793.31 (0.277 sec)\n",
            "train - step 15540: loss = 2182.88 (0.262 sec)\n",
            "train - step 15541: loss = 2020.32 (0.258 sec)\n",
            "train - step 15542: loss = 2281.34 (0.264 sec)\n",
            "train - step 15543: loss = 2159.03 (0.267 sec)\n",
            "train - step 15544: loss = 2159.83 (0.270 sec)\n",
            "train - step 15545: loss = 1875.33 (0.268 sec)\n",
            "train - step 15546: loss = 2063.15 (0.277 sec)\n",
            "train - step 15547: loss = 2268.00 (0.269 sec)\n",
            "train - step 15548: loss = 2392.25 (0.263 sec)\n",
            "train - step 15549: loss = 2029.82 (0.265 sec)\n",
            "train - step 15550: loss = 1755.10 (0.268 sec)\n",
            "train - step 15551: loss = 2297.92 (0.271 sec)\n",
            "train - step 15552: loss = 2355.23 (0.266 sec)\n",
            "train - step 15553: loss = 2787.40 (0.268 sec)\n",
            "train - step 15554: loss = 1342.33 (0.273 sec)\n",
            "train - step 15555: loss = 1767.74 (0.279 sec)\n",
            "train - step 15556: loss = 2380.27 (0.263 sec)\n",
            "train - step 15557: loss = 2346.09 (0.263 sec)\n",
            "train - step 15558: loss = 1894.31 (0.269 sec)\n",
            "train - step 15559: loss = 1876.92 (0.271 sec)\n",
            "train - step 15560: loss = 1932.20 (0.271 sec)\n",
            "train - step 15561: loss = 2500.59 (0.269 sec)\n",
            "train - step 15562: loss = 2360.16 (0.272 sec)\n",
            "train - step 15563: loss = 2027.74 (0.271 sec)\n",
            "train - step 15564: loss = 1824.74 (0.281 sec)\n",
            "train - step 15565: loss = 2356.24 (0.263 sec)\n",
            "train - step 15566: loss = 2294.54 (0.275 sec)\n",
            "train - step 15567: loss = 2486.26 (0.266 sec)\n",
            "train - step 15568: loss = 597.66 (0.259 sec)\n",
            "train - step 15569: loss = 1628.83 (0.265 sec)\n",
            "train - step 15570: loss = 2020.38 (0.263 sec)\n",
            "train - step 15571: loss = 2381.44 (0.269 sec)\n",
            "train - step 15572: loss = 2236.96 (0.268 sec)\n",
            "train - step 15573: loss = 2058.73 (0.276 sec)\n",
            "train - step 15574: loss = 1960.20 (0.269 sec)\n",
            "train - step 15575: loss = 1896.54 (0.268 sec)\n",
            "train - step 15576: loss = 2443.67 (0.268 sec)\n",
            "train - step 15577: loss = 1876.38 (0.275 sec)\n",
            "train - step 15578: loss = 1728.72 (0.273 sec)\n",
            "train - step 15579: loss = 2077.97 (0.268 sec)\n",
            "train - step 15580: loss = 1722.70 (0.268 sec)\n",
            "train - step 15581: loss = 1720.39 (0.274 sec)\n",
            "train - step 15582: loss = 1965.77 (0.262 sec)\n",
            "train - step 15583: loss = 1539.98 (1.010 sec)\n",
            "train - step 15584: loss = 1890.65 (0.265 sec)\n",
            "train - step 15585: loss = 2364.80 (0.265 sec)\n",
            "train - step 15586: loss = 2406.47 (0.276 sec)\n",
            "train - step 15587: loss = 2034.17 (0.282 sec)\n",
            "train - step 15588: loss = 2176.69 (0.265 sec)\n",
            "train - step 15589: loss = 2076.08 (0.272 sec)\n",
            "train - step 15590: loss = 2061.46 (0.263 sec)\n",
            "train - step 15591: loss = 1901.37 (0.268 sec)\n",
            "train - step 15592: loss = 2140.17 (0.275 sec)\n",
            "train - step 15593: loss = 2151.93 (0.267 sec)\n",
            "train - step 15594: loss = 2538.81 (0.270 sec)\n",
            "train - step 15595: loss = 1605.16 (0.259 sec)\n",
            "train - step 15596: loss = 2178.47 (0.270 sec)\n",
            "train - step 15597: loss = 1608.13 (0.272 sec)\n",
            "train - step 15598: loss = 1617.01 (0.261 sec)\n",
            "train - step 15599: loss = 2169.29 (0.265 sec)\n",
            "train - step 15600: loss = 1958.53 (0.273 sec)\n",
            "train - step 15601: loss = 1887.70 (0.269 sec)\n",
            "train - step 15602: loss = 2200.33 (0.261 sec)\n",
            "train - step 15603: loss = 2124.71 (0.274 sec)\n",
            "train - step 15604: loss = 1886.64 (0.263 sec)\n",
            "train - step 15605: loss = 2175.58 (0.270 sec)\n",
            "train - step 15606: loss = 1780.75 (0.268 sec)\n",
            "train - step 15607: loss = 2375.19 (0.271 sec)\n",
            "train - step 15608: loss = 2310.75 (0.266 sec)\n",
            "train - step 15609: loss = 2100.13 (0.281 sec)\n",
            "train - step 15610: loss = 1807.80 (0.271 sec)\n",
            "train - step 15611: loss = 1912.38 (0.260 sec)\n",
            "train - step 15612: loss = 2183.09 (0.259 sec)\n",
            "train - step 15613: loss = 1795.22 (0.264 sec)\n",
            "train - step 15614: loss = 2341.15 (0.264 sec)\n",
            "train - step 15615: loss = 1520.31 (0.272 sec)\n",
            "train - step 15616: loss = 2145.73 (0.269 sec)\n",
            "train - step 15617: loss = 1970.24 (0.264 sec)\n",
            "train - step 15618: loss = 2522.44 (0.258 sec)\n",
            "train - step 15619: loss = 1906.02 (0.263 sec)\n",
            "train - step 15620: loss = 2625.20 (0.263 sec)\n",
            "train - step 15621: loss = 2024.64 (0.270 sec)\n",
            "train - step 15622: loss = 1679.32 (0.271 sec)\n",
            "train - step 15623: loss = 2092.02 (0.264 sec)\n",
            "train - step 15624: loss = 1718.17 (0.271 sec)\n",
            "train - step 15625: loss = 2360.55 (0.264 sec)\n",
            "train - step 15626: loss = 2237.80 (0.270 sec)\n",
            "train - step 15627: loss = 1959.62 (0.271 sec)\n",
            "train - step 15628: loss = 1795.58 (0.268 sec)\n",
            "train - step 15629: loss = 2377.93 (0.258 sec)\n",
            "train - step 15630: loss = 2174.98 (0.273 sec)\n",
            "train - step 15631: loss = 1844.58 (0.268 sec)\n",
            "train - step 15632: loss = 1757.23 (0.270 sec)\n",
            "train - step 15633: loss = 2366.34 (0.266 sec)\n",
            "train - step 15634: loss = 1796.61 (0.264 sec)\n",
            "train - step 15635: loss = 1900.78 (0.266 sec)\n",
            "train - step 15636: loss = 1814.14 (0.272 sec)\n",
            "train - step 15637: loss = 2392.05 (0.271 sec)\n",
            "train - step 15638: loss = 1954.50 (0.276 sec)\n",
            "train - step 15639: loss = 1734.69 (0.276 sec)\n",
            "train - step 15640: loss = 1911.55 (0.263 sec)\n",
            "train - step 15641: loss = 2001.21 (0.274 sec)\n",
            "train - step 15642: loss = 1953.80 (0.267 sec)\n",
            "train - step 15643: loss = 1916.12 (0.271 sec)\n",
            "train - step 15644: loss = 1763.76 (0.271 sec)\n",
            "train - step 15645: loss = 1974.34 (0.275 sec)\n",
            "train - step 15646: loss = 1505.27 (0.269 sec)\n",
            "train - step 15647: loss = 2132.01 (0.272 sec)\n",
            "train - step 15648: loss = 2191.55 (0.269 sec)\n",
            "train - step 15649: loss = 1744.60 (0.270 sec)\n",
            "train - step 15650: loss = 1997.72 (0.265 sec)\n",
            "train - step 15651: loss = 1929.76 (0.269 sec)\n",
            "train - step 15652: loss = 2372.86 (0.260 sec)\n",
            "train - step 15653: loss = 2309.39 (0.263 sec)\n",
            "train - step 15654: loss = 1374.74 (0.265 sec)\n",
            "train - step 15655: loss = 2213.53 (0.265 sec)\n",
            "train - step 15656: loss = 1887.71 (0.269 sec)\n",
            "train - step 15657: loss = 2254.90 (0.268 sec)\n",
            "train - step 15658: loss = 2278.63 (0.265 sec)\n",
            "train - step 15659: loss = 1939.41 (0.273 sec)\n",
            "train - step 15660: loss = 1896.52 (0.264 sec)\n",
            "train - step 15661: loss = 1922.22 (0.270 sec)\n",
            "train - step 15662: loss = 2715.18 (0.263 sec)\n",
            "train - step 15663: loss = 1818.01 (0.268 sec)\n",
            "train - step 15664: loss = 2052.65 (0.270 sec)\n",
            "train - step 15665: loss = 1951.37 (0.266 sec)\n",
            "train - step 15666: loss = 1925.95 (0.265 sec)\n",
            "train - step 15667: loss = 1917.07 (0.272 sec)\n",
            "train - step 15668: loss = 1772.53 (0.266 sec)\n",
            "train - step 15669: loss = 1945.00 (0.263 sec)\n",
            "train - step 15670: loss = 1991.72 (0.283 sec)\n",
            "train - step 15671: loss = 1955.57 (0.262 sec)\n",
            "train - step 15672: loss = 2323.68 (0.260 sec)\n",
            "train - step 15673: loss = 1907.77 (0.270 sec)\n",
            "train - step 15674: loss = 2514.23 (0.272 sec)\n",
            "train - step 15675: loss = 2353.23 (0.260 sec)\n",
            "train - step 15676: loss = 2514.11 (0.258 sec)\n",
            "train - step 15677: loss = 2304.49 (0.269 sec)\n",
            "train - step 15678: loss = 2034.82 (0.268 sec)\n",
            "train - step 15679: loss = 2185.99 (0.274 sec)\n",
            "train - step 15680: loss = 2209.44 (0.261 sec)\n",
            "train - step 15681: loss = 2356.85 (0.270 sec)\n",
            "train - step 15682: loss = 1961.01 (0.272 sec)\n",
            "train - step 15683: loss = 1709.56 (0.271 sec)\n",
            "train - step 15684: loss = 2079.96 (0.269 sec)\n",
            "train - step 15685: loss = 2202.74 (0.267 sec)\n",
            "train - step 15686: loss = 1564.91 (0.268 sec)\n",
            "train - step 15687: loss = 1369.24 (0.261 sec)\n",
            "train - step 15688: loss = 2290.36 (0.263 sec)\n",
            "train - step 15689: loss = 2377.58 (0.264 sec)\n",
            "train - step 15690: loss = 2107.88 (0.288 sec)\n",
            "train - step 15691: loss = 2157.73 (0.262 sec)\n",
            "train - step 15692: loss = 1819.56 (0.263 sec)\n",
            "train - step 15693: loss = 1980.05 (0.278 sec)\n",
            "train - step 15694: loss = 2662.49 (0.263 sec)\n",
            "train - step 15695: loss = 2155.54 (0.261 sec)\n",
            "train - step 15696: loss = 2261.49 (0.268 sec)\n",
            "train - step 15697: loss = 2116.41 (0.270 sec)\n",
            "train - step 15698: loss = 1982.17 (0.272 sec)\n",
            "train - step 15699: loss = 1719.85 (0.267 sec)\n",
            "train - step 15700: loss = 2394.13 (0.265 sec)\n",
            "train - step 15701: loss = 2055.50 (0.275 sec)\n",
            "train - step 15702: loss = 2121.60 (0.273 sec)\n",
            "train - step 15703: loss = 2206.55 (0.267 sec)\n",
            "train - step 15704: loss = 2458.44 (0.270 sec)\n",
            "train - step 15705: loss = 2122.42 (0.268 sec)\n",
            "train - step 15706: loss = 2288.90 (0.284 sec)\n",
            "train - step 15707: loss = 2194.71 (0.262 sec)\n",
            "train - step 15708: loss = 1876.58 (0.269 sec)\n",
            "train - step 15709: loss = 1880.73 (0.276 sec)\n",
            "train - step 15710: loss = 1944.88 (0.265 sec)\n",
            "train - step 15711: loss = 1650.53 (0.278 sec)\n",
            "train - step 15712: loss = 1800.55 (0.277 sec)\n",
            "train - step 15713: loss = 1903.04 (0.270 sec)\n",
            "train - step 15714: loss = 2043.61 (0.272 sec)\n",
            "train - step 15715: loss = 2141.19 (0.264 sec)\n",
            "train - step 15716: loss = 1245.56 (0.275 sec)\n",
            "train - step 15717: loss = 2091.66 (0.269 sec)\n",
            "train - step 15718: loss = 2061.41 (0.269 sec)\n",
            "train - step 15719: loss = 621.26 (0.258 sec)\n",
            "train - step 15720: loss = 2599.07 (0.272 sec)\n",
            "train - step 15721: loss = 2635.38 (0.264 sec)\n",
            "train - step 15722: loss = 1789.66 (0.263 sec)\n",
            "train - step 15723: loss = 2202.64 (0.272 sec)\n",
            "train - step 15724: loss = 1790.05 (0.277 sec)\n",
            "train - step 15725: loss = 2666.68 (0.266 sec)\n",
            "train - step 15726: loss = 1701.03 (0.266 sec)\n",
            "train - step 15727: loss = 1917.06 (0.263 sec)\n",
            "train - step 15728: loss = 2006.11 (0.269 sec)\n",
            "train - step 15729: loss = 1755.49 (0.269 sec)\n",
            "train - step 15730: loss = 2063.30 (0.274 sec)\n",
            "train - step 15731: loss = 1390.62 (0.269 sec)\n",
            "train - step 15732: loss = 2230.16 (0.262 sec)\n",
            "train - step 15733: loss = 1937.44 (0.267 sec)\n",
            "train - step 15734: loss = 1969.84 (0.269 sec)\n",
            "train - step 15735: loss = 1689.13 (0.264 sec)\n",
            "train - step 15736: loss = 1838.39 (0.264 sec)\n",
            "train - step 15737: loss = 2325.32 (0.265 sec)\n",
            "train - step 15738: loss = 1591.01 (0.266 sec)\n",
            "train - step 15739: loss = 2068.71 (0.275 sec)\n",
            "train - step 15740: loss = 2105.66 (0.271 sec)\n",
            "train - step 15741: loss = 2814.19 (0.264 sec)\n",
            "train - step 15742: loss = 2274.24 (0.275 sec)\n",
            "train - step 15743: loss = 1760.02 (0.276 sec)\n",
            "train - step 15744: loss = 1927.98 (0.277 sec)\n",
            "train - step 15745: loss = 1996.48 (0.289 sec)\n",
            "train - step 15746: loss = 2267.63 (0.277 sec)\n",
            "train - step 15747: loss = 2178.84 (0.276 sec)\n",
            "train - step 15748: loss = 2664.39 (0.275 sec)\n",
            "train - step 15749: loss = 1978.43 (0.273 sec)\n",
            "train - step 15750: loss = 2538.53 (0.283 sec)\n",
            "train - step 15751: loss = 1490.33 (0.265 sec)\n",
            "train - step 15752: loss = 1863.98 (0.269 sec)\n",
            "train - step 15753: loss = 1481.77 (0.275 sec)\n",
            "train - step 15754: loss = 2311.02 (0.280 sec)\n",
            "train - step 15755: loss = 1606.39 (0.281 sec)\n",
            "train - step 15756: loss = 1442.24 (0.277 sec)\n",
            "train - step 15757: loss = 2088.82 (0.269 sec)\n",
            "train - step 15758: loss = 2193.79 (0.283 sec)\n",
            "train - step 15759: loss = 2442.59 (0.267 sec)\n",
            "train - step 15760: loss = 2385.09 (0.283 sec)\n",
            "train - step 15761: loss = 1938.01 (0.276 sec)\n",
            "train - step 15762: loss = 1824.30 (0.271 sec)\n",
            "train - step 15763: loss = 2150.70 (0.277 sec)\n",
            "train - step 15764: loss = 1561.75 (0.278 sec)\n",
            "train - step 15765: loss = 1590.77 (0.279 sec)\n",
            "train - step 15766: loss = 2024.08 (0.282 sec)\n",
            "train - step 15767: loss = 1536.12 (0.270 sec)\n",
            "train - step 15768: loss = 1927.03 (0.283 sec)\n",
            "train - step 15769: loss = 1778.57 (0.272 sec)\n",
            "train - step 15770: loss = 2003.83 (0.271 sec)\n",
            "train - step 15771: loss = 1894.42 (0.275 sec)\n",
            "train - step 15772: loss = 1354.87 (0.277 sec)\n",
            "train - step 15773: loss = 1978.82 (0.277 sec)\n",
            "train - step 15774: loss = 1438.47 (0.268 sec)\n",
            "train - step 15775: loss = 2180.04 (0.274 sec)\n",
            "train - step 15776: loss = 2230.88 (0.276 sec)\n",
            "train - step 15777: loss = 2322.73 (0.270 sec)\n",
            "train - step 15778: loss = 1968.66 (0.271 sec)\n",
            "train - step 15779: loss = 2160.33 (0.271 sec)\n",
            "train - step 15780: loss = 1700.25 (0.272 sec)\n",
            "train - step 15781: loss = 1838.10 (0.263 sec)\n",
            "train - step 15782: loss = 2054.68 (0.262 sec)\n",
            "train - step 15783: loss = 1709.38 (0.270 sec)\n",
            "train - step 15784: loss = 2032.47 (0.268 sec)\n",
            "train - step 15785: loss = 1878.22 (0.266 sec)\n",
            "train - step 15786: loss = 1813.05 (0.269 sec)\n",
            "train - step 15787: loss = 1632.68 (0.269 sec)\n",
            "train - step 15788: loss = 2067.35 (0.266 sec)\n",
            "train - step 15789: loss = 2023.33 (0.271 sec)\n",
            "train - step 15790: loss = 1848.81 (0.268 sec)\n",
            "train - step 15791: loss = 2157.35 (0.271 sec)\n",
            "train - step 15792: loss = 2388.73 (0.268 sec)\n",
            "train - step 15793: loss = 2217.69 (0.267 sec)\n",
            "train - step 15794: loss = 640.58 (0.261 sec)\n",
            "train - step 15795: loss = 1761.04 (0.268 sec)\n",
            "train - step 15796: loss = 2414.42 (0.270 sec)\n",
            "train - step 15797: loss = 1719.27 (0.266 sec)\n",
            "train - step 15798: loss = 1777.07 (0.263 sec)\n",
            "train - step 15799: loss = 2423.69 (0.265 sec)\n",
            "train - step 15800: loss = 2008.01 (0.265 sec)\n",
            "train - step 15801: loss = 1975.49 (0.270 sec)\n",
            "train - step 15802: loss = 1255.83 (0.264 sec)\n",
            "train - step 15803: loss = 2111.91 (0.277 sec)\n",
            "train - step 15804: loss = 1882.06 (0.900 sec)\n",
            "train - step 15805: loss = 2136.19 (0.268 sec)\n",
            "train - step 15806: loss = 1704.71 (0.271 sec)\n",
            "train - step 15807: loss = 2046.47 (0.261 sec)\n",
            "train - step 15808: loss = 1952.65 (0.272 sec)\n",
            "train - step 15809: loss = 1786.84 (0.267 sec)\n",
            "train - step 15810: loss = 2268.39 (0.272 sec)\n",
            "train - step 15811: loss = 1857.22 (0.267 sec)\n",
            "train - step 15812: loss = 2262.70 (0.272 sec)\n",
            "train - step 15813: loss = 1537.45 (0.263 sec)\n",
            "train - step 15814: loss = 1984.67 (0.263 sec)\n",
            "train - step 15815: loss = 2436.26 (0.259 sec)\n",
            "train - step 15816: loss = 2083.10 (0.268 sec)\n",
            "train - step 15817: loss = 2009.65 (0.268 sec)\n",
            "train - step 15818: loss = 2082.82 (0.274 sec)\n",
            "train - step 15819: loss = 2116.50 (0.262 sec)\n",
            "train - step 15820: loss = 2203.55 (0.274 sec)\n",
            "train - step 15821: loss = 2023.22 (0.279 sec)\n",
            "train - step 15822: loss = 1381.19 (0.265 sec)\n",
            "train - step 15823: loss = 1687.46 (0.267 sec)\n",
            "train - step 15824: loss = 2005.11 (0.273 sec)\n",
            "train - step 15825: loss = 2037.79 (0.265 sec)\n",
            "train - step 15826: loss = 1712.44 (0.270 sec)\n",
            "train - step 15827: loss = 2222.35 (0.266 sec)\n",
            "train - step 15828: loss = 1990.25 (0.271 sec)\n",
            "train - step 15829: loss = 1677.20 (0.269 sec)\n",
            "train - step 15830: loss = 2072.78 (0.270 sec)\n",
            "train - step 15831: loss = 1731.93 (0.268 sec)\n",
            "train - step 15832: loss = 1564.82 (0.263 sec)\n",
            "train - step 15833: loss = 1424.00 (0.276 sec)\n",
            "train - step 15834: loss = 2219.59 (0.267 sec)\n",
            "train - step 15835: loss = 2126.32 (0.280 sec)\n",
            "train - step 15836: loss = 2438.84 (0.267 sec)\n",
            "train - step 15837: loss = 1886.31 (0.265 sec)\n",
            "train - step 15838: loss = 2263.52 (0.269 sec)\n",
            "train - step 15839: loss = 2307.03 (0.278 sec)\n",
            "train - step 15840: loss = 1903.41 (0.269 sec)\n",
            "train - step 15841: loss = 2434.65 (0.257 sec)\n",
            "train - step 15842: loss = 1888.68 (0.273 sec)\n",
            "train - step 15843: loss = 1665.41 (0.276 sec)\n",
            "train - step 15844: loss = 1722.18 (0.264 sec)\n",
            "train - step 15845: loss = 1478.55 (0.274 sec)\n",
            "train - step 15846: loss = 1534.91 (0.270 sec)\n",
            "train - step 15847: loss = 1481.74 (0.267 sec)\n",
            "train - step 15848: loss = 2405.90 (0.273 sec)\n",
            "train - step 15849: loss = 2092.69 (0.273 sec)\n",
            "train - step 15850: loss = 1790.21 (0.268 sec)\n",
            "train - step 15851: loss = 2342.12 (0.262 sec)\n",
            "train - step 15852: loss = 2188.79 (0.260 sec)\n",
            "train - step 15853: loss = 2339.08 (0.268 sec)\n",
            "train - step 15854: loss = 1510.01 (0.264 sec)\n",
            "train - step 15855: loss = 1398.63 (0.258 sec)\n",
            "train - step 15856: loss = 2227.52 (0.262 sec)\n",
            "train - step 15857: loss = 2039.98 (0.265 sec)\n",
            "train - step 15858: loss = 2090.98 (0.281 sec)\n",
            "train - step 15859: loss = 2294.64 (0.266 sec)\n",
            "train - step 15860: loss = 2166.48 (0.271 sec)\n",
            "train - step 15861: loss = 2511.88 (0.265 sec)\n",
            "train - step 15862: loss = 1899.35 (0.278 sec)\n",
            "train - step 15863: loss = 2344.69 (0.266 sec)\n",
            "train - step 15864: loss = 2055.43 (0.260 sec)\n",
            "train - step 15865: loss = 1690.64 (0.269 sec)\n",
            "train - step 15866: loss = 1989.42 (0.269 sec)\n",
            "train - step 15867: loss = 2736.69 (0.266 sec)\n",
            "train - step 15868: loss = 2269.93 (0.271 sec)\n",
            "train - step 15869: loss = 2135.05 (0.261 sec)\n",
            "train - step 15870: loss = 2181.56 (0.276 sec)\n",
            "train - step 15871: loss = 1816.97 (0.255 sec)\n",
            "train - step 15872: loss = 2109.49 (0.275 sec)\n",
            "train - step 15873: loss = 2168.82 (0.271 sec)\n",
            "train - step 15874: loss = 1799.95 (0.264 sec)\n",
            "train - step 15875: loss = 1632.19 (0.266 sec)\n",
            "train - step 15876: loss = 1955.43 (0.267 sec)\n",
            "train - step 15877: loss = 1898.59 (0.268 sec)\n",
            "train - step 15878: loss = 1737.86 (0.266 sec)\n",
            "train - step 15879: loss = 2019.58 (0.271 sec)\n",
            "train - step 15880: loss = 2667.16 (0.260 sec)\n",
            "train - step 15881: loss = 520.86 (0.275 sec)\n",
            "train - step 15882: loss = 2044.84 (0.263 sec)\n",
            "train - step 15883: loss = 2030.79 (0.260 sec)\n",
            "train - step 15884: loss = 2370.59 (0.268 sec)\n",
            "train - step 15885: loss = 1572.71 (0.273 sec)\n",
            "train - step 15886: loss = 1841.73 (0.265 sec)\n",
            "train - step 15887: loss = 2015.37 (0.266 sec)\n",
            "train - step 15888: loss = 2209.38 (0.257 sec)\n",
            "train - step 15889: loss = 2384.22 (0.271 sec)\n",
            "train - step 15890: loss = 1568.79 (0.258 sec)\n",
            "train - step 15891: loss = 1829.46 (0.273 sec)\n",
            "train - step 15892: loss = 1615.25 (0.266 sec)\n",
            "train - step 15893: loss = 1923.70 (0.272 sec)\n",
            "train - step 15894: loss = 1856.77 (0.275 sec)\n",
            "train - step 15895: loss = 2099.70 (0.258 sec)\n",
            "train - step 15896: loss = 1887.01 (0.270 sec)\n",
            "train - step 15897: loss = 2334.44 (0.269 sec)\n",
            "train - step 15898: loss = 2286.50 (0.265 sec)\n",
            "train - step 15899: loss = 2363.55 (0.267 sec)\n",
            "train - step 15900: loss = 2078.13 (0.270 sec)\n",
            "train - step 15901: loss = 1462.49 (0.267 sec)\n",
            "train - step 15902: loss = 2036.99 (0.263 sec)\n",
            "train - step 15903: loss = 1952.17 (0.265 sec)\n",
            "train - step 15904: loss = 1640.29 (0.275 sec)\n",
            "train - step 15905: loss = 1663.37 (0.267 sec)\n",
            "train - step 15906: loss = 1681.90 (0.271 sec)\n",
            "train - step 15907: loss = 1919.94 (0.265 sec)\n",
            "train - step 15908: loss = 1774.86 (0.275 sec)\n",
            "train - step 15909: loss = 1623.32 (0.270 sec)\n",
            "train - step 15910: loss = 2223.40 (0.274 sec)\n",
            "train - step 15911: loss = 2135.17 (0.264 sec)\n",
            "train - step 15912: loss = 1574.55 (0.273 sec)\n",
            "train - step 15913: loss = 1417.09 (0.270 sec)\n",
            "train - step 15914: loss = 1994.68 (0.261 sec)\n",
            "train - step 15915: loss = 2010.78 (0.263 sec)\n",
            "train - step 15916: loss = 2096.33 (0.266 sec)\n",
            "train - step 15917: loss = 2211.78 (0.273 sec)\n",
            "train - step 15918: loss = 1963.43 (0.261 sec)\n",
            "train - step 15919: loss = 1601.84 (0.261 sec)\n",
            "train - step 15920: loss = 1652.11 (0.267 sec)\n",
            "train - step 15921: loss = 1825.79 (0.265 sec)\n",
            "train - step 15922: loss = 2326.36 (0.269 sec)\n",
            "train - step 15923: loss = 1304.64 (0.267 sec)\n",
            "train - step 15924: loss = 1921.23 (0.276 sec)\n",
            "train - step 15925: loss = 1990.34 (0.269 sec)\n",
            "train - step 15926: loss = 2051.98 (0.259 sec)\n",
            "train - step 15927: loss = 2258.27 (0.269 sec)\n",
            "train - step 15928: loss = 1842.72 (0.267 sec)\n",
            "train - step 15929: loss = 1755.06 (0.265 sec)\n",
            "train - step 15930: loss = 2049.93 (0.262 sec)\n",
            "train - step 15931: loss = 2110.47 (0.281 sec)\n",
            "train - step 15932: loss = 1888.34 (0.265 sec)\n",
            "train - step 15933: loss = 1792.94 (0.274 sec)\n",
            "train - step 15934: loss = 2055.24 (0.268 sec)\n",
            "train - step 15935: loss = 2005.27 (0.266 sec)\n",
            "train - step 15936: loss = 1770.20 (0.269 sec)\n",
            "train - step 15937: loss = 1699.87 (0.270 sec)\n",
            "train - step 15938: loss = 1955.39 (0.264 sec)\n",
            "train - step 15939: loss = 1849.01 (0.272 sec)\n",
            "train - step 15940: loss = 1950.79 (0.268 sec)\n",
            "train - step 15941: loss = 2076.67 (0.262 sec)\n",
            "train - step 15942: loss = 2481.75 (0.266 sec)\n",
            "train - step 15943: loss = 1835.08 (0.273 sec)\n",
            "train - step 15944: loss = 2113.94 (0.273 sec)\n",
            "train - step 15945: loss = 1667.46 (0.264 sec)\n",
            "train - step 15946: loss = 2111.97 (0.267 sec)\n",
            "train - step 15947: loss = 2300.19 (0.267 sec)\n",
            "train - step 15948: loss = 1839.57 (0.266 sec)\n",
            "train - step 15949: loss = 2103.08 (0.260 sec)\n",
            "train - step 15950: loss = 2246.47 (0.264 sec)\n",
            "train - step 15951: loss = 1855.59 (0.268 sec)\n",
            "train - step 15952: loss = 2595.99 (0.268 sec)\n",
            "train - step 15953: loss = 1951.03 (0.264 sec)\n",
            "train - step 15954: loss = 1476.12 (0.267 sec)\n",
            "train - step 15955: loss = 1347.81 (0.262 sec)\n",
            "train - step 15956: loss = 2216.52 (0.265 sec)\n",
            "train - step 15957: loss = 1983.93 (0.265 sec)\n",
            "train - step 15958: loss = 1770.86 (0.269 sec)\n",
            "train - step 15959: loss = 1957.43 (0.270 sec)\n",
            "train - step 15960: loss = 2318.90 (0.267 sec)\n",
            "train - step 15961: loss = 2504.15 (0.270 sec)\n",
            "train - step 15962: loss = 1653.10 (0.270 sec)\n",
            "train - step 15963: loss = 1659.83 (0.265 sec)\n",
            "train - step 15964: loss = 2213.62 (0.261 sec)\n",
            "train - step 15965: loss = 1973.22 (0.268 sec)\n",
            "train - step 15966: loss = 2226.16 (0.270 sec)\n",
            "train - step 15967: loss = 2323.49 (0.260 sec)\n",
            "train - step 15968: loss = 2133.77 (0.266 sec)\n",
            "train - step 15969: loss = 2019.11 (0.265 sec)\n",
            "train - step 15970: loss = 1492.03 (0.267 sec)\n",
            "train - step 15971: loss = 1954.67 (0.274 sec)\n",
            "train - step 15972: loss = 1916.73 (0.258 sec)\n",
            "train - step 15973: loss = 2318.02 (0.273 sec)\n",
            "train - step 15974: loss = 2356.03 (0.266 sec)\n",
            "train - step 15975: loss = 1712.04 (0.267 sec)\n",
            "train - step 15976: loss = 1945.95 (0.267 sec)\n",
            "train - step 15977: loss = 1982.19 (0.276 sec)\n",
            "train - step 15978: loss = 2671.37 (0.253 sec)\n",
            "train - step 15979: loss = 1720.21 (0.275 sec)\n",
            "train - step 15980: loss = 2051.69 (0.267 sec)\n",
            "train - step 15981: loss = 2032.80 (0.276 sec)\n",
            "train - step 15982: loss = 2693.44 (0.257 sec)\n",
            "train - step 15983: loss = 1522.30 (0.260 sec)\n",
            "train - step 15984: loss = 1907.01 (0.269 sec)\n",
            "train - step 15985: loss = 2385.06 (0.265 sec)\n",
            "train - step 15986: loss = 2131.16 (0.265 sec)\n",
            "train - step 15987: loss = 1845.47 (0.275 sec)\n",
            "train - step 15988: loss = 1952.01 (0.275 sec)\n",
            "train - step 15989: loss = 1614.13 (0.274 sec)\n",
            "train - step 15990: loss = 2273.71 (0.266 sec)\n",
            "train - step 15991: loss = 1999.39 (0.270 sec)\n",
            "train - step 15992: loss = 2062.53 (0.263 sec)\n",
            "train - step 15993: loss = 1775.23 (0.273 sec)\n",
            "train - step 15994: loss = 1281.95 (0.267 sec)\n",
            "train - step 15995: loss = 1616.02 (0.259 sec)\n",
            "train - step 15996: loss = 546.16 (0.260 sec)\n",
            "train - step 15997: loss = 2194.46 (0.269 sec)\n",
            "train - step 15998: loss = 2370.70 (0.268 sec)\n",
            "train - step 15999: loss = 2224.19 (0.265 sec)\n",
            "train - step 16000: loss = 2410.33 (0.264 sec)\n",
            "train - step 16001: loss = 1745.12 (0.280 sec)\n",
            "train - step 16002: loss = 2080.02 (0.266 sec)\n",
            "train - step 16003: loss = 2338.78 (0.261 sec)\n",
            "train - step 16004: loss = 1892.00 (0.272 sec)\n",
            "train - step 16005: loss = 1844.67 (0.269 sec)\n",
            "train - step 16006: loss = 2004.57 (0.260 sec)\n",
            "train - step 16007: loss = 1706.95 (0.266 sec)\n",
            "train - step 16008: loss = 2066.08 (0.269 sec)\n",
            "train - step 16009: loss = 1746.20 (0.265 sec)\n",
            "train - step 16010: loss = 1709.09 (0.268 sec)\n",
            "train - step 16011: loss = 1998.83 (0.271 sec)\n",
            "train - step 16012: loss = 1959.06 (0.268 sec)\n",
            "train - step 16013: loss = 2124.44 (0.271 sec)\n",
            "train - step 16014: loss = 1754.81 (0.262 sec)\n",
            "train - step 16015: loss = 2079.95 (0.272 sec)\n",
            "train - step 16016: loss = 1708.96 (0.265 sec)\n",
            "train - step 16017: loss = 1896.54 (0.261 sec)\n",
            "train - step 16018: loss = 1859.95 (0.266 sec)\n",
            "train - step 16019: loss = 1880.45 (0.273 sec)\n",
            "train - step 16020: loss = 1743.09 (0.269 sec)\n",
            "train - step 16021: loss = 1758.72 (0.265 sec)\n",
            "train - step 16022: loss = 1917.28 (0.271 sec)\n",
            "train - step 16023: loss = 1939.93 (0.269 sec)\n",
            "train - step 16024: loss = 1748.36 (0.270 sec)\n",
            "train - step 16025: loss = 1998.28 (0.264 sec)\n",
            "train - step 16026: loss = 2099.81 (0.270 sec)\n",
            "train - step 16027: loss = 1913.20 (0.986 sec)\n",
            "train - step 16028: loss = 2090.88 (0.262 sec)\n",
            "train - step 16029: loss = 1724.19 (0.266 sec)\n",
            "train - step 16030: loss = 2612.63 (0.260 sec)\n",
            "train - step 16031: loss = 2043.69 (0.267 sec)\n",
            "train - step 16032: loss = 2052.04 (0.278 sec)\n",
            "train - step 16033: loss = 2042.07 (0.269 sec)\n",
            "train - step 16034: loss = 1965.48 (0.268 sec)\n",
            "train - step 16035: loss = 2114.86 (0.282 sec)\n",
            "train - step 16036: loss = 1963.13 (0.284 sec)\n",
            "train - step 16037: loss = 1750.94 (0.275 sec)\n",
            "train - step 16038: loss = 2147.49 (0.271 sec)\n",
            "train - step 16039: loss = 1616.53 (0.275 sec)\n",
            "train - step 16040: loss = 2060.08 (0.278 sec)\n",
            "train - step 16041: loss = 2200.48 (0.269 sec)\n",
            "train - step 16042: loss = 1806.06 (0.278 sec)\n",
            "train - step 16043: loss = 1017.98 (0.265 sec)\n",
            "train - step 16044: loss = 1824.17 (0.268 sec)\n",
            "train - step 16045: loss = 1808.52 (0.272 sec)\n",
            "train - step 16046: loss = 1834.52 (0.270 sec)\n",
            "train - step 16047: loss = 1961.39 (0.278 sec)\n",
            "train - step 16048: loss = 1891.15 (0.278 sec)\n",
            "train - step 16049: loss = 2492.69 (0.277 sec)\n",
            "train - step 16050: loss = 666.45 (0.269 sec)\n",
            "train - step 16051: loss = 2320.56 (0.276 sec)\n",
            "train - step 16052: loss = 1840.20 (0.281 sec)\n",
            "train - step 16053: loss = 1849.49 (0.276 sec)\n",
            "train - step 16054: loss = 2063.57 (0.282 sec)\n",
            "train - step 16055: loss = 2010.89 (0.283 sec)\n",
            "train - step 16056: loss = 2017.92 (0.276 sec)\n",
            "train - step 16057: loss = 1824.94 (0.268 sec)\n",
            "train - step 16058: loss = 2086.65 (0.272 sec)\n",
            "train - step 16059: loss = 1776.32 (0.283 sec)\n",
            "train - step 16060: loss = 2191.86 (0.274 sec)\n",
            "train - step 16061: loss = 2453.36 (0.274 sec)\n",
            "train - step 16062: loss = 2010.94 (0.272 sec)\n",
            "train - step 16063: loss = 1970.52 (0.285 sec)\n",
            "train - step 16064: loss = 1692.70 (0.267 sec)\n",
            "train - step 16065: loss = 1314.01 (0.264 sec)\n",
            "train - step 16066: loss = 1801.49 (0.277 sec)\n",
            "train - step 16067: loss = 1857.59 (0.269 sec)\n",
            "train - step 16068: loss = 2112.73 (0.270 sec)\n",
            "train - step 16069: loss = 1766.20 (0.285 sec)\n",
            "train - step 16070: loss = 1997.52 (0.278 sec)\n",
            "train - step 16071: loss = 1873.96 (0.272 sec)\n",
            "train - step 16072: loss = 1919.86 (0.263 sec)\n",
            "train - step 16073: loss = 2013.65 (0.275 sec)\n",
            "train - step 16074: loss = 1830.98 (0.270 sec)\n",
            "train - step 16075: loss = 1838.28 (0.268 sec)\n",
            "train - step 16076: loss = 1880.77 (0.273 sec)\n",
            "train - step 16077: loss = 1939.71 (0.264 sec)\n",
            "train - step 16078: loss = 2113.70 (0.261 sec)\n",
            "train - step 16079: loss = 1489.00 (0.281 sec)\n",
            "train - step 16080: loss = 2100.15 (0.267 sec)\n",
            "train - step 16081: loss = 1953.65 (0.270 sec)\n",
            "train - step 16082: loss = 1866.06 (0.273 sec)\n",
            "train - step 16083: loss = 2106.11 (0.267 sec)\n",
            "train - step 16084: loss = 1810.05 (0.264 sec)\n",
            "train - step 16085: loss = 589.32 (0.267 sec)\n",
            "train - step 16086: loss = 1995.73 (0.283 sec)\n",
            "train - step 16087: loss = 2195.48 (0.269 sec)\n",
            "train - step 16088: loss = 1809.19 (0.283 sec)\n",
            "train - step 16089: loss = 1904.66 (0.280 sec)\n",
            "train - step 16090: loss = 2009.60 (0.273 sec)\n",
            "train - step 16091: loss = 2269.22 (0.280 sec)\n",
            "train - step 16092: loss = 2033.77 (0.278 sec)\n",
            "train - step 16093: loss = 1769.84 (0.269 sec)\n",
            "train - step 16094: loss = 2006.41 (0.276 sec)\n",
            "train - step 16095: loss = 2143.53 (0.282 sec)\n",
            "train - step 16096: loss = 1863.40 (0.286 sec)\n",
            "train - step 16097: loss = 1862.50 (0.273 sec)\n",
            "train - step 16098: loss = 2404.51 (0.273 sec)\n",
            "train - step 16099: loss = 2076.44 (0.274 sec)\n",
            "train - step 16100: loss = 1952.64 (0.276 sec)\n",
            "train - step 16101: loss = 1908.29 (0.281 sec)\n",
            "train - step 16102: loss = 1693.20 (0.282 sec)\n",
            "train - step 16103: loss = 2092.98 (0.280 sec)\n",
            "train - step 16104: loss = 2100.70 (0.267 sec)\n",
            "train - step 16105: loss = 1927.14 (0.264 sec)\n",
            "train - step 16106: loss = 2275.09 (0.269 sec)\n",
            "train - step 16107: loss = 2107.36 (0.262 sec)\n",
            "train - step 16108: loss = 2059.95 (0.263 sec)\n",
            "train - step 16109: loss = 1856.81 (0.270 sec)\n",
            "train - step 16110: loss = 2082.78 (0.267 sec)\n",
            "train - step 16111: loss = 1770.56 (0.280 sec)\n",
            "train - step 16112: loss = 2388.51 (0.260 sec)\n",
            "train - step 16113: loss = 1664.02 (0.270 sec)\n",
            "train - step 16114: loss = 1567.87 (0.267 sec)\n",
            "train - step 16115: loss = 1017.83 (0.275 sec)\n",
            "train - step 16116: loss = 2474.23 (0.270 sec)\n",
            "train - step 16117: loss = 2725.68 (0.266 sec)\n",
            "train - step 16118: loss = 2190.93 (0.267 sec)\n",
            "train - step 16119: loss = 1750.17 (0.275 sec)\n",
            "train - step 16120: loss = 1973.28 (0.263 sec)\n",
            "train - step 16121: loss = 2053.68 (0.265 sec)\n",
            "train - step 16122: loss = 1734.43 (0.277 sec)\n",
            "train - step 16123: loss = 1723.97 (0.269 sec)\n",
            "train - step 16124: loss = 1725.86 (0.260 sec)\n",
            "train - step 16125: loss = 1871.70 (0.261 sec)\n",
            "train - step 16126: loss = 2054.75 (0.262 sec)\n",
            "train - step 16127: loss = 1928.17 (0.267 sec)\n",
            "train - step 16128: loss = 2294.07 (0.262 sec)\n",
            "train - step 16129: loss = 1347.88 (0.263 sec)\n",
            "train - step 16130: loss = 1729.87 (0.267 sec)\n",
            "train - step 16131: loss = 1616.10 (0.270 sec)\n",
            "train - step 16132: loss = 1716.21 (0.261 sec)\n",
            "train - step 16133: loss = 2533.30 (0.263 sec)\n",
            "train - step 16134: loss = 2026.52 (0.268 sec)\n",
            "train - step 16135: loss = 2037.47 (0.265 sec)\n",
            "train - step 16136: loss = 1514.15 (0.264 sec)\n",
            "train - step 16137: loss = 1963.16 (0.264 sec)\n",
            "train - step 16138: loss = 1962.95 (0.282 sec)\n",
            "train - step 16139: loss = 1785.23 (0.265 sec)\n",
            "train - step 16140: loss = 1651.90 (0.269 sec)\n",
            "train - step 16141: loss = 2160.98 (0.278 sec)\n",
            "train - step 16142: loss = 1811.18 (0.277 sec)\n",
            "train - step 16143: loss = 1887.01 (0.275 sec)\n",
            "train - step 16144: loss = 2280.00 (0.266 sec)\n",
            "train - step 16145: loss = 1907.84 (0.261 sec)\n",
            "train - step 16146: loss = 1864.03 (0.270 sec)\n",
            "train - step 16147: loss = 1550.50 (0.269 sec)\n",
            "train - step 16148: loss = 2219.11 (0.266 sec)\n",
            "train - step 16149: loss = 2203.86 (0.273 sec)\n",
            "train - step 16150: loss = 2145.23 (0.270 sec)\n",
            "train - step 16151: loss = 2145.89 (0.272 sec)\n",
            "train - step 16152: loss = 2136.77 (0.271 sec)\n",
            "train - step 16153: loss = 2074.18 (0.271 sec)\n",
            "train - step 16154: loss = 2021.06 (0.258 sec)\n",
            "train - step 16155: loss = 1779.78 (0.267 sec)\n",
            "train - step 16156: loss = 1810.28 (0.258 sec)\n",
            "train - step 16157: loss = 2046.12 (0.284 sec)\n",
            "train - step 16158: loss = 2306.80 (0.259 sec)\n",
            "train - step 16159: loss = 2323.51 (0.262 sec)\n",
            "train - step 16160: loss = 1663.41 (0.266 sec)\n",
            "train - step 16161: loss = 2218.31 (0.270 sec)\n",
            "train - step 16162: loss = 2134.77 (0.264 sec)\n",
            "train - step 16163: loss = 1718.07 (0.273 sec)\n",
            "train - step 16164: loss = 1884.94 (0.270 sec)\n",
            "train - step 16165: loss = 568.06 (0.266 sec)\n",
            "train - step 16166: loss = 1967.91 (0.265 sec)\n",
            "train - step 16167: loss = 1778.33 (0.263 sec)\n",
            "train - step 16168: loss = 2081.88 (0.264 sec)\n",
            "train - step 16169: loss = 2005.94 (0.267 sec)\n",
            "train - step 16170: loss = 2054.66 (0.260 sec)\n",
            "train - step 16171: loss = 1931.87 (0.267 sec)\n",
            "train - step 16172: loss = 2325.84 (0.273 sec)\n",
            "train - step 16173: loss = 1862.63 (0.269 sec)\n",
            "train - step 16174: loss = 2063.62 (0.267 sec)\n",
            "train - step 16175: loss = 1886.85 (0.267 sec)\n",
            "train - step 16176: loss = 1964.68 (0.261 sec)\n",
            "train - step 16177: loss = 1618.64 (0.258 sec)\n",
            "train - step 16178: loss = 2189.10 (0.260 sec)\n",
            "train - step 16179: loss = 1902.46 (0.271 sec)\n",
            "train - step 16180: loss = 2338.15 (0.268 sec)\n",
            "train - step 16181: loss = 1587.24 (0.277 sec)\n",
            "train - step 16182: loss = 1224.77 (0.266 sec)\n",
            "train - step 16183: loss = 2082.50 (0.269 sec)\n",
            "train - step 16184: loss = 600.77 (0.262 sec)\n",
            "train - step 16185: loss = 1821.13 (0.265 sec)\n",
            "train - step 16186: loss = 1799.96 (0.273 sec)\n",
            "train - step 16187: loss = 2031.20 (0.275 sec)\n",
            "train - step 16188: loss = 1940.73 (0.275 sec)\n",
            "train - step 16189: loss = 1729.54 (0.267 sec)\n",
            "train - step 16190: loss = 1639.41 (0.266 sec)\n",
            "train - step 16191: loss = 1783.96 (0.272 sec)\n",
            "train - step 16192: loss = 1947.40 (0.275 sec)\n",
            "train - step 16193: loss = 2580.35 (0.260 sec)\n",
            "train - step 16194: loss = 2064.44 (0.265 sec)\n",
            "train - step 16195: loss = 1663.35 (0.274 sec)\n",
            "train - step 16196: loss = 1930.55 (0.265 sec)\n",
            "train - step 16197: loss = 1219.57 (0.272 sec)\n",
            "train - step 16198: loss = 1697.09 (0.263 sec)\n",
            "train - step 16199: loss = 1696.86 (0.269 sec)\n",
            "train - step 16200: loss = 1960.70 (0.271 sec)\n",
            "train - step 16201: loss = 2541.07 (0.267 sec)\n",
            "train - step 16202: loss = 2094.61 (0.266 sec)\n",
            "train - step 16203: loss = 1199.35 (0.267 sec)\n",
            "train - step 16204: loss = 2681.94 (0.258 sec)\n",
            "train - step 16205: loss = 798.51 (0.274 sec)\n",
            "train - step 16206: loss = 2436.58 (0.264 sec)\n",
            "train - step 16207: loss = 2153.67 (0.271 sec)\n",
            "train - step 16208: loss = 1928.41 (0.268 sec)\n",
            "train - step 16209: loss = 2643.79 (0.260 sec)\n",
            "train - step 16210: loss = 2099.86 (0.270 sec)\n",
            "train - step 16211: loss = 2258.34 (0.263 sec)\n",
            "train - step 16212: loss = 1840.51 (0.261 sec)\n",
            "train - step 16213: loss = 1668.53 (0.278 sec)\n",
            "train - step 16214: loss = 1646.38 (0.269 sec)\n",
            "train - step 16215: loss = 1947.94 (0.259 sec)\n",
            "train - step 16216: loss = 1699.00 (0.263 sec)\n",
            "train - step 16217: loss = 2044.60 (0.262 sec)\n",
            "train - step 16218: loss = 1940.89 (0.266 sec)\n",
            "train - step 16219: loss = 1476.17 (0.273 sec)\n",
            "train - step 16220: loss = 1645.82 (0.263 sec)\n",
            "train - step 16221: loss = 1724.13 (0.277 sec)\n",
            "train - step 16222: loss = 1721.92 (0.266 sec)\n",
            "train - step 16223: loss = 2051.15 (0.258 sec)\n",
            "train - step 16224: loss = 2179.00 (0.267 sec)\n",
            "train - step 16225: loss = 2218.91 (0.263 sec)\n",
            "train - step 16226: loss = 1935.39 (0.271 sec)\n",
            "train - step 16227: loss = 1622.63 (0.267 sec)\n",
            "train - step 16228: loss = 1816.89 (0.259 sec)\n",
            "train - step 16229: loss = 2301.88 (0.260 sec)\n",
            "train - step 16230: loss = 2172.73 (0.267 sec)\n",
            "train - step 16231: loss = 1767.50 (0.264 sec)\n",
            "train - step 16232: loss = 1949.03 (0.265 sec)\n",
            "train - step 16233: loss = 2145.11 (0.263 sec)\n",
            "train - step 16234: loss = 2491.55 (0.280 sec)\n",
            "train - step 16235: loss = 2203.80 (0.262 sec)\n",
            "train - step 16236: loss = 1621.27 (0.263 sec)\n",
            "train - step 16237: loss = 2002.53 (0.274 sec)\n",
            "train - step 16238: loss = 2180.06 (0.274 sec)\n",
            "train - step 16239: loss = 1832.55 (0.270 sec)\n",
            "train - step 16240: loss = 1346.02 (0.275 sec)\n",
            "train - step 16241: loss = 2063.08 (0.270 sec)\n",
            "train - step 16242: loss = 1670.66 (0.271 sec)\n",
            "train - step 16243: loss = 2598.17 (0.261 sec)\n",
            "train - step 16244: loss = 1956.71 (0.275 sec)\n",
            "train - step 16245: loss = 2087.53 (0.267 sec)\n",
            "train - step 16246: loss = 2026.76 (0.264 sec)\n",
            "train - step 16247: loss = 1977.90 (0.263 sec)\n",
            "train - step 16248: loss = 2019.14 (1.075 sec)\n",
            "train - step 16249: loss = 1718.87 (0.267 sec)\n",
            "train - step 16250: loss = 1666.46 (0.272 sec)\n",
            "train - step 16251: loss = 2217.61 (0.269 sec)\n",
            "train - step 16252: loss = 2092.75 (0.266 sec)\n",
            "train - step 16253: loss = 1880.79 (0.264 sec)\n",
            "train - step 16254: loss = 2019.59 (0.260 sec)\n",
            "train - step 16255: loss = 1927.12 (0.265 sec)\n",
            "train - step 16256: loss = 1830.49 (0.269 sec)\n",
            "train - step 16257: loss = 2250.11 (0.274 sec)\n",
            "train - step 16258: loss = 1932.19 (0.266 sec)\n",
            "train - step 16259: loss = 1702.33 (0.269 sec)\n",
            "train - step 16260: loss = 1733.50 (0.279 sec)\n",
            "train - step 16261: loss = 1675.95 (0.264 sec)\n",
            "train - step 16262: loss = 1636.93 (0.274 sec)\n",
            "train - step 16263: loss = 2288.47 (0.265 sec)\n",
            "train - step 16264: loss = 1978.38 (0.262 sec)\n",
            "train - step 16265: loss = 2279.53 (0.263 sec)\n",
            "train - step 16266: loss = 2104.92 (0.269 sec)\n",
            "train - step 16267: loss = 1915.29 (0.268 sec)\n",
            "train - step 16268: loss = 1787.09 (0.264 sec)\n",
            "train - step 16269: loss = 1905.38 (0.263 sec)\n",
            "train - step 16270: loss = 2207.19 (0.258 sec)\n",
            "train - step 16271: loss = 1915.55 (0.272 sec)\n",
            "train - step 16272: loss = 1569.91 (0.270 sec)\n",
            "train - step 16273: loss = 1693.14 (0.274 sec)\n",
            "train - step 16274: loss = 1909.18 (0.271 sec)\n",
            "train - step 16275: loss = 2504.63 (0.263 sec)\n",
            "train - step 16276: loss = 1910.03 (0.265 sec)\n",
            "train - step 16277: loss = 1531.33 (0.277 sec)\n",
            "train - step 16278: loss = 1555.40 (0.264 sec)\n",
            "train - step 16279: loss = 2146.13 (0.263 sec)\n",
            "train - step 16280: loss = 1300.59 (0.260 sec)\n",
            "train - step 16281: loss = 1678.22 (0.274 sec)\n",
            "train - step 16282: loss = 2326.06 (0.269 sec)\n",
            "train - step 16283: loss = 1876.98 (0.271 sec)\n",
            "train - step 16284: loss = 2186.12 (0.263 sec)\n",
            "train - step 16285: loss = 2129.03 (0.268 sec)\n",
            "train - step 16286: loss = 1905.53 (0.264 sec)\n",
            "train - step 16287: loss = 1874.94 (0.267 sec)\n",
            "train - step 16288: loss = 1861.12 (0.275 sec)\n",
            "train - step 16289: loss = 2239.37 (0.264 sec)\n",
            "train - step 16290: loss = 2145.28 (0.259 sec)\n",
            "train - step 16291: loss = 2215.75 (0.264 sec)\n",
            "train - step 16292: loss = 2280.70 (0.269 sec)\n",
            "train - step 16293: loss = 1685.57 (0.267 sec)\n",
            "train - step 16294: loss = 2155.92 (0.263 sec)\n",
            "train - step 16295: loss = 1980.56 (0.272 sec)\n",
            "train - step 16296: loss = 2298.76 (0.273 sec)\n",
            "train - step 16297: loss = 1612.39 (0.263 sec)\n",
            "train - step 16298: loss = 2113.19 (0.275 sec)\n",
            "train - step 16299: loss = 2014.32 (0.268 sec)\n",
            "train - step 16300: loss = 1626.47 (0.277 sec)\n",
            "train - step 16301: loss = 1696.06 (0.269 sec)\n",
            "train - step 16302: loss = 1547.45 (0.268 sec)\n",
            "train - step 16303: loss = 1722.04 (0.266 sec)\n",
            "train - step 16304: loss = 1853.24 (0.272 sec)\n",
            "train - step 16305: loss = 1612.57 (0.274 sec)\n",
            "train - step 16306: loss = 808.07 (0.266 sec)\n",
            "train - step 16307: loss = 2443.49 (0.264 sec)\n",
            "train - step 16308: loss = 1383.84 (0.276 sec)\n",
            "train - step 16309: loss = 2016.69 (0.260 sec)\n",
            "train - step 16310: loss = 2086.82 (0.265 sec)\n",
            "train - step 16311: loss = 1680.16 (0.257 sec)\n",
            "train - step 16312: loss = 1972.70 (0.263 sec)\n",
            "train - step 16313: loss = 1970.02 (0.263 sec)\n",
            "train - step 16314: loss = 2043.74 (0.273 sec)\n",
            "train - step 16315: loss = 2011.47 (0.261 sec)\n",
            "train - step 16316: loss = 1637.15 (0.270 sec)\n",
            "train - step 16317: loss = 1198.68 (0.260 sec)\n",
            "train - step 16318: loss = 1783.48 (0.266 sec)\n",
            "train - step 16319: loss = 1561.72 (0.274 sec)\n",
            "train - step 16320: loss = 2199.02 (0.262 sec)\n",
            "train - step 16321: loss = 1876.04 (0.264 sec)\n",
            "train - step 16322: loss = 2320.23 (0.268 sec)\n",
            "train - step 16323: loss = 1678.03 (0.278 sec)\n",
            "train - step 16324: loss = 2088.79 (0.267 sec)\n",
            "train - step 16325: loss = 1934.34 (0.268 sec)\n",
            "train - step 16326: loss = 1779.79 (0.270 sec)\n",
            "train - step 16327: loss = 2316.84 (0.260 sec)\n",
            "train - step 16328: loss = 1997.40 (0.266 sec)\n",
            "train - step 16329: loss = 2032.50 (0.267 sec)\n",
            "train - step 16330: loss = 2229.04 (0.277 sec)\n",
            "train - step 16331: loss = 1925.06 (0.278 sec)\n",
            "train - step 16332: loss = 1899.62 (0.279 sec)\n",
            "train - step 16333: loss = 1569.97 (0.278 sec)\n",
            "train - step 16334: loss = 655.85 (0.274 sec)\n",
            "train - step 16335: loss = 1975.14 (0.289 sec)\n",
            "train - step 16336: loss = 1653.34 (0.280 sec)\n",
            "train - step 16337: loss = 1815.33 (0.280 sec)\n",
            "train - step 16338: loss = 1858.36 (0.282 sec)\n",
            "train - step 16339: loss = 2362.36 (0.273 sec)\n",
            "train - step 16340: loss = 2309.21 (0.276 sec)\n",
            "train - step 16341: loss = 2000.26 (0.272 sec)\n",
            "train - step 16342: loss = 1974.02 (0.281 sec)\n",
            "train - step 16343: loss = 2457.20 (0.266 sec)\n",
            "train - step 16344: loss = 1745.96 (0.271 sec)\n",
            "train - step 16345: loss = 1851.59 (0.278 sec)\n",
            "train - step 16346: loss = 2057.78 (0.271 sec)\n",
            "train - step 16347: loss = 1904.49 (0.273 sec)\n",
            "train - step 16348: loss = 2119.29 (0.276 sec)\n",
            "train - step 16349: loss = 2063.79 (0.276 sec)\n",
            "train - step 16350: loss = 1925.73 (0.269 sec)\n",
            "train - step 16351: loss = 1281.96 (0.263 sec)\n",
            "train - step 16352: loss = 1733.84 (0.274 sec)\n",
            "train - step 16353: loss = 1974.36 (0.281 sec)\n",
            "train - step 16354: loss = 1760.44 (0.272 sec)\n",
            "train - step 16355: loss = 2182.33 (0.283 sec)\n",
            "train - step 16356: loss = 2029.91 (0.280 sec)\n",
            "train - step 16357: loss = 2018.37 (0.278 sec)\n",
            "train - step 16358: loss = 1509.54 (0.271 sec)\n",
            "train - step 16359: loss = 1465.34 (0.277 sec)\n",
            "train - step 16360: loss = 2333.56 (0.268 sec)\n",
            "train - step 16361: loss = 1593.97 (0.279 sec)\n",
            "train - step 16362: loss = 2073.59 (0.273 sec)\n",
            "train - step 16363: loss = 1999.14 (0.279 sec)\n",
            "train - step 16364: loss = 1810.88 (0.284 sec)\n",
            "train - step 16365: loss = 1998.21 (0.277 sec)\n",
            "train - step 16366: loss = 1859.00 (0.275 sec)\n",
            "train - step 16367: loss = 1552.71 (0.263 sec)\n",
            "train - step 16368: loss = 2158.13 (0.269 sec)\n",
            "train - step 16369: loss = 1163.39 (0.264 sec)\n",
            "train - step 16370: loss = 2279.67 (0.263 sec)\n",
            "train - step 16371: loss = 1719.12 (0.263 sec)\n",
            "train - step 16372: loss = 2160.70 (0.274 sec)\n",
            "train - step 16373: loss = 1728.48 (0.269 sec)\n",
            "train - step 16374: loss = 1829.08 (0.264 sec)\n",
            "train - step 16375: loss = 1844.87 (0.263 sec)\n",
            "train - step 16376: loss = 1684.91 (0.285 sec)\n",
            "train - step 16377: loss = 2103.60 (0.269 sec)\n",
            "train - step 16378: loss = 1885.19 (0.266 sec)\n",
            "train - step 16379: loss = 1915.16 (0.260 sec)\n",
            "train - step 16380: loss = 1908.60 (0.271 sec)\n",
            "train - step 16381: loss = 1682.44 (0.266 sec)\n",
            "train - step 16382: loss = 1716.32 (0.261 sec)\n",
            "train - step 16383: loss = 2099.16 (0.268 sec)\n",
            "train - step 16384: loss = 1765.98 (0.279 sec)\n",
            "train - step 16385: loss = 1951.38 (0.264 sec)\n",
            "train - step 16386: loss = 1232.34 (0.268 sec)\n",
            "train - step 16387: loss = 2296.68 (0.268 sec)\n",
            "train - step 16388: loss = 1817.49 (0.268 sec)\n",
            "train - step 16389: loss = 1757.23 (0.264 sec)\n",
            "train - step 16390: loss = 1916.81 (0.272 sec)\n",
            "train - step 16391: loss = 1936.58 (0.278 sec)\n",
            "train - step 16392: loss = 2104.07 (0.267 sec)\n",
            "train - step 16393: loss = 2026.54 (0.266 sec)\n",
            "train - step 16394: loss = 1894.08 (0.259 sec)\n",
            "train - step 16395: loss = 2168.54 (0.267 sec)\n",
            "train - step 16396: loss = 1816.84 (0.261 sec)\n",
            "train - step 16397: loss = 1358.25 (0.259 sec)\n",
            "train - step 16398: loss = 1809.04 (0.266 sec)\n",
            "train - step 16399: loss = 2144.63 (0.271 sec)\n",
            "train - step 16400: loss = 1366.39 (0.275 sec)\n",
            "train - step 16401: loss = 2333.75 (0.263 sec)\n",
            "train - step 16402: loss = 2142.34 (0.268 sec)\n",
            "train - step 16403: loss = 1748.04 (0.272 sec)\n",
            "train - step 16404: loss = 1731.67 (0.271 sec)\n",
            "train - step 16405: loss = 1764.97 (0.259 sec)\n",
            "train - step 16406: loss = 2097.02 (0.265 sec)\n",
            "train - step 16407: loss = 1581.74 (0.274 sec)\n",
            "train - step 16408: loss = 1947.93 (0.262 sec)\n",
            "train - step 16409: loss = 1912.31 (0.266 sec)\n",
            "train - step 16410: loss = 1692.96 (0.265 sec)\n",
            "train - step 16411: loss = 1832.33 (0.265 sec)\n",
            "train - step 16412: loss = 2042.39 (0.263 sec)\n",
            "train - step 16413: loss = 1692.77 (0.263 sec)\n",
            "train - step 16414: loss = 1986.10 (0.268 sec)\n",
            "train - step 16415: loss = 2181.11 (0.275 sec)\n",
            "train - step 16416: loss = 1313.81 (0.260 sec)\n",
            "train - step 16417: loss = 1244.57 (0.264 sec)\n",
            "train - step 16418: loss = 2157.47 (0.271 sec)\n",
            "train - step 16419: loss = 2118.18 (0.264 sec)\n",
            "train - step 16420: loss = 1899.08 (0.259 sec)\n",
            "train - step 16421: loss = 2006.88 (0.261 sec)\n",
            "train - step 16422: loss = 1704.27 (0.274 sec)\n",
            "train - step 16423: loss = 1376.61 (0.268 sec)\n",
            "train - step 16424: loss = 1251.22 (0.266 sec)\n",
            "train - step 16425: loss = 1994.74 (0.272 sec)\n",
            "train - step 16426: loss = 1636.72 (0.273 sec)\n",
            "train - step 16427: loss = 2108.28 (0.260 sec)\n",
            "train - step 16428: loss = 1623.65 (0.269 sec)\n",
            "train - step 16429: loss = 1893.52 (0.272 sec)\n",
            "train - step 16430: loss = 2269.07 (0.275 sec)\n",
            "train - step 16431: loss = 1658.66 (0.262 sec)\n",
            "train - step 16432: loss = 1935.60 (0.258 sec)\n",
            "train - step 16433: loss = 1144.53 (0.273 sec)\n",
            "train - step 16434: loss = 2181.81 (0.259 sec)\n",
            "train - step 16435: loss = 1807.86 (0.273 sec)\n",
            "train - step 16436: loss = 1964.21 (0.268 sec)\n",
            "train - step 16437: loss = 1720.82 (0.270 sec)\n",
            "train - step 16438: loss = 2205.29 (0.262 sec)\n",
            "train - step 16439: loss = 2402.16 (0.262 sec)\n",
            "train - step 16440: loss = 1120.12 (0.268 sec)\n",
            "train - step 16441: loss = 2135.66 (0.271 sec)\n",
            "train - step 16442: loss = 1746.18 (0.261 sec)\n",
            "train - step 16443: loss = 1943.45 (0.270 sec)\n",
            "train - step 16444: loss = 1852.81 (0.262 sec)\n",
            "train - step 16445: loss = 1724.12 (0.270 sec)\n",
            "train - step 16446: loss = 1832.38 (0.262 sec)\n",
            "train - step 16447: loss = 2136.12 (0.265 sec)\n",
            "train - step 16448: loss = 1937.80 (0.274 sec)\n",
            "train - step 16449: loss = 1790.22 (0.271 sec)\n",
            "train - step 16450: loss = 1599.23 (0.271 sec)\n",
            "train - step 16451: loss = 1976.87 (0.262 sec)\n",
            "train - step 16452: loss = 2066.42 (0.267 sec)\n",
            "train - step 16453: loss = 1916.10 (0.272 sec)\n",
            "train - step 16454: loss = 1924.21 (0.271 sec)\n",
            "train - step 16455: loss = 1888.64 (0.272 sec)\n",
            "train - step 16456: loss = 539.60 (0.257 sec)\n",
            "train - step 16457: loss = 1931.22 (0.281 sec)\n",
            "train - step 16458: loss = 1502.37 (0.269 sec)\n",
            "train - step 16459: loss = 1838.27 (0.269 sec)\n",
            "train - step 16460: loss = 1766.43 (0.267 sec)\n",
            "train - step 16461: loss = 2001.84 (0.261 sec)\n",
            "train - step 16462: loss = 1490.26 (0.262 sec)\n",
            "train - step 16463: loss = 1715.98 (0.261 sec)\n",
            "train - step 16464: loss = 2234.04 (0.268 sec)\n",
            "train - step 16465: loss = 1830.59 (0.267 sec)\n",
            "train - step 16466: loss = 1769.11 (0.268 sec)\n",
            "train - step 16467: loss = 524.35 (0.266 sec)\n",
            "train - step 16468: loss = 1954.54 (0.281 sec)\n",
            "train - step 16469: loss = 1662.13 (0.940 sec)\n",
            "train - step 16470: loss = 2156.46 (0.263 sec)\n",
            "train - step 16471: loss = 1620.95 (0.262 sec)\n",
            "train - step 16472: loss = 1824.80 (0.260 sec)\n",
            "train - step 16473: loss = 1657.95 (0.268 sec)\n",
            "train - step 16474: loss = 1094.52 (0.274 sec)\n",
            "train - step 16475: loss = 1674.55 (0.265 sec)\n",
            "train - step 16476: loss = 1872.53 (0.264 sec)\n",
            "train - step 16477: loss = 1873.66 (0.269 sec)\n",
            "train - step 16478: loss = 2151.43 (0.263 sec)\n",
            "train - step 16479: loss = 1430.11 (0.267 sec)\n",
            "train - step 16480: loss = 1570.69 (0.263 sec)\n",
            "train - step 16481: loss = 2478.45 (0.265 sec)\n",
            "train - step 16482: loss = 1348.48 (0.267 sec)\n",
            "train - step 16483: loss = 1989.48 (0.266 sec)\n",
            "train - step 16484: loss = 2223.48 (0.261 sec)\n",
            "train - step 16485: loss = 1549.08 (0.268 sec)\n",
            "train - step 16486: loss = 2443.74 (0.263 sec)\n",
            "train - step 16487: loss = 2066.77 (0.265 sec)\n",
            "train - step 16488: loss = 1764.84 (0.260 sec)\n",
            "train - step 16489: loss = 1763.12 (0.276 sec)\n",
            "train - step 16490: loss = 2289.88 (0.267 sec)\n",
            "train - step 16491: loss = 2104.53 (0.263 sec)\n",
            "train - step 16492: loss = 1773.28 (0.277 sec)\n",
            "train - step 16493: loss = 2209.25 (0.270 sec)\n",
            "train - step 16494: loss = 2174.11 (0.261 sec)\n",
            "train - step 16495: loss = 1843.41 (0.265 sec)\n",
            "train - step 16496: loss = 1729.31 (0.264 sec)\n",
            "train - step 16497: loss = 1775.40 (0.274 sec)\n",
            "train - step 16498: loss = 1702.80 (0.270 sec)\n",
            "train - step 16499: loss = 2463.58 (0.262 sec)\n",
            "train - step 16500: loss = 1991.30 (0.278 sec)\n",
            "train - step 16501: loss = 1808.92 (0.265 sec)\n",
            "train - step 16502: loss = 1964.93 (0.266 sec)\n",
            "train - step 16503: loss = 1902.67 (0.265 sec)\n",
            "train - step 16504: loss = 1873.59 (0.271 sec)\n",
            "train - step 16505: loss = 2545.78 (0.264 sec)\n",
            "train - step 16506: loss = 2572.67 (0.254 sec)\n",
            "train - step 16507: loss = 1934.16 (0.270 sec)\n",
            "train - step 16508: loss = 2169.31 (0.264 sec)\n",
            "train - step 16509: loss = 2343.24 (0.268 sec)\n",
            "train - step 16510: loss = 1933.90 (0.271 sec)\n",
            "train - step 16511: loss = 2393.14 (0.258 sec)\n",
            "train - step 16512: loss = 2134.01 (0.276 sec)\n",
            "train - step 16513: loss = 1956.29 (0.260 sec)\n",
            "train - step 16514: loss = 2122.34 (0.264 sec)\n",
            "train - step 16515: loss = 1759.69 (0.269 sec)\n",
            "train - step 16516: loss = 1798.06 (0.269 sec)\n",
            "train - step 16517: loss = 1721.43 (0.267 sec)\n",
            "train - step 16518: loss = 1636.21 (0.272 sec)\n",
            "train - step 16519: loss = 2118.24 (0.267 sec)\n",
            "train - step 16520: loss = 2092.81 (0.262 sec)\n",
            "train - step 16521: loss = 2105.57 (0.263 sec)\n",
            "train - step 16522: loss = 1859.87 (0.268 sec)\n",
            "train - step 16523: loss = 2056.18 (0.270 sec)\n",
            "train - step 16524: loss = 1869.74 (0.268 sec)\n",
            "train - step 16525: loss = 1753.58 (0.262 sec)\n",
            "train - step 16526: loss = 1722.99 (0.266 sec)\n",
            "train - step 16527: loss = 1269.14 (0.279 sec)\n",
            "train - step 16528: loss = 1594.64 (0.261 sec)\n",
            "train - step 16529: loss = 1681.30 (0.262 sec)\n",
            "train - step 16530: loss = 1987.55 (0.267 sec)\n",
            "train - step 16531: loss = 2027.34 (0.267 sec)\n",
            "train - step 16532: loss = 1800.03 (0.271 sec)\n",
            "train - step 16533: loss = 2141.11 (0.265 sec)\n",
            "train - step 16534: loss = 1872.97 (0.267 sec)\n",
            "train - step 16535: loss = 2177.50 (0.267 sec)\n",
            "train - step 16536: loss = 1662.23 (0.263 sec)\n",
            "train - step 16537: loss = 2161.08 (0.270 sec)\n",
            "train - step 16538: loss = 1777.81 (0.267 sec)\n",
            "train - step 16539: loss = 1965.47 (0.270 sec)\n",
            "train - step 16540: loss = 1773.97 (0.267 sec)\n",
            "train - step 16541: loss = 2223.15 (0.261 sec)\n",
            "train - step 16542: loss = 1869.87 (0.267 sec)\n",
            "train - step 16543: loss = 1893.96 (0.272 sec)\n",
            "train - step 16544: loss = 1813.18 (0.268 sec)\n",
            "train - step 16545: loss = 1610.29 (0.268 sec)\n",
            "train - step 16546: loss = 1704.74 (0.289 sec)\n",
            "train - step 16547: loss = 1989.09 (0.266 sec)\n",
            "train - step 16548: loss = 1735.61 (0.273 sec)\n",
            "train - step 16549: loss = 1948.21 (0.276 sec)\n",
            "train - step 16550: loss = 1763.69 (0.279 sec)\n",
            "train - step 16551: loss = 1932.74 (0.276 sec)\n",
            "train - step 16552: loss = 2382.94 (0.269 sec)\n",
            "train - step 16553: loss = 1384.17 (0.272 sec)\n",
            "train - step 16554: loss = 1271.03 (0.289 sec)\n",
            "train - step 16555: loss = 2179.05 (0.274 sec)\n",
            "train - step 16556: loss = 1921.16 (0.272 sec)\n",
            "train - step 16557: loss = 2316.23 (0.291 sec)\n",
            "train - step 16558: loss = 2028.05 (0.269 sec)\n",
            "train - step 16559: loss = 2124.39 (0.278 sec)\n",
            "train - step 16560: loss = 1790.25 (0.285 sec)\n",
            "train - step 16561: loss = 2159.99 (0.287 sec)\n",
            "train - step 16562: loss = 1819.33 (0.275 sec)\n",
            "train - step 16563: loss = 1607.82 (0.278 sec)\n",
            "train - step 16564: loss = 1948.69 (0.278 sec)\n",
            "train - step 16565: loss = 2133.97 (0.283 sec)\n",
            "train - step 16566: loss = 2216.30 (0.273 sec)\n",
            "train - step 16567: loss = 1999.01 (0.266 sec)\n",
            "train - step 16568: loss = 1473.36 (0.282 sec)\n",
            "train - step 16569: loss = 1651.61 (0.289 sec)\n",
            "train - step 16570: loss = 1687.80 (0.289 sec)\n",
            "train - step 16571: loss = 2045.37 (0.279 sec)\n",
            "train - step 16572: loss = 1385.29 (0.283 sec)\n",
            "train - step 16573: loss = 2018.67 (0.272 sec)\n",
            "train - step 16574: loss = 2178.32 (0.278 sec)\n",
            "train - step 16575: loss = 1740.96 (0.280 sec)\n",
            "train - step 16576: loss = 1800.00 (0.287 sec)\n",
            "train - step 16577: loss = 1796.92 (0.279 sec)\n",
            "train - step 16578: loss = 1619.79 (0.285 sec)\n",
            "train - step 16579: loss = 1579.52 (0.278 sec)\n",
            "train - step 16580: loss = 2167.36 (0.279 sec)\n",
            "train - step 16581: loss = 1714.89 (0.276 sec)\n",
            "train - step 16582: loss = 2126.96 (0.280 sec)\n",
            "train - step 16583: loss = 1962.00 (0.265 sec)\n",
            "train - step 16584: loss = 1696.79 (0.269 sec)\n",
            "train - step 16585: loss = 1959.40 (0.267 sec)\n",
            "train - step 16586: loss = 1753.83 (0.271 sec)\n",
            "train - step 16587: loss = 1867.01 (0.274 sec)\n",
            "train - step 16588: loss = 2399.52 (0.273 sec)\n",
            "train - step 16589: loss = 2167.77 (0.269 sec)\n",
            "train - step 16590: loss = 1853.59 (0.264 sec)\n",
            "train - step 16591: loss = 1943.33 (0.274 sec)\n",
            "train - step 16592: loss = 1465.23 (0.274 sec)\n",
            "train - step 16593: loss = 1884.88 (0.277 sec)\n",
            "train - step 16594: loss = 2311.35 (0.278 sec)\n",
            "train - step 16595: loss = 1548.55 (0.295 sec)\n",
            "train - step 16596: loss = 2138.05 (0.265 sec)\n",
            "train - step 16597: loss = 2254.74 (0.262 sec)\n",
            "train - step 16598: loss = 1726.01 (0.265 sec)\n",
            "train - step 16599: loss = 1977.16 (0.257 sec)\n",
            "train - step 16600: loss = 568.40 (0.264 sec)\n",
            "train - step 16601: loss = 1713.83 (0.272 sec)\n",
            "train - step 16602: loss = 1906.46 (0.261 sec)\n",
            "train - step 16603: loss = 1311.95 (0.269 sec)\n",
            "train - step 16604: loss = 1913.75 (0.265 sec)\n",
            "train - step 16605: loss = 1811.14 (0.260 sec)\n",
            "train - step 16606: loss = 1738.22 (0.267 sec)\n",
            "train - step 16607: loss = 2081.08 (0.257 sec)\n",
            "train - step 16608: loss = 1949.73 (0.267 sec)\n",
            "train - step 16609: loss = 1862.44 (0.270 sec)\n",
            "train - step 16610: loss = 2388.59 (0.266 sec)\n",
            "train - step 16611: loss = 584.97 (0.261 sec)\n",
            "train - step 16612: loss = 1766.10 (0.267 sec)\n",
            "train - step 16613: loss = 1376.62 (0.269 sec)\n",
            "train - step 16614: loss = 1762.35 (0.265 sec)\n",
            "train - step 16615: loss = 1775.01 (0.269 sec)\n",
            "train - step 16616: loss = 2029.62 (0.277 sec)\n",
            "train - step 16617: loss = 2065.25 (0.262 sec)\n",
            "train - step 16618: loss = 1758.63 (0.269 sec)\n",
            "train - step 16619: loss = 1756.13 (0.265 sec)\n",
            "train - step 16620: loss = 2448.91 (0.266 sec)\n",
            "train - step 16621: loss = 2117.58 (0.273 sec)\n",
            "train - step 16622: loss = 2011.28 (0.272 sec)\n",
            "train - step 16623: loss = 1873.67 (0.267 sec)\n",
            "train - step 16624: loss = 1594.61 (0.271 sec)\n",
            "train - step 16625: loss = 1783.89 (0.271 sec)\n",
            "train - step 16626: loss = 1438.11 (0.267 sec)\n",
            "train - step 16627: loss = 1958.58 (0.273 sec)\n",
            "train - step 16628: loss = 2094.88 (0.276 sec)\n",
            "train - step 16629: loss = 1932.18 (0.277 sec)\n",
            "train - step 16630: loss = 1786.68 (0.283 sec)\n",
            "train - step 16631: loss = 1703.30 (0.288 sec)\n",
            "train - step 16632: loss = 2182.91 (0.290 sec)\n",
            "train - step 16633: loss = 1766.83 (0.294 sec)\n",
            "train - step 16634: loss = 2439.64 (0.273 sec)\n",
            "train - step 16635: loss = 2188.66 (0.279 sec)\n",
            "train - step 16636: loss = 1979.57 (0.308 sec)\n",
            "train - step 16637: loss = 1466.54 (0.279 sec)\n",
            "train - step 16638: loss = 1423.84 (0.273 sec)\n",
            "train - step 16639: loss = 1903.20 (0.283 sec)\n",
            "train - step 16640: loss = 1785.45 (0.295 sec)\n",
            "train - step 16641: loss = 1847.40 (0.289 sec)\n",
            "train - step 16642: loss = 2470.43 (0.270 sec)\n",
            "train - step 16643: loss = 1839.30 (0.291 sec)\n",
            "train - step 16644: loss = 1449.67 (0.271 sec)\n",
            "train - step 16645: loss = 1949.28 (0.279 sec)\n",
            "train - step 16646: loss = 1827.76 (0.268 sec)\n",
            "train - step 16647: loss = 1978.86 (0.297 sec)\n",
            "train - step 16648: loss = 1598.89 (0.270 sec)\n",
            "train - step 16649: loss = 2423.96 (0.282 sec)\n",
            "train - step 16650: loss = 1827.97 (0.278 sec)\n",
            "train - step 16651: loss = 1593.76 (0.283 sec)\n",
            "train - step 16652: loss = 1659.80 (0.276 sec)\n",
            "train - step 16653: loss = 2055.86 (0.275 sec)\n",
            "train - step 16654: loss = 1999.79 (0.283 sec)\n",
            "train - step 16655: loss = 1847.82 (0.294 sec)\n",
            "train - step 16656: loss = 2019.59 (0.267 sec)\n",
            "train - step 16657: loss = 1669.16 (0.275 sec)\n",
            "train - step 16658: loss = 1424.48 (0.274 sec)\n",
            "train - step 16659: loss = 1740.41 (0.286 sec)\n",
            "train - step 16660: loss = 1856.64 (0.276 sec)\n",
            "train - step 16661: loss = 1847.21 (0.273 sec)\n",
            "train - step 16662: loss = 1928.21 (0.283 sec)\n",
            "train - step 16663: loss = 1842.51 (0.268 sec)\n",
            "train - step 16664: loss = 1570.32 (0.265 sec)\n",
            "train - step 16665: loss = 1721.86 (0.267 sec)\n",
            "train - step 16666: loss = 1826.59 (0.280 sec)\n",
            "train - step 16667: loss = 1928.99 (0.271 sec)\n",
            "train - step 16668: loss = 2015.34 (0.271 sec)\n",
            "train - step 16669: loss = 2045.34 (0.265 sec)\n",
            "train - step 16670: loss = 1783.73 (0.265 sec)\n",
            "train - step 16671: loss = 1948.95 (0.266 sec)\n",
            "train - step 16672: loss = 1791.99 (0.266 sec)\n",
            "train - step 16673: loss = 2004.60 (0.275 sec)\n",
            "train - step 16674: loss = 1938.58 (0.268 sec)\n",
            "train - step 16675: loss = 1656.28 (0.267 sec)\n",
            "train - step 16676: loss = 1767.65 (0.267 sec)\n",
            "train - step 16677: loss = 1954.24 (0.269 sec)\n",
            "train - step 16678: loss = 1225.59 (0.259 sec)\n",
            "train - step 16679: loss = 2081.77 (0.263 sec)\n",
            "train - step 16680: loss = 2317.34 (0.266 sec)\n",
            "train - step 16681: loss = 1876.61 (0.270 sec)\n",
            "train - step 16682: loss = 1876.48 (0.262 sec)\n",
            "train - step 16683: loss = 2333.11 (0.258 sec)\n",
            "train - step 16684: loss = 2160.73 (0.266 sec)\n",
            "train - step 16685: loss = 1682.82 (0.279 sec)\n",
            "train - step 16686: loss = 1952.45 (0.256 sec)\n",
            "train - step 16687: loss = 2142.51 (0.265 sec)\n",
            "train - step 16688: loss = 1994.05 (0.268 sec)\n",
            "train - step 16689: loss = 2052.20 (0.973 sec)\n",
            "train - step 16690: loss = 1725.30 (0.264 sec)\n",
            "train - step 16691: loss = 2332.80 (0.261 sec)\n",
            "train - step 16692: loss = 1596.09 (0.263 sec)\n",
            "train - step 16693: loss = 2029.87 (0.275 sec)\n",
            "train - step 16694: loss = 2068.47 (0.268 sec)\n",
            "train - step 16695: loss = 1954.11 (0.264 sec)\n",
            "train - step 16696: loss = 1857.82 (0.278 sec)\n",
            "train - step 16697: loss = 1439.14 (0.274 sec)\n",
            "train - step 16698: loss = 2113.08 (0.261 sec)\n",
            "train - step 16699: loss = 1641.65 (0.273 sec)\n",
            "train - step 16700: loss = 1364.82 (0.265 sec)\n",
            "train - step 16701: loss = 1968.52 (0.272 sec)\n",
            "train - step 16702: loss = 2205.26 (0.278 sec)\n",
            "train - step 16703: loss = 2062.01 (0.265 sec)\n",
            "train - step 16704: loss = 2213.29 (0.263 sec)\n",
            "train - step 16705: loss = 1947.12 (0.268 sec)\n",
            "train - step 16706: loss = 2073.24 (0.262 sec)\n",
            "train - step 16707: loss = 1754.41 (0.275 sec)\n",
            "train - step 16708: loss = 1829.93 (0.273 sec)\n",
            "train - step 16709: loss = 2320.71 (0.267 sec)\n",
            "train - step 16710: loss = 2023.76 (0.266 sec)\n",
            "train - step 16711: loss = 1226.70 (0.271 sec)\n",
            "train - step 16712: loss = 1619.81 (0.265 sec)\n",
            "train - step 16713: loss = 2246.26 (0.281 sec)\n",
            "train - step 16714: loss = 1831.72 (0.265 sec)\n",
            "train - step 16715: loss = 2004.80 (0.279 sec)\n",
            "train - step 16716: loss = 1860.38 (0.270 sec)\n",
            "train - step 16717: loss = 1984.74 (0.292 sec)\n",
            "train - step 16718: loss = 2196.14 (0.277 sec)\n",
            "train - step 16719: loss = 2069.08 (0.269 sec)\n",
            "train - step 16720: loss = 1223.67 (0.280 sec)\n",
            "train - step 16721: loss = 2140.13 (0.282 sec)\n",
            "train - step 16722: loss = 1998.92 (0.265 sec)\n",
            "train - step 16723: loss = 1705.23 (0.285 sec)\n",
            "train - step 16724: loss = 1057.83 (0.278 sec)\n",
            "train - step 16725: loss = 1877.22 (0.275 sec)\n",
            "train - step 16726: loss = 1867.01 (0.265 sec)\n",
            "train - step 16727: loss = 1280.09 (0.274 sec)\n",
            "train - step 16728: loss = 1712.13 (0.274 sec)\n",
            "train - step 16729: loss = 1428.28 (0.271 sec)\n",
            "train - step 16730: loss = 1845.88 (0.267 sec)\n",
            "train - step 16731: loss = 1910.25 (0.273 sec)\n",
            "train - step 16732: loss = 1696.92 (0.273 sec)\n",
            "train - step 16733: loss = 1658.37 (0.259 sec)\n",
            "train - step 16734: loss = 2226.33 (0.271 sec)\n",
            "train - step 16735: loss = 1806.90 (0.267 sec)\n",
            "train - step 16736: loss = 1848.72 (0.274 sec)\n",
            "train - step 16737: loss = 1298.64 (0.269 sec)\n",
            "train - step 16738: loss = 2150.76 (0.274 sec)\n",
            "train - step 16739: loss = 1989.18 (0.269 sec)\n",
            "train - step 16740: loss = 2075.47 (0.270 sec)\n",
            "train - step 16741: loss = 1743.95 (0.267 sec)\n",
            "train - step 16742: loss = 2087.00 (0.270 sec)\n",
            "train - step 16743: loss = 498.88 (0.271 sec)\n",
            "train - step 16744: loss = 2043.39 (0.257 sec)\n",
            "train - step 16745: loss = 1760.10 (0.263 sec)\n",
            "train - step 16746: loss = 1942.93 (0.269 sec)\n",
            "train - step 16747: loss = 1738.94 (0.268 sec)\n",
            "train - step 16748: loss = 2052.94 (0.261 sec)\n",
            "train - step 16749: loss = 1415.00 (0.263 sec)\n",
            "train - step 16750: loss = 1724.23 (0.264 sec)\n",
            "train - step 16751: loss = 2030.91 (0.277 sec)\n",
            "train - step 16752: loss = 1991.94 (0.270 sec)\n",
            "train - step 16753: loss = 2153.65 (0.273 sec)\n",
            "train - step 16754: loss = 1514.80 (0.271 sec)\n",
            "train - step 16755: loss = 1778.19 (0.272 sec)\n",
            "train - step 16756: loss = 1505.94 (0.271 sec)\n",
            "train - step 16757: loss = 2280.61 (0.262 sec)\n",
            "train - step 16758: loss = 1918.33 (0.265 sec)\n",
            "train - step 16759: loss = 1459.91 (0.278 sec)\n",
            "train - step 16760: loss = 1830.79 (0.278 sec)\n",
            "train - step 16761: loss = 2392.50 (0.261 sec)\n",
            "train - step 16762: loss = 2051.45 (0.278 sec)\n",
            "train - step 16763: loss = 1630.10 (0.270 sec)\n",
            "train - step 16764: loss = 1746.94 (0.265 sec)\n",
            "train - step 16765: loss = 1274.40 (0.268 sec)\n",
            "train - step 16766: loss = 1578.95 (0.264 sec)\n",
            "train - step 16767: loss = 1942.16 (0.267 sec)\n",
            "train - step 16768: loss = 2154.12 (0.261 sec)\n",
            "train - step 16769: loss = 1560.84 (0.282 sec)\n",
            "train - step 16770: loss = 546.36 (0.266 sec)\n",
            "train - step 16771: loss = 1822.64 (0.264 sec)\n",
            "train - step 16772: loss = 1746.09 (0.267 sec)\n",
            "train - step 16773: loss = 1810.19 (0.267 sec)\n",
            "train - step 16774: loss = 2273.82 (0.262 sec)\n",
            "train - step 16775: loss = 2085.37 (0.268 sec)\n",
            "train - step 16776: loss = 1926.39 (0.269 sec)\n",
            "train - step 16777: loss = 1746.89 (0.271 sec)\n",
            "train - step 16778: loss = 1281.93 (0.276 sec)\n",
            "train - step 16779: loss = 1867.62 (0.278 sec)\n",
            "train - step 16780: loss = 2066.77 (0.266 sec)\n",
            "train - step 16781: loss = 2242.78 (0.278 sec)\n",
            "train - step 16782: loss = 2206.73 (0.269 sec)\n",
            "train - step 16783: loss = 1782.10 (0.266 sec)\n",
            "train - step 16784: loss = 2318.02 (0.276 sec)\n",
            "train - step 16785: loss = 1739.68 (0.269 sec)\n",
            "train - step 16786: loss = 1831.44 (0.266 sec)\n",
            "train - step 16787: loss = 1772.62 (0.274 sec)\n",
            "train - step 16788: loss = 1925.35 (0.264 sec)\n",
            "train - step 16789: loss = 2083.80 (0.277 sec)\n",
            "train - step 16790: loss = 1785.42 (0.265 sec)\n",
            "train - step 16791: loss = 1658.50 (0.268 sec)\n",
            "train - step 16792: loss = 2005.59 (0.267 sec)\n",
            "train - step 16793: loss = 2079.18 (0.275 sec)\n",
            "train - step 16794: loss = 1524.17 (0.274 sec)\n",
            "train - step 16795: loss = 2005.34 (0.266 sec)\n",
            "train - step 16796: loss = 2117.91 (0.273 sec)\n",
            "train - step 16797: loss = 1309.55 (0.273 sec)\n",
            "train - step 16798: loss = 2420.60 (0.268 sec)\n",
            "train - step 16799: loss = 1968.87 (0.272 sec)\n",
            "train - step 16800: loss = 1885.12 (0.272 sec)\n",
            "train - step 16801: loss = 1714.58 (0.266 sec)\n",
            "train - step 16802: loss = 1681.92 (0.267 sec)\n",
            "train - step 16803: loss = 2100.63 (0.269 sec)\n",
            "train - step 16804: loss = 2102.41 (0.262 sec)\n",
            "train - step 16805: loss = 2132.05 (0.260 sec)\n",
            "train - step 16806: loss = 1971.96 (0.262 sec)\n",
            "train - step 16807: loss = 1803.46 (0.272 sec)\n",
            "train - step 16808: loss = 1619.23 (0.279 sec)\n",
            "train - step 16809: loss = 1949.38 (0.264 sec)\n",
            "train - step 16810: loss = 1595.18 (0.266 sec)\n",
            "train - step 16811: loss = 1296.88 (0.279 sec)\n",
            "train - step 16812: loss = 1681.52 (0.269 sec)\n",
            "train - step 16813: loss = 1595.95 (0.272 sec)\n",
            "train - step 16814: loss = 1925.50 (0.269 sec)\n",
            "train - step 16815: loss = 2190.63 (0.268 sec)\n",
            "train - step 16816: loss = 1994.43 (0.267 sec)\n",
            "train - step 16817: loss = 1588.49 (0.269 sec)\n",
            "train - step 16818: loss = 2042.02 (0.264 sec)\n",
            "train - step 16819: loss = 1785.67 (0.270 sec)\n",
            "train - step 16820: loss = 1958.83 (0.266 sec)\n",
            "train - step 16821: loss = 1918.32 (0.265 sec)\n",
            "train - step 16822: loss = 1965.92 (0.262 sec)\n",
            "train - step 16823: loss = 2021.71 (0.264 sec)\n",
            "train - step 16824: loss = 2235.14 (0.273 sec)\n",
            "train - step 16825: loss = 2048.62 (0.268 sec)\n",
            "train - step 16826: loss = 1866.83 (0.263 sec)\n",
            "train - step 16827: loss = 1762.78 (0.273 sec)\n",
            "train - step 16828: loss = 1736.73 (0.265 sec)\n",
            "train - step 16829: loss = 1761.72 (0.263 sec)\n",
            "train - step 16830: loss = 1753.46 (0.268 sec)\n",
            "train - step 16831: loss = 2300.77 (0.264 sec)\n",
            "train - step 16832: loss = 1598.18 (0.266 sec)\n",
            "train - step 16833: loss = 1546.50 (0.265 sec)\n",
            "train - step 16834: loss = 1814.82 (0.258 sec)\n",
            "train - step 16835: loss = 1861.25 (0.273 sec)\n",
            "train - step 16836: loss = 1798.63 (0.264 sec)\n",
            "train - step 16837: loss = 1754.28 (0.269 sec)\n",
            "train - step 16838: loss = 2029.93 (0.274 sec)\n",
            "train - step 16839: loss = 1682.47 (0.268 sec)\n",
            "train - step 16840: loss = 1707.29 (0.266 sec)\n",
            "train - step 16841: loss = 1942.44 (0.276 sec)\n",
            "train - step 16842: loss = 2175.70 (0.266 sec)\n",
            "train - step 16843: loss = 1828.45 (0.269 sec)\n",
            "train - step 16844: loss = 2053.06 (0.272 sec)\n",
            "train - step 16845: loss = 1581.52 (0.266 sec)\n",
            "train - step 16846: loss = 2001.21 (0.268 sec)\n",
            "train - step 16847: loss = 1759.57 (0.272 sec)\n",
            "train - step 16848: loss = 1430.49 (0.278 sec)\n",
            "train - step 16849: loss = 1988.60 (0.267 sec)\n",
            "train - step 16850: loss = 2266.68 (0.269 sec)\n",
            "train - step 16851: loss = 1861.29 (0.259 sec)\n",
            "train - step 16852: loss = 1784.82 (0.273 sec)\n",
            "train - step 16853: loss = 1660.73 (0.266 sec)\n",
            "train - step 16854: loss = 1746.57 (0.270 sec)\n",
            "train - step 16855: loss = 1733.97 (0.263 sec)\n",
            "train - step 16856: loss = 1992.57 (0.270 sec)\n",
            "train - step 16857: loss = 1901.51 (0.264 sec)\n",
            "train - step 16858: loss = 2396.24 (0.263 sec)\n",
            "train - step 16859: loss = 2316.44 (0.264 sec)\n",
            "train - step 16860: loss = 2536.22 (0.265 sec)\n",
            "train - step 16861: loss = 1758.05 (0.270 sec)\n",
            "train - step 16862: loss = 1968.36 (0.259 sec)\n",
            "train - step 16863: loss = 1766.38 (0.266 sec)\n",
            "train - step 16864: loss = 1488.66 (0.270 sec)\n",
            "train - step 16865: loss = 1824.57 (0.261 sec)\n",
            "train - step 16866: loss = 1313.09 (0.274 sec)\n",
            "train - step 16867: loss = 1734.82 (0.262 sec)\n",
            "train - step 16868: loss = 2215.91 (0.270 sec)\n",
            "train - step 16869: loss = 1527.28 (0.272 sec)\n",
            "train - step 16870: loss = 1733.53 (0.268 sec)\n",
            "train - step 16871: loss = 1990.87 (0.269 sec)\n",
            "train - step 16872: loss = 2241.59 (0.266 sec)\n",
            "train - step 16873: loss = 1916.95 (0.266 sec)\n",
            "train - step 16874: loss = 2086.47 (0.258 sec)\n",
            "train - step 16875: loss = 1101.57 (0.270 sec)\n",
            "train - step 16876: loss = 1906.53 (0.265 sec)\n",
            "train - step 16877: loss = 1691.81 (0.262 sec)\n",
            "train - step 16878: loss = 1918.78 (0.267 sec)\n",
            "train - step 16879: loss = 1604.79 (0.268 sec)\n",
            "train - step 16880: loss = 1817.96 (0.270 sec)\n",
            "train - step 16881: loss = 1188.54 (0.270 sec)\n",
            "train - step 16882: loss = 1599.39 (0.265 sec)\n",
            "train - step 16883: loss = 1595.82 (0.267 sec)\n",
            "train - step 16884: loss = 2023.74 (0.275 sec)\n",
            "train - step 16885: loss = 2488.82 (0.274 sec)\n",
            "train - step 16886: loss = 1957.17 (0.261 sec)\n",
            "train - step 16887: loss = 1970.88 (0.263 sec)\n",
            "train - step 16888: loss = 854.24 (0.272 sec)\n",
            "train - step 16889: loss = 2130.81 (0.274 sec)\n",
            "train - step 16890: loss = 1483.84 (0.261 sec)\n",
            "train - step 16891: loss = 1701.15 (0.267 sec)\n",
            "train - step 16892: loss = 1685.50 (0.267 sec)\n",
            "train - step 16893: loss = 1991.27 (0.264 sec)\n",
            "train - step 16894: loss = 1828.17 (0.268 sec)\n",
            "train - step 16895: loss = 2074.00 (0.268 sec)\n",
            "train - step 16896: loss = 2362.29 (0.266 sec)\n",
            "train - step 16897: loss = 2096.22 (0.268 sec)\n",
            "train - step 16898: loss = 2033.49 (0.263 sec)\n",
            "train - step 16899: loss = 2090.13 (0.269 sec)\n",
            "train - step 16900: loss = 1878.07 (0.268 sec)\n",
            "train - step 16901: loss = 2191.63 (0.264 sec)\n",
            "train - step 16902: loss = 2156.55 (0.267 sec)\n",
            "train - step 16903: loss = 1967.86 (0.268 sec)\n",
            "train - step 16904: loss = 2278.99 (0.271 sec)\n",
            "train - step 16905: loss = 1613.82 (0.266 sec)\n",
            "train - step 16906: loss = 1442.27 (0.255 sec)\n",
            "train - step 16907: loss = 2659.65 (0.262 sec)\n",
            "train - step 16908: loss = 2081.03 (0.269 sec)\n",
            "train - step 16909: loss = 1875.44 (0.263 sec)\n",
            "train - step 16910: loss = 1796.41 (0.261 sec)\n",
            "train - step 16911: loss = 1697.07 (0.954 sec)\n",
            "train - step 16912: loss = 2038.09 (0.259 sec)\n",
            "train - step 16913: loss = 1542.56 (0.272 sec)\n",
            "train - step 16914: loss = 1227.00 (0.262 sec)\n",
            "train - step 16915: loss = 1963.61 (0.261 sec)\n",
            "train - step 16916: loss = 2073.99 (0.271 sec)\n",
            "train - step 16917: loss = 548.46 (0.272 sec)\n",
            "train - step 16918: loss = 1991.12 (0.267 sec)\n",
            "train - step 16919: loss = 2081.57 (0.267 sec)\n",
            "train - step 16920: loss = 1562.40 (0.282 sec)\n",
            "train - step 16921: loss = 2022.60 (0.270 sec)\n",
            "train - step 16922: loss = 1847.57 (0.276 sec)\n",
            "train - step 16923: loss = 1806.25 (0.277 sec)\n",
            "train - step 16924: loss = 1829.95 (0.277 sec)\n",
            "train - step 16925: loss = 2174.90 (0.280 sec)\n",
            "train - step 16926: loss = 2126.05 (0.277 sec)\n",
            "train - step 16927: loss = 2153.55 (0.281 sec)\n",
            "train - step 16928: loss = 1853.53 (0.280 sec)\n",
            "train - step 16929: loss = 1945.86 (0.275 sec)\n",
            "train - step 16930: loss = 1668.66 (0.279 sec)\n",
            "train - step 16931: loss = 1761.11 (0.275 sec)\n",
            "train - step 16932: loss = 1117.71 (0.285 sec)\n",
            "train - step 16933: loss = 1774.51 (0.270 sec)\n",
            "train - step 16934: loss = 1554.65 (0.276 sec)\n",
            "train - step 16935: loss = 1832.32 (0.271 sec)\n",
            "train - step 16936: loss = 2021.41 (0.282 sec)\n",
            "train - step 16937: loss = 1834.01 (0.277 sec)\n",
            "train - step 16938: loss = 1934.30 (0.278 sec)\n",
            "train - step 16939: loss = 1926.98 (0.277 sec)\n",
            "train - step 16940: loss = 1680.28 (0.276 sec)\n",
            "train - step 16941: loss = 1992.16 (0.280 sec)\n",
            "train - step 16942: loss = 1836.52 (0.273 sec)\n",
            "train - step 16943: loss = 1634.32 (0.287 sec)\n",
            "train - step 16944: loss = 1897.28 (0.277 sec)\n",
            "train - step 16945: loss = 1726.42 (0.275 sec)\n",
            "train - step 16946: loss = 2291.86 (0.271 sec)\n",
            "train - step 16947: loss = 2444.64 (0.270 sec)\n",
            "train - step 16948: loss = 1809.01 (0.276 sec)\n",
            "train - step 16949: loss = 1577.80 (0.277 sec)\n",
            "train - step 16950: loss = 1861.02 (0.276 sec)\n",
            "train - step 16951: loss = 1695.79 (0.274 sec)\n",
            "train - step 16952: loss = 1958.34 (0.277 sec)\n",
            "train - step 16953: loss = 2061.67 (0.277 sec)\n",
            "train - step 16954: loss = 1877.53 (0.279 sec)\n",
            "train - step 16955: loss = 2258.40 (0.265 sec)\n",
            "train - step 16956: loss = 1782.73 (0.267 sec)\n",
            "train - step 16957: loss = 2261.67 (0.261 sec)\n",
            "train - step 16958: loss = 2125.19 (0.269 sec)\n",
            "train - step 16959: loss = 1654.07 (0.271 sec)\n",
            "train - step 16960: loss = 2113.00 (0.268 sec)\n",
            "train - step 16961: loss = 1692.32 (0.265 sec)\n",
            "train - step 16962: loss = 2205.57 (0.271 sec)\n",
            "train - step 16963: loss = 1548.21 (0.263 sec)\n",
            "train - step 16964: loss = 1614.38 (0.263 sec)\n",
            "train - step 16965: loss = 2256.77 (0.269 sec)\n",
            "train - step 16966: loss = 1879.01 (0.265 sec)\n",
            "train - step 16967: loss = 2084.74 (0.269 sec)\n",
            "train - step 16968: loss = 1243.10 (0.264 sec)\n",
            "train - step 16969: loss = 1888.79 (0.272 sec)\n",
            "train - step 16970: loss = 2098.32 (0.265 sec)\n",
            "train - step 16971: loss = 1522.67 (0.265 sec)\n",
            "train - step 16972: loss = 1702.77 (0.264 sec)\n",
            "train - step 16973: loss = 2040.96 (0.267 sec)\n",
            "train - step 16974: loss = 1650.83 (0.270 sec)\n",
            "train - step 16975: loss = 1666.06 (0.271 sec)\n",
            "train - step 16976: loss = 2219.55 (0.267 sec)\n",
            "train - step 16977: loss = 1782.06 (0.264 sec)\n",
            "train - step 16978: loss = 1815.94 (0.264 sec)\n",
            "train - step 16979: loss = 2111.71 (0.269 sec)\n",
            "train - step 16980: loss = 2030.12 (0.272 sec)\n",
            "train - step 16981: loss = 1654.87 (0.266 sec)\n",
            "train - step 16982: loss = 1518.52 (0.275 sec)\n",
            "train - step 16983: loss = 1734.20 (0.268 sec)\n",
            "train - step 16984: loss = 1727.96 (0.262 sec)\n",
            "train - step 16985: loss = 1947.24 (0.271 sec)\n",
            "train - step 16986: loss = 1916.08 (0.264 sec)\n",
            "train - step 16987: loss = 2409.20 (0.258 sec)\n",
            "train - step 16988: loss = 2140.27 (0.263 sec)\n",
            "train - step 16989: loss = 1952.32 (0.263 sec)\n",
            "train - step 16990: loss = 2169.15 (0.268 sec)\n",
            "train - step 16991: loss = 2008.35 (0.270 sec)\n",
            "train - step 16992: loss = 1973.12 (0.261 sec)\n",
            "train - step 16993: loss = 1713.05 (0.265 sec)\n",
            "train - step 16994: loss = 1923.86 (0.268 sec)\n",
            "train - step 16995: loss = 1663.14 (0.263 sec)\n",
            "train - step 16996: loss = 1608.19 (0.268 sec)\n",
            "train - step 16997: loss = 1737.59 (0.268 sec)\n",
            "train - step 16998: loss = 1815.89 (0.261 sec)\n",
            "train - step 16999: loss = 2165.76 (0.268 sec)\n",
            "train - step 17000: loss = 2035.18 (0.263 sec)\n",
            "train - step 17001: loss = 1467.74 (0.277 sec)\n",
            "train - step 17002: loss = 1832.08 (0.272 sec)\n",
            "train - step 17003: loss = 1803.96 (0.267 sec)\n",
            "train - step 17004: loss = 1970.61 (0.273 sec)\n",
            "train - step 17005: loss = 1745.70 (0.264 sec)\n",
            "train - step 17006: loss = 1556.70 (0.262 sec)\n",
            "train - step 17007: loss = 1884.03 (0.266 sec)\n",
            "train - step 17008: loss = 1711.73 (0.277 sec)\n",
            "train - step 17009: loss = 1680.80 (0.270 sec)\n",
            "train - step 17010: loss = 1796.23 (0.274 sec)\n",
            "train - step 17011: loss = 1834.67 (0.261 sec)\n",
            "train - step 17012: loss = 1685.44 (0.264 sec)\n",
            "train - step 17013: loss = 1637.15 (0.267 sec)\n",
            "train - step 17014: loss = 1797.99 (0.270 sec)\n",
            "train - step 17015: loss = 1881.32 (0.265 sec)\n",
            "train - step 17016: loss = 1681.02 (0.271 sec)\n",
            "train - step 17017: loss = 1570.03 (0.263 sec)\n",
            "train - step 17018: loss = 1220.85 (0.269 sec)\n",
            "train - step 17019: loss = 1301.79 (0.268 sec)\n",
            "train - step 17020: loss = 1872.90 (0.264 sec)\n",
            "train - step 17021: loss = 1794.49 (0.266 sec)\n",
            "train - step 17022: loss = 2315.91 (0.265 sec)\n",
            "train - step 17023: loss = 1714.31 (0.265 sec)\n",
            "train - step 17024: loss = 1657.73 (0.270 sec)\n",
            "train - step 17025: loss = 2019.00 (0.259 sec)\n",
            "train - step 17026: loss = 1947.02 (0.266 sec)\n",
            "train - step 17027: loss = 2120.03 (0.270 sec)\n",
            "train - step 17028: loss = 2301.16 (0.265 sec)\n",
            "train - step 17029: loss = 2039.23 (0.264 sec)\n",
            "train - step 17030: loss = 1621.16 (0.258 sec)\n",
            "train - step 17031: loss = 1701.27 (0.269 sec)\n",
            "train - step 17032: loss = 2051.24 (0.269 sec)\n",
            "train - step 17033: loss = 2023.05 (0.268 sec)\n",
            "train - step 17034: loss = 1822.74 (0.264 sec)\n",
            "train - step 17035: loss = 1708.90 (0.260 sec)\n",
            "train - step 17036: loss = 1770.87 (0.261 sec)\n",
            "train - step 17037: loss = 2144.92 (0.260 sec)\n",
            "train - step 17038: loss = 1646.12 (0.264 sec)\n",
            "train - step 17039: loss = 2132.12 (0.280 sec)\n",
            "train - step 17040: loss = 1926.06 (0.258 sec)\n",
            "train - step 17041: loss = 1912.14 (0.265 sec)\n",
            "train - step 17042: loss = 1743.41 (0.265 sec)\n",
            "train - step 17043: loss = 1793.17 (0.268 sec)\n",
            "train - step 17044: loss = 1818.96 (0.260 sec)\n",
            "train - step 17045: loss = 1704.47 (0.270 sec)\n",
            "train - step 17046: loss = 1706.05 (0.265 sec)\n",
            "train - step 17047: loss = 2076.14 (0.277 sec)\n",
            "train - step 17048: loss = 1865.96 (0.264 sec)\n",
            "train - step 17049: loss = 1597.00 (0.260 sec)\n",
            "train - step 17050: loss = 1596.73 (0.273 sec)\n",
            "train - step 17051: loss = 2052.91 (0.270 sec)\n",
            "train - step 17052: loss = 1817.35 (0.260 sec)\n",
            "train - step 17053: loss = 1488.35 (0.269 sec)\n",
            "train - step 17054: loss = 1762.63 (0.267 sec)\n",
            "train - step 17055: loss = 1204.13 (0.269 sec)\n",
            "train - step 17056: loss = 1465.16 (0.264 sec)\n",
            "train - step 17057: loss = 1888.32 (0.266 sec)\n",
            "train - step 17058: loss = 2050.93 (0.268 sec)\n",
            "train - step 17059: loss = 1819.20 (0.264 sec)\n",
            "train - step 17060: loss = 2047.48 (0.263 sec)\n",
            "train - step 17061: loss = 1740.16 (0.267 sec)\n",
            "train - step 17062: loss = 2172.47 (0.262 sec)\n",
            "train - step 17063: loss = 1658.18 (0.269 sec)\n",
            "train - step 17064: loss = 1912.43 (0.265 sec)\n",
            "train - step 17065: loss = 1645.84 (0.271 sec)\n",
            "train - step 17066: loss = 2157.13 (0.276 sec)\n",
            "train - step 17067: loss = 2037.83 (0.270 sec)\n",
            "train - step 17068: loss = 1857.27 (0.274 sec)\n",
            "train - step 17069: loss = 1849.87 (0.271 sec)\n",
            "train - step 17070: loss = 1957.48 (0.275 sec)\n",
            "train - step 17071: loss = 1631.88 (0.263 sec)\n",
            "train - step 17072: loss = 1785.30 (0.262 sec)\n",
            "train - step 17073: loss = 1760.94 (0.266 sec)\n",
            "train - step 17074: loss = 2243.46 (0.267 sec)\n",
            "train - step 17075: loss = 2124.23 (0.268 sec)\n",
            "train - step 17076: loss = 1746.23 (0.270 sec)\n",
            "train - step 17077: loss = 1608.62 (0.260 sec)\n",
            "train - step 17078: loss = 1910.70 (0.267 sec)\n",
            "train - step 17079: loss = 1803.65 (0.270 sec)\n",
            "train - step 17080: loss = 1811.70 (0.261 sec)\n",
            "train - step 17081: loss = 1607.15 (0.266 sec)\n",
            "train - step 17082: loss = 1859.22 (0.264 sec)\n",
            "train - step 17083: loss = 1741.34 (0.261 sec)\n",
            "train - step 17084: loss = 1866.49 (0.270 sec)\n",
            "train - step 17085: loss = 2130.84 (0.273 sec)\n",
            "train - step 17086: loss = 1796.51 (0.264 sec)\n",
            "train - step 17087: loss = 1883.45 (0.268 sec)\n",
            "train - step 17088: loss = 2100.04 (0.275 sec)\n",
            "train - step 17089: loss = 505.42 (0.271 sec)\n",
            "train - step 17090: loss = 1272.24 (0.274 sec)\n",
            "train - step 17091: loss = 2106.69 (0.267 sec)\n",
            "train - step 17092: loss = 1761.08 (0.267 sec)\n",
            "train - step 17093: loss = 1788.02 (0.272 sec)\n",
            "train - step 17094: loss = 1862.05 (0.259 sec)\n",
            "train - step 17095: loss = 1972.95 (0.273 sec)\n",
            "train - step 17096: loss = 1808.12 (0.266 sec)\n",
            "train - step 17097: loss = 2156.09 (0.268 sec)\n",
            "train - step 17098: loss = 2114.28 (0.263 sec)\n",
            "train - step 17099: loss = 2137.33 (0.257 sec)\n",
            "train - step 17100: loss = 1908.85 (0.268 sec)\n",
            "train - step 17101: loss = 1661.36 (0.270 sec)\n",
            "train - step 17102: loss = 537.48 (0.248 sec)\n",
            "train - step 17103: loss = 1848.23 (0.282 sec)\n",
            "train - step 17104: loss = 2221.08 (0.257 sec)\n",
            "train - step 17105: loss = 2317.16 (0.264 sec)\n",
            "train - step 17106: loss = 1894.71 (0.272 sec)\n",
            "train - step 17107: loss = 1782.01 (0.264 sec)\n",
            "train - step 17108: loss = 1796.94 (0.265 sec)\n",
            "train - step 17109: loss = 1393.01 (0.268 sec)\n",
            "train - step 17110: loss = 2264.17 (0.262 sec)\n",
            "train - step 17111: loss = 1951.01 (0.269 sec)\n",
            "train - step 17112: loss = 1867.70 (0.261 sec)\n",
            "train - step 17113: loss = 1908.74 (0.270 sec)\n",
            "train - step 17114: loss = 1568.70 (0.273 sec)\n",
            "train - step 17115: loss = 1832.99 (0.269 sec)\n",
            "train - step 17116: loss = 1904.44 (0.285 sec)\n",
            "train - step 17117: loss = 2082.29 (0.279 sec)\n",
            "train - step 17118: loss = 2282.33 (0.301 sec)\n",
            "train - step 17119: loss = 2028.33 (0.279 sec)\n",
            "train - step 17120: loss = 1916.16 (0.272 sec)\n",
            "train - step 17121: loss = 1785.32 (0.273 sec)\n",
            "train - step 17122: loss = 2298.08 (0.281 sec)\n",
            "train - step 17123: loss = 2280.29 (0.271 sec)\n",
            "train - step 17124: loss = 1738.25 (0.272 sec)\n",
            "train - step 17125: loss = 2341.70 (0.269 sec)\n",
            "train - step 17126: loss = 1885.04 (0.267 sec)\n",
            "train - step 17127: loss = 1896.58 (0.267 sec)\n",
            "train - step 17128: loss = 1981.90 (0.276 sec)\n",
            "train - step 17129: loss = 1603.93 (0.269 sec)\n",
            "train - step 17130: loss = 2437.86 (0.268 sec)\n",
            "train - step 17131: loss = 1569.33 (0.297 sec)\n",
            "train - step 17132: loss = 1826.89 (0.265 sec)\n",
            "train - step 17133: loss = 2452.90 (1.027 sec)\n",
            "train - step 17134: loss = 2146.05 (0.279 sec)\n",
            "train - step 17135: loss = 2097.43 (0.264 sec)\n",
            "train - step 17136: loss = 1930.82 (0.276 sec)\n",
            "train - step 17137: loss = 1856.82 (0.272 sec)\n",
            "train - step 17138: loss = 2122.99 (0.275 sec)\n",
            "train - step 17139: loss = 2189.04 (0.266 sec)\n",
            "train - step 17140: loss = 1880.35 (0.267 sec)\n",
            "train - step 17141: loss = 1848.64 (0.269 sec)\n",
            "train - step 17142: loss = 1979.73 (0.272 sec)\n",
            "train - step 17143: loss = 1702.08 (0.272 sec)\n",
            "train - step 17144: loss = 1979.25 (0.262 sec)\n",
            "train - step 17145: loss = 2183.90 (0.272 sec)\n",
            "train - step 17146: loss = 1695.66 (0.260 sec)\n",
            "train - step 17147: loss = 1978.76 (0.281 sec)\n",
            "train - step 17148: loss = 2355.62 (0.262 sec)\n",
            "train - step 17149: loss = 1866.78 (0.277 sec)\n",
            "train - step 17150: loss = 1868.22 (0.276 sec)\n",
            "train - step 17151: loss = 1702.46 (0.264 sec)\n",
            "train - step 17152: loss = 1671.90 (0.263 sec)\n",
            "train - step 17153: loss = 2000.06 (0.260 sec)\n",
            "train - step 17154: loss = 1623.89 (0.271 sec)\n",
            "train - step 17155: loss = 1692.18 (0.270 sec)\n",
            "train - step 17156: loss = 1750.96 (0.266 sec)\n",
            "train - step 17157: loss = 1601.82 (0.262 sec)\n",
            "train - step 17158: loss = 1637.28 (0.266 sec)\n",
            "train - step 17159: loss = 1577.36 (0.275 sec)\n",
            "train - step 17160: loss = 1871.64 (0.277 sec)\n",
            "train - step 17161: loss = 1718.08 (0.263 sec)\n",
            "train - step 17162: loss = 1798.91 (0.263 sec)\n",
            "train - step 17163: loss = 2026.72 (0.266 sec)\n",
            "train - step 17164: loss = 2452.44 (0.266 sec)\n",
            "train - step 17165: loss = 1670.22 (0.266 sec)\n",
            "train - step 17166: loss = 1508.08 (0.285 sec)\n",
            "train - step 17167: loss = 2267.22 (0.261 sec)\n",
            "train - step 17168: loss = 1684.35 (0.270 sec)\n",
            "train - step 17169: loss = 1937.72 (0.258 sec)\n",
            "train - step 17170: loss = 2069.90 (0.270 sec)\n",
            "train - step 17171: loss = 1664.86 (0.260 sec)\n",
            "train - step 17172: loss = 1726.69 (0.275 sec)\n",
            "train - step 17173: loss = 1701.64 (0.263 sec)\n",
            "train - step 17174: loss = 1986.85 (0.270 sec)\n",
            "train - step 17175: loss = 1911.85 (0.270 sec)\n",
            "train - step 17176: loss = 1821.49 (0.267 sec)\n",
            "train - step 17177: loss = 1839.10 (0.262 sec)\n",
            "train - step 17178: loss = 1513.93 (0.284 sec)\n",
            "train - step 17179: loss = 2249.88 (0.536 sec)\n",
            "train - step 17180: loss = 1740.60 (0.279 sec)\n",
            "train - step 17181: loss = 1942.30 (0.259 sec)\n",
            "train - step 17182: loss = 1960.65 (0.268 sec)\n",
            "train - step 17183: loss = 1897.37 (0.279 sec)\n",
            "train - step 17184: loss = 1514.28 (0.266 sec)\n",
            "train - step 17185: loss = 1974.84 (0.267 sec)\n",
            "train - step 17186: loss = 1781.21 (0.275 sec)\n",
            "train - step 17187: loss = 2063.75 (0.271 sec)\n",
            "train - step 17188: loss = 2230.44 (0.295 sec)\n",
            "train - step 17189: loss = 1804.23 (0.262 sec)\n",
            "train - step 17190: loss = 1895.78 (0.266 sec)\n",
            "train - step 17191: loss = 1539.66 (0.271 sec)\n",
            "train - step 17192: loss = 1844.18 (0.273 sec)\n",
            "train - step 17193: loss = 2458.25 (0.266 sec)\n",
            "train - step 17194: loss = 1989.14 (0.290 sec)\n",
            "train - step 17195: loss = 1521.10 (0.266 sec)\n",
            "train - step 17196: loss = 1439.98 (0.265 sec)\n",
            "train - step 17197: loss = 1975.71 (0.266 sec)\n",
            "train - step 17198: loss = 1644.46 (0.270 sec)\n",
            "train - step 17199: loss = 1525.58 (0.270 sec)\n",
            "train - step 17200: loss = 1620.02 (0.264 sec)\n",
            "train - step 17201: loss = 1711.62 (0.268 sec)\n",
            "train - step 17202: loss = 2373.54 (0.268 sec)\n",
            "train - step 17203: loss = 1756.73 (0.262 sec)\n",
            "train - step 17204: loss = 2101.30 (0.278 sec)\n",
            "train - step 17205: loss = 1518.61 (0.298 sec)\n",
            "train - step 17206: loss = 1735.62 (0.279 sec)\n",
            "train - step 17207: loss = 1637.77 (0.285 sec)\n",
            "train - step 17208: loss = 1647.01 (0.284 sec)\n",
            "train - step 17209: loss = 1953.36 (0.275 sec)\n",
            "train - step 17210: loss = 2777.93 (0.276 sec)\n",
            "train - step 17211: loss = 2386.07 (0.276 sec)\n",
            "train - step 17212: loss = 1640.67 (0.281 sec)\n",
            "train - step 17213: loss = 2135.86 (0.280 sec)\n",
            "train - step 17214: loss = 1996.42 (0.276 sec)\n",
            "train - step 17215: loss = 1994.59 (0.279 sec)\n",
            "train - step 17216: loss = 1811.37 (0.281 sec)\n",
            "train - step 17217: loss = 1936.02 (0.271 sec)\n",
            "train - step 17218: loss = 2253.21 (0.285 sec)\n",
            "train - step 17219: loss = 2347.07 (0.271 sec)\n",
            "train - step 17220: loss = 2298.62 (0.284 sec)\n",
            "train - step 17221: loss = 1706.81 (0.272 sec)\n",
            "train - step 17222: loss = 2263.83 (0.286 sec)\n",
            "train - step 17223: loss = 2308.08 (0.268 sec)\n",
            "train - step 17224: loss = 1773.17 (0.267 sec)\n",
            "train - step 17225: loss = 1675.12 (0.282 sec)\n",
            "train - step 17226: loss = 1772.12 (0.284 sec)\n",
            "train - step 17227: loss = 1559.45 (0.273 sec)\n",
            "train - step 17228: loss = 1787.28 (0.276 sec)\n",
            "train - step 17229: loss = 2324.03 (0.276 sec)\n",
            "train - step 17230: loss = 2130.85 (0.271 sec)\n",
            "train - step 17231: loss = 1721.56 (0.282 sec)\n",
            "train - step 17232: loss = 1771.35 (0.278 sec)\n",
            "train - step 17233: loss = 1421.47 (0.287 sec)\n",
            "train - step 17234: loss = 2051.92 (0.272 sec)\n",
            "train - step 17235: loss = 2131.10 (0.274 sec)\n",
            "train - step 17236: loss = 1590.30 (0.281 sec)\n",
            "train - step 17237: loss = 1771.46 (0.290 sec)\n",
            "train - step 17238: loss = 2011.17 (0.278 sec)\n",
            "train - step 17239: loss = 1827.07 (0.275 sec)\n",
            "train - step 17240: loss = 1957.11 (0.276 sec)\n",
            "train - step 17241: loss = 1827.98 (0.266 sec)\n",
            "train - step 17242: loss = 1530.49 (0.280 sec)\n",
            "train - step 17243: loss = 2093.38 (0.273 sec)\n",
            "train - step 17244: loss = 2020.09 (0.300 sec)\n",
            "train - step 17245: loss = 1628.56 (0.284 sec)\n",
            "train - step 17246: loss = 1641.45 (0.269 sec)\n",
            "train - step 17247: loss = 1526.94 (0.279 sec)\n",
            "train - step 17248: loss = 1853.78 (0.272 sec)\n",
            "train - step 17249: loss = 1965.03 (0.279 sec)\n",
            "train - step 17250: loss = 2052.70 (0.267 sec)\n",
            "train - step 17251: loss = 1475.41 (0.271 sec)\n",
            "train - step 17252: loss = 1426.67 (0.274 sec)\n",
            "train - step 17253: loss = 2344.08 (0.264 sec)\n",
            "train - step 17254: loss = 1626.93 (0.273 sec)\n",
            "train - step 17255: loss = 1956.48 (0.265 sec)\n",
            "train - step 17256: loss = 2326.67 (0.265 sec)\n",
            "train - step 17257: loss = 1629.16 (0.261 sec)\n",
            "train - step 17258: loss = 1875.02 (0.271 sec)\n",
            "train - step 17259: loss = 1698.07 (0.262 sec)\n",
            "train - step 17260: loss = 2053.35 (0.270 sec)\n",
            "train - step 17261: loss = 1275.39 (0.267 sec)\n",
            "train - step 17262: loss = 1798.58 (0.271 sec)\n",
            "train - step 17263: loss = 1882.79 (0.269 sec)\n",
            "train - step 17264: loss = 1959.06 (0.267 sec)\n",
            "train - step 17265: loss = 1726.74 (0.264 sec)\n",
            "train - step 17266: loss = 1895.25 (0.272 sec)\n",
            "train - step 17267: loss = 1677.34 (0.273 sec)\n",
            "train - step 17268: loss = 1802.72 (0.268 sec)\n",
            "train - step 17269: loss = 1956.71 (0.264 sec)\n",
            "train - step 17270: loss = 1620.23 (0.263 sec)\n",
            "train - step 17271: loss = 1700.18 (0.262 sec)\n",
            "train - step 17272: loss = 1381.18 (0.266 sec)\n",
            "train - step 17273: loss = 1857.07 (0.262 sec)\n",
            "train - step 17274: loss = 2057.59 (0.272 sec)\n",
            "train - step 17275: loss = 1941.84 (0.271 sec)\n",
            "train - step 17276: loss = 2009.45 (0.270 sec)\n",
            "train - step 17277: loss = 1735.57 (0.266 sec)\n",
            "train - step 17278: loss = 555.05 (0.268 sec)\n",
            "train - step 17279: loss = 1513.40 (0.266 sec)\n",
            "train - step 17280: loss = 1818.31 (0.270 sec)\n",
            "train - step 17281: loss = 1846.52 (0.267 sec)\n",
            "train - step 17282: loss = 1874.75 (0.267 sec)\n",
            "train - step 17283: loss = 2087.55 (0.264 sec)\n",
            "train - step 17284: loss = 1937.76 (0.281 sec)\n",
            "train - step 17285: loss = 1428.41 (0.264 sec)\n",
            "train - step 17286: loss = 1737.32 (0.269 sec)\n",
            "train - step 17287: loss = 1758.31 (0.266 sec)\n",
            "train - step 17288: loss = 1956.44 (0.272 sec)\n",
            "train - step 17289: loss = 1478.32 (0.271 sec)\n",
            "train - step 17290: loss = 1464.32 (0.267 sec)\n",
            "train - step 17291: loss = 1912.80 (0.262 sec)\n",
            "train - step 17292: loss = 1602.60 (0.268 sec)\n",
            "train - step 17293: loss = 1720.55 (0.269 sec)\n",
            "train - step 17294: loss = 1595.72 (0.267 sec)\n",
            "train - step 17295: loss = 1711.56 (0.270 sec)\n",
            "train - step 17296: loss = 1499.59 (0.269 sec)\n",
            "train - step 17297: loss = 1782.34 (0.268 sec)\n",
            "train - step 17298: loss = 1535.89 (0.270 sec)\n",
            "train - step 17299: loss = 2383.65 (0.259 sec)\n",
            "train - step 17300: loss = 2003.66 (0.266 sec)\n",
            "train - step 17301: loss = 1878.81 (0.266 sec)\n",
            "train - step 17302: loss = 2280.82 (0.262 sec)\n",
            "train - step 17303: loss = 2016.17 (0.272 sec)\n",
            "train - step 17304: loss = 1805.89 (0.260 sec)\n",
            "train - step 17305: loss = 1690.48 (0.262 sec)\n",
            "train - step 17306: loss = 1767.37 (0.273 sec)\n",
            "train - step 17307: loss = 1650.12 (0.265 sec)\n",
            "train - step 17308: loss = 2142.87 (0.266 sec)\n",
            "train - step 17309: loss = 1958.91 (0.275 sec)\n",
            "train - step 17310: loss = 2228.27 (0.266 sec)\n",
            "train - step 17311: loss = 1855.70 (0.261 sec)\n",
            "train - step 17312: loss = 2010.78 (0.265 sec)\n",
            "train - step 17313: loss = 1849.13 (0.265 sec)\n",
            "train - step 17314: loss = 526.21 (0.263 sec)\n",
            "train - step 17315: loss = 1759.51 (0.270 sec)\n",
            "train - step 17316: loss = 2231.07 (0.258 sec)\n",
            "train - step 17317: loss = 2200.13 (0.265 sec)\n",
            "train - step 17318: loss = 1589.57 (0.264 sec)\n",
            "train - step 17319: loss = 1543.02 (0.267 sec)\n",
            "train - step 17320: loss = 1519.25 (0.266 sec)\n",
            "train - step 17321: loss = 1821.72 (0.269 sec)\n",
            "train - step 17322: loss = 1869.52 (0.270 sec)\n",
            "train - step 17323: loss = 1819.26 (0.270 sec)\n",
            "train - step 17324: loss = 2469.39 (0.261 sec)\n",
            "train - step 17325: loss = 1758.17 (0.267 sec)\n",
            "train - step 17326: loss = 1599.52 (0.265 sec)\n",
            "train - step 17327: loss = 1898.74 (0.260 sec)\n",
            "train - step 17328: loss = 1663.77 (0.272 sec)\n",
            "train - step 17329: loss = 1788.08 (0.262 sec)\n",
            "train - step 17330: loss = 1602.13 (0.270 sec)\n",
            "train - step 17331: loss = 2176.55 (0.263 sec)\n",
            "train - step 17332: loss = 2075.00 (0.263 sec)\n",
            "train - step 17333: loss = 1773.74 (0.273 sec)\n",
            "train - step 17334: loss = 1663.60 (0.265 sec)\n",
            "train - step 17335: loss = 1459.71 (0.275 sec)\n",
            "train - step 17336: loss = 1467.15 (0.267 sec)\n",
            "train - step 17337: loss = 1791.35 (0.267 sec)\n",
            "train - step 17338: loss = 2367.19 (0.262 sec)\n",
            "train - step 17339: loss = 1976.36 (0.265 sec)\n",
            "train - step 17340: loss = 2170.48 (0.267 sec)\n",
            "train - step 17341: loss = 2033.91 (0.267 sec)\n",
            "train - step 17342: loss = 1412.15 (0.265 sec)\n",
            "train - step 17343: loss = 545.83 (0.251 sec)\n",
            "train - step 17344: loss = 1373.47 (0.273 sec)\n",
            "train - step 17345: loss = 1951.38 (0.268 sec)\n",
            "train - step 17346: loss = 1727.59 (0.268 sec)\n",
            "train - step 17347: loss = 2306.39 (0.260 sec)\n",
            "train - step 17348: loss = 1683.07 (0.268 sec)\n",
            "train - step 17349: loss = 1802.32 (0.281 sec)\n",
            "train - step 17350: loss = 1623.12 (0.264 sec)\n",
            "train - step 17351: loss = 1002.05 (0.254 sec)\n",
            "train - step 17352: loss = 1603.94 (1.031 sec)\n",
            "train - step 17353: loss = 1900.57 (0.269 sec)\n",
            "train - step 17354: loss = 1806.60 (0.269 sec)\n",
            "train - step 17355: loss = 1897.81 (0.257 sec)\n",
            "train - step 17356: loss = 1857.47 (0.270 sec)\n",
            "train - step 17357: loss = 2006.30 (0.266 sec)\n",
            "train - step 17358: loss = 2036.60 (0.263 sec)\n",
            "train - step 17359: loss = 2232.44 (0.269 sec)\n",
            "train - step 17360: loss = 1789.33 (0.276 sec)\n",
            "train - step 17361: loss = 1689.00 (0.266 sec)\n",
            "train - step 17362: loss = 1546.62 (0.264 sec)\n",
            "train - step 17363: loss = 1454.36 (0.268 sec)\n",
            "train - step 17364: loss = 1891.29 (0.275 sec)\n",
            "train - step 17365: loss = 1730.59 (0.265 sec)\n",
            "train - step 17366: loss = 2023.82 (0.264 sec)\n",
            "train - step 17367: loss = 1694.42 (0.266 sec)\n",
            "train - step 17368: loss = 1896.50 (0.274 sec)\n",
            "train - step 17369: loss = 1765.44 (0.264 sec)\n",
            "train - step 17370: loss = 1593.66 (0.274 sec)\n",
            "train - step 17371: loss = 2425.02 (0.260 sec)\n",
            "train - step 17372: loss = 1479.13 (0.284 sec)\n",
            "train - step 17373: loss = 1737.39 (0.266 sec)\n",
            "train - step 17374: loss = 2483.59 (0.259 sec)\n",
            "train - step 17375: loss = 2046.57 (0.266 sec)\n",
            "train - step 17376: loss = 1877.33 (0.275 sec)\n",
            "train - step 17377: loss = 1963.34 (0.261 sec)\n",
            "train - step 17378: loss = 1593.49 (0.265 sec)\n",
            "train - step 17379: loss = 1650.04 (0.269 sec)\n",
            "train - step 17380: loss = 2221.15 (0.266 sec)\n",
            "train - step 17381: loss = 1731.64 (0.264 sec)\n",
            "train - step 17382: loss = 1794.68 (0.267 sec)\n",
            "train - step 17383: loss = 2143.58 (0.257 sec)\n",
            "train - step 17384: loss = 1801.92 (0.269 sec)\n",
            "train - step 17385: loss = 1919.52 (0.267 sec)\n",
            "train - step 17386: loss = 1740.23 (0.264 sec)\n",
            "train - step 17387: loss = 1636.90 (0.279 sec)\n",
            "train - step 17388: loss = 2010.93 (0.268 sec)\n",
            "train - step 17389: loss = 1789.40 (0.265 sec)\n",
            "train - step 17390: loss = 2118.31 (0.262 sec)\n",
            "train - step 17391: loss = 1587.27 (0.274 sec)\n",
            "train - step 17392: loss = 1302.92 (0.264 sec)\n",
            "train - step 17393: loss = 1622.84 (0.270 sec)\n",
            "train - step 17394: loss = 2242.76 (0.270 sec)\n",
            "train - step 17395: loss = 1683.51 (0.268 sec)\n",
            "train - step 17396: loss = 1497.68 (0.269 sec)\n",
            "train - step 17397: loss = 2247.59 (0.264 sec)\n",
            "train - step 17398: loss = 1971.39 (0.267 sec)\n",
            "train - step 17399: loss = 1960.13 (0.285 sec)\n",
            "train - step 17400: loss = 1574.54 (0.269 sec)\n",
            "train - step 17401: loss = 1752.13 (0.268 sec)\n",
            "train - step 17402: loss = 1841.55 (0.268 sec)\n",
            "train - step 17403: loss = 1170.71 (0.263 sec)\n",
            "train - step 17404: loss = 1603.27 (0.270 sec)\n",
            "train - step 17405: loss = 1812.16 (0.269 sec)\n",
            "train - step 17406: loss = 2098.36 (0.259 sec)\n",
            "train - step 17407: loss = 2217.39 (0.265 sec)\n",
            "train - step 17408: loss = 1496.90 (0.265 sec)\n",
            "train - step 17409: loss = 2339.68 (0.261 sec)\n",
            "train - step 17410: loss = 1688.96 (0.275 sec)\n",
            "train - step 17411: loss = 1813.68 (0.265 sec)\n",
            "train - step 17412: loss = 1915.40 (0.268 sec)\n",
            "train - step 17413: loss = 2207.87 (0.261 sec)\n",
            "train - step 17414: loss = 2064.36 (0.277 sec)\n",
            "train - step 17415: loss = 1933.52 (0.265 sec)\n",
            "train - step 17416: loss = 1538.31 (0.270 sec)\n",
            "train - step 17417: loss = 1913.69 (0.260 sec)\n",
            "train - step 17418: loss = 1811.54 (0.284 sec)\n",
            "train - step 17419: loss = 1683.87 (0.272 sec)\n",
            "train - step 17420: loss = 1868.15 (0.263 sec)\n",
            "train - step 17421: loss = 1748.01 (0.256 sec)\n",
            "train - step 17422: loss = 1426.11 (0.264 sec)\n",
            "train - step 17423: loss = 2086.60 (0.263 sec)\n",
            "train - step 17424: loss = 1548.05 (0.275 sec)\n",
            "train - step 17425: loss = 1643.90 (0.261 sec)\n",
            "train - step 17426: loss = 1515.52 (0.271 sec)\n",
            "train - step 17427: loss = 1875.59 (0.263 sec)\n",
            "train - step 17428: loss = 1955.98 (0.260 sec)\n",
            "train - step 17429: loss = 1700.62 (0.258 sec)\n",
            "train - step 17430: loss = 1921.02 (0.265 sec)\n",
            "train - step 17431: loss = 1677.96 (0.263 sec)\n",
            "train - step 17432: loss = 2237.60 (0.265 sec)\n",
            "train - step 17433: loss = 1867.78 (0.266 sec)\n",
            "train - step 17434: loss = 1723.59 (0.263 sec)\n",
            "train - step 17435: loss = 1517.49 (0.265 sec)\n",
            "train - step 17436: loss = 2036.99 (0.267 sec)\n",
            "train - step 17437: loss = 1982.71 (0.267 sec)\n",
            "train - step 17438: loss = 2028.89 (0.261 sec)\n",
            "train - step 17439: loss = 1950.89 (0.271 sec)\n",
            "train - step 17440: loss = 1843.12 (0.264 sec)\n",
            "train - step 17441: loss = 1863.57 (0.276 sec)\n",
            "train - step 17442: loss = 1926.19 (0.267 sec)\n",
            "train - step 17443: loss = 2070.80 (0.269 sec)\n",
            "train - step 17444: loss = 2070.85 (0.273 sec)\n",
            "train - step 17445: loss = 1556.86 (0.267 sec)\n",
            "train - step 17446: loss = 1816.24 (0.267 sec)\n",
            "train - step 17447: loss = 1663.46 (0.260 sec)\n",
            "train - step 17448: loss = 1403.94 (0.270 sec)\n",
            "train - step 17449: loss = 1668.08 (0.270 sec)\n",
            "train - step 17450: loss = 1617.65 (0.267 sec)\n",
            "train - step 17451: loss = 1452.04 (0.260 sec)\n",
            "train - step 17452: loss = 1845.30 (0.273 sec)\n",
            "train - step 17453: loss = 1573.03 (0.274 sec)\n",
            "train - step 17454: loss = 2040.39 (0.273 sec)\n",
            "train - step 17455: loss = 2171.82 (0.258 sec)\n",
            "train - step 17456: loss = 1659.06 (0.269 sec)\n",
            "train - step 17457: loss = 1944.99 (0.262 sec)\n",
            "train - step 17458: loss = 1968.99 (0.267 sec)\n",
            "train - step 17459: loss = 1503.16 (0.273 sec)\n",
            "train - step 17460: loss = 1562.64 (0.268 sec)\n",
            "train - step 17461: loss = 2060.66 (0.269 sec)\n",
            "train - step 17462: loss = 1972.32 (0.264 sec)\n",
            "train - step 17463: loss = 1863.86 (0.265 sec)\n",
            "train - step 17464: loss = 2498.71 (0.265 sec)\n",
            "train - step 17465: loss = 2131.11 (0.262 sec)\n",
            "train - step 17466: loss = 2054.97 (0.265 sec)\n",
            "train - step 17467: loss = 1848.33 (0.268 sec)\n",
            "train - step 17468: loss = 1995.37 (0.272 sec)\n",
            "train - step 17469: loss = 2020.81 (0.262 sec)\n",
            "train - step 17470: loss = 1880.94 (0.257 sec)\n",
            "train - step 17471: loss = 1809.31 (0.259 sec)\n",
            "train - step 17472: loss = 1879.54 (0.277 sec)\n",
            "train - step 17473: loss = 1721.58 (0.269 sec)\n",
            "train - step 17474: loss = 1855.39 (0.262 sec)\n",
            "train - step 17475: loss = 1843.04 (0.270 sec)\n",
            "train - step 17476: loss = 1711.39 (0.277 sec)\n",
            "train - step 17477: loss = 1470.23 (0.269 sec)\n",
            "train - step 17478: loss = 1735.73 (0.266 sec)\n",
            "train - step 17479: loss = 1579.70 (0.272 sec)\n",
            "train - step 17480: loss = 1652.29 (0.269 sec)\n",
            "train - step 17481: loss = 1739.08 (0.263 sec)\n",
            "train - step 17482: loss = 1843.47 (0.266 sec)\n",
            "train - step 17483: loss = 2683.30 (0.254 sec)\n",
            "train - step 17484: loss = 1872.35 (0.269 sec)\n",
            "train - step 17485: loss = 1809.31 (0.267 sec)\n",
            "train - step 17486: loss = 1270.25 (0.269 sec)\n",
            "train - step 17487: loss = 2080.76 (0.259 sec)\n",
            "train - step 17488: loss = 2042.31 (0.266 sec)\n",
            "train - step 17489: loss = 2493.00 (0.266 sec)\n",
            "train - step 17490: loss = 2310.64 (0.270 sec)\n",
            "train - step 17491: loss = 1886.64 (0.268 sec)\n",
            "train - step 17492: loss = 1947.19 (0.270 sec)\n",
            "train - step 17493: loss = 2032.04 (0.267 sec)\n",
            "train - step 17494: loss = 1763.69 (0.262 sec)\n",
            "train - step 17495: loss = 1726.27 (0.275 sec)\n",
            "train - step 17496: loss = 1717.33 (0.266 sec)\n",
            "train - step 17497: loss = 2047.73 (0.257 sec)\n",
            "train - step 17498: loss = 1919.83 (0.269 sec)\n",
            "train - step 17499: loss = 1766.74 (0.272 sec)\n",
            "train - step 17500: loss = 1910.01 (0.259 sec)\n",
            "train - step 17501: loss = 1663.56 (0.273 sec)\n",
            "train - step 17502: loss = 1697.05 (0.268 sec)\n",
            "train - step 17503: loss = 2281.03 (0.265 sec)\n",
            "train - step 17504: loss = 1899.14 (0.261 sec)\n",
            "train - step 17505: loss = 2219.81 (0.259 sec)\n",
            "train - step 17506: loss = 2182.22 (0.275 sec)\n",
            "train - step 17507: loss = 2082.30 (0.273 sec)\n",
            "train - step 17508: loss = 1573.96 (0.265 sec)\n",
            "train - step 17509: loss = 1930.48 (0.265 sec)\n",
            "train - step 17510: loss = 1666.37 (0.274 sec)\n",
            "train - step 17511: loss = 1597.08 (0.278 sec)\n",
            "train - step 17512: loss = 1737.93 (0.275 sec)\n",
            "train - step 17513: loss = 1706.01 (0.277 sec)\n",
            "train - step 17514: loss = 1958.12 (0.279 sec)\n",
            "train - step 17515: loss = 1765.71 (0.265 sec)\n",
            "train - step 17516: loss = 2034.56 (0.282 sec)\n",
            "train - step 17517: loss = 2042.46 (0.271 sec)\n",
            "train - step 17518: loss = 2035.99 (0.285 sec)\n",
            "train - step 17519: loss = 1993.64 (0.277 sec)\n",
            "train - step 17520: loss = 1876.40 (0.275 sec)\n",
            "train - step 17521: loss = 1452.02 (0.272 sec)\n",
            "train - step 17522: loss = 1804.95 (0.271 sec)\n",
            "train - step 17523: loss = 1811.18 (0.276 sec)\n",
            "train - step 17524: loss = 2050.09 (0.276 sec)\n",
            "train - step 17525: loss = 1821.79 (0.276 sec)\n",
            "train - step 17526: loss = 2168.96 (0.282 sec)\n",
            "train - step 17527: loss = 1842.11 (0.274 sec)\n",
            "train - step 17528: loss = 1876.47 (0.277 sec)\n",
            "train - step 17529: loss = 1738.69 (0.272 sec)\n",
            "train - step 17530: loss = 2553.24 (0.275 sec)\n",
            "train - step 17531: loss = 1750.75 (0.275 sec)\n",
            "train - step 17532: loss = 1991.59 (0.278 sec)\n",
            "train - step 17533: loss = 1834.16 (0.276 sec)\n",
            "train - step 17534: loss = 1938.29 (0.276 sec)\n",
            "train - step 17535: loss = 1923.91 (0.268 sec)\n",
            "train - step 17536: loss = 2231.01 (0.269 sec)\n",
            "train - step 17537: loss = 1669.35 (0.275 sec)\n",
            "train - step 17538: loss = 1795.90 (0.278 sec)\n",
            "train - step 17539: loss = 1981.84 (0.274 sec)\n",
            "train - step 17540: loss = 2626.31 (0.275 sec)\n",
            "train - step 17541: loss = 1785.33 (0.273 sec)\n",
            "train - step 17542: loss = 1934.14 (0.273 sec)\n",
            "train - step 17543: loss = 1570.60 (0.280 sec)\n",
            "train - step 17544: loss = 1605.08 (0.279 sec)\n",
            "train - step 17545: loss = 1745.62 (0.270 sec)\n",
            "train - step 17546: loss = 2381.53 (0.282 sec)\n",
            "train - step 17547: loss = 2031.01 (0.275 sec)\n",
            "train - step 17548: loss = 1676.86 (0.281 sec)\n",
            "train - step 17549: loss = 1959.06 (0.266 sec)\n",
            "train - step 17550: loss = 1658.10 (0.260 sec)\n",
            "train - step 17551: loss = 1749.99 (0.270 sec)\n",
            "train - step 17552: loss = 2210.77 (0.269 sec)\n",
            "train - step 17553: loss = 1715.30 (0.268 sec)\n",
            "train - step 17554: loss = 2209.85 (0.271 sec)\n",
            "train - step 17555: loss = 1896.14 (0.282 sec)\n",
            "train - step 17556: loss = 1864.39 (0.271 sec)\n",
            "train - step 17557: loss = 1517.66 (0.279 sec)\n",
            "train - step 17558: loss = 1695.44 (0.271 sec)\n",
            "train - step 17559: loss = 1750.25 (0.271 sec)\n",
            "train - step 17560: loss = 2294.59 (0.268 sec)\n",
            "train - step 17561: loss = 1946.40 (0.277 sec)\n",
            "train - step 17562: loss = 1671.90 (0.274 sec)\n",
            "train - step 17563: loss = 1722.81 (0.270 sec)\n",
            "train - step 17564: loss = 1556.67 (0.267 sec)\n",
            "train - step 17565: loss = 1953.88 (0.263 sec)\n",
            "train - step 17566: loss = 1536.78 (0.268 sec)\n",
            "train - step 17567: loss = 1958.07 (0.275 sec)\n",
            "train - step 17568: loss = 1966.76 (0.271 sec)\n",
            "train - step 17569: loss = 1508.28 (0.271 sec)\n",
            "train - step 17570: loss = 1846.71 (0.278 sec)\n",
            "train - step 17571: loss = 1236.49 (0.263 sec)\n",
            "train - step 17572: loss = 1752.29 (0.264 sec)\n",
            "train - step 17573: loss = 2057.96 (0.266 sec)\n",
            "train - step 17574: loss = 1522.37 (0.902 sec)\n",
            "train - step 17575: loss = 1717.90 (0.269 sec)\n",
            "train - step 17576: loss = 1902.05 (0.279 sec)\n",
            "train - step 17577: loss = 1655.15 (0.269 sec)\n",
            "train - step 17578: loss = 2010.43 (0.267 sec)\n",
            "train - step 17579: loss = 1758.42 (0.272 sec)\n",
            "train - step 17580: loss = 1906.58 (0.269 sec)\n",
            "train - step 17581: loss = 1683.70 (0.267 sec)\n",
            "train - step 17582: loss = 1742.52 (0.260 sec)\n",
            "train - step 17583: loss = 1736.36 (0.264 sec)\n",
            "train - step 17584: loss = 1711.47 (0.266 sec)\n",
            "train - step 17585: loss = 1585.32 (0.271 sec)\n",
            "train - step 17586: loss = 1973.94 (0.270 sec)\n",
            "train - step 17587: loss = 1689.71 (0.274 sec)\n",
            "train - step 17588: loss = 1477.27 (0.274 sec)\n",
            "train - step 17589: loss = 1826.45 (0.262 sec)\n",
            "train - step 17590: loss = 1351.52 (0.267 sec)\n",
            "train - step 17591: loss = 1733.00 (0.276 sec)\n",
            "train - step 17592: loss = 2238.26 (0.270 sec)\n",
            "train - step 17593: loss = 1219.51 (0.265 sec)\n",
            "train - step 17594: loss = 1630.71 (0.284 sec)\n",
            "train - step 17595: loss = 2288.87 (0.267 sec)\n",
            "train - step 17596: loss = 2065.48 (0.262 sec)\n",
            "train - step 17597: loss = 1511.36 (0.264 sec)\n",
            "train - step 17598: loss = 1835.40 (0.266 sec)\n",
            "train - step 17599: loss = 2372.41 (0.263 sec)\n",
            "train - step 17600: loss = 1859.31 (0.269 sec)\n",
            "train - step 17601: loss = 1787.30 (0.262 sec)\n",
            "train - step 17602: loss = 1792.69 (0.266 sec)\n",
            "train - step 17603: loss = 1846.53 (0.258 sec)\n",
            "train - step 17604: loss = 1715.51 (0.266 sec)\n",
            "train - step 17605: loss = 1562.87 (0.260 sec)\n",
            "train - step 17606: loss = 1748.46 (0.261 sec)\n",
            "train - step 17607: loss = 1711.48 (0.261 sec)\n",
            "train - step 17608: loss = 1880.14 (0.264 sec)\n",
            "train - step 17609: loss = 2224.40 (0.262 sec)\n",
            "train - step 17610: loss = 1854.89 (0.279 sec)\n",
            "train - step 17611: loss = 1801.54 (0.268 sec)\n",
            "train - step 17612: loss = 2124.00 (0.268 sec)\n",
            "train - step 17613: loss = 1939.91 (0.269 sec)\n",
            "train - step 17614: loss = 1737.46 (0.275 sec)\n",
            "train - step 17615: loss = 1416.49 (0.260 sec)\n",
            "train - step 17616: loss = 1349.12 (0.257 sec)\n",
            "train - step 17617: loss = 1823.23 (0.266 sec)\n",
            "train - step 17618: loss = 1792.86 (0.275 sec)\n",
            "train - step 17619: loss = 1775.40 (0.259 sec)\n",
            "train - step 17620: loss = 2430.55 (0.260 sec)\n",
            "train - step 17621: loss = 2024.59 (0.267 sec)\n",
            "train - step 17622: loss = 1622.72 (0.264 sec)\n",
            "train - step 17623: loss = 1591.49 (0.261 sec)\n",
            "train - step 17624: loss = 1713.94 (0.267 sec)\n",
            "train - step 17625: loss = 1883.02 (0.259 sec)\n",
            "train - step 17626: loss = 1643.01 (0.272 sec)\n",
            "train - step 17627: loss = 1579.14 (0.266 sec)\n",
            "train - step 17628: loss = 2027.35 (0.267 sec)\n",
            "train - step 17629: loss = 1991.88 (0.266 sec)\n",
            "train - step 17630: loss = 1680.98 (0.260 sec)\n",
            "train - step 17631: loss = 2287.99 (0.260 sec)\n",
            "train - step 17632: loss = 2122.60 (0.270 sec)\n",
            "train - step 17633: loss = 1855.28 (0.263 sec)\n",
            "train - step 17634: loss = 1481.05 (0.261 sec)\n",
            "train - step 17635: loss = 1801.51 (0.278 sec)\n",
            "train - step 17636: loss = 1693.13 (0.261 sec)\n",
            "train - step 17637: loss = 2004.99 (0.273 sec)\n",
            "train - step 17638: loss = 1766.56 (0.268 sec)\n",
            "train - step 17639: loss = 1313.32 (0.269 sec)\n",
            "train - step 17640: loss = 1682.36 (0.260 sec)\n",
            "train - step 17641: loss = 2070.12 (0.269 sec)\n",
            "train - step 17642: loss = 1958.77 (0.266 sec)\n",
            "train - step 17643: loss = 2518.32 (0.264 sec)\n",
            "train - step 17644: loss = 2011.07 (0.269 sec)\n",
            "train - step 17645: loss = 2374.84 (0.264 sec)\n",
            "train - step 17646: loss = 1954.79 (0.262 sec)\n",
            "train - step 17647: loss = 1629.73 (0.274 sec)\n",
            "train - step 17648: loss = 1917.57 (0.271 sec)\n",
            "train - step 17649: loss = 1796.96 (0.266 sec)\n",
            "train - step 17650: loss = 1621.28 (0.270 sec)\n",
            "train - step 17651: loss = 1665.68 (0.263 sec)\n",
            "train - step 17652: loss = 2421.67 (0.254 sec)\n",
            "train - step 17653: loss = 2007.31 (0.282 sec)\n",
            "train - step 17654: loss = 1556.46 (0.268 sec)\n",
            "train - step 17655: loss = 1711.42 (0.261 sec)\n",
            "train - step 17656: loss = 1825.19 (0.271 sec)\n",
            "train - step 17657: loss = 1809.32 (0.268 sec)\n",
            "train - step 17658: loss = 1784.54 (0.266 sec)\n",
            "train - step 17659: loss = 2154.53 (0.268 sec)\n",
            "train - step 17660: loss = 2001.27 (0.263 sec)\n",
            "train - step 17661: loss = 2212.68 (0.266 sec)\n",
            "train - step 17662: loss = 1721.51 (0.264 sec)\n",
            "train - step 17663: loss = 1975.53 (0.270 sec)\n",
            "train - step 17664: loss = 1970.84 (0.267 sec)\n",
            "train - step 17665: loss = 1736.61 (0.258 sec)\n",
            "train - step 17666: loss = 1818.16 (0.266 sec)\n",
            "train - step 17667: loss = 1488.02 (0.272 sec)\n",
            "train - step 17668: loss = 1705.84 (0.269 sec)\n",
            "train - step 17669: loss = 2188.80 (0.271 sec)\n",
            "train - step 17670: loss = 1480.44 (0.272 sec)\n",
            "train - step 17671: loss = 2011.42 (0.269 sec)\n",
            "train - step 17672: loss = 2621.70 (0.262 sec)\n",
            "train - step 17673: loss = 1974.33 (0.263 sec)\n",
            "train - step 17674: loss = 1818.63 (0.267 sec)\n",
            "train - step 17675: loss = 1492.20 (0.279 sec)\n",
            "train - step 17676: loss = 1786.67 (0.272 sec)\n",
            "train - step 17677: loss = 1926.44 (0.269 sec)\n",
            "train - step 17678: loss = 1662.40 (0.267 sec)\n",
            "train - step 17679: loss = 2410.16 (0.260 sec)\n",
            "train - step 17680: loss = 1538.35 (0.279 sec)\n",
            "train - step 17681: loss = 1947.12 (0.274 sec)\n",
            "train - step 17682: loss = 1641.50 (0.282 sec)\n",
            "train - step 17683: loss = 1795.56 (0.282 sec)\n",
            "train - step 17684: loss = 1617.78 (0.283 sec)\n",
            "train - step 17685: loss = 1622.52 (0.275 sec)\n",
            "train - step 17686: loss = 1732.16 (0.278 sec)\n",
            "train - step 17687: loss = 1653.07 (0.291 sec)\n",
            "train - step 17688: loss = 2160.73 (0.271 sec)\n",
            "train - step 17689: loss = 2026.27 (0.275 sec)\n",
            "train - step 17690: loss = 1300.60 (0.275 sec)\n",
            "train - step 17691: loss = 2064.73 (0.285 sec)\n",
            "train - step 17692: loss = 578.86 (0.273 sec)\n",
            "train - step 17693: loss = 1915.02 (0.287 sec)\n",
            "train - step 17694: loss = 1630.26 (0.281 sec)\n",
            "train - step 17695: loss = 1939.99 (0.271 sec)\n",
            "train - step 17696: loss = 1837.07 (0.283 sec)\n",
            "train - step 17697: loss = 1864.20 (0.283 sec)\n",
            "train - step 17698: loss = 1723.08 (0.286 sec)\n",
            "train - step 17699: loss = 1694.02 (0.276 sec)\n",
            "train - step 17700: loss = 1695.41 (0.277 sec)\n",
            "train - step 17701: loss = 2009.08 (0.281 sec)\n",
            "train - step 17702: loss = 2043.32 (0.286 sec)\n",
            "train - step 17703: loss = 1797.09 (0.276 sec)\n",
            "train - step 17704: loss = 1972.57 (0.286 sec)\n",
            "train - step 17705: loss = 1697.91 (0.281 sec)\n",
            "train - step 17706: loss = 1762.10 (0.278 sec)\n",
            "train - step 17707: loss = 1894.36 (0.278 sec)\n",
            "train - step 17708: loss = 1378.99 (0.283 sec)\n",
            "train - step 17709: loss = 1639.17 (0.276 sec)\n",
            "train - step 17710: loss = 1958.65 (0.280 sec)\n",
            "train - step 17711: loss = 2080.52 (0.273 sec)\n",
            "train - step 17712: loss = 1887.65 (0.282 sec)\n",
            "train - step 17713: loss = 2010.18 (0.278 sec)\n",
            "train - step 17714: loss = 1692.82 (0.285 sec)\n",
            "train - step 17715: loss = 1047.22 (0.288 sec)\n",
            "train - step 17716: loss = 1111.50 (0.281 sec)\n",
            "train - step 17717: loss = 2119.24 (0.274 sec)\n",
            "train - step 17718: loss = 1764.22 (0.281 sec)\n",
            "train - step 17719: loss = 1913.60 (0.266 sec)\n",
            "train - step 17720: loss = 1461.17 (0.273 sec)\n",
            "train - step 17721: loss = 1389.49 (0.259 sec)\n",
            "train - step 17722: loss = 1785.16 (0.274 sec)\n",
            "train - step 17723: loss = 1686.19 (0.265 sec)\n",
            "train - step 17724: loss = 1773.92 (0.274 sec)\n",
            "train - step 17725: loss = 2042.31 (0.262 sec)\n",
            "train - step 17726: loss = 1652.56 (0.273 sec)\n",
            "train - step 17727: loss = 2223.29 (0.268 sec)\n",
            "train - step 17728: loss = 2209.73 (0.263 sec)\n",
            "train - step 17729: loss = 1424.55 (0.259 sec)\n",
            "train - step 17730: loss = 1936.26 (0.263 sec)\n",
            "train - step 17731: loss = 1950.73 (0.263 sec)\n",
            "train - step 17732: loss = 1746.53 (0.267 sec)\n",
            "train - step 17733: loss = 1492.97 (0.266 sec)\n",
            "train - step 17734: loss = 1808.37 (0.266 sec)\n",
            "train - step 17735: loss = 1935.64 (0.270 sec)\n",
            "train - step 17736: loss = 1872.82 (0.265 sec)\n",
            "train - step 17737: loss = 2053.59 (0.270 sec)\n",
            "train - step 17738: loss = 2055.28 (0.272 sec)\n",
            "train - step 17739: loss = 1746.28 (0.266 sec)\n",
            "train - step 17740: loss = 2095.94 (0.264 sec)\n",
            "train - step 17741: loss = 1835.05 (0.270 sec)\n",
            "train - step 17742: loss = 1593.19 (0.265 sec)\n",
            "train - step 17743: loss = 1794.02 (0.279 sec)\n",
            "train - step 17744: loss = 1850.77 (0.265 sec)\n",
            "train - step 17745: loss = 1717.62 (0.265 sec)\n",
            "train - step 17746: loss = 1829.56 (0.271 sec)\n",
            "train - step 17747: loss = 1603.47 (0.274 sec)\n",
            "train - step 17748: loss = 1936.43 (0.284 sec)\n",
            "train - step 17749: loss = 1311.65 (0.269 sec)\n",
            "train - step 17750: loss = 2092.44 (0.270 sec)\n",
            "train - step 17751: loss = 1670.50 (0.272 sec)\n",
            "train - step 17752: loss = 1545.04 (0.265 sec)\n",
            "train - step 17753: loss = 1713.15 (0.268 sec)\n",
            "train - step 17754: loss = 1764.19 (0.286 sec)\n",
            "train - step 17755: loss = 2056.69 (0.267 sec)\n",
            "train - step 17756: loss = 1821.04 (0.266 sec)\n",
            "train - step 17757: loss = 1874.14 (0.265 sec)\n",
            "train - step 17758: loss = 1810.69 (0.276 sec)\n",
            "train - step 17759: loss = 1836.08 (0.268 sec)\n",
            "train - step 17760: loss = 1470.94 (0.270 sec)\n",
            "train - step 17761: loss = 2082.16 (0.262 sec)\n",
            "train - step 17762: loss = 1435.36 (0.272 sec)\n",
            "train - step 17763: loss = 1800.80 (0.271 sec)\n",
            "train - step 17764: loss = 1824.12 (0.273 sec)\n",
            "train - step 17765: loss = 1936.84 (0.265 sec)\n",
            "train - step 17766: loss = 1624.42 (0.272 sec)\n",
            "train - step 17767: loss = 2018.87 (0.260 sec)\n",
            "train - step 17768: loss = 1890.49 (0.273 sec)\n",
            "train - step 17769: loss = 1849.44 (0.268 sec)\n",
            "train - step 17770: loss = 1903.87 (0.273 sec)\n",
            "train - step 17771: loss = 2044.10 (0.267 sec)\n",
            "train - step 17772: loss = 1649.76 (0.275 sec)\n",
            "train - step 17773: loss = 2030.73 (0.272 sec)\n",
            "train - step 17774: loss = 1895.69 (0.275 sec)\n",
            "train - step 17775: loss = 1982.92 (0.266 sec)\n",
            "train - step 17776: loss = 1230.34 (0.261 sec)\n",
            "train - step 17777: loss = 2457.87 (0.260 sec)\n",
            "train - step 17778: loss = 1943.66 (0.257 sec)\n",
            "train - step 17779: loss = 1517.05 (0.269 sec)\n",
            "train - step 17780: loss = 1934.70 (0.265 sec)\n",
            "train - step 17781: loss = 597.85 (0.260 sec)\n",
            "train - step 17782: loss = 1870.39 (0.272 sec)\n",
            "train - step 17783: loss = 1708.25 (0.266 sec)\n",
            "train - step 17784: loss = 2189.10 (0.257 sec)\n",
            "train - step 17785: loss = 1350.79 (0.275 sec)\n",
            "train - step 17786: loss = 1906.09 (0.265 sec)\n",
            "train - step 17787: loss = 2008.82 (0.257 sec)\n",
            "train - step 17788: loss = 1888.50 (0.272 sec)\n",
            "train - step 17789: loss = 1337.08 (0.260 sec)\n",
            "train - step 17790: loss = 1890.04 (0.262 sec)\n",
            "train - step 17791: loss = 2144.84 (0.264 sec)\n",
            "train - step 17792: loss = 1618.10 (0.274 sec)\n",
            "train - step 17793: loss = 1783.95 (0.267 sec)\n",
            "train - step 17794: loss = 1821.04 (0.259 sec)\n",
            "train - step 17795: loss = 1822.44 (0.923 sec)\n",
            "train - step 17796: loss = 1667.66 (0.265 sec)\n",
            "train - step 17797: loss = 2201.67 (0.263 sec)\n",
            "train - step 17798: loss = 2009.35 (0.276 sec)\n",
            "train - step 17799: loss = 1852.01 (0.267 sec)\n",
            "train - step 17800: loss = 1767.18 (0.266 sec)\n",
            "train - step 17801: loss = 2094.52 (0.273 sec)\n",
            "train - step 17802: loss = 1561.38 (0.275 sec)\n",
            "train - step 17803: loss = 2174.18 (0.266 sec)\n",
            "train - step 17804: loss = 1991.24 (0.273 sec)\n",
            "train - step 17805: loss = 2307.73 (0.273 sec)\n",
            "train - step 17806: loss = 1576.03 (0.286 sec)\n",
            "train - step 17807: loss = 1838.26 (0.277 sec)\n",
            "train - step 17808: loss = 1801.70 (0.270 sec)\n",
            "train - step 17809: loss = 1720.03 (0.280 sec)\n",
            "train - step 17810: loss = 2048.71 (0.277 sec)\n",
            "train - step 17811: loss = 1728.38 (0.272 sec)\n",
            "train - step 17812: loss = 2518.83 (0.276 sec)\n",
            "train - step 17813: loss = 1484.71 (0.289 sec)\n",
            "train - step 17814: loss = 1613.73 (0.277 sec)\n",
            "train - step 17815: loss = 1950.78 (0.271 sec)\n",
            "train - step 17816: loss = 1888.44 (0.278 sec)\n",
            "train - step 17817: loss = 1878.65 (0.276 sec)\n",
            "train - step 17818: loss = 1788.95 (0.274 sec)\n",
            "train - step 17819: loss = 2713.15 (0.265 sec)\n",
            "train - step 17820: loss = 1311.81 (0.284 sec)\n",
            "train - step 17821: loss = 1815.47 (0.278 sec)\n",
            "train - step 17822: loss = 1969.72 (0.277 sec)\n",
            "train - step 17823: loss = 1559.20 (0.272 sec)\n",
            "train - step 17824: loss = 1533.31 (0.283 sec)\n",
            "train - step 17825: loss = 2024.04 (0.279 sec)\n",
            "train - step 17826: loss = 1995.92 (0.281 sec)\n",
            "train - step 17827: loss = 1987.88 (0.270 sec)\n",
            "train - step 17828: loss = 1747.80 (0.285 sec)\n",
            "train - step 17829: loss = 2205.51 (0.278 sec)\n",
            "train - step 17830: loss = 1513.58 (0.275 sec)\n",
            "train - step 17831: loss = 1689.78 (0.271 sec)\n",
            "train - step 17832: loss = 2353.54 (0.282 sec)\n",
            "train - step 17833: loss = 1921.06 (0.277 sec)\n",
            "train - step 17834: loss = 1852.94 (0.277 sec)\n",
            "train - step 17835: loss = 1885.84 (0.279 sec)\n",
            "train - step 17836: loss = 1716.63 (0.279 sec)\n",
            "train - step 17837: loss = 2365.40 (0.270 sec)\n",
            "train - step 17838: loss = 2109.65 (0.272 sec)\n",
            "train - step 17839: loss = 1333.92 (0.287 sec)\n",
            "train - step 17840: loss = 1862.39 (0.270 sec)\n",
            "train - step 17841: loss = 2026.92 (0.265 sec)\n",
            "train - step 17842: loss = 1735.83 (0.259 sec)\n",
            "train - step 17843: loss = 1818.21 (0.265 sec)\n",
            "train - step 17844: loss = 2179.87 (0.262 sec)\n",
            "train - step 17845: loss = 1592.17 (0.265 sec)\n",
            "train - step 17846: loss = 1771.97 (0.265 sec)\n",
            "train - step 17847: loss = 2096.86 (0.271 sec)\n",
            "train - step 17848: loss = 1957.62 (0.271 sec)\n",
            "train - step 17849: loss = 1756.62 (0.262 sec)\n",
            "train - step 17850: loss = 1865.29 (0.269 sec)\n",
            "train - step 17851: loss = 1868.63 (0.270 sec)\n",
            "train - step 17852: loss = 1786.65 (0.270 sec)\n",
            "train - step 17853: loss = 2324.28 (0.255 sec)\n",
            "train - step 17854: loss = 1836.23 (0.268 sec)\n",
            "train - step 17855: loss = 1529.25 (0.263 sec)\n",
            "train - step 17856: loss = 2027.42 (0.269 sec)\n",
            "train - step 17857: loss = 1953.26 (0.263 sec)\n",
            "train - step 17858: loss = 1829.32 (0.266 sec)\n",
            "train - step 17859: loss = 2208.94 (0.271 sec)\n",
            "train - step 17860: loss = 1627.28 (0.269 sec)\n",
            "train - step 17861: loss = 1670.86 (0.260 sec)\n",
            "train - step 17862: loss = 1361.79 (0.258 sec)\n",
            "train - step 17863: loss = 1767.82 (0.265 sec)\n",
            "train - step 17864: loss = 1757.42 (0.273 sec)\n",
            "train - step 17865: loss = 1741.80 (0.270 sec)\n",
            "train - step 17866: loss = 2235.79 (0.279 sec)\n",
            "train - step 17867: loss = 1821.20 (0.267 sec)\n",
            "train - step 17868: loss = 1593.82 (0.271 sec)\n",
            "train - step 17869: loss = 2061.55 (0.277 sec)\n",
            "train - step 17870: loss = 1626.06 (0.270 sec)\n",
            "train - step 17871: loss = 1896.15 (0.276 sec)\n",
            "train - step 17872: loss = 2097.75 (0.261 sec)\n",
            "train - step 17873: loss = 1228.89 (0.269 sec)\n",
            "train - step 17874: loss = 1589.56 (0.270 sec)\n",
            "train - step 17875: loss = 2128.61 (0.266 sec)\n",
            "train - step 17876: loss = 2188.83 (0.257 sec)\n",
            "train - step 17877: loss = 1848.36 (0.268 sec)\n",
            "train - step 17878: loss = 1977.89 (0.263 sec)\n",
            "train - step 17879: loss = 1624.68 (0.268 sec)\n",
            "train - step 17880: loss = 2160.42 (0.269 sec)\n",
            "train - step 17881: loss = 1759.41 (0.263 sec)\n",
            "train - step 17882: loss = 1659.80 (0.269 sec)\n",
            "train - step 17883: loss = 2363.39 (0.259 sec)\n",
            "train - step 17884: loss = 2487.46 (0.265 sec)\n",
            "train - step 17885: loss = 1582.39 (0.264 sec)\n",
            "train - step 17886: loss = 1890.47 (0.271 sec)\n",
            "train - step 17887: loss = 1515.59 (0.270 sec)\n",
            "train - step 17888: loss = 1521.27 (0.265 sec)\n",
            "train - step 17889: loss = 1235.80 (0.260 sec)\n",
            "train - step 17890: loss = 2044.30 (0.268 sec)\n",
            "train - step 17891: loss = 2220.21 (0.258 sec)\n",
            "train - step 17892: loss = 1695.91 (0.266 sec)\n",
            "train - step 17893: loss = 1898.91 (0.270 sec)\n",
            "train - step 17894: loss = 1116.40 (0.266 sec)\n",
            "train - step 17895: loss = 2039.89 (0.266 sec)\n",
            "train - step 17896: loss = 2491.62 (0.263 sec)\n",
            "train - step 17897: loss = 2090.37 (0.268 sec)\n",
            "train - step 17898: loss = 1461.83 (0.270 sec)\n",
            "train - step 17899: loss = 1773.73 (0.268 sec)\n",
            "train - step 17900: loss = 1730.09 (0.271 sec)\n",
            "train - step 17901: loss = 1587.84 (0.276 sec)\n",
            "train - step 17902: loss = 1737.58 (0.263 sec)\n",
            "train - step 17903: loss = 1966.18 (0.277 sec)\n",
            "train - step 17904: loss = 1784.61 (0.275 sec)\n",
            "train - step 17905: loss = 1778.21 (0.274 sec)\n",
            "train - step 17906: loss = 1687.51 (0.266 sec)\n",
            "train - step 17907: loss = 1441.92 (0.266 sec)\n",
            "train - step 17908: loss = 1722.20 (0.274 sec)\n",
            "train - step 17909: loss = 1781.51 (0.265 sec)\n",
            "train - step 17910: loss = 1739.50 (0.271 sec)\n",
            "train - step 17911: loss = 2028.95 (0.264 sec)\n",
            "train - step 17912: loss = 1750.49 (0.281 sec)\n",
            "train - step 17913: loss = 1646.02 (0.262 sec)\n",
            "train - step 17914: loss = 1759.61 (0.266 sec)\n",
            "train - step 17915: loss = 1813.52 (0.270 sec)\n",
            "train - step 17916: loss = 2010.41 (0.265 sec)\n",
            "train - step 17917: loss = 1604.99 (0.263 sec)\n",
            "train - step 17918: loss = 1679.55 (0.267 sec)\n",
            "train - step 17919: loss = 2312.70 (0.266 sec)\n",
            "train - step 17920: loss = 1844.89 (0.266 sec)\n",
            "train - step 17921: loss = 1889.69 (0.275 sec)\n",
            "train - step 17922: loss = 1957.88 (0.264 sec)\n",
            "train - step 17923: loss = 1577.71 (0.268 sec)\n",
            "train - step 17924: loss = 2311.00 (0.266 sec)\n",
            "train - step 17925: loss = 1819.83 (0.271 sec)\n",
            "train - step 17926: loss = 1981.33 (0.258 sec)\n",
            "train - step 17927: loss = 2003.02 (0.265 sec)\n",
            "train - step 17928: loss = 1824.92 (0.263 sec)\n",
            "train - step 17929: loss = 2100.15 (0.276 sec)\n",
            "train - step 17930: loss = 1701.31 (0.271 sec)\n",
            "train - step 17931: loss = 1732.28 (0.285 sec)\n",
            "train - step 17932: loss = 1868.78 (0.262 sec)\n",
            "train - step 17933: loss = 1750.65 (0.263 sec)\n",
            "train - step 17934: loss = 1990.88 (0.265 sec)\n",
            "train - step 17935: loss = 1518.06 (0.273 sec)\n",
            "train - step 17936: loss = 1884.58 (0.263 sec)\n",
            "train - step 17937: loss = 1653.70 (0.260 sec)\n",
            "train - step 17938: loss = 2348.43 (0.260 sec)\n",
            "train - step 17939: loss = 1720.29 (0.271 sec)\n",
            "train - step 17940: loss = 1909.93 (0.263 sec)\n",
            "train - step 17941: loss = 2466.15 (0.261 sec)\n",
            "train - step 17942: loss = 1928.43 (0.274 sec)\n",
            "train - step 17943: loss = 1912.38 (0.276 sec)\n",
            "train - step 17944: loss = 1755.41 (0.270 sec)\n",
            "train - step 17945: loss = 1926.05 (0.271 sec)\n",
            "train - step 17946: loss = 1649.53 (0.263 sec)\n",
            "train - step 17947: loss = 1777.64 (0.269 sec)\n",
            "train - step 17948: loss = 2264.62 (0.265 sec)\n",
            "train - step 17949: loss = 2167.70 (0.264 sec)\n",
            "train - step 17950: loss = 1918.08 (0.265 sec)\n",
            "train - step 17951: loss = 1976.12 (0.261 sec)\n",
            "train - step 17952: loss = 1508.41 (0.265 sec)\n",
            "train - step 17953: loss = 2138.82 (0.261 sec)\n",
            "train - step 17954: loss = 1861.59 (0.269 sec)\n",
            "train - step 17955: loss = 2381.08 (0.267 sec)\n",
            "train - step 17956: loss = 1946.72 (0.265 sec)\n",
            "train - step 17957: loss = 1752.20 (0.265 sec)\n",
            "train - step 17958: loss = 2100.00 (0.264 sec)\n",
            "train - step 17959: loss = 1968.30 (0.258 sec)\n",
            "train - step 17960: loss = 2257.61 (0.262 sec)\n",
            "train - step 17961: loss = 2185.23 (0.266 sec)\n",
            "train - step 17962: loss = 1927.54 (0.268 sec)\n",
            "train - step 17963: loss = 1656.45 (0.267 sec)\n",
            "train - step 17964: loss = 2102.79 (0.263 sec)\n",
            "train - step 17965: loss = 2081.64 (0.265 sec)\n",
            "train - step 17966: loss = 1755.36 (0.270 sec)\n",
            "train - step 17967: loss = 1633.22 (0.271 sec)\n",
            "train - step 17968: loss = 1750.56 (0.265 sec)\n",
            "train - step 17969: loss = 1210.82 (0.262 sec)\n",
            "train - step 17970: loss = 1649.26 (0.275 sec)\n",
            "train - step 17971: loss = 1939.59 (0.264 sec)\n",
            "train - step 17972: loss = 2188.47 (0.261 sec)\n",
            "train - step 17973: loss = 1804.49 (0.275 sec)\n",
            "train - step 17974: loss = 1625.05 (0.274 sec)\n",
            "train - step 17975: loss = 2260.87 (0.265 sec)\n",
            "train - step 17976: loss = 1682.03 (0.268 sec)\n",
            "train - step 17977: loss = 1711.90 (0.268 sec)\n",
            "train - step 17978: loss = 1325.09 (0.265 sec)\n",
            "train - step 17979: loss = 1861.67 (0.273 sec)\n",
            "train - step 17980: loss = 1869.61 (0.263 sec)\n",
            "train - step 17981: loss = 1808.36 (0.269 sec)\n",
            "train - step 17982: loss = 2098.29 (0.261 sec)\n",
            "train - step 17983: loss = 2184.21 (0.269 sec)\n",
            "train - step 17984: loss = 2428.40 (0.259 sec)\n",
            "train - step 17985: loss = 1883.50 (0.266 sec)\n",
            "train - step 17986: loss = 2181.50 (0.264 sec)\n",
            "train - step 17987: loss = 1974.72 (0.271 sec)\n",
            "train - step 17988: loss = 1536.00 (0.270 sec)\n",
            "train - step 17989: loss = 1853.65 (0.265 sec)\n",
            "train - step 17990: loss = 1735.79 (0.260 sec)\n",
            "train - step 17991: loss = 1769.63 (0.259 sec)\n",
            "train - step 17992: loss = 1977.52 (0.270 sec)\n",
            "train - step 17993: loss = 1634.08 (0.265 sec)\n",
            "train - step 17994: loss = 1838.61 (0.269 sec)\n",
            "train - step 17995: loss = 2197.11 (0.263 sec)\n",
            "train - step 17996: loss = 2075.95 (0.264 sec)\n",
            "train - step 17997: loss = 1576.72 (0.267 sec)\n",
            "train - step 17998: loss = 1604.19 (0.264 sec)\n",
            "train - step 17999: loss = 2057.15 (0.270 sec)\n",
            "train - step 18000: loss = 565.60 (0.260 sec)\n",
            "train - step 18001: loss = 2045.75 (0.263 sec)\n",
            "train - step 18002: loss = 2166.11 (0.263 sec)\n",
            "train - step 18003: loss = 2050.11 (0.256 sec)\n",
            "train - step 18004: loss = 2042.49 (0.265 sec)\n",
            "train - step 18005: loss = 1737.83 (0.264 sec)\n",
            "train - step 18006: loss = 1756.85 (0.269 sec)\n",
            "train - step 18007: loss = 1908.34 (0.269 sec)\n",
            "train - step 18008: loss = 2173.60 (0.269 sec)\n",
            "train - step 18009: loss = 2076.45 (0.262 sec)\n",
            "train - step 18010: loss = 2247.36 (0.265 sec)\n",
            "train - step 18011: loss = 1752.00 (0.259 sec)\n",
            "train - step 18012: loss = 1941.57 (0.262 sec)\n",
            "train - step 18013: loss = 1682.94 (0.278 sec)\n",
            "train - step 18014: loss = 1588.47 (0.268 sec)\n",
            "train - step 18015: loss = 2142.42 (0.264 sec)\n",
            "train - step 18016: loss = 1996.51 (0.268 sec)\n",
            "train - step 18017: loss = 2108.44 (0.898 sec)\n",
            "train - step 18018: loss = 2075.79 (0.278 sec)\n",
            "train - step 18019: loss = 1807.31 (0.266 sec)\n",
            "train - step 18020: loss = 2259.81 (0.261 sec)\n",
            "train - step 18021: loss = 1808.72 (0.260 sec)\n",
            "train - step 18022: loss = 2008.13 (0.268 sec)\n",
            "train - step 18023: loss = 1602.60 (0.265 sec)\n",
            "train - step 18024: loss = 1893.77 (0.262 sec)\n",
            "train - step 18025: loss = 1807.26 (0.259 sec)\n",
            "train - step 18026: loss = 1247.06 (0.256 sec)\n",
            "train - step 18027: loss = 1873.18 (0.267 sec)\n",
            "train - step 18028: loss = 1803.40 (0.266 sec)\n",
            "train - step 18029: loss = 2075.51 (0.274 sec)\n",
            "train - step 18030: loss = 1699.13 (0.272 sec)\n",
            "train - step 18031: loss = 2260.78 (0.255 sec)\n",
            "train - step 18032: loss = 1658.21 (0.269 sec)\n",
            "train - step 18033: loss = 2223.71 (0.267 sec)\n",
            "train - step 18034: loss = 1802.68 (0.272 sec)\n",
            "train - step 18035: loss = 1864.68 (0.263 sec)\n",
            "train - step 18036: loss = 1815.83 (0.273 sec)\n",
            "train - step 18037: loss = 1755.50 (0.271 sec)\n",
            "train - step 18038: loss = 2000.53 (0.265 sec)\n",
            "train - step 18039: loss = 2057.34 (0.263 sec)\n",
            "train - step 18040: loss = 1666.63 (0.264 sec)\n",
            "train - step 18041: loss = 1391.58 (0.272 sec)\n",
            "train - step 18042: loss = 1925.41 (0.267 sec)\n",
            "train - step 18043: loss = 1427.17 (0.279 sec)\n",
            "train - step 18044: loss = 1627.64 (0.273 sec)\n",
            "train - step 18045: loss = 2001.85 (0.262 sec)\n",
            "train - step 18046: loss = 1941.39 (0.263 sec)\n",
            "train - step 18047: loss = 2029.97 (0.265 sec)\n",
            "train - step 18048: loss = 1627.80 (0.263 sec)\n",
            "train - step 18049: loss = 1884.21 (0.269 sec)\n",
            "train - step 18050: loss = 2173.05 (0.261 sec)\n",
            "train - step 18051: loss = 1362.46 (0.267 sec)\n",
            "train - step 18052: loss = 1704.57 (0.274 sec)\n",
            "train - step 18053: loss = 1591.99 (0.267 sec)\n",
            "train - step 18054: loss = 1704.94 (0.271 sec)\n",
            "train - step 18055: loss = 1812.41 (0.269 sec)\n",
            "train - step 18056: loss = 1642.45 (0.267 sec)\n",
            "train - step 18057: loss = 1724.90 (0.268 sec)\n",
            "train - step 18058: loss = 1473.71 (0.262 sec)\n",
            "train - step 18059: loss = 2427.17 (0.260 sec)\n",
            "train - step 18060: loss = 1622.82 (0.267 sec)\n",
            "train - step 18061: loss = 1584.93 (0.266 sec)\n",
            "train - step 18062: loss = 623.72 (0.262 sec)\n",
            "train - step 18063: loss = 1844.73 (0.271 sec)\n",
            "train - step 18064: loss = 2181.68 (0.271 sec)\n",
            "train - step 18065: loss = 2202.53 (0.264 sec)\n",
            "train - step 18066: loss = 1801.26 (0.263 sec)\n",
            "train - step 18067: loss = 1627.37 (0.264 sec)\n",
            "train - step 18068: loss = 1407.36 (0.273 sec)\n",
            "train - step 18069: loss = 1752.55 (0.269 sec)\n",
            "train - step 18070: loss = 1455.00 (0.267 sec)\n",
            "train - step 18071: loss = 1435.97 (0.262 sec)\n",
            "train - step 18072: loss = 2014.12 (0.265 sec)\n",
            "train - step 18073: loss = 2191.31 (0.263 sec)\n",
            "train - step 18074: loss = 1701.19 (0.266 sec)\n",
            "train - step 18075: loss = 1840.50 (0.265 sec)\n",
            "train - step 18076: loss = 1962.49 (0.258 sec)\n",
            "train - step 18077: loss = 1653.03 (0.266 sec)\n",
            "train - step 18078: loss = 2105.57 (0.258 sec)\n",
            "train - step 18079: loss = 1889.70 (0.275 sec)\n",
            "train - step 18080: loss = 1494.35 (0.275 sec)\n",
            "train - step 18081: loss = 1636.26 (0.263 sec)\n",
            "train - step 18082: loss = 1845.96 (0.262 sec)\n",
            "train - step 18083: loss = 2053.67 (0.272 sec)\n",
            "train - step 18084: loss = 2240.57 (0.262 sec)\n",
            "train - step 18085: loss = 1289.99 (0.271 sec)\n",
            "train - step 18086: loss = 1965.92 (0.272 sec)\n",
            "train - step 18087: loss = 1718.97 (0.271 sec)\n",
            "train - step 18088: loss = 1800.50 (0.268 sec)\n",
            "train - step 18089: loss = 1666.75 (0.269 sec)\n",
            "train - step 18090: loss = 1558.04 (0.266 sec)\n",
            "train - step 18091: loss = 1718.37 (0.271 sec)\n",
            "train - step 18092: loss = 2046.88 (0.266 sec)\n",
            "train - step 18093: loss = 1686.27 (0.266 sec)\n",
            "train - step 18094: loss = 1733.29 (0.274 sec)\n",
            "train - step 18095: loss = 1847.98 (0.272 sec)\n",
            "train - step 18096: loss = 2036.35 (0.269 sec)\n",
            "train - step 18097: loss = 1595.53 (0.271 sec)\n",
            "train - step 18098: loss = 2102.43 (0.261 sec)\n",
            "train - step 18099: loss = 1831.28 (0.269 sec)\n",
            "train - step 18100: loss = 1510.81 (0.266 sec)\n",
            "train - step 18101: loss = 1847.68 (0.264 sec)\n",
            "train - step 18102: loss = 2148.66 (0.285 sec)\n",
            "train - step 18103: loss = 1759.57 (0.273 sec)\n",
            "train - step 18104: loss = 2013.73 (0.275 sec)\n",
            "train - step 18105: loss = 1831.47 (0.276 sec)\n",
            "train - step 18106: loss = 1743.93 (0.284 sec)\n",
            "train - step 18107: loss = 1879.37 (0.274 sec)\n",
            "train - step 18108: loss = 2001.14 (0.280 sec)\n",
            "train - step 18109: loss = 1828.08 (0.274 sec)\n",
            "train - step 18110: loss = 2135.67 (0.278 sec)\n",
            "train - step 18111: loss = 2196.46 (0.270 sec)\n",
            "train - step 18112: loss = 1989.19 (0.277 sec)\n",
            "train - step 18113: loss = 1794.45 (0.292 sec)\n",
            "train - step 18114: loss = 1551.39 (0.273 sec)\n",
            "train - step 18115: loss = 1147.95 (0.274 sec)\n",
            "train - step 18116: loss = 1582.92 (0.273 sec)\n",
            "train - step 18117: loss = 1942.70 (0.292 sec)\n",
            "train - step 18118: loss = 1979.01 (0.278 sec)\n",
            "train - step 18119: loss = 1962.13 (0.269 sec)\n",
            "train - step 18120: loss = 1838.29 (0.277 sec)\n",
            "train - step 18121: loss = 2081.26 (0.277 sec)\n",
            "train - step 18122: loss = 2003.03 (0.274 sec)\n",
            "train - step 18123: loss = 2042.97 (0.269 sec)\n",
            "train - step 18124: loss = 1793.42 (0.284 sec)\n",
            "train - step 18125: loss = 1993.73 (0.275 sec)\n",
            "train - step 18126: loss = 1369.39 (0.279 sec)\n",
            "train - step 18127: loss = 1810.68 (0.278 sec)\n",
            "train - step 18128: loss = 1953.13 (0.271 sec)\n",
            "train - step 18129: loss = 1945.91 (0.270 sec)\n",
            "train - step 18130: loss = 1716.79 (0.278 sec)\n",
            "train - step 18131: loss = 1822.58 (0.273 sec)\n",
            "train - step 18132: loss = 1632.26 (0.291 sec)\n",
            "train - step 18133: loss = 1674.98 (0.278 sec)\n",
            "train - step 18134: loss = 1740.27 (0.271 sec)\n",
            "train - step 18135: loss = 1815.62 (0.271 sec)\n",
            "train - step 18136: loss = 2120.55 (0.285 sec)\n",
            "train - step 18137: loss = 1768.72 (0.269 sec)\n",
            "train - step 18138: loss = 2267.27 (0.263 sec)\n",
            "train - step 18139: loss = 1677.17 (0.262 sec)\n",
            "train - step 18140: loss = 1857.56 (0.266 sec)\n",
            "train - step 18141: loss = 1472.27 (0.259 sec)\n",
            "train - step 18142: loss = 1746.87 (0.270 sec)\n",
            "train - step 18143: loss = 1949.90 (0.267 sec)\n",
            "train - step 18144: loss = 1536.80 (0.267 sec)\n",
            "train - step 18145: loss = 2298.79 (0.260 sec)\n",
            "train - step 18146: loss = 1769.02 (0.265 sec)\n",
            "train - step 18147: loss = 1822.63 (0.269 sec)\n",
            "train - step 18148: loss = 1568.44 (0.266 sec)\n",
            "train - step 18149: loss = 2094.88 (0.262 sec)\n",
            "train - step 18150: loss = 1609.74 (0.269 sec)\n",
            "train - step 18151: loss = 1499.28 (0.275 sec)\n",
            "train - step 18152: loss = 1707.58 (0.276 sec)\n",
            "train - step 18153: loss = 1802.80 (0.265 sec)\n",
            "train - step 18154: loss = 1419.13 (0.266 sec)\n",
            "train - step 18155: loss = 1640.13 (0.272 sec)\n",
            "train - step 18156: loss = 1911.25 (0.267 sec)\n",
            "train - step 18157: loss = 2081.72 (0.261 sec)\n",
            "train - step 18158: loss = 1650.70 (0.276 sec)\n",
            "train - step 18159: loss = 1809.31 (0.264 sec)\n",
            "train - step 18160: loss = 1966.26 (0.266 sec)\n",
            "train - step 18161: loss = 1871.60 (0.265 sec)\n",
            "train - step 18162: loss = 1498.78 (0.269 sec)\n",
            "train - step 18163: loss = 2115.90 (0.258 sec)\n",
            "train - step 18164: loss = 1563.13 (0.271 sec)\n",
            "train - step 18165: loss = 2125.67 (0.271 sec)\n",
            "train - step 18166: loss = 1719.70 (0.264 sec)\n",
            "train - step 18167: loss = 1849.80 (0.267 sec)\n",
            "train - step 18168: loss = 1773.53 (0.266 sec)\n",
            "train - step 18169: loss = 1906.08 (0.269 sec)\n",
            "train - step 18170: loss = 2039.59 (0.261 sec)\n",
            "train - step 18171: loss = 1677.97 (0.263 sec)\n",
            "train - step 18172: loss = 1923.26 (0.268 sec)\n",
            "train - step 18173: loss = 1768.88 (0.267 sec)\n",
            "train - step 18174: loss = 1551.21 (0.268 sec)\n",
            "train - step 18175: loss = 1658.01 (0.267 sec)\n",
            "train - step 18176: loss = 1947.43 (0.260 sec)\n",
            "train - step 18177: loss = 1872.51 (0.272 sec)\n",
            "train - step 18178: loss = 1720.57 (0.273 sec)\n",
            "train - step 18179: loss = 1966.31 (0.261 sec)\n",
            "train - step 18180: loss = 1814.89 (0.267 sec)\n",
            "train - step 18181: loss = 2002.65 (0.266 sec)\n",
            "train - step 18182: loss = 1546.89 (0.272 sec)\n",
            "train - step 18183: loss = 1605.90 (0.270 sec)\n",
            "train - step 18184: loss = 2490.55 (0.259 sec)\n",
            "train - step 18185: loss = 1700.50 (0.254 sec)\n",
            "train - step 18186: loss = 2095.30 (0.275 sec)\n",
            "train - step 18187: loss = 1230.74 (0.253 sec)\n",
            "train - step 18188: loss = 1858.04 (0.274 sec)\n",
            "train - step 18189: loss = 1802.27 (0.261 sec)\n",
            "train - step 18190: loss = 2003.77 (0.271 sec)\n",
            "train - step 18191: loss = 1570.36 (0.267 sec)\n",
            "train - step 18192: loss = 896.89 (0.274 sec)\n",
            "train - step 18193: loss = 1758.07 (0.267 sec)\n",
            "train - step 18194: loss = 1609.45 (0.262 sec)\n",
            "train - step 18195: loss = 2190.66 (0.267 sec)\n",
            "train - step 18196: loss = 2212.17 (0.268 sec)\n",
            "train - step 18197: loss = 1624.34 (0.268 sec)\n",
            "train - step 18198: loss = 2421.91 (0.256 sec)\n",
            "train - step 18199: loss = 1788.47 (0.264 sec)\n",
            "train - step 18200: loss = 2001.60 (0.265 sec)\n",
            "train - step 18201: loss = 1829.60 (0.262 sec)\n",
            "train - step 18202: loss = 1768.56 (0.269 sec)\n",
            "train - step 18203: loss = 1751.47 (0.269 sec)\n",
            "train - step 18204: loss = 1294.15 (0.269 sec)\n",
            "train - step 18205: loss = 1630.96 (0.279 sec)\n",
            "train - step 18206: loss = 1637.54 (0.270 sec)\n",
            "train - step 18207: loss = 1463.77 (0.265 sec)\n",
            "train - step 18208: loss = 1863.32 (0.265 sec)\n",
            "train - step 18209: loss = 1485.51 (0.270 sec)\n",
            "train - step 18210: loss = 1616.83 (0.266 sec)\n",
            "train - step 18211: loss = 1385.95 (0.270 sec)\n",
            "train - step 18212: loss = 1693.82 (0.264 sec)\n",
            "train - step 18213: loss = 1696.05 (0.270 sec)\n",
            "train - step 18214: loss = 2084.57 (0.272 sec)\n",
            "train - step 18215: loss = 1564.45 (0.265 sec)\n",
            "train - step 18216: loss = 1777.59 (0.273 sec)\n",
            "train - step 18217: loss = 1546.89 (0.268 sec)\n",
            "train - step 18218: loss = 1875.47 (0.272 sec)\n",
            "train - step 18219: loss = 1862.17 (0.265 sec)\n",
            "train - step 18220: loss = 1936.59 (0.270 sec)\n",
            "train - step 18221: loss = 1256.44 (0.270 sec)\n",
            "train - step 18222: loss = 1266.64 (0.260 sec)\n",
            "train - step 18223: loss = 1190.33 (0.267 sec)\n",
            "train - step 18224: loss = 1897.49 (0.268 sec)\n",
            "train - step 18225: loss = 1799.25 (0.267 sec)\n",
            "train - step 18226: loss = 2272.24 (0.264 sec)\n",
            "train - step 18227: loss = 1815.12 (0.267 sec)\n",
            "train - step 18228: loss = 2226.38 (0.270 sec)\n",
            "train - step 18229: loss = 1828.44 (0.263 sec)\n",
            "train - step 18230: loss = 1730.80 (0.266 sec)\n",
            "train - step 18231: loss = 1583.44 (0.274 sec)\n",
            "train - step 18232: loss = 2073.60 (0.271 sec)\n",
            "train - step 18233: loss = 1717.50 (0.269 sec)\n",
            "train - step 18234: loss = 1505.08 (0.265 sec)\n",
            "train - step 18235: loss = 1443.59 (0.265 sec)\n",
            "train - step 18236: loss = 1881.04 (0.282 sec)\n",
            "train - step 18237: loss = 2229.71 (0.265 sec)\n",
            "train - step 18238: loss = 1309.51 (0.253 sec)\n",
            "train - step 18239: loss = 1691.54 (1.053 sec)\n",
            "train - step 18240: loss = 1201.21 (0.261 sec)\n",
            "train - step 18241: loss = 1497.08 (0.279 sec)\n",
            "train - step 18242: loss = 1636.17 (0.265 sec)\n",
            "train - step 18243: loss = 1595.53 (0.275 sec)\n",
            "train - step 18244: loss = 1699.93 (0.267 sec)\n",
            "train - step 18245: loss = 1573.35 (0.268 sec)\n",
            "train - step 18246: loss = 2230.46 (0.262 sec)\n",
            "train - step 18247: loss = 1693.24 (0.269 sec)\n",
            "train - step 18248: loss = 1505.13 (0.275 sec)\n",
            "train - step 18249: loss = 2000.07 (0.272 sec)\n",
            "train - step 18250: loss = 1708.34 (0.268 sec)\n",
            "train - step 18251: loss = 2012.30 (0.266 sec)\n",
            "train - step 18252: loss = 1784.83 (0.270 sec)\n",
            "train - step 18253: loss = 1817.76 (0.267 sec)\n",
            "train - step 18254: loss = 1800.60 (0.258 sec)\n",
            "train - step 18255: loss = 1730.28 (0.263 sec)\n",
            "train - step 18256: loss = 1449.33 (0.280 sec)\n",
            "train - step 18257: loss = 1840.70 (0.268 sec)\n",
            "train - step 18258: loss = 1768.53 (0.267 sec)\n",
            "train - step 18259: loss = 1753.76 (0.269 sec)\n",
            "train - step 18260: loss = 1890.00 (0.270 sec)\n",
            "train - step 18261: loss = 2257.89 (0.265 sec)\n",
            "train - step 18262: loss = 2153.43 (0.259 sec)\n",
            "train - step 18263: loss = 1605.30 (0.263 sec)\n",
            "train - step 18264: loss = 1705.05 (0.272 sec)\n",
            "train - step 18265: loss = 1776.32 (0.262 sec)\n",
            "train - step 18266: loss = 1736.13 (0.268 sec)\n",
            "train - step 18267: loss = 1894.09 (0.271 sec)\n",
            "train - step 18268: loss = 1694.98 (0.268 sec)\n",
            "train - step 18269: loss = 1717.10 (0.266 sec)\n",
            "train - step 18270: loss = 1702.48 (0.268 sec)\n",
            "train - step 18271: loss = 1884.43 (0.289 sec)\n",
            "train - step 18272: loss = 1588.79 (0.270 sec)\n",
            "train - step 18273: loss = 1203.49 (0.263 sec)\n",
            "train - step 18274: loss = 1946.97 (0.267 sec)\n",
            "train - step 18275: loss = 1877.06 (0.268 sec)\n",
            "train - step 18276: loss = 1428.11 (0.269 sec)\n",
            "train - step 18277: loss = 1746.74 (0.262 sec)\n",
            "train - step 18278: loss = 550.11 (0.258 sec)\n",
            "train - step 18279: loss = 2154.75 (0.274 sec)\n",
            "train - step 18280: loss = 1865.79 (0.268 sec)\n",
            "train - step 18281: loss = 1970.67 (0.264 sec)\n",
            "train - step 18282: loss = 2010.04 (0.263 sec)\n",
            "train - step 18283: loss = 2178.56 (0.269 sec)\n",
            "train - step 18284: loss = 1767.61 (0.271 sec)\n",
            "train - step 18285: loss = 2407.34 (0.259 sec)\n",
            "train - step 18286: loss = 1251.77 (0.269 sec)\n",
            "train - step 18287: loss = 1440.90 (0.263 sec)\n",
            "train - step 18288: loss = 1266.84 (0.259 sec)\n",
            "train - step 18289: loss = 1233.78 (0.269 sec)\n",
            "train - step 18290: loss = 2384.10 (0.259 sec)\n",
            "train - step 18291: loss = 1739.92 (0.271 sec)\n",
            "train - step 18292: loss = 2030.18 (0.269 sec)\n",
            "train - step 18293: loss = 1764.66 (0.260 sec)\n",
            "train - step 18294: loss = 1461.19 (0.276 sec)\n",
            "train - step 18295: loss = 1595.57 (0.270 sec)\n",
            "train - step 18296: loss = 1977.49 (0.264 sec)\n",
            "train - step 18297: loss = 1738.51 (0.262 sec)\n",
            "train - step 18298: loss = 1681.80 (0.272 sec)\n",
            "train - step 18299: loss = 2292.64 (0.270 sec)\n",
            "train - step 18300: loss = 1681.32 (0.259 sec)\n",
            "train - step 18301: loss = 1573.44 (0.265 sec)\n",
            "train - step 18302: loss = 1684.39 (0.266 sec)\n",
            "train - step 18303: loss = 1702.45 (0.271 sec)\n",
            "train - step 18304: loss = 1829.64 (0.266 sec)\n",
            "train - step 18305: loss = 1743.21 (0.257 sec)\n",
            "train - step 18306: loss = 1933.41 (0.278 sec)\n",
            "train - step 18307: loss = 1771.57 (0.261 sec)\n",
            "train - step 18308: loss = 1645.28 (0.262 sec)\n",
            "train - step 18309: loss = 1779.69 (0.260 sec)\n",
            "train - step 18310: loss = 1709.17 (0.275 sec)\n",
            "train - step 18311: loss = 2033.89 (0.266 sec)\n",
            "train - step 18312: loss = 2075.82 (0.262 sec)\n",
            "train - step 18313: loss = 1626.15 (0.262 sec)\n",
            "train - step 18314: loss = 1886.97 (0.265 sec)\n",
            "train - step 18315: loss = 1789.58 (0.262 sec)\n",
            "train - step 18316: loss = 1974.71 (0.271 sec)\n",
            "train - step 18317: loss = 2170.32 (0.266 sec)\n",
            "train - step 18318: loss = 1554.01 (0.261 sec)\n",
            "train - step 18319: loss = 1744.20 (0.264 sec)\n",
            "train - step 18320: loss = 2093.65 (0.270 sec)\n",
            "train - step 18321: loss = 2293.57 (0.270 sec)\n",
            "train - step 18322: loss = 1501.07 (0.269 sec)\n",
            "train - step 18323: loss = 1848.19 (0.264 sec)\n",
            "train - step 18324: loss = 1685.18 (0.271 sec)\n",
            "train - step 18325: loss = 1589.41 (0.275 sec)\n",
            "train - step 18326: loss = 1511.07 (0.284 sec)\n",
            "train - step 18327: loss = 1687.26 (0.279 sec)\n",
            "train - step 18328: loss = 1452.94 (0.279 sec)\n",
            "train - step 18329: loss = 1668.03 (0.277 sec)\n",
            "train - step 18330: loss = 1770.07 (0.276 sec)\n",
            "train - step 18331: loss = 2283.27 (0.277 sec)\n",
            "train - step 18332: loss = 1902.05 (0.278 sec)\n",
            "train - step 18333: loss = 1604.26 (0.281 sec)\n",
            "train - step 18334: loss = 1802.67 (0.271 sec)\n",
            "train - step 18335: loss = 1765.33 (0.282 sec)\n",
            "train - step 18336: loss = 1857.57 (0.288 sec)\n",
            "train - step 18337: loss = 1479.66 (0.284 sec)\n",
            "train - step 18338: loss = 1922.26 (0.277 sec)\n",
            "train - step 18339: loss = 1640.02 (0.279 sec)\n",
            "train - step 18340: loss = 1671.73 (0.286 sec)\n",
            "train - step 18341: loss = 2008.31 (0.278 sec)\n",
            "train - step 18342: loss = 1893.71 (0.280 sec)\n",
            "train - step 18343: loss = 1812.77 (0.276 sec)\n",
            "train - step 18344: loss = 1859.71 (0.278 sec)\n",
            "train - step 18345: loss = 2117.60 (0.259 sec)\n",
            "train - step 18346: loss = 2123.07 (0.273 sec)\n",
            "train - step 18347: loss = 1852.42 (0.264 sec)\n",
            "train - step 18348: loss = 1470.90 (0.273 sec)\n",
            "train - step 18349: loss = 1964.74 (0.276 sec)\n",
            "train - step 18350: loss = 1835.90 (0.267 sec)\n",
            "train - step 18351: loss = 1775.36 (0.275 sec)\n",
            "train - step 18352: loss = 1811.01 (0.256 sec)\n",
            "train - step 18353: loss = 1748.69 (0.263 sec)\n",
            "train - step 18354: loss = 1923.35 (0.269 sec)\n",
            "train - step 18355: loss = 1914.67 (0.270 sec)\n",
            "train - step 18356: loss = 1812.18 (0.271 sec)\n",
            "train - step 18357: loss = 1911.59 (0.266 sec)\n",
            "train - step 18358: loss = 1834.00 (0.265 sec)\n",
            "train - step 18359: loss = 1592.26 (0.263 sec)\n",
            "train - step 18360: loss = 1701.81 (0.268 sec)\n",
            "train - step 18361: loss = 1797.50 (0.265 sec)\n",
            "train - step 18362: loss = 1770.96 (0.268 sec)\n",
            "train - step 18363: loss = 2004.21 (0.275 sec)\n",
            "train - step 18364: loss = 1676.98 (0.266 sec)\n",
            "train - step 18365: loss = 1872.45 (0.262 sec)\n",
            "train - step 18366: loss = 1701.51 (0.271 sec)\n",
            "train - step 18367: loss = 1418.77 (0.262 sec)\n",
            "train - step 18368: loss = 2149.79 (0.270 sec)\n",
            "train - step 18369: loss = 1623.13 (0.263 sec)\n",
            "train - step 18370: loss = 1352.03 (0.267 sec)\n",
            "train - step 18371: loss = 1950.87 (0.260 sec)\n",
            "train - step 18372: loss = 1857.04 (0.267 sec)\n",
            "train - step 18373: loss = 1690.12 (0.260 sec)\n",
            "train - step 18374: loss = 1949.30 (0.272 sec)\n",
            "train - step 18375: loss = 2274.92 (0.255 sec)\n",
            "train - step 18376: loss = 2511.50 (0.266 sec)\n",
            "train - step 18377: loss = 1633.12 (0.264 sec)\n",
            "train - step 18378: loss = 2027.52 (0.279 sec)\n",
            "train - step 18379: loss = 2136.44 (0.269 sec)\n",
            "train - step 18380: loss = 1614.14 (0.272 sec)\n",
            "train - step 18381: loss = 1722.74 (0.267 sec)\n",
            "train - step 18382: loss = 1924.75 (0.272 sec)\n",
            "train - step 18383: loss = 1378.88 (0.257 sec)\n",
            "train - step 18384: loss = 1816.20 (0.265 sec)\n",
            "train - step 18385: loss = 2040.47 (0.255 sec)\n",
            "train - step 18386: loss = 1378.29 (0.269 sec)\n",
            "train - step 18387: loss = 1614.76 (0.265 sec)\n",
            "train - step 18388: loss = 1782.50 (0.255 sec)\n",
            "train - step 18389: loss = 2381.55 (0.266 sec)\n",
            "train - step 18390: loss = 2016.05 (0.285 sec)\n",
            "train - step 18391: loss = 1694.55 (0.260 sec)\n",
            "train - step 18392: loss = 1995.18 (0.269 sec)\n",
            "train - step 18393: loss = 1474.71 (0.269 sec)\n",
            "train - step 18394: loss = 2018.55 (0.264 sec)\n",
            "train - step 18395: loss = 1663.56 (0.258 sec)\n",
            "train - step 18396: loss = 1733.54 (0.265 sec)\n",
            "train - step 18397: loss = 2080.03 (0.265 sec)\n",
            "train - step 18398: loss = 1822.41 (0.277 sec)\n",
            "train - step 18399: loss = 2128.65 (0.275 sec)\n",
            "train - step 18400: loss = 1941.06 (0.274 sec)\n",
            "train - step 18401: loss = 2076.25 (0.277 sec)\n",
            "train - step 18402: loss = 1717.98 (0.271 sec)\n",
            "train - step 18403: loss = 1522.49 (0.278 sec)\n",
            "train - step 18404: loss = 2319.03 (0.270 sec)\n",
            "train - step 18405: loss = 1709.13 (0.275 sec)\n",
            "train - step 18406: loss = 1839.19 (0.277 sec)\n",
            "train - step 18407: loss = 1582.29 (0.282 sec)\n",
            "train - step 18408: loss = 1882.94 (0.281 sec)\n",
            "train - step 18409: loss = 2149.61 (0.279 sec)\n",
            "train - step 18410: loss = 1366.52 (0.272 sec)\n",
            "train - step 18411: loss = 1736.50 (0.272 sec)\n",
            "train - step 18412: loss = 2006.24 (0.272 sec)\n",
            "train - step 18413: loss = 1867.13 (0.273 sec)\n",
            "train - step 18414: loss = 2100.37 (0.272 sec)\n",
            "train - step 18415: loss = 1675.85 (0.282 sec)\n",
            "train - step 18416: loss = 1718.27 (0.283 sec)\n",
            "train - step 18417: loss = 1800.97 (0.271 sec)\n",
            "train - step 18418: loss = 1962.36 (0.272 sec)\n",
            "train - step 18419: loss = 1494.65 (0.284 sec)\n",
            "train - step 18420: loss = 441.73 (0.268 sec)\n",
            "train - step 18421: loss = 1700.61 (0.281 sec)\n",
            "train - step 18422: loss = 1793.53 (0.281 sec)\n",
            "train - step 18423: loss = 1616.60 (0.275 sec)\n",
            "train - step 18424: loss = 2095.70 (0.274 sec)\n",
            "train - step 18425: loss = 1618.60 (0.285 sec)\n",
            "train - step 18426: loss = 1762.89 (0.274 sec)\n",
            "train - step 18427: loss = 1572.72 (0.279 sec)\n",
            "train - step 18428: loss = 1897.68 (0.282 sec)\n",
            "train - step 18429: loss = 1841.96 (0.276 sec)\n",
            "train - step 18430: loss = 2261.12 (0.283 sec)\n",
            "train - step 18431: loss = 1681.12 (0.277 sec)\n",
            "train - step 18432: loss = 1666.07 (0.283 sec)\n",
            "train - step 18433: loss = 2065.18 (0.270 sec)\n",
            "train - step 18434: loss = 1930.58 (0.266 sec)\n",
            "train - step 18435: loss = 1819.88 (0.274 sec)\n",
            "train - step 18436: loss = 2238.79 (0.260 sec)\n",
            "train - step 18437: loss = 1986.81 (0.266 sec)\n",
            "train - step 18438: loss = 1505.98 (0.263 sec)\n",
            "train - step 18439: loss = 1464.81 (0.266 sec)\n",
            "train - step 18440: loss = 1728.90 (0.273 sec)\n",
            "train - step 18441: loss = 456.87 (0.251 sec)\n",
            "train - step 18442: loss = 1530.87 (0.259 sec)\n",
            "train - step 18443: loss = 2136.14 (0.267 sec)\n",
            "train - step 18444: loss = 2065.17 (0.267 sec)\n",
            "train - step 18445: loss = 1702.10 (0.260 sec)\n",
            "train - step 18446: loss = 2014.84 (0.257 sec)\n",
            "train - step 18447: loss = 2084.03 (0.260 sec)\n",
            "train - step 18448: loss = 2259.27 (0.258 sec)\n",
            "train - step 18449: loss = 1738.42 (0.262 sec)\n",
            "train - step 18450: loss = 1873.33 (0.269 sec)\n",
            "train - step 18451: loss = 2027.56 (0.257 sec)\n",
            "train - step 18452: loss = 1975.79 (0.269 sec)\n",
            "train - step 18453: loss = 1616.81 (0.262 sec)\n",
            "train - step 18454: loss = 1535.45 (0.269 sec)\n",
            "train - step 18455: loss = 1195.52 (0.271 sec)\n",
            "train - step 18456: loss = 1743.56 (0.258 sec)\n",
            "train - step 18457: loss = 1861.45 (0.271 sec)\n",
            "train - step 18458: loss = 2076.44 (0.274 sec)\n",
            "train - step 18459: loss = 2146.18 (0.268 sec)\n",
            "train - step 18460: loss = 1572.74 (0.923 sec)\n",
            "train - step 18461: loss = 1828.23 (0.266 sec)\n",
            "train - step 18462: loss = 1891.57 (0.264 sec)\n",
            "train - step 18463: loss = 1851.60 (0.272 sec)\n",
            "train - step 18464: loss = 1542.01 (0.270 sec)\n",
            "train - step 18465: loss = 2014.50 (0.263 sec)\n",
            "train - step 18466: loss = 1472.93 (0.269 sec)\n",
            "train - step 18467: loss = 1929.54 (0.284 sec)\n",
            "train - step 18468: loss = 1859.93 (0.267 sec)\n",
            "train - step 18469: loss = 1749.15 (0.264 sec)\n",
            "train - step 18470: loss = 1403.47 (0.264 sec)\n",
            "train - step 18471: loss = 1320.18 (0.272 sec)\n",
            "train - step 18472: loss = 2101.79 (0.264 sec)\n",
            "train - step 18473: loss = 1635.52 (0.270 sec)\n",
            "train - step 18474: loss = 1346.49 (0.278 sec)\n",
            "train - step 18475: loss = 2056.05 (0.269 sec)\n",
            "train - step 18476: loss = 1871.69 (0.264 sec)\n",
            "train - step 18477: loss = 2075.72 (0.265 sec)\n",
            "train - step 18478: loss = 1564.51 (0.268 sec)\n",
            "train - step 18479: loss = 1720.53 (0.266 sec)\n",
            "train - step 18480: loss = 1702.04 (0.260 sec)\n",
            "train - step 18481: loss = 1861.50 (0.265 sec)\n",
            "train - step 18482: loss = 2085.70 (0.266 sec)\n",
            "train - step 18483: loss = 1532.22 (0.264 sec)\n",
            "train - step 18484: loss = 1936.67 (0.274 sec)\n",
            "train - step 18485: loss = 2404.36 (0.262 sec)\n",
            "train - step 18486: loss = 2056.20 (0.265 sec)\n",
            "train - step 18487: loss = 1436.80 (0.268 sec)\n",
            "train - step 18488: loss = 2218.25 (0.256 sec)\n",
            "train - step 18489: loss = 2342.91 (0.264 sec)\n",
            "train - step 18490: loss = 1996.41 (0.274 sec)\n",
            "train - step 18491: loss = 2228.31 (0.261 sec)\n",
            "train - step 18492: loss = 1917.02 (0.271 sec)\n",
            "train - step 18493: loss = 1636.97 (0.265 sec)\n",
            "train - step 18494: loss = 2040.47 (0.265 sec)\n",
            "train - step 18495: loss = 2001.80 (0.268 sec)\n",
            "train - step 18496: loss = 1698.35 (0.273 sec)\n",
            "train - step 18497: loss = 1495.95 (0.262 sec)\n",
            "train - step 18498: loss = 1810.69 (0.278 sec)\n",
            "train - step 18499: loss = 1863.73 (0.268 sec)\n",
            "train - step 18500: loss = 2102.98 (0.264 sec)\n",
            "train - step 18501: loss = 1908.33 (0.267 sec)\n",
            "train - step 18502: loss = 1989.33 (0.267 sec)\n",
            "train - step 18503: loss = 1642.19 (0.267 sec)\n",
            "train - step 18504: loss = 2366.09 (0.261 sec)\n",
            "train - step 18505: loss = 1405.41 (0.265 sec)\n",
            "train - step 18506: loss = 1679.48 (0.267 sec)\n",
            "train - step 18507: loss = 1651.74 (0.271 sec)\n",
            "train - step 18508: loss = 1798.61 (0.262 sec)\n",
            "train - step 18509: loss = 1619.29 (0.272 sec)\n",
            "train - step 18510: loss = 1352.19 (0.267 sec)\n",
            "train - step 18511: loss = 2217.10 (0.264 sec)\n",
            "train - step 18512: loss = 2114.96 (0.263 sec)\n",
            "train - step 18513: loss = 1741.26 (0.275 sec)\n",
            "train - step 18514: loss = 1658.63 (0.265 sec)\n",
            "train - step 18515: loss = 2218.38 (0.263 sec)\n",
            "train - step 18516: loss = 1630.55 (0.261 sec)\n",
            "train - step 18517: loss = 1711.47 (0.277 sec)\n",
            "train - step 18518: loss = 1276.56 (0.269 sec)\n",
            "train - step 18519: loss = 1728.09 (0.267 sec)\n",
            "train - step 18520: loss = 1519.49 (0.263 sec)\n",
            "train - step 18521: loss = 1531.14 (0.272 sec)\n",
            "train - step 18522: loss = 1347.89 (0.259 sec)\n",
            "train - step 18523: loss = 1632.20 (0.274 sec)\n",
            "train - step 18524: loss = 2002.25 (0.268 sec)\n",
            "train - step 18525: loss = 1858.14 (0.266 sec)\n",
            "train - step 18526: loss = 1623.68 (0.263 sec)\n",
            "train - step 18527: loss = 1850.86 (0.263 sec)\n",
            "train - step 18528: loss = 2044.03 (0.266 sec)\n",
            "train - step 18529: loss = 2163.38 (0.262 sec)\n",
            "train - step 18530: loss = 1906.24 (0.261 sec)\n",
            "train - step 18531: loss = 1820.96 (0.266 sec)\n",
            "train - step 18532: loss = 1699.27 (0.265 sec)\n",
            "train - step 18533: loss = 1565.61 (0.268 sec)\n",
            "train - step 18534: loss = 1646.34 (0.267 sec)\n",
            "train - step 18535: loss = 1669.62 (0.256 sec)\n",
            "train - step 18536: loss = 1333.06 (0.260 sec)\n",
            "train - step 18537: loss = 2010.79 (0.258 sec)\n",
            "train - step 18538: loss = 1441.62 (0.272 sec)\n",
            "train - step 18539: loss = 1695.54 (0.266 sec)\n",
            "train - step 18540: loss = 1813.05 (0.270 sec)\n",
            "train - step 18541: loss = 1895.29 (0.269 sec)\n",
            "train - step 18542: loss = 1533.90 (0.267 sec)\n",
            "train - step 18543: loss = 1964.58 (0.266 sec)\n",
            "train - step 18544: loss = 1572.15 (0.271 sec)\n",
            "train - step 18545: loss = 2199.08 (0.258 sec)\n",
            "train - step 18546: loss = 1831.49 (0.279 sec)\n",
            "train - step 18547: loss = 1661.60 (0.265 sec)\n",
            "train - step 18548: loss = 2046.68 (0.271 sec)\n",
            "train - step 18549: loss = 1643.17 (0.271 sec)\n",
            "train - step 18550: loss = 2076.64 (0.262 sec)\n",
            "train - step 18551: loss = 1590.86 (0.262 sec)\n",
            "train - step 18552: loss = 1843.02 (0.279 sec)\n",
            "train - step 18553: loss = 1623.77 (0.266 sec)\n",
            "train - step 18554: loss = 1636.43 (0.267 sec)\n",
            "train - step 18555: loss = 1869.30 (0.274 sec)\n",
            "train - step 18556: loss = 2066.70 (0.268 sec)\n",
            "train - step 18557: loss = 1635.08 (0.261 sec)\n",
            "train - step 18558: loss = 1980.31 (0.263 sec)\n",
            "train - step 18559: loss = 1682.30 (0.261 sec)\n",
            "train - step 18560: loss = 1876.12 (0.273 sec)\n",
            "train - step 18561: loss = 1954.07 (0.264 sec)\n",
            "train - step 18562: loss = 1842.95 (0.263 sec)\n",
            "train - step 18563: loss = 2238.06 (0.269 sec)\n",
            "train - step 18564: loss = 1380.71 (0.266 sec)\n",
            "train - step 18565: loss = 1641.14 (0.265 sec)\n",
            "train - step 18566: loss = 2038.43 (0.274 sec)\n",
            "train - step 18567: loss = 1226.49 (0.273 sec)\n",
            "train - step 18568: loss = 1617.85 (0.267 sec)\n",
            "train - step 18569: loss = 2234.03 (0.275 sec)\n",
            "train - step 18570: loss = 1873.02 (0.259 sec)\n",
            "train - step 18571: loss = 1584.21 (0.273 sec)\n",
            "train - step 18572: loss = 2046.09 (0.270 sec)\n",
            "train - step 18573: loss = 1901.22 (0.268 sec)\n",
            "train - step 18574: loss = 1798.51 (0.264 sec)\n",
            "train - step 18575: loss = 1659.64 (0.277 sec)\n",
            "train - step 18576: loss = 1756.98 (0.268 sec)\n",
            "train - step 18577: loss = 1730.83 (0.265 sec)\n",
            "train - step 18578: loss = 1414.28 (0.268 sec)\n",
            "train - step 18579: loss = 2588.55 (0.259 sec)\n",
            "train - step 18580: loss = 1491.52 (0.267 sec)\n",
            "train - step 18581: loss = 1856.73 (0.266 sec)\n",
            "train - step 18582: loss = 1781.19 (0.273 sec)\n",
            "train - step 18583: loss = 1738.96 (0.266 sec)\n",
            "train - step 18584: loss = 1963.67 (0.266 sec)\n",
            "train - step 18585: loss = 2076.31 (0.263 sec)\n",
            "train - step 18586: loss = 1930.07 (0.270 sec)\n",
            "train - step 18587: loss = 1887.71 (0.264 sec)\n",
            "train - step 18588: loss = 2114.88 (0.266 sec)\n",
            "train - step 18589: loss = 1363.48 (0.263 sec)\n",
            "train - step 18590: loss = 1809.57 (0.268 sec)\n",
            "train - step 18591: loss = 1744.88 (0.274 sec)\n",
            "train - step 18592: loss = 2309.57 (0.265 sec)\n",
            "train - step 18593: loss = 1935.18 (0.264 sec)\n",
            "train - step 18594: loss = 1847.99 (0.275 sec)\n",
            "train - step 18595: loss = 1765.37 (0.266 sec)\n",
            "train - step 18596: loss = 1971.08 (0.269 sec)\n",
            "train - step 18597: loss = 2024.38 (0.264 sec)\n",
            "train - step 18598: loss = 2001.63 (0.266 sec)\n",
            "train - step 18599: loss = 1827.07 (0.269 sec)\n",
            "train - step 18600: loss = 1794.80 (0.264 sec)\n",
            "train - step 18601: loss = 1950.34 (0.258 sec)\n",
            "train - step 18602: loss = 1653.64 (0.271 sec)\n",
            "train - step 18603: loss = 1476.02 (0.266 sec)\n",
            "train - step 18604: loss = 1946.63 (0.266 sec)\n",
            "train - step 18605: loss = 1668.35 (0.269 sec)\n",
            "train - step 18606: loss = 567.77 (0.262 sec)\n",
            "train - step 18607: loss = 1874.73 (0.268 sec)\n",
            "train - step 18608: loss = 1701.21 (0.269 sec)\n",
            "train - step 18609: loss = 1563.59 (0.268 sec)\n",
            "train - step 18610: loss = 1591.76 (0.265 sec)\n",
            "train - step 18611: loss = 1497.41 (0.265 sec)\n",
            "train - step 18612: loss = 1851.10 (0.262 sec)\n",
            "train - step 18613: loss = 1905.29 (0.274 sec)\n",
            "train - step 18614: loss = 1795.60 (0.268 sec)\n",
            "train - step 18615: loss = 1729.15 (0.257 sec)\n",
            "train - step 18616: loss = 1393.88 (0.270 sec)\n",
            "train - step 18617: loss = 2024.54 (0.266 sec)\n",
            "train - step 18618: loss = 1999.84 (0.266 sec)\n",
            "train - step 18619: loss = 2457.41 (0.264 sec)\n",
            "train - step 18620: loss = 2267.09 (0.258 sec)\n",
            "train - step 18621: loss = 2053.32 (0.269 sec)\n",
            "train - step 18622: loss = 2020.51 (0.262 sec)\n",
            "train - step 18623: loss = 1295.94 (0.257 sec)\n",
            "train - step 18624: loss = 1895.90 (0.263 sec)\n",
            "train - step 18625: loss = 1630.09 (0.270 sec)\n",
            "train - step 18626: loss = 2079.05 (0.278 sec)\n",
            "train - step 18627: loss = 1913.14 (0.270 sec)\n",
            "train - step 18628: loss = 2338.45 (0.272 sec)\n",
            "train - step 18629: loss = 1451.31 (0.267 sec)\n",
            "train - step 18630: loss = 1541.08 (0.271 sec)\n",
            "train - step 18631: loss = 1737.19 (0.267 sec)\n",
            "train - step 18632: loss = 1781.61 (0.268 sec)\n",
            "train - step 18633: loss = 1839.05 (0.258 sec)\n",
            "train - step 18634: loss = 1811.92 (0.262 sec)\n",
            "train - step 18635: loss = 1924.51 (0.268 sec)\n",
            "train - step 18636: loss = 2008.92 (0.262 sec)\n",
            "train - step 18637: loss = 1837.78 (0.264 sec)\n",
            "train - step 18638: loss = 1846.96 (0.271 sec)\n",
            "train - step 18639: loss = 1945.58 (0.266 sec)\n",
            "train - step 18640: loss = 1846.80 (0.281 sec)\n",
            "train - step 18641: loss = 1588.41 (0.273 sec)\n",
            "train - step 18642: loss = 1949.57 (0.263 sec)\n",
            "train - step 18643: loss = 1515.91 (0.266 sec)\n",
            "train - step 18644: loss = 1969.43 (0.265 sec)\n",
            "train - step 18645: loss = 1420.31 (0.266 sec)\n",
            "train - step 18646: loss = 1903.66 (0.268 sec)\n",
            "train - step 18647: loss = 1808.10 (0.268 sec)\n",
            "train - step 18648: loss = 1920.20 (0.272 sec)\n",
            "train - step 18649: loss = 2218.27 (0.269 sec)\n",
            "train - step 18650: loss = 1655.64 (0.272 sec)\n",
            "train - step 18651: loss = 1481.33 (0.270 sec)\n",
            "train - step 18652: loss = 2092.41 (0.275 sec)\n",
            "train - step 18653: loss = 1861.48 (0.261 sec)\n",
            "train - step 18654: loss = 1930.43 (0.278 sec)\n",
            "train - step 18655: loss = 2166.88 (0.266 sec)\n",
            "train - step 18656: loss = 1696.20 (0.270 sec)\n",
            "train - step 18657: loss = 1530.91 (0.266 sec)\n",
            "train - step 18658: loss = 2210.22 (0.267 sec)\n",
            "train - step 18659: loss = 1864.32 (0.264 sec)\n",
            "train - step 18660: loss = 1491.09 (0.264 sec)\n",
            "train - step 18661: loss = 1911.18 (0.272 sec)\n",
            "train - step 18662: loss = 1740.92 (0.265 sec)\n",
            "train - step 18663: loss = 1517.34 (0.267 sec)\n",
            "train - step 18664: loss = 2004.86 (0.264 sec)\n",
            "train - step 18665: loss = 2445.29 (0.276 sec)\n",
            "train - step 18666: loss = 1976.79 (0.259 sec)\n",
            "train - step 18667: loss = 1676.13 (0.268 sec)\n",
            "train - step 18668: loss = 2084.02 (0.280 sec)\n",
            "train - step 18669: loss = 1835.31 (0.267 sec)\n",
            "train - step 18670: loss = 1905.60 (0.266 sec)\n",
            "train - step 18671: loss = 1768.25 (0.267 sec)\n",
            "train - step 18672: loss = 1852.59 (0.272 sec)\n",
            "train - step 18673: loss = 1919.40 (0.268 sec)\n",
            "train - step 18674: loss = 2056.61 (0.269 sec)\n",
            "train - step 18675: loss = 1609.72 (0.268 sec)\n",
            "train - step 18676: loss = 1885.90 (0.266 sec)\n",
            "train - step 18677: loss = 1746.71 (0.261 sec)\n",
            "train - step 18678: loss = 1723.76 (0.264 sec)\n",
            "train - step 18679: loss = 1845.34 (0.270 sec)\n",
            "train - step 18680: loss = 1419.90 (0.268 sec)\n",
            "train - step 18681: loss = 1944.25 (0.268 sec)\n",
            "train - step 18682: loss = 1884.62 (0.271 sec)\n",
            "train - step 18683: loss = 2114.51 (0.926 sec)\n",
            "train - step 18684: loss = 2178.92 (0.259 sec)\n",
            "train - step 18685: loss = 1856.68 (0.270 sec)\n",
            "train - step 18686: loss = 1403.25 (0.270 sec)\n",
            "train - step 18687: loss = 1353.18 (0.270 sec)\n",
            "train - step 18688: loss = 2138.07 (0.264 sec)\n",
            "train - step 18689: loss = 1789.44 (0.261 sec)\n",
            "train - step 18690: loss = 1724.88 (0.263 sec)\n",
            "train - step 18691: loss = 1913.58 (0.263 sec)\n",
            "train - step 18692: loss = 2327.67 (0.260 sec)\n",
            "train - step 18693: loss = 1741.63 (0.274 sec)\n",
            "train - step 18694: loss = 559.92 (0.274 sec)\n",
            "train - step 18695: loss = 1497.29 (0.287 sec)\n",
            "train - step 18696: loss = 1801.07 (0.271 sec)\n",
            "train - step 18697: loss = 1763.34 (0.277 sec)\n",
            "train - step 18698: loss = 1655.14 (0.266 sec)\n",
            "train - step 18699: loss = 1613.98 (0.275 sec)\n",
            "train - step 18700: loss = 1834.55 (0.278 sec)\n",
            "train - step 18701: loss = 1848.89 (0.269 sec)\n",
            "train - step 18702: loss = 579.68 (0.261 sec)\n",
            "train - step 18703: loss = 1754.33 (0.289 sec)\n",
            "train - step 18704: loss = 2035.87 (0.272 sec)\n",
            "train - step 18705: loss = 1616.64 (0.276 sec)\n",
            "train - step 18706: loss = 1718.61 (0.272 sec)\n",
            "train - step 18707: loss = 1733.47 (0.275 sec)\n",
            "train - step 18708: loss = 1293.79 (0.279 sec)\n",
            "train - step 18709: loss = 1677.94 (0.278 sec)\n",
            "train - step 18710: loss = 1896.01 (0.287 sec)\n",
            "train - step 18711: loss = 2544.13 (0.267 sec)\n",
            "train - step 18712: loss = 2013.36 (0.269 sec)\n",
            "train - step 18713: loss = 1706.83 (0.276 sec)\n",
            "train - step 18714: loss = 1584.25 (0.277 sec)\n",
            "train - step 18715: loss = 1735.86 (0.274 sec)\n",
            "train - step 18716: loss = 1928.68 (0.278 sec)\n",
            "train - step 18717: loss = 1596.69 (0.278 sec)\n",
            "train - step 18718: loss = 1641.63 (0.285 sec)\n",
            "train - step 18719: loss = 1540.48 (0.272 sec)\n",
            "train - step 18720: loss = 1746.68 (0.264 sec)\n",
            "train - step 18721: loss = 1949.63 (0.276 sec)\n",
            "train - step 18722: loss = 2213.12 (0.273 sec)\n",
            "train - step 18723: loss = 1853.82 (0.273 sec)\n",
            "train - step 18724: loss = 1976.82 (0.267 sec)\n",
            "train - step 18725: loss = 1599.50 (0.284 sec)\n",
            "train - step 18726: loss = 1795.00 (0.267 sec)\n",
            "train - step 18727: loss = 1671.47 (0.274 sec)\n",
            "train - step 18728: loss = 2096.01 (0.274 sec)\n",
            "train - step 18729: loss = 1380.97 (0.283 sec)\n",
            "train - step 18730: loss = 1362.45 (0.263 sec)\n",
            "train - step 18731: loss = 2229.00 (0.267 sec)\n",
            "train - step 18732: loss = 2271.83 (0.257 sec)\n",
            "train - step 18733: loss = 1653.26 (0.270 sec)\n",
            "train - step 18734: loss = 1219.40 (0.270 sec)\n",
            "train - step 18735: loss = 1624.80 (0.262 sec)\n",
            "train - step 18736: loss = 1865.65 (0.270 sec)\n",
            "train - step 18737: loss = 1766.85 (0.276 sec)\n",
            "train - step 18738: loss = 2095.26 (0.263 sec)\n",
            "train - step 18739: loss = 1905.25 (0.264 sec)\n",
            "train - step 18740: loss = 2188.86 (0.269 sec)\n",
            "train - step 18741: loss = 1200.48 (0.272 sec)\n",
            "train - step 18742: loss = 1882.36 (0.266 sec)\n",
            "train - step 18743: loss = 1906.55 (0.264 sec)\n",
            "train - step 18744: loss = 459.25 (0.248 sec)\n",
            "train - step 18745: loss = 1554.08 (0.261 sec)\n",
            "train - step 18746: loss = 1994.59 (0.264 sec)\n",
            "train - step 18747: loss = 1802.84 (0.266 sec)\n",
            "train - step 18748: loss = 1772.08 (0.274 sec)\n",
            "train - step 18749: loss = 2008.71 (0.266 sec)\n",
            "train - step 18750: loss = 1872.10 (0.270 sec)\n",
            "train - step 18751: loss = 1865.53 (0.265 sec)\n",
            "train - step 18752: loss = 1725.12 (0.270 sec)\n",
            "train - step 18753: loss = 1465.63 (0.274 sec)\n",
            "train - step 18754: loss = 1892.07 (0.267 sec)\n",
            "train - step 18755: loss = 1906.47 (0.260 sec)\n",
            "train - step 18756: loss = 1758.29 (0.274 sec)\n",
            "train - step 18757: loss = 1027.70 (0.262 sec)\n",
            "train - step 18758: loss = 1711.22 (0.271 sec)\n",
            "train - step 18759: loss = 1676.57 (0.270 sec)\n",
            "train - step 18760: loss = 1671.64 (0.272 sec)\n",
            "train - step 18761: loss = 1854.98 (0.263 sec)\n",
            "train - step 18762: loss = 1567.67 (0.261 sec)\n",
            "train - step 18763: loss = 1909.64 (0.261 sec)\n",
            "train - step 18764: loss = 1710.99 (0.275 sec)\n",
            "train - step 18765: loss = 1413.87 (0.269 sec)\n",
            "train - step 18766: loss = 1868.15 (0.268 sec)\n",
            "train - step 18767: loss = 1828.13 (0.263 sec)\n",
            "train - step 18768: loss = 1843.71 (0.267 sec)\n",
            "train - step 18769: loss = 1858.70 (0.265 sec)\n",
            "train - step 18770: loss = 1012.89 (0.269 sec)\n",
            "train - step 18771: loss = 1844.11 (0.270 sec)\n",
            "train - step 18772: loss = 1769.87 (0.264 sec)\n",
            "train - step 18773: loss = 1689.98 (0.268 sec)\n",
            "train - step 18774: loss = 1610.28 (0.265 sec)\n",
            "train - step 18775: loss = 1682.71 (0.273 sec)\n",
            "train - step 18776: loss = 2038.98 (0.265 sec)\n",
            "train - step 18777: loss = 1766.76 (0.264 sec)\n",
            "train - step 18778: loss = 1825.80 (0.264 sec)\n",
            "train - step 18779: loss = 1872.63 (0.266 sec)\n",
            "train - step 18780: loss = 1913.42 (0.264 sec)\n",
            "train - step 18781: loss = 1759.15 (0.265 sec)\n",
            "train - step 18782: loss = 1732.49 (0.267 sec)\n",
            "train - step 18783: loss = 1925.47 (0.275 sec)\n",
            "train - step 18784: loss = 1800.29 (0.262 sec)\n",
            "train - step 18785: loss = 1747.72 (0.266 sec)\n",
            "train - step 18786: loss = 1301.96 (0.262 sec)\n",
            "train - step 18787: loss = 1786.74 (0.272 sec)\n",
            "train - step 18788: loss = 1880.43 (0.264 sec)\n",
            "train - step 18789: loss = 1871.56 (0.269 sec)\n",
            "train - step 18790: loss = 2293.36 (0.269 sec)\n",
            "train - step 18791: loss = 1969.32 (0.264 sec)\n",
            "train - step 18792: loss = 1883.74 (0.267 sec)\n",
            "train - step 18793: loss = 1878.26 (0.257 sec)\n",
            "train - step 18794: loss = 1662.66 (0.283 sec)\n",
            "train - step 18795: loss = 1561.97 (0.267 sec)\n",
            "train - step 18796: loss = 1759.60 (0.267 sec)\n",
            "train - step 18797: loss = 1572.91 (0.270 sec)\n",
            "train - step 18798: loss = 1182.28 (0.278 sec)\n",
            "train - step 18799: loss = 1669.05 (0.257 sec)\n",
            "train - step 18800: loss = 1843.50 (0.275 sec)\n",
            "train - step 18801: loss = 2040.35 (0.260 sec)\n",
            "train - step 18802: loss = 1622.76 (0.270 sec)\n",
            "train - step 18803: loss = 1700.86 (0.265 sec)\n",
            "train - step 18804: loss = 1668.30 (0.278 sec)\n",
            "train - step 18805: loss = 2242.28 (0.257 sec)\n",
            "train - step 18806: loss = 1924.75 (0.282 sec)\n",
            "train - step 18807: loss = 1702.56 (0.267 sec)\n",
            "train - step 18808: loss = 1555.59 (0.266 sec)\n",
            "train - step 18809: loss = 1894.95 (0.269 sec)\n",
            "train - step 18810: loss = 1776.20 (0.258 sec)\n",
            "train - step 18811: loss = 1899.78 (0.272 sec)\n",
            "train - step 18812: loss = 1681.49 (0.263 sec)\n",
            "train - step 18813: loss = 1671.20 (0.261 sec)\n",
            "train - step 18814: loss = 1923.91 (0.271 sec)\n",
            "train - step 18815: loss = 1666.46 (0.262 sec)\n",
            "train - step 18816: loss = 1350.72 (0.266 sec)\n",
            "train - step 18817: loss = 2021.09 (0.270 sec)\n",
            "train - step 18818: loss = 1737.91 (0.259 sec)\n",
            "train - step 18819: loss = 1790.04 (0.265 sec)\n",
            "train - step 18820: loss = 1573.10 (0.268 sec)\n",
            "train - step 18821: loss = 2005.89 (0.275 sec)\n",
            "train - step 18822: loss = 1595.49 (0.271 sec)\n",
            "train - step 18823: loss = 2299.71 (0.258 sec)\n",
            "train - step 18824: loss = 1893.78 (0.268 sec)\n",
            "train - step 18825: loss = 1729.25 (0.285 sec)\n",
            "train - step 18826: loss = 1603.71 (0.277 sec)\n",
            "train - step 18827: loss = 1292.00 (0.283 sec)\n",
            "train - step 18828: loss = 1764.41 (0.274 sec)\n",
            "train - step 18829: loss = 1940.85 (0.286 sec)\n",
            "train - step 18830: loss = 1744.45 (0.273 sec)\n",
            "train - step 18831: loss = 1801.20 (0.280 sec)\n",
            "train - step 18832: loss = 1795.16 (0.280 sec)\n",
            "train - step 18833: loss = 1854.89 (0.284 sec)\n",
            "train - step 18834: loss = 1732.18 (0.278 sec)\n",
            "train - step 18835: loss = 1588.18 (0.274 sec)\n",
            "train - step 18836: loss = 2170.64 (0.283 sec)\n",
            "train - step 18837: loss = 1952.46 (0.280 sec)\n",
            "train - step 18838: loss = 1772.16 (0.273 sec)\n",
            "train - step 18839: loss = 1581.75 (0.271 sec)\n",
            "train - step 18840: loss = 1609.64 (0.276 sec)\n",
            "train - step 18841: loss = 1574.04 (0.281 sec)\n",
            "train - step 18842: loss = 2027.23 (0.282 sec)\n",
            "train - step 18843: loss = 2006.46 (0.275 sec)\n",
            "train - step 18844: loss = 1439.36 (0.277 sec)\n",
            "train - step 18845: loss = 1713.86 (0.280 sec)\n",
            "train - step 18846: loss = 1776.71 (0.274 sec)\n",
            "train - step 18847: loss = 1991.72 (0.286 sec)\n",
            "train - step 18848: loss = 1505.85 (0.282 sec)\n",
            "train - step 18849: loss = 1772.90 (0.282 sec)\n",
            "train - step 18850: loss = 1709.13 (0.281 sec)\n",
            "train - step 18851: loss = 1426.53 (0.284 sec)\n",
            "train - step 18852: loss = 2342.04 (0.281 sec)\n",
            "train - step 18853: loss = 1704.59 (0.277 sec)\n",
            "train - step 18854: loss = 1782.90 (0.285 sec)\n",
            "train - step 18855: loss = 1876.10 (0.279 sec)\n",
            "train - step 18856: loss = 1635.41 (0.273 sec)\n",
            "train - step 18857: loss = 1611.75 (0.274 sec)\n",
            "train - step 18858: loss = 1362.94 (0.282 sec)\n",
            "train - step 18859: loss = 1957.70 (0.272 sec)\n",
            "train - step 18860: loss = 1728.45 (0.289 sec)\n",
            "train - step 18861: loss = 1679.09 (0.270 sec)\n",
            "train - step 18862: loss = 1507.78 (0.270 sec)\n",
            "train - step 18863: loss = 1777.12 (0.272 sec)\n",
            "train - step 18864: loss = 1785.29 (0.267 sec)\n",
            "train - step 18865: loss = 1853.15 (0.264 sec)\n",
            "train - step 18866: loss = 2027.42 (0.272 sec)\n",
            "train - step 18867: loss = 1625.87 (0.259 sec)\n",
            "train - step 18868: loss = 1573.48 (0.261 sec)\n",
            "train - step 18869: loss = 2028.93 (0.272 sec)\n",
            "train - step 18870: loss = 1860.35 (0.260 sec)\n",
            "train - step 18871: loss = 1990.60 (0.259 sec)\n",
            "train - step 18872: loss = 2164.19 (0.262 sec)\n",
            "train - step 18873: loss = 1547.84 (0.273 sec)\n",
            "train - step 18874: loss = 1902.65 (0.269 sec)\n",
            "train - step 18875: loss = 1707.86 (0.267 sec)\n",
            "train - step 18876: loss = 1908.65 (0.266 sec)\n",
            "train - step 18877: loss = 1702.82 (0.263 sec)\n",
            "train - step 18878: loss = 2037.89 (0.267 sec)\n",
            "train - step 18879: loss = 1785.29 (0.261 sec)\n",
            "train - step 18880: loss = 1162.67 (0.260 sec)\n",
            "train - step 18881: loss = 1962.16 (0.273 sec)\n",
            "train - step 18882: loss = 1531.30 (0.270 sec)\n",
            "train - step 18883: loss = 1623.45 (0.265 sec)\n",
            "train - step 18884: loss = 1760.93 (0.268 sec)\n",
            "train - step 18885: loss = 1438.73 (0.271 sec)\n",
            "train - step 18886: loss = 2171.49 (0.265 sec)\n",
            "train - step 18887: loss = 1455.91 (0.272 sec)\n",
            "train - step 18888: loss = 1553.17 (0.270 sec)\n",
            "train - step 18889: loss = 1777.85 (0.270 sec)\n",
            "train - step 18890: loss = 2025.10 (0.265 sec)\n",
            "train - step 18891: loss = 1745.03 (0.265 sec)\n",
            "train - step 18892: loss = 1642.43 (0.265 sec)\n",
            "train - step 18893: loss = 1445.27 (0.274 sec)\n",
            "train - step 18894: loss = 1737.72 (0.263 sec)\n",
            "train - step 18895: loss = 1587.10 (0.277 sec)\n",
            "train - step 18896: loss = 1745.62 (0.271 sec)\n",
            "train - step 18897: loss = 1797.54 (0.259 sec)\n",
            "train - step 18898: loss = 1590.07 (0.268 sec)\n",
            "train - step 18899: loss = 1852.45 (0.267 sec)\n",
            "train - step 18900: loss = 1654.90 (0.267 sec)\n",
            "train - step 18901: loss = 1893.84 (0.267 sec)\n",
            "train - step 18902: loss = 1913.32 (0.265 sec)\n",
            "train - step 18903: loss = 1182.88 (0.257 sec)\n",
            "train - step 18904: loss = 1619.34 (0.893 sec)\n",
            "train - step 18905: loss = 2207.63 (0.262 sec)\n",
            "train - step 18906: loss = 1964.44 (0.269 sec)\n",
            "train - step 18907: loss = 1637.95 (0.264 sec)\n",
            "train - step 18908: loss = 1795.25 (0.263 sec)\n",
            "train - step 18909: loss = 1883.83 (0.272 sec)\n",
            "train - step 18910: loss = 1671.49 (0.265 sec)\n",
            "train - step 18911: loss = 1719.34 (0.274 sec)\n",
            "train - step 18912: loss = 1833.24 (0.269 sec)\n",
            "train - step 18913: loss = 1928.51 (0.266 sec)\n",
            "train - step 18914: loss = 1881.26 (0.272 sec)\n",
            "train - step 18915: loss = 2050.26 (0.271 sec)\n",
            "train - step 18916: loss = 1809.12 (0.262 sec)\n",
            "train - step 18917: loss = 1755.00 (0.272 sec)\n",
            "train - step 18918: loss = 2075.65 (0.275 sec)\n",
            "train - step 18919: loss = 1965.85 (0.267 sec)\n",
            "train - step 18920: loss = 1915.70 (0.274 sec)\n",
            "train - step 18921: loss = 1921.79 (0.274 sec)\n",
            "train - step 18922: loss = 2159.99 (0.262 sec)\n",
            "train - step 18923: loss = 1849.88 (0.268 sec)\n",
            "train - step 18924: loss = 2161.93 (0.271 sec)\n",
            "train - step 18925: loss = 1502.12 (0.267 sec)\n",
            "train - step 18926: loss = 1832.00 (0.266 sec)\n",
            "train - step 18927: loss = 1482.27 (0.271 sec)\n",
            "train - step 18928: loss = 1763.50 (0.265 sec)\n",
            "train - step 18929: loss = 1776.80 (0.270 sec)\n",
            "train - step 18930: loss = 1820.52 (0.270 sec)\n",
            "train - step 18931: loss = 1582.78 (0.261 sec)\n",
            "train - step 18932: loss = 1658.60 (0.259 sec)\n",
            "train - step 18933: loss = 1751.40 (0.266 sec)\n",
            "train - step 18934: loss = 1831.83 (0.269 sec)\n",
            "train - step 18935: loss = 1417.22 (0.273 sec)\n",
            "train - step 18936: loss = 2046.03 (0.263 sec)\n",
            "train - step 18937: loss = 1955.57 (0.278 sec)\n",
            "train - step 18938: loss = 1520.75 (0.260 sec)\n",
            "train - step 18939: loss = 1852.63 (0.268 sec)\n",
            "train - step 18940: loss = 1934.15 (0.277 sec)\n",
            "train - step 18941: loss = 1847.05 (0.268 sec)\n",
            "train - step 18942: loss = 2193.37 (0.264 sec)\n",
            "train - step 18943: loss = 2190.53 (0.255 sec)\n",
            "train - step 18944: loss = 2275.47 (0.270 sec)\n",
            "train - step 18945: loss = 1678.55 (0.271 sec)\n",
            "train - step 18946: loss = 1540.07 (0.265 sec)\n",
            "train - step 18947: loss = 1691.38 (0.275 sec)\n",
            "train - step 18948: loss = 1448.15 (0.269 sec)\n",
            "train - step 18949: loss = 1789.17 (0.269 sec)\n",
            "train - step 18950: loss = 1947.06 (0.270 sec)\n",
            "train - step 18951: loss = 1357.32 (0.290 sec)\n",
            "train - step 18952: loss = 1694.29 (0.274 sec)\n",
            "train - step 18953: loss = 1478.35 (0.295 sec)\n",
            "train - step 18954: loss = 1702.86 (0.274 sec)\n",
            "train - step 18955: loss = 1662.18 (0.268 sec)\n",
            "train - step 18956: loss = 1791.78 (0.277 sec)\n",
            "train - step 18957: loss = 1899.81 (0.276 sec)\n",
            "train - step 18958: loss = 1646.94 (0.280 sec)\n",
            "train - step 18959: loss = 1827.21 (0.273 sec)\n",
            "train - step 18960: loss = 2039.75 (0.266 sec)\n",
            "train - step 18961: loss = 1655.15 (0.270 sec)\n",
            "train - step 18962: loss = 1907.80 (0.266 sec)\n",
            "train - step 18963: loss = 1656.70 (0.284 sec)\n",
            "train - step 18964: loss = 1999.31 (0.275 sec)\n",
            "train - step 18965: loss = 1882.41 (0.264 sec)\n",
            "train - step 18966: loss = 1568.42 (0.270 sec)\n",
            "train - step 18967: loss = 1784.47 (0.278 sec)\n",
            "train - step 18968: loss = 1817.45 (0.267 sec)\n",
            "train - step 18969: loss = 1620.23 (0.276 sec)\n",
            "train - step 18970: loss = 1820.96 (0.264 sec)\n",
            "train - step 18971: loss = 1772.11 (0.285 sec)\n",
            "train - step 18972: loss = 1538.53 (0.265 sec)\n",
            "train - step 18973: loss = 1743.83 (0.260 sec)\n",
            "train - step 18974: loss = 1255.48 (0.270 sec)\n",
            "train - step 18975: loss = 1476.83 (0.275 sec)\n",
            "train - step 18976: loss = 2020.10 (0.265 sec)\n",
            "train - step 18977: loss = 1914.59 (0.288 sec)\n",
            "train - step 18978: loss = 1736.63 (0.269 sec)\n",
            "train - step 18979: loss = 1915.19 (0.264 sec)\n",
            "train - step 18980: loss = 1759.01 (0.261 sec)\n",
            "train - step 18981: loss = 1467.91 (0.260 sec)\n",
            "train - step 18982: loss = 1763.41 (0.267 sec)\n",
            "train - step 18983: loss = 1878.23 (0.268 sec)\n",
            "train - step 18984: loss = 1939.12 (0.254 sec)\n",
            "train - step 18985: loss = 1879.58 (0.275 sec)\n",
            "train - step 18986: loss = 2166.74 (0.271 sec)\n",
            "train - step 18987: loss = 1461.76 (0.256 sec)\n",
            "train - step 18988: loss = 1775.36 (0.269 sec)\n",
            "train - step 18989: loss = 2075.45 (0.271 sec)\n",
            "train - step 18990: loss = 1695.65 (0.280 sec)\n",
            "train - step 18991: loss = 2076.94 (0.266 sec)\n",
            "train - step 18992: loss = 1904.95 (0.268 sec)\n",
            "train - step 18993: loss = 1844.78 (0.280 sec)\n",
            "train - step 18994: loss = 1637.56 (0.275 sec)\n",
            "train - step 18995: loss = 1752.10 (0.278 sec)\n",
            "train - step 18996: loss = 2147.72 (0.266 sec)\n",
            "train - step 18997: loss = 1897.74 (0.294 sec)\n",
            "train - step 18998: loss = 1888.80 (0.277 sec)\n",
            "train - step 18999: loss = 1741.54 (0.266 sec)\n",
            "train - step 19000: loss = 1896.93 (0.275 sec)\n",
            "train - step 19001: loss = 1600.45 (0.288 sec)\n",
            "train - step 19002: loss = 1600.61 (0.275 sec)\n",
            "train - step 19003: loss = 1805.00 (0.279 sec)\n",
            "train - step 19004: loss = 1648.38 (0.281 sec)\n",
            "train - step 19005: loss = 2101.03 (0.281 sec)\n",
            "train - step 19006: loss = 1415.55 (0.273 sec)\n",
            "train - step 19007: loss = 1598.25 (0.272 sec)\n",
            "train - step 19008: loss = 1730.95 (0.282 sec)\n",
            "train - step 19009: loss = 1913.18 (0.285 sec)\n",
            "train - step 19010: loss = 2088.35 (0.280 sec)\n",
            "train - step 19011: loss = 1304.16 (0.274 sec)\n",
            "train - step 19012: loss = 1520.63 (0.284 sec)\n",
            "train - step 19013: loss = 2072.01 (0.268 sec)\n",
            "train - step 19014: loss = 1919.95 (0.293 sec)\n",
            "train - step 19015: loss = 1769.14 (0.494 sec)\n",
            "train - step 19016: loss = 1168.87 (0.415 sec)\n",
            "train - step 19017: loss = 1972.64 (0.285 sec)\n",
            "train - step 19018: loss = 1855.13 (0.281 sec)\n",
            "train - step 19019: loss = 1915.78 (0.275 sec)\n",
            "train - step 19020: loss = 2380.23 (0.284 sec)\n",
            "train - step 19021: loss = 2052.20 (0.282 sec)\n",
            "train - step 19022: loss = 1624.31 (0.291 sec)\n",
            "train - step 19023: loss = 2040.66 (0.267 sec)\n",
            "train - step 19024: loss = 1686.54 (0.273 sec)\n",
            "train - step 19025: loss = 1747.37 (0.264 sec)\n",
            "train - step 19026: loss = 1594.65 (0.263 sec)\n",
            "train - step 19027: loss = 2107.33 (0.267 sec)\n",
            "train - step 19028: loss = 1531.75 (0.261 sec)\n",
            "train - step 19029: loss = 1643.99 (0.273 sec)\n",
            "train - step 19030: loss = 1565.03 (0.357 sec)\n",
            "train - step 19031: loss = 2126.70 (0.254 sec)\n",
            "train - step 19032: loss = 1470.71 (0.263 sec)\n",
            "train - step 19033: loss = 2001.82 (0.272 sec)\n",
            "train - step 19034: loss = 1684.75 (0.270 sec)\n",
            "train - step 19035: loss = 1757.65 (0.264 sec)\n",
            "train - step 19036: loss = 1662.56 (0.261 sec)\n",
            "train - step 19037: loss = 1755.83 (0.278 sec)\n",
            "train - step 19038: loss = 1861.57 (0.264 sec)\n",
            "train - step 19039: loss = 1764.52 (0.262 sec)\n",
            "train - step 19040: loss = 1682.69 (0.268 sec)\n",
            "train - step 19041: loss = 1931.33 (0.267 sec)\n",
            "train - step 19042: loss = 1972.16 (0.260 sec)\n",
            "train - step 19043: loss = 1780.92 (0.268 sec)\n",
            "train - step 19044: loss = 1780.65 (0.265 sec)\n",
            "train - step 19045: loss = 1855.94 (0.266 sec)\n",
            "train - step 19046: loss = 2003.18 (0.261 sec)\n",
            "train - step 19047: loss = 1749.96 (0.261 sec)\n",
            "train - step 19048: loss = 1492.47 (0.273 sec)\n",
            "train - step 19049: loss = 1796.93 (0.268 sec)\n",
            "train - step 19050: loss = 1577.36 (0.262 sec)\n",
            "train - step 19051: loss = 1496.42 (0.265 sec)\n",
            "train - step 19052: loss = 1680.47 (0.271 sec)\n",
            "train - step 19053: loss = 1945.16 (0.279 sec)\n",
            "train - step 19054: loss = 1906.68 (0.270 sec)\n",
            "train - step 19055: loss = 1984.15 (0.273 sec)\n",
            "train - step 19056: loss = 1961.70 (0.265 sec)\n",
            "train - step 19057: loss = 1666.78 (0.268 sec)\n",
            "train - step 19058: loss = 2049.32 (0.265 sec)\n",
            "train - step 19059: loss = 1052.76 (0.267 sec)\n",
            "train - step 19060: loss = 2176.75 (0.268 sec)\n",
            "train - step 19061: loss = 1834.07 (0.263 sec)\n",
            "train - step 19062: loss = 1746.98 (0.270 sec)\n",
            "train - step 19063: loss = 1851.40 (0.269 sec)\n",
            "train - step 19064: loss = 1962.84 (0.266 sec)\n",
            "train - step 19065: loss = 1587.90 (0.273 sec)\n",
            "train - step 19066: loss = 1462.66 (0.265 sec)\n",
            "train - step 19067: loss = 1794.12 (0.263 sec)\n",
            "train - step 19068: loss = 1927.65 (0.260 sec)\n",
            "train - step 19069: loss = 1718.29 (0.263 sec)\n",
            "train - step 19070: loss = 1496.19 (0.270 sec)\n",
            "train - step 19071: loss = 1988.82 (0.264 sec)\n",
            "train - step 19072: loss = 1962.41 (0.262 sec)\n",
            "train - step 19073: loss = 2350.88 (0.266 sec)\n",
            "train - step 19074: loss = 2003.17 (0.274 sec)\n",
            "train - step 19075: loss = 1767.72 (0.271 sec)\n",
            "train - step 19076: loss = 1499.12 (0.275 sec)\n",
            "train - step 19077: loss = 1745.90 (0.267 sec)\n",
            "train - step 19078: loss = 1786.81 (0.263 sec)\n",
            "train - step 19079: loss = 1996.65 (0.268 sec)\n",
            "train - step 19080: loss = 1691.97 (0.267 sec)\n",
            "train - step 19081: loss = 2013.58 (0.267 sec)\n",
            "train - step 19082: loss = 1951.29 (0.266 sec)\n",
            "train - step 19083: loss = 2016.49 (0.271 sec)\n",
            "train - step 19084: loss = 1623.63 (0.270 sec)\n",
            "train - step 19085: loss = 1987.19 (0.257 sec)\n",
            "train - step 19086: loss = 1578.06 (0.265 sec)\n",
            "train - step 19087: loss = 1769.34 (0.269 sec)\n",
            "train - step 19088: loss = 1773.06 (0.263 sec)\n",
            "train - step 19089: loss = 1656.63 (0.262 sec)\n",
            "train - step 19090: loss = 1508.17 (0.267 sec)\n",
            "train - step 19091: loss = 2095.11 (0.267 sec)\n",
            "train - step 19092: loss = 1715.55 (0.263 sec)\n",
            "train - step 19093: loss = 1784.52 (0.260 sec)\n",
            "train - step 19094: loss = 1974.08 (0.272 sec)\n",
            "train - step 19095: loss = 1602.93 (0.273 sec)\n",
            "train - step 19096: loss = 1811.67 (0.257 sec)\n",
            "train - step 19097: loss = 1815.68 (0.271 sec)\n",
            "train - step 19098: loss = 1888.44 (0.273 sec)\n",
            "train - step 19099: loss = 1748.08 (0.276 sec)\n",
            "train - step 19100: loss = 1584.56 (0.267 sec)\n",
            "train - step 19101: loss = 1064.04 (0.265 sec)\n",
            "train - step 19102: loss = 1696.52 (0.268 sec)\n",
            "train - step 19103: loss = 1698.88 (0.265 sec)\n",
            "train - step 19104: loss = 1833.75 (0.268 sec)\n",
            "train - step 19105: loss = 1690.07 (0.257 sec)\n",
            "train - step 19106: loss = 1758.03 (0.270 sec)\n",
            "train - step 19107: loss = 1871.49 (0.264 sec)\n",
            "train - step 19108: loss = 2120.29 (0.262 sec)\n",
            "train - step 19109: loss = 1585.42 (0.258 sec)\n",
            "train - step 19110: loss = 2328.45 (0.265 sec)\n",
            "train - step 19111: loss = 1263.48 (0.267 sec)\n",
            "train - step 19112: loss = 2082.72 (0.260 sec)\n",
            "train - step 19113: loss = 1705.08 (0.269 sec)\n",
            "train - step 19114: loss = 1945.58 (0.270 sec)\n",
            "train - step 19115: loss = 1397.84 (0.262 sec)\n",
            "train - step 19116: loss = 1514.51 (0.271 sec)\n",
            "train - step 19117: loss = 1703.91 (0.263 sec)\n",
            "train - step 19118: loss = 1448.37 (0.267 sec)\n",
            "train - step 19119: loss = 1932.20 (0.261 sec)\n",
            "train - step 19120: loss = 1944.58 (0.264 sec)\n",
            "train - step 19121: loss = 1807.64 (0.275 sec)\n",
            "train - step 19122: loss = 1886.44 (0.264 sec)\n",
            "train - step 19123: loss = 1956.94 (0.259 sec)\n",
            "train - step 19124: loss = 1176.29 (0.965 sec)\n",
            "train - step 19125: loss = 1621.68 (0.261 sec)\n",
            "train - step 19126: loss = 2023.49 (0.270 sec)\n",
            "train - step 19127: loss = 1374.75 (0.257 sec)\n",
            "train - step 19128: loss = 1838.43 (0.260 sec)\n",
            "train - step 19129: loss = 1633.00 (0.263 sec)\n",
            "train - step 19130: loss = 1724.70 (0.267 sec)\n",
            "train - step 19131: loss = 1663.67 (0.263 sec)\n",
            "train - step 19132: loss = 1751.19 (0.267 sec)\n",
            "train - step 19133: loss = 1786.05 (0.274 sec)\n",
            "train - step 19134: loss = 1959.06 (0.267 sec)\n",
            "train - step 19135: loss = 1694.04 (0.264 sec)\n",
            "train - step 19136: loss = 1831.44 (0.262 sec)\n",
            "train - step 19137: loss = 1212.61 (0.256 sec)\n",
            "train - step 19138: loss = 2086.35 (0.285 sec)\n",
            "train - step 19139: loss = 1428.06 (0.266 sec)\n",
            "train - step 19140: loss = 2004.11 (0.263 sec)\n",
            "train - step 19141: loss = 1519.48 (0.259 sec)\n",
            "train - step 19142: loss = 1690.05 (0.275 sec)\n",
            "train - step 19143: loss = 1638.80 (0.263 sec)\n",
            "train - step 19144: loss = 670.81 (0.248 sec)\n",
            "train - step 19145: loss = 2331.71 (0.256 sec)\n",
            "train - step 19146: loss = 1686.79 (0.273 sec)\n",
            "train - step 19147: loss = 1805.59 (0.272 sec)\n",
            "train - step 19148: loss = 1807.18 (0.271 sec)\n",
            "train - step 19149: loss = 2146.21 (0.264 sec)\n",
            "train - step 19150: loss = 1889.47 (0.284 sec)\n",
            "train - step 19151: loss = 1896.42 (0.267 sec)\n",
            "train - step 19152: loss = 1797.02 (0.270 sec)\n",
            "train - step 19153: loss = 1749.59 (0.266 sec)\n",
            "train - step 19154: loss = 1955.08 (0.262 sec)\n",
            "train - step 19155: loss = 2375.31 (0.262 sec)\n",
            "train - step 19156: loss = 1690.89 (0.268 sec)\n",
            "train - step 19157: loss = 1734.10 (0.266 sec)\n",
            "train - step 19158: loss = 1041.16 (0.255 sec)\n",
            "train - step 19159: loss = 1877.12 (0.266 sec)\n",
            "train - step 19160: loss = 1862.14 (0.262 sec)\n",
            "train - step 19161: loss = 2202.41 (0.257 sec)\n",
            "train - step 19162: loss = 1596.19 (0.267 sec)\n",
            "train - step 19163: loss = 1723.61 (0.266 sec)\n",
            "train - step 19164: loss = 1579.87 (0.264 sec)\n",
            "train - step 19165: loss = 1838.22 (0.269 sec)\n",
            "train - step 19166: loss = 1788.34 (0.270 sec)\n",
            "train - step 19167: loss = 1569.44 (0.265 sec)\n",
            "train - step 19168: loss = 1706.01 (0.260 sec)\n",
            "train - step 19169: loss = 1809.92 (0.276 sec)\n",
            "train - step 19170: loss = 1689.24 (0.256 sec)\n",
            "train - step 19171: loss = 1492.14 (0.273 sec)\n",
            "train - step 19172: loss = 2036.82 (0.278 sec)\n",
            "train - step 19173: loss = 2104.10 (0.269 sec)\n",
            "train - step 19174: loss = 2052.79 (0.266 sec)\n",
            "train - step 19175: loss = 1999.01 (0.266 sec)\n",
            "train - step 19176: loss = 1634.30 (0.272 sec)\n",
            "train - step 19177: loss = 1794.17 (0.273 sec)\n",
            "train - step 19178: loss = 1789.62 (0.260 sec)\n",
            "train - step 19179: loss = 1738.33 (0.265 sec)\n",
            "train - step 19180: loss = 1731.32 (0.276 sec)\n",
            "train - step 19181: loss = 1772.05 (0.264 sec)\n",
            "train - step 19182: loss = 2372.50 (0.263 sec)\n",
            "train - step 19183: loss = 1863.23 (0.266 sec)\n",
            "train - step 19184: loss = 1659.37 (0.267 sec)\n",
            "train - step 19185: loss = 1689.98 (0.262 sec)\n",
            "train - step 19186: loss = 1480.23 (0.265 sec)\n",
            "train - step 19187: loss = 1562.42 (0.263 sec)\n",
            "train - step 19188: loss = 1982.61 (0.268 sec)\n",
            "train - step 19189: loss = 1830.73 (0.264 sec)\n",
            "train - step 19190: loss = 2075.85 (0.264 sec)\n",
            "train - step 19191: loss = 1833.83 (0.274 sec)\n",
            "train - step 19192: loss = 1699.73 (0.279 sec)\n",
            "train - step 19193: loss = 1760.63 (0.264 sec)\n",
            "train - step 19194: loss = 1683.04 (0.270 sec)\n",
            "train - step 19195: loss = 2114.10 (0.269 sec)\n",
            "train - step 19196: loss = 1741.43 (0.276 sec)\n",
            "train - step 19197: loss = 1657.90 (0.265 sec)\n",
            "train - step 19198: loss = 1700.29 (0.254 sec)\n",
            "train - step 19199: loss = 1798.24 (0.269 sec)\n",
            "train - step 19200: loss = 1495.21 (0.268 sec)\n",
            "train - step 19201: loss = 1596.43 (0.265 sec)\n",
            "train - step 19202: loss = 1945.46 (0.268 sec)\n",
            "train - step 19203: loss = 1433.28 (0.269 sec)\n",
            "train - step 19204: loss = 1814.09 (0.262 sec)\n",
            "train - step 19205: loss = 1652.30 (0.263 sec)\n",
            "train - step 19206: loss = 2046.69 (0.267 sec)\n",
            "train - step 19207: loss = 1745.21 (0.262 sec)\n",
            "train - step 19208: loss = 1164.23 (0.270 sec)\n",
            "train - step 19209: loss = 2088.53 (0.259 sec)\n",
            "train - step 19210: loss = 2301.67 (0.258 sec)\n",
            "train - step 19211: loss = 1600.31 (0.271 sec)\n",
            "train - step 19212: loss = 1923.71 (0.277 sec)\n",
            "train - step 19213: loss = 1855.75 (0.259 sec)\n",
            "train - step 19214: loss = 1592.85 (0.271 sec)\n",
            "train - step 19215: loss = 1820.64 (0.272 sec)\n",
            "train - step 19216: loss = 1275.35 (0.273 sec)\n",
            "train - step 19217: loss = 1657.43 (0.260 sec)\n",
            "train - step 19218: loss = 1843.34 (0.268 sec)\n",
            "train - step 19219: loss = 1489.17 (0.272 sec)\n",
            "train - step 19220: loss = 1792.76 (0.270 sec)\n",
            "train - step 19221: loss = 1769.44 (0.308 sec)\n",
            "train - step 19222: loss = 1716.52 (0.266 sec)\n",
            "train - step 19223: loss = 1871.66 (0.272 sec)\n",
            "train - step 19224: loss = 1754.48 (0.262 sec)\n",
            "train - step 19225: loss = 2046.09 (0.263 sec)\n",
            "train - step 19226: loss = 1979.30 (0.270 sec)\n",
            "train - step 19227: loss = 1251.07 (0.255 sec)\n",
            "train - step 19228: loss = 1708.46 (0.259 sec)\n",
            "train - step 19229: loss = 1525.74 (0.264 sec)\n",
            "train - step 19230: loss = 1733.25 (0.269 sec)\n",
            "train - step 19231: loss = 2066.85 (0.259 sec)\n",
            "train - step 19232: loss = 1579.72 (0.270 sec)\n",
            "train - step 19233: loss = 1959.45 (0.266 sec)\n",
            "train - step 19234: loss = 1899.45 (0.265 sec)\n",
            "train - step 19235: loss = 1876.94 (0.270 sec)\n",
            "train - step 19236: loss = 1704.94 (0.269 sec)\n",
            "train - step 19237: loss = 1645.52 (0.266 sec)\n",
            "train - step 19238: loss = 1610.77 (0.275 sec)\n",
            "train - step 19239: loss = 1803.00 (0.266 sec)\n",
            "train - step 19240: loss = 2194.16 (0.259 sec)\n",
            "train - step 19241: loss = 1540.49 (0.267 sec)\n",
            "train - step 19242: loss = 1768.95 (0.270 sec)\n",
            "train - step 19243: loss = 1839.79 (0.258 sec)\n",
            "train - step 19244: loss = 2058.51 (0.256 sec)\n",
            "train - step 19245: loss = 1903.50 (0.258 sec)\n",
            "train - step 19246: loss = 1645.64 (0.275 sec)\n",
            "train - step 19247: loss = 2218.38 (0.260 sec)\n",
            "train - step 19248: loss = 1819.97 (0.257 sec)\n",
            "train - step 19249: loss = 1800.03 (0.259 sec)\n",
            "train - step 19250: loss = 1970.69 (0.265 sec)\n",
            "train - step 19251: loss = 2007.02 (0.266 sec)\n",
            "train - step 19252: loss = 1643.09 (0.270 sec)\n",
            "train - step 19253: loss = 2160.99 (0.263 sec)\n",
            "train - step 19254: loss = 1773.89 (0.268 sec)\n",
            "train - step 19255: loss = 1065.85 (0.270 sec)\n",
            "train - step 19256: loss = 1896.51 (0.277 sec)\n",
            "train - step 19257: loss = 1783.43 (0.264 sec)\n",
            "train - step 19258: loss = 2136.43 (0.260 sec)\n",
            "train - step 19259: loss = 1976.82 (0.269 sec)\n",
            "train - step 19260: loss = 2132.87 (0.262 sec)\n",
            "train - step 19261: loss = 1498.50 (0.268 sec)\n",
            "train - step 19262: loss = 1662.03 (0.267 sec)\n",
            "train - step 19263: loss = 2087.61 (0.258 sec)\n",
            "train - step 19264: loss = 1823.31 (0.267 sec)\n",
            "train - step 19265: loss = 2085.29 (0.273 sec)\n",
            "train - step 19266: loss = 1714.31 (0.265 sec)\n",
            "train - step 19267: loss = 1738.46 (0.263 sec)\n",
            "train - step 19268: loss = 2478.06 (0.261 sec)\n",
            "train - step 19269: loss = 1773.15 (0.278 sec)\n",
            "train - step 19270: loss = 1753.82 (0.266 sec)\n",
            "train - step 19271: loss = 2141.48 (0.259 sec)\n",
            "train - step 19272: loss = 1622.88 (0.271 sec)\n",
            "train - step 19273: loss = 2397.50 (0.269 sec)\n",
            "train - step 19274: loss = 1773.44 (0.259 sec)\n",
            "train - step 19275: loss = 1749.67 (0.262 sec)\n",
            "train - step 19276: loss = 1525.99 (0.268 sec)\n",
            "train - step 19277: loss = 1370.52 (0.265 sec)\n",
            "train - step 19278: loss = 1785.22 (0.272 sec)\n",
            "train - step 19279: loss = 1448.03 (0.269 sec)\n",
            "train - step 19280: loss = 1318.87 (0.270 sec)\n",
            "train - step 19281: loss = 2132.81 (0.259 sec)\n",
            "train - step 19282: loss = 1974.00 (0.265 sec)\n",
            "train - step 19283: loss = 1854.30 (0.258 sec)\n",
            "train - step 19284: loss = 1689.98 (0.262 sec)\n",
            "train - step 19285: loss = 1466.72 (0.280 sec)\n",
            "train - step 19286: loss = 1754.72 (0.275 sec)\n",
            "train - step 19287: loss = 1823.81 (0.278 sec)\n",
            "train - step 19288: loss = 1709.45 (0.278 sec)\n",
            "train - step 19289: loss = 2436.82 (0.268 sec)\n",
            "train - step 19290: loss = 1598.90 (0.277 sec)\n",
            "train - step 19291: loss = 1822.63 (0.281 sec)\n",
            "train - step 19292: loss = 2088.81 (0.284 sec)\n",
            "train - step 19293: loss = 1840.78 (0.269 sec)\n",
            "train - step 19294: loss = 1923.72 (0.278 sec)\n",
            "train - step 19295: loss = 1416.24 (0.270 sec)\n",
            "train - step 19296: loss = 1518.11 (0.286 sec)\n",
            "train - step 19297: loss = 1406.80 (0.276 sec)\n",
            "train - step 19298: loss = 1823.69 (0.270 sec)\n",
            "train - step 19299: loss = 1435.60 (0.281 sec)\n",
            "train - step 19300: loss = 2114.19 (0.269 sec)\n",
            "train - step 19301: loss = 2292.48 (0.272 sec)\n",
            "train - step 19302: loss = 1968.73 (0.272 sec)\n",
            "train - step 19303: loss = 1724.57 (0.266 sec)\n",
            "train - step 19304: loss = 1715.79 (0.275 sec)\n",
            "train - step 19305: loss = 2045.79 (0.271 sec)\n",
            "train - step 19306: loss = 1642.26 (0.275 sec)\n",
            "train - step 19307: loss = 1662.45 (0.279 sec)\n",
            "train - step 19308: loss = 1907.51 (0.272 sec)\n",
            "train - step 19309: loss = 1651.71 (0.275 sec)\n",
            "train - step 19310: loss = 1169.72 (0.270 sec)\n",
            "train - step 19311: loss = 1929.97 (0.277 sec)\n",
            "train - step 19312: loss = 1652.51 (0.279 sec)\n",
            "train - step 19313: loss = 1672.43 (0.275 sec)\n",
            "train - step 19314: loss = 1833.24 (0.279 sec)\n",
            "train - step 19315: loss = 1780.45 (0.277 sec)\n",
            "train - step 19316: loss = 1566.88 (0.280 sec)\n",
            "train - step 19317: loss = 1989.79 (0.272 sec)\n",
            "train - step 19318: loss = 1513.00 (0.280 sec)\n",
            "train - step 19319: loss = 2381.96 (0.266 sec)\n",
            "train - step 19320: loss = 1464.07 (0.275 sec)\n",
            "train - step 19321: loss = 2329.80 (0.260 sec)\n",
            "train - step 19322: loss = 1781.56 (0.275 sec)\n",
            "train - step 19323: loss = 2023.65 (0.260 sec)\n",
            "train - step 19324: loss = 1560.45 (0.268 sec)\n",
            "train - step 19325: loss = 2107.67 (0.266 sec)\n",
            "train - step 19326: loss = 1820.49 (0.273 sec)\n",
            "train - step 19327: loss = 1827.79 (0.262 sec)\n",
            "train - step 19328: loss = 1798.37 (0.269 sec)\n",
            "train - step 19329: loss = 1678.39 (0.275 sec)\n",
            "train - step 19330: loss = 1745.42 (0.273 sec)\n",
            "train - step 19331: loss = 1876.10 (0.268 sec)\n",
            "train - step 19332: loss = 1763.24 (0.270 sec)\n",
            "train - step 19333: loss = 1581.73 (0.276 sec)\n",
            "train - step 19334: loss = 1828.41 (0.272 sec)\n",
            "train - step 19335: loss = 2466.92 (0.259 sec)\n",
            "train - step 19336: loss = 1553.76 (0.264 sec)\n",
            "train - step 19337: loss = 1317.63 (0.265 sec)\n",
            "train - step 19338: loss = 1657.37 (0.258 sec)\n",
            "train - step 19339: loss = 1762.74 (0.266 sec)\n",
            "train - step 19340: loss = 1834.51 (0.265 sec)\n",
            "train - step 19341: loss = 1642.87 (0.268 sec)\n",
            "train - step 19342: loss = 2386.89 (0.260 sec)\n",
            "train - step 19343: loss = 1852.94 (0.255 sec)\n",
            "train - step 19344: loss = 1847.49 (0.281 sec)\n",
            "train - step 19345: loss = 1681.28 (0.270 sec)\n",
            "train - step 19346: loss = 1989.48 (0.268 sec)\n",
            "train - step 19347: loss = 2013.18 (0.865 sec)\n",
            "train - step 19348: loss = 1871.68 (0.265 sec)\n",
            "train - step 19349: loss = 1787.94 (0.265 sec)\n",
            "train - step 19350: loss = 2047.47 (0.259 sec)\n",
            "train - step 19351: loss = 1292.90 (0.266 sec)\n",
            "train - step 19352: loss = 1557.91 (0.265 sec)\n",
            "train - step 19353: loss = 1954.20 (0.267 sec)\n",
            "train - step 19354: loss = 1670.66 (0.267 sec)\n",
            "train - step 19355: loss = 1541.92 (0.267 sec)\n",
            "train - step 19356: loss = 2117.58 (0.259 sec)\n",
            "train - step 19357: loss = 1682.86 (0.261 sec)\n",
            "train - step 19358: loss = 1415.35 (0.264 sec)\n",
            "train - step 19359: loss = 1990.73 (0.262 sec)\n",
            "train - step 19360: loss = 1864.33 (0.274 sec)\n",
            "train - step 19361: loss = 1517.17 (0.269 sec)\n",
            "train - step 19362: loss = 1689.69 (0.270 sec)\n",
            "train - step 19363: loss = 1343.27 (0.268 sec)\n",
            "train - step 19364: loss = 1419.48 (0.276 sec)\n",
            "train - step 19365: loss = 1608.68 (0.265 sec)\n",
            "train - step 19366: loss = 2122.00 (0.273 sec)\n",
            "train - step 19367: loss = 1956.15 (0.268 sec)\n",
            "train - step 19368: loss = 1581.73 (0.268 sec)\n",
            "train - step 19369: loss = 1551.70 (0.261 sec)\n",
            "train - step 19370: loss = 1810.96 (0.275 sec)\n",
            "train - step 19371: loss = 1710.74 (0.270 sec)\n",
            "train - step 19372: loss = 1837.88 (0.264 sec)\n",
            "train - step 19373: loss = 1972.24 (0.277 sec)\n",
            "train - step 19374: loss = 1663.22 (0.267 sec)\n",
            "train - step 19375: loss = 2061.10 (0.265 sec)\n",
            "train - step 19376: loss = 1689.49 (0.272 sec)\n",
            "train - step 19377: loss = 1512.61 (0.270 sec)\n",
            "train - step 19378: loss = 1795.99 (0.269 sec)\n",
            "train - step 19379: loss = 1459.49 (0.268 sec)\n",
            "train - step 19380: loss = 1860.94 (0.274 sec)\n",
            "train - step 19381: loss = 1810.82 (0.270 sec)\n",
            "train - step 19382: loss = 1594.20 (0.274 sec)\n",
            "train - step 19383: loss = 1507.15 (0.258 sec)\n",
            "train - step 19384: loss = 1581.72 (0.267 sec)\n",
            "train - step 19385: loss = 1866.57 (0.267 sec)\n",
            "train - step 19386: loss = 1982.54 (0.267 sec)\n",
            "train - step 19387: loss = 1905.61 (0.266 sec)\n",
            "train - step 19388: loss = 2090.39 (0.260 sec)\n",
            "train - step 19389: loss = 1747.70 (0.274 sec)\n",
            "train - step 19390: loss = 2390.06 (0.260 sec)\n",
            "train - step 19391: loss = 1628.53 (0.267 sec)\n",
            "train - step 19392: loss = 1898.84 (0.263 sec)\n",
            "train - step 19393: loss = 1233.13 (0.263 sec)\n",
            "train - step 19394: loss = 1724.08 (0.269 sec)\n",
            "train - step 19395: loss = 1526.36 (0.265 sec)\n",
            "train - step 19396: loss = 1585.31 (0.274 sec)\n",
            "train - step 19397: loss = 1876.85 (0.273 sec)\n",
            "train - step 19398: loss = 1845.69 (0.271 sec)\n",
            "train - step 19399: loss = 1625.18 (0.266 sec)\n",
            "train - step 19400: loss = 2026.82 (0.266 sec)\n",
            "train - step 19401: loss = 2075.27 (0.267 sec)\n",
            "train - step 19402: loss = 1948.45 (0.257 sec)\n",
            "train - step 19403: loss = 2021.53 (0.259 sec)\n",
            "train - step 19404: loss = 1768.73 (0.255 sec)\n",
            "train - step 19405: loss = 1573.83 (0.265 sec)\n",
            "train - step 19406: loss = 2228.34 (0.268 sec)\n",
            "train - step 19407: loss = 1633.80 (0.258 sec)\n",
            "train - step 19408: loss = 1939.71 (0.271 sec)\n",
            "train - step 19409: loss = 2208.34 (0.266 sec)\n",
            "train - step 19410: loss = 1543.98 (0.275 sec)\n",
            "train - step 19411: loss = 1615.95 (0.268 sec)\n",
            "train - step 19412: loss = 1705.51 (0.274 sec)\n",
            "train - step 19413: loss = 1920.11 (0.272 sec)\n",
            "train - step 19414: loss = 1048.45 (0.256 sec)\n",
            "train - step 19415: loss = 1644.91 (0.262 sec)\n",
            "train - step 19416: loss = 1904.51 (0.274 sec)\n",
            "train - step 19417: loss = 2167.72 (0.263 sec)\n",
            "train - step 19418: loss = 1763.35 (0.273 sec)\n",
            "train - step 19419: loss = 2184.36 (0.266 sec)\n",
            "train - step 19420: loss = 1362.84 (0.273 sec)\n",
            "train - step 19421: loss = 1401.97 (0.259 sec)\n",
            "train - step 19422: loss = 1579.01 (0.266 sec)\n",
            "train - step 19423: loss = 2215.76 (0.266 sec)\n",
            "train - step 19424: loss = 1855.96 (0.268 sec)\n",
            "train - step 19425: loss = 1901.41 (0.262 sec)\n",
            "train - step 19426: loss = 1781.92 (0.267 sec)\n",
            "train - step 19427: loss = 1930.92 (0.271 sec)\n",
            "train - step 19428: loss = 1864.33 (0.270 sec)\n",
            "train - step 19429: loss = 2209.50 (0.262 sec)\n",
            "train - step 19430: loss = 2005.77 (0.259 sec)\n",
            "train - step 19431: loss = 1928.51 (0.263 sec)\n",
            "train - step 19432: loss = 1888.44 (0.267 sec)\n",
            "train - step 19433: loss = 1461.09 (0.268 sec)\n",
            "train - step 19434: loss = 1907.17 (0.266 sec)\n",
            "train - step 19435: loss = 2077.67 (0.276 sec)\n",
            "train - step 19436: loss = 1788.25 (0.265 sec)\n",
            "train - step 19437: loss = 1672.05 (0.265 sec)\n",
            "train - step 19438: loss = 649.07 (0.260 sec)\n",
            "train - step 19439: loss = 1695.86 (0.273 sec)\n",
            "train - step 19440: loss = 1750.38 (0.262 sec)\n",
            "train - step 19441: loss = 1689.84 (0.268 sec)\n",
            "train - step 19442: loss = 1712.40 (0.259 sec)\n",
            "train - step 19443: loss = 1706.31 (0.273 sec)\n",
            "train - step 19444: loss = 1566.79 (0.266 sec)\n",
            "train - step 19445: loss = 1703.67 (0.263 sec)\n",
            "train - step 19446: loss = 1161.33 (0.272 sec)\n",
            "train - step 19447: loss = 1442.78 (0.269 sec)\n",
            "train - step 19448: loss = 1756.18 (0.276 sec)\n",
            "train - step 19449: loss = 1683.94 (0.275 sec)\n",
            "train - step 19450: loss = 1573.84 (0.282 sec)\n",
            "train - step 19451: loss = 1334.58 (0.268 sec)\n",
            "train - step 19452: loss = 1815.94 (0.277 sec)\n",
            "train - step 19453: loss = 1736.41 (0.285 sec)\n",
            "train - step 19454: loss = 1494.43 (0.289 sec)\n",
            "train - step 19455: loss = 1980.77 (0.278 sec)\n",
            "train - step 19456: loss = 1794.93 (0.280 sec)\n",
            "train - step 19457: loss = 1820.92 (0.278 sec)\n",
            "train - step 19458: loss = 1993.10 (0.283 sec)\n",
            "train - step 19459: loss = 2113.57 (0.271 sec)\n",
            "train - step 19460: loss = 1905.35 (0.279 sec)\n",
            "train - step 19461: loss = 2370.53 (0.276 sec)\n",
            "train - step 19462: loss = 2016.39 (0.281 sec)\n",
            "train - step 19463: loss = 1641.95 (0.274 sec)\n",
            "train - step 19464: loss = 1985.03 (0.273 sec)\n",
            "train - step 19465: loss = 1983.53 (0.274 sec)\n",
            "train - step 19466: loss = 1658.31 (0.277 sec)\n",
            "train - step 19467: loss = 1241.23 (0.265 sec)\n",
            "train - step 19468: loss = 1931.81 (0.269 sec)\n",
            "train - step 19469: loss = 1503.15 (0.272 sec)\n",
            "train - step 19470: loss = 1640.01 (0.263 sec)\n",
            "train - step 19471: loss = 1131.91 (0.266 sec)\n",
            "train - step 19472: loss = 1764.56 (0.261 sec)\n",
            "train - step 19473: loss = 586.25 (0.262 sec)\n",
            "train - step 19474: loss = 1894.03 (0.262 sec)\n",
            "train - step 19475: loss = 1929.52 (0.269 sec)\n",
            "train - step 19476: loss = 1728.22 (0.269 sec)\n",
            "train - step 19477: loss = 1853.49 (0.265 sec)\n",
            "train - step 19478: loss = 1224.50 (0.264 sec)\n",
            "train - step 19479: loss = 1720.04 (0.271 sec)\n",
            "train - step 19480: loss = 1702.30 (0.268 sec)\n",
            "train - step 19481: loss = 499.48 (0.267 sec)\n",
            "train - step 19482: loss = 1486.15 (0.263 sec)\n",
            "train - step 19483: loss = 1782.05 (0.258 sec)\n",
            "train - step 19484: loss = 1952.84 (0.268 sec)\n",
            "train - step 19485: loss = 1892.38 (0.267 sec)\n",
            "train - step 19486: loss = 1623.08 (0.262 sec)\n",
            "train - step 19487: loss = 1571.92 (0.271 sec)\n",
            "train - step 19488: loss = 1965.91 (0.265 sec)\n",
            "train - step 19489: loss = 1731.46 (0.266 sec)\n",
            "train - step 19490: loss = 1702.01 (0.267 sec)\n",
            "train - step 19491: loss = 1787.07 (0.269 sec)\n",
            "train - step 19492: loss = 1736.32 (0.268 sec)\n",
            "train - step 19493: loss = 1633.88 (0.263 sec)\n",
            "train - step 19494: loss = 1319.14 (0.259 sec)\n",
            "train - step 19495: loss = 1912.77 (0.273 sec)\n",
            "train - step 19496: loss = 1756.36 (0.273 sec)\n",
            "train - step 19497: loss = 1678.03 (0.268 sec)\n",
            "train - step 19498: loss = 1796.75 (0.276 sec)\n",
            "train - step 19499: loss = 1453.11 (0.267 sec)\n",
            "train - step 19500: loss = 2193.66 (0.260 sec)\n",
            "train - step 19501: loss = 1656.45 (0.269 sec)\n",
            "train - step 19502: loss = 1909.43 (0.265 sec)\n",
            "train - step 19503: loss = 1704.48 (0.270 sec)\n",
            "train - step 19504: loss = 1585.40 (0.275 sec)\n",
            "train - step 19505: loss = 2126.63 (0.261 sec)\n",
            "train - step 19506: loss = 1821.71 (0.262 sec)\n",
            "train - step 19507: loss = 1628.28 (0.266 sec)\n",
            "train - step 19508: loss = 1825.81 (0.269 sec)\n",
            "train - step 19509: loss = 2528.26 (0.260 sec)\n",
            "train - step 19510: loss = 1512.15 (0.263 sec)\n",
            "train - step 19511: loss = 1861.85 (0.262 sec)\n",
            "train - step 19512: loss = 1504.27 (0.268 sec)\n",
            "train - step 19513: loss = 1538.27 (0.266 sec)\n",
            "train - step 19514: loss = 1702.29 (0.263 sec)\n",
            "train - step 19515: loss = 1826.77 (0.265 sec)\n",
            "train - step 19516: loss = 2054.84 (0.261 sec)\n",
            "train - step 19517: loss = 1693.56 (0.264 sec)\n",
            "train - step 19518: loss = 2059.98 (0.261 sec)\n",
            "train - step 19519: loss = 2075.18 (0.263 sec)\n",
            "train - step 19520: loss = 1631.08 (0.274 sec)\n",
            "train - step 19521: loss = 1631.21 (0.265 sec)\n",
            "train - step 19522: loss = 1731.78 (0.266 sec)\n",
            "train - step 19523: loss = 1795.13 (0.271 sec)\n",
            "train - step 19524: loss = 1202.45 (0.267 sec)\n",
            "train - step 19525: loss = 1875.56 (0.271 sec)\n",
            "train - step 19526: loss = 1520.35 (0.264 sec)\n",
            "train - step 19527: loss = 1779.56 (0.266 sec)\n",
            "train - step 19528: loss = 1560.49 (0.266 sec)\n",
            "train - step 19529: loss = 1611.73 (0.267 sec)\n",
            "train - step 19530: loss = 1823.38 (0.260 sec)\n",
            "train - step 19531: loss = 1796.43 (0.272 sec)\n",
            "train - step 19532: loss = 1759.44 (0.261 sec)\n",
            "train - step 19533: loss = 1617.40 (0.271 sec)\n",
            "train - step 19534: loss = 1684.51 (0.266 sec)\n",
            "train - step 19535: loss = 1947.56 (0.273 sec)\n",
            "train - step 19536: loss = 2400.70 (0.264 sec)\n",
            "train - step 19537: loss = 1660.76 (0.260 sec)\n",
            "train - step 19538: loss = 1469.82 (0.266 sec)\n",
            "train - step 19539: loss = 1841.79 (0.263 sec)\n",
            "train - step 19540: loss = 1660.50 (0.268 sec)\n",
            "train - step 19541: loss = 1949.58 (0.265 sec)\n",
            "train - step 19542: loss = 1629.55 (0.259 sec)\n",
            "train - step 19543: loss = 1762.45 (0.255 sec)\n",
            "train - step 19544: loss = 1764.34 (0.263 sec)\n",
            "train - step 19545: loss = 1786.18 (0.262 sec)\n",
            "train - step 19546: loss = 1727.76 (0.274 sec)\n",
            "train - step 19547: loss = 1689.84 (0.260 sec)\n",
            "train - step 19548: loss = 1893.06 (0.269 sec)\n",
            "train - step 19549: loss = 1776.69 (0.262 sec)\n",
            "train - step 19550: loss = 1983.35 (0.267 sec)\n",
            "train - step 19551: loss = 1880.19 (0.266 sec)\n",
            "train - step 19552: loss = 1823.33 (0.262 sec)\n",
            "train - step 19553: loss = 1658.89 (0.260 sec)\n",
            "train - step 19554: loss = 1765.32 (0.272 sec)\n",
            "train - step 19555: loss = 1671.43 (0.259 sec)\n",
            "train - step 19556: loss = 1228.88 (0.270 sec)\n",
            "train - step 19557: loss = 2022.66 (0.259 sec)\n",
            "train - step 19558: loss = 1709.79 (0.274 sec)\n",
            "train - step 19559: loss = 1573.83 (0.256 sec)\n",
            "train - step 19560: loss = 1388.92 (0.270 sec)\n",
            "train - step 19561: loss = 1583.07 (0.263 sec)\n",
            "train - step 19562: loss = 1820.46 (0.278 sec)\n",
            "train - step 19563: loss = 1562.72 (0.262 sec)\n",
            "train - step 19564: loss = 1770.98 (0.264 sec)\n",
            "train - step 19565: loss = 1746.81 (0.265 sec)\n",
            "train - step 19566: loss = 1697.79 (0.271 sec)\n",
            "train - step 19567: loss = 1771.19 (0.260 sec)\n",
            "train - step 19568: loss = 1911.08 (0.269 sec)\n",
            "train - step 19569: loss = 1526.39 (0.267 sec)\n",
            "train - step 19570: loss = 1778.60 (0.967 sec)\n",
            "train - step 19571: loss = 1564.47 (0.265 sec)\n",
            "train - step 19572: loss = 1668.19 (0.265 sec)\n",
            "train - step 19573: loss = 1703.35 (0.267 sec)\n",
            "train - step 19574: loss = 1778.16 (0.272 sec)\n",
            "train - step 19575: loss = 1379.60 (0.269 sec)\n",
            "train - step 19576: loss = 1588.07 (0.267 sec)\n",
            "train - step 19577: loss = 1631.51 (0.267 sec)\n",
            "train - step 19578: loss = 1663.54 (0.266 sec)\n",
            "train - step 19579: loss = 1846.13 (0.275 sec)\n",
            "train - step 19580: loss = 1994.69 (0.269 sec)\n",
            "train - step 19581: loss = 2087.32 (0.273 sec)\n",
            "train - step 19582: loss = 2004.79 (0.281 sec)\n",
            "train - step 19583: loss = 1834.56 (0.273 sec)\n",
            "train - step 19584: loss = 2248.54 (0.271 sec)\n",
            "train - step 19585: loss = 1843.50 (0.281 sec)\n",
            "train - step 19586: loss = 1813.15 (0.272 sec)\n",
            "train - step 19587: loss = 1598.58 (0.272 sec)\n",
            "train - step 19588: loss = 982.93 (0.268 sec)\n",
            "train - step 19589: loss = 1520.40 (0.279 sec)\n",
            "train - step 19590: loss = 2099.48 (0.278 sec)\n",
            "train - step 19591: loss = 2518.23 (0.265 sec)\n",
            "train - step 19592: loss = 1764.36 (0.277 sec)\n",
            "train - step 19593: loss = 1754.81 (0.270 sec)\n",
            "train - step 19594: loss = 1784.43 (0.278 sec)\n",
            "train - step 19595: loss = 1987.63 (0.271 sec)\n",
            "train - step 19596: loss = 2122.01 (0.282 sec)\n",
            "train - step 19597: loss = 1532.68 (0.279 sec)\n",
            "train - step 19598: loss = 1750.04 (0.275 sec)\n",
            "train - step 19599: loss = 1463.58 (0.277 sec)\n",
            "train - step 19600: loss = 1935.99 (0.274 sec)\n",
            "train - step 19601: loss = 1516.15 (0.277 sec)\n",
            "train - step 19602: loss = 2202.10 (0.270 sec)\n",
            "train - step 19603: loss = 2227.36 (0.270 sec)\n",
            "train - step 19604: loss = 1644.64 (0.276 sec)\n",
            "train - step 19605: loss = 2013.27 (0.279 sec)\n",
            "train - step 19606: loss = 999.29 (0.279 sec)\n",
            "train - step 19607: loss = 1299.48 (0.277 sec)\n",
            "train - step 19608: loss = 1845.23 (0.276 sec)\n",
            "train - step 19609: loss = 1420.94 (0.283 sec)\n",
            "train - step 19610: loss = 2297.07 (0.276 sec)\n",
            "train - step 19611: loss = 1694.15 (0.281 sec)\n",
            "train - step 19612: loss = 1701.30 (0.284 sec)\n",
            "train - step 19613: loss = 2030.14 (0.268 sec)\n",
            "train - step 19614: loss = 1528.90 (0.286 sec)\n",
            "train - step 19615: loss = 1653.44 (0.279 sec)\n",
            "train - step 19616: loss = 1291.95 (0.269 sec)\n",
            "train - step 19617: loss = 1769.73 (0.262 sec)\n",
            "train - step 19618: loss = 1999.24 (0.262 sec)\n",
            "train - step 19619: loss = 1568.92 (0.268 sec)\n",
            "train - step 19620: loss = 1738.50 (0.272 sec)\n",
            "train - step 19621: loss = 1630.74 (0.268 sec)\n",
            "train - step 19622: loss = 1681.16 (0.267 sec)\n",
            "train - step 19623: loss = 1625.09 (0.260 sec)\n",
            "train - step 19624: loss = 1824.78 (0.264 sec)\n",
            "train - step 19625: loss = 1761.91 (0.269 sec)\n",
            "train - step 19626: loss = 1791.01 (0.272 sec)\n",
            "train - step 19627: loss = 1710.66 (0.260 sec)\n",
            "train - step 19628: loss = 1916.76 (0.256 sec)\n",
            "train - step 19629: loss = 1978.89 (0.260 sec)\n",
            "train - step 19630: loss = 1963.44 (0.266 sec)\n",
            "train - step 19631: loss = 2078.88 (0.268 sec)\n",
            "train - step 19632: loss = 2229.58 (0.263 sec)\n",
            "train - step 19633: loss = 1901.39 (0.272 sec)\n",
            "train - step 19634: loss = 1665.74 (0.268 sec)\n",
            "train - step 19635: loss = 1891.80 (0.273 sec)\n",
            "train - step 19636: loss = 1274.75 (0.261 sec)\n",
            "train - step 19637: loss = 1158.67 (0.270 sec)\n",
            "train - step 19638: loss = 2363.29 (0.268 sec)\n",
            "train - step 19639: loss = 2024.54 (0.270 sec)\n",
            "train - step 19640: loss = 1565.61 (0.266 sec)\n",
            "train - step 19641: loss = 2303.34 (0.258 sec)\n",
            "train - step 19642: loss = 1782.78 (0.268 sec)\n",
            "train - step 19643: loss = 2137.98 (0.266 sec)\n",
            "train - step 19644: loss = 1900.72 (0.275 sec)\n",
            "train - step 19645: loss = 1392.93 (0.264 sec)\n",
            "train - step 19646: loss = 1529.97 (0.263 sec)\n",
            "train - step 19647: loss = 2188.00 (0.269 sec)\n",
            "train - step 19648: loss = 1045.09 (0.266 sec)\n",
            "train - step 19649: loss = 1867.07 (0.266 sec)\n",
            "train - step 19650: loss = 1606.96 (0.261 sec)\n",
            "train - step 19651: loss = 1612.82 (0.271 sec)\n",
            "train - step 19652: loss = 1745.02 (0.262 sec)\n",
            "train - step 19653: loss = 1593.93 (0.265 sec)\n",
            "train - step 19654: loss = 2052.77 (0.275 sec)\n",
            "train - step 19655: loss = 1967.02 (0.267 sec)\n",
            "train - step 19656: loss = 1469.11 (0.267 sec)\n",
            "train - step 19657: loss = 1932.81 (0.260 sec)\n",
            "train - step 19658: loss = 1554.40 (0.275 sec)\n",
            "train - step 19659: loss = 2074.84 (0.263 sec)\n",
            "train - step 19660: loss = 1758.78 (0.267 sec)\n",
            "train - step 19661: loss = 1617.87 (0.262 sec)\n",
            "train - step 19662: loss = 1640.44 (0.271 sec)\n",
            "train - step 19663: loss = 1978.11 (0.260 sec)\n",
            "train - step 19664: loss = 1567.39 (0.265 sec)\n",
            "train - step 19665: loss = 990.49 (0.271 sec)\n",
            "train - step 19666: loss = 1931.58 (0.269 sec)\n",
            "train - step 19667: loss = 1854.65 (0.260 sec)\n",
            "train - step 19668: loss = 1717.05 (0.269 sec)\n",
            "train - step 19669: loss = 1793.06 (0.252 sec)\n",
            "train - step 19670: loss = 1553.18 (0.267 sec)\n",
            "train - step 19671: loss = 455.61 (0.260 sec)\n",
            "train - step 19672: loss = 1588.94 (0.266 sec)\n",
            "train - step 19673: loss = 2421.16 (0.261 sec)\n",
            "train - step 19674: loss = 1766.16 (0.277 sec)\n",
            "train - step 19675: loss = 1732.04 (0.283 sec)\n",
            "train - step 19676: loss = 1528.07 (0.275 sec)\n",
            "train - step 19677: loss = 1128.10 (0.257 sec)\n",
            "train - step 19678: loss = 1747.99 (0.266 sec)\n",
            "train - step 19679: loss = 1624.81 (0.270 sec)\n",
            "train - step 19680: loss = 1774.99 (0.263 sec)\n",
            "train - step 19681: loss = 1743.21 (0.266 sec)\n",
            "train - step 19682: loss = 1436.02 (0.270 sec)\n",
            "train - step 19683: loss = 1344.86 (0.267 sec)\n",
            "train - step 19684: loss = 1605.35 (0.267 sec)\n",
            "train - step 19685: loss = 1234.44 (0.271 sec)\n",
            "train - step 19686: loss = 1186.66 (0.271 sec)\n",
            "train - step 19687: loss = 1899.49 (0.272 sec)\n",
            "train - step 19688: loss = 2151.83 (0.259 sec)\n",
            "train - step 19689: loss = 1611.62 (0.264 sec)\n",
            "train - step 19690: loss = 1739.79 (0.264 sec)\n",
            "train - step 19691: loss = 1518.91 (0.266 sec)\n",
            "train - step 19692: loss = 1718.59 (0.275 sec)\n",
            "train - step 19693: loss = 1836.80 (0.269 sec)\n",
            "train - step 19694: loss = 1614.44 (0.274 sec)\n",
            "train - step 19695: loss = 2145.10 (0.255 sec)\n",
            "train - step 19696: loss = 1917.99 (0.265 sec)\n",
            "train - step 19697: loss = 1887.47 (0.268 sec)\n",
            "train - step 19698: loss = 1765.25 (0.266 sec)\n",
            "train - step 19699: loss = 1608.43 (0.264 sec)\n",
            "train - step 19700: loss = 2095.27 (0.266 sec)\n",
            "train - step 19701: loss = 1739.90 (0.265 sec)\n",
            "train - step 19702: loss = 1338.62 (0.262 sec)\n",
            "train - step 19703: loss = 1949.83 (0.265 sec)\n",
            "train - step 19704: loss = 1667.08 (0.260 sec)\n",
            "train - step 19705: loss = 1822.22 (0.261 sec)\n",
            "train - step 19706: loss = 1039.61 (0.272 sec)\n",
            "train - step 19707: loss = 1710.91 (0.273 sec)\n",
            "train - step 19708: loss = 1465.48 (0.262 sec)\n",
            "train - step 19709: loss = 1964.43 (0.266 sec)\n",
            "train - step 19710: loss = 1674.91 (0.265 sec)\n",
            "train - step 19711: loss = 1865.03 (0.272 sec)\n",
            "train - step 19712: loss = 1697.60 (0.276 sec)\n",
            "train - step 19713: loss = 1649.18 (0.268 sec)\n",
            "train - step 19714: loss = 1943.60 (0.267 sec)\n",
            "train - step 19715: loss = 1657.25 (0.266 sec)\n",
            "train - step 19716: loss = 1709.86 (0.276 sec)\n",
            "train - step 19717: loss = 1561.76 (0.262 sec)\n",
            "train - step 19718: loss = 2118.01 (0.265 sec)\n",
            "train - step 19719: loss = 1667.72 (0.265 sec)\n",
            "train - step 19720: loss = 1912.55 (0.269 sec)\n",
            "train - step 19721: loss = 1817.24 (0.261 sec)\n",
            "train - step 19722: loss = 1803.19 (0.268 sec)\n",
            "train - step 19723: loss = 1670.62 (0.268 sec)\n",
            "train - step 19724: loss = 1914.35 (0.277 sec)\n",
            "train - step 19725: loss = 1802.67 (0.265 sec)\n",
            "train - step 19726: loss = 1223.79 (0.264 sec)\n",
            "train - step 19727: loss = 1846.95 (0.266 sec)\n",
            "train - step 19728: loss = 1767.99 (0.271 sec)\n",
            "train - step 19729: loss = 1644.12 (0.270 sec)\n",
            "train - step 19730: loss = 1882.98 (0.269 sec)\n",
            "train - step 19731: loss = 1559.38 (0.270 sec)\n",
            "train - step 19732: loss = 1732.42 (0.267 sec)\n",
            "train - step 19733: loss = 1703.96 (0.267 sec)\n",
            "train - step 19734: loss = 1818.96 (0.260 sec)\n",
            "train - step 19735: loss = 1724.63 (0.290 sec)\n",
            "train - step 19736: loss = 1862.24 (0.269 sec)\n",
            "train - step 19737: loss = 2049.61 (0.269 sec)\n",
            "train - step 19738: loss = 1670.45 (0.270 sec)\n",
            "train - step 19739: loss = 2014.59 (0.264 sec)\n",
            "train - step 19740: loss = 2001.25 (0.264 sec)\n",
            "train - step 19741: loss = 1737.56 (0.267 sec)\n",
            "train - step 19742: loss = 1866.99 (0.262 sec)\n",
            "train - step 19743: loss = 1483.96 (0.267 sec)\n",
            "train - step 19744: loss = 1732.79 (0.265 sec)\n",
            "train - step 19745: loss = 1803.28 (0.266 sec)\n",
            "train - step 19746: loss = 1688.35 (0.264 sec)\n",
            "train - step 19747: loss = 1673.04 (0.275 sec)\n",
            "train - step 19748: loss = 2130.46 (0.261 sec)\n",
            "train - step 19749: loss = 1397.08 (0.264 sec)\n",
            "train - step 19750: loss = 1483.53 (0.261 sec)\n",
            "train - step 19751: loss = 813.27 (0.264 sec)\n",
            "train - step 19752: loss = 1800.96 (0.262 sec)\n",
            "train - step 19753: loss = 1547.14 (0.265 sec)\n",
            "train - step 19754: loss = 1735.90 (0.268 sec)\n",
            "train - step 19755: loss = 1648.50 (0.271 sec)\n",
            "train - step 19756: loss = 1545.17 (0.268 sec)\n",
            "train - step 19757: loss = 1777.93 (0.264 sec)\n",
            "train - step 19758: loss = 1555.51 (0.278 sec)\n",
            "train - step 19759: loss = 2106.14 (0.269 sec)\n",
            "train - step 19760: loss = 2004.73 (0.268 sec)\n",
            "train - step 19761: loss = 1631.63 (0.261 sec)\n",
            "train - step 19762: loss = 1631.51 (0.275 sec)\n",
            "train - step 19763: loss = 1759.96 (0.262 sec)\n",
            "train - step 19764: loss = 1349.76 (0.271 sec)\n",
            "train - step 19765: loss = 1692.67 (0.264 sec)\n",
            "train - step 19766: loss = 1730.93 (0.265 sec)\n",
            "train - step 19767: loss = 1650.50 (0.266 sec)\n",
            "train - step 19768: loss = 1965.46 (0.267 sec)\n",
            "train - step 19769: loss = 1622.63 (0.261 sec)\n",
            "train - step 19770: loss = 1706.30 (0.270 sec)\n",
            "train - step 19771: loss = 1898.11 (0.257 sec)\n",
            "train - step 19772: loss = 1742.19 (0.268 sec)\n",
            "train - step 19773: loss = 1502.77 (0.268 sec)\n",
            "train - step 19774: loss = 1676.96 (0.276 sec)\n",
            "train - step 19775: loss = 1504.61 (0.274 sec)\n",
            "train - step 19776: loss = 1816.61 (0.268 sec)\n",
            "train - step 19777: loss = 1859.75 (0.263 sec)\n",
            "train - step 19778: loss = 1909.81 (0.256 sec)\n",
            "train - step 19779: loss = 1831.98 (0.261 sec)\n",
            "train - step 19780: loss = 1621.33 (0.263 sec)\n",
            "train - step 19781: loss = 1126.62 (0.269 sec)\n",
            "train - step 19782: loss = 1642.78 (0.260 sec)\n",
            "train - step 19783: loss = 1722.81 (0.266 sec)\n",
            "train - step 19784: loss = 1965.15 (0.270 sec)\n",
            "train - step 19785: loss = 1664.88 (0.266 sec)\n",
            "train - step 19786: loss = 1658.25 (0.270 sec)\n",
            "train - step 19787: loss = 2026.31 (0.264 sec)\n",
            "train - step 19788: loss = 1802.07 (0.262 sec)\n",
            "train - step 19789: loss = 1738.33 (0.268 sec)\n",
            "train - step 19790: loss = 2061.57 (0.267 sec)\n",
            "train - step 19791: loss = 1345.41 (0.265 sec)\n",
            "train - step 19792: loss = 1592.50 (0.911 sec)\n",
            "train - step 19793: loss = 1786.52 (0.260 sec)\n",
            "train - step 19794: loss = 1820.32 (0.265 sec)\n",
            "train - step 19795: loss = 1869.19 (0.263 sec)\n",
            "train - step 19796: loss = 1782.27 (0.272 sec)\n",
            "train - step 19797: loss = 1795.07 (0.263 sec)\n",
            "train - step 19798: loss = 1504.36 (0.264 sec)\n",
            "train - step 19799: loss = 1600.14 (0.266 sec)\n",
            "train - step 19800: loss = 1542.57 (0.266 sec)\n",
            "train - step 19801: loss = 1682.66 (0.265 sec)\n",
            "train - step 19802: loss = 2066.83 (0.269 sec)\n",
            "train - step 19803: loss = 1968.39 (0.262 sec)\n",
            "train - step 19804: loss = 1665.03 (0.273 sec)\n",
            "train - step 19805: loss = 523.23 (0.255 sec)\n",
            "train - step 19806: loss = 1607.06 (0.274 sec)\n",
            "train - step 19807: loss = 1910.19 (0.273 sec)\n",
            "train - step 19808: loss = 1778.10 (0.268 sec)\n",
            "train - step 19809: loss = 1492.39 (0.266 sec)\n",
            "train - step 19810: loss = 1939.44 (0.271 sec)\n",
            "train - step 19811: loss = 1965.12 (0.262 sec)\n",
            "train - step 19812: loss = 1810.45 (0.273 sec)\n",
            "train - step 19813: loss = 1850.09 (0.268 sec)\n",
            "train - step 19814: loss = 2181.10 (0.273 sec)\n",
            "train - step 19815: loss = 1325.60 (0.257 sec)\n",
            "train - step 19816: loss = 2041.73 (0.268 sec)\n",
            "train - step 19817: loss = 1976.35 (0.265 sec)\n",
            "train - step 19818: loss = 1460.54 (0.271 sec)\n",
            "train - step 19819: loss = 490.15 (0.262 sec)\n",
            "train - step 19820: loss = 1827.74 (0.264 sec)\n",
            "train - step 19821: loss = 1734.25 (0.259 sec)\n",
            "train - step 19822: loss = 1664.26 (0.266 sec)\n",
            "train - step 19823: loss = 1939.41 (0.267 sec)\n",
            "train - step 19824: loss = 1689.96 (0.269 sec)\n",
            "train - step 19825: loss = 1697.76 (0.260 sec)\n",
            "train - step 19826: loss = 2085.00 (0.266 sec)\n",
            "train - step 19827: loss = 1749.48 (0.265 sec)\n",
            "train - step 19828: loss = 2252.35 (0.264 sec)\n",
            "train - step 19829: loss = 1848.94 (0.265 sec)\n",
            "train - step 19830: loss = 1773.99 (0.263 sec)\n",
            "train - step 19831: loss = 1770.98 (0.266 sec)\n",
            "train - step 19832: loss = 1488.78 (0.267 sec)\n",
            "train - step 19833: loss = 1590.30 (0.267 sec)\n",
            "train - step 19834: loss = 1584.67 (0.269 sec)\n",
            "train - step 19835: loss = 2068.09 (0.256 sec)\n",
            "train - step 19836: loss = 1733.66 (0.271 sec)\n",
            "train - step 19837: loss = 1626.85 (0.262 sec)\n",
            "train - step 19838: loss = 2009.58 (0.269 sec)\n",
            "train - step 19839: loss = 1788.03 (0.270 sec)\n",
            "train - step 19840: loss = 1908.25 (0.272 sec)\n",
            "train - step 19841: loss = 1559.33 (0.265 sec)\n",
            "train - step 19842: loss = 1798.74 (0.270 sec)\n",
            "train - step 19843: loss = 1576.28 (0.266 sec)\n",
            "train - step 19844: loss = 1146.59 (0.269 sec)\n",
            "train - step 19845: loss = 1814.00 (0.261 sec)\n",
            "train - step 19846: loss = 1514.95 (0.268 sec)\n",
            "train - step 19847: loss = 1682.14 (0.270 sec)\n",
            "train - step 19848: loss = 1585.21 (0.262 sec)\n",
            "train - step 19849: loss = 1820.65 (0.261 sec)\n",
            "train - step 19850: loss = 1757.87 (0.261 sec)\n",
            "train - step 19851: loss = 1805.63 (0.264 sec)\n",
            "train - step 19852: loss = 1801.45 (0.272 sec)\n",
            "train - step 19853: loss = 1902.32 (0.268 sec)\n",
            "train - step 19854: loss = 1715.22 (0.267 sec)\n",
            "train - step 19855: loss = 1609.37 (0.266 sec)\n",
            "train - step 19856: loss = 1941.92 (0.272 sec)\n",
            "train - step 19857: loss = 1512.10 (0.267 sec)\n",
            "train - step 19858: loss = 1327.82 (0.266 sec)\n",
            "train - step 19859: loss = 2119.36 (0.267 sec)\n",
            "train - step 19860: loss = 1795.20 (0.270 sec)\n",
            "train - step 19861: loss = 1870.83 (0.267 sec)\n",
            "train - step 19862: loss = 1657.95 (0.261 sec)\n",
            "train - step 19863: loss = 1269.04 (0.267 sec)\n",
            "train - step 19864: loss = 1673.67 (0.282 sec)\n",
            "train - step 19865: loss = 1076.81 (0.255 sec)\n",
            "train - step 19866: loss = 1581.62 (0.260 sec)\n",
            "train - step 19867: loss = 1801.00 (0.270 sec)\n",
            "train - step 19868: loss = 1683.96 (0.276 sec)\n",
            "train - step 19869: loss = 1692.21 (0.267 sec)\n",
            "train - step 19870: loss = 1709.25 (0.266 sec)\n",
            "train - step 19871: loss = 2142.09 (0.259 sec)\n",
            "train - step 19872: loss = 1969.03 (0.269 sec)\n",
            "train - step 19873: loss = 1728.73 (0.266 sec)\n",
            "train - step 19874: loss = 1881.25 (0.261 sec)\n",
            "train - step 19875: loss = 1814.67 (0.270 sec)\n",
            "train - step 19876: loss = 1288.74 (0.269 sec)\n",
            "train - step 19877: loss = 1786.66 (0.270 sec)\n",
            "train - step 19878: loss = 1740.76 (0.274 sec)\n",
            "train - step 19879: loss = 1865.83 (0.280 sec)\n",
            "train - step 19880: loss = 448.70 (0.272 sec)\n",
            "train - step 19881: loss = 1675.33 (0.278 sec)\n",
            "train - step 19882: loss = 1537.61 (0.274 sec)\n",
            "train - step 19883: loss = 1978.28 (0.277 sec)\n",
            "train - step 19884: loss = 1315.41 (0.279 sec)\n",
            "train - step 19885: loss = 2050.52 (0.275 sec)\n",
            "train - step 19886: loss = 1614.65 (0.275 sec)\n",
            "train - step 19887: loss = 1772.58 (0.278 sec)\n",
            "train - step 19888: loss = 1785.64 (0.273 sec)\n",
            "train - step 19889: loss = 2264.79 (0.276 sec)\n",
            "train - step 19890: loss = 1509.31 (0.280 sec)\n",
            "train - step 19891: loss = 1710.06 (0.287 sec)\n",
            "train - step 19892: loss = 1832.09 (0.275 sec)\n",
            "train - step 19893: loss = 1964.47 (0.280 sec)\n",
            "train - step 19894: loss = 1676.06 (0.280 sec)\n",
            "train - step 19895: loss = 1895.09 (0.263 sec)\n",
            "train - step 19896: loss = 1785.13 (0.274 sec)\n",
            "train - step 19897: loss = 2028.99 (0.273 sec)\n",
            "train - step 19898: loss = 1877.30 (0.271 sec)\n",
            "train - step 19899: loss = 1428.27 (0.271 sec)\n",
            "train - step 19900: loss = 1975.13 (0.274 sec)\n",
            "train - step 19901: loss = 1765.65 (0.283 sec)\n",
            "train - step 19902: loss = 2173.06 (0.270 sec)\n",
            "train - step 19903: loss = 539.37 (0.274 sec)\n",
            "train - step 19904: loss = 1774.16 (0.276 sec)\n",
            "train - step 19905: loss = 1802.52 (0.266 sec)\n",
            "train - step 19906: loss = 1452.18 (0.278 sec)\n",
            "train - step 19907: loss = 1756.17 (0.271 sec)\n",
            "train - step 19908: loss = 1754.74 (0.276 sec)\n",
            "train - step 19909: loss = 2212.30 (0.279 sec)\n",
            "train - step 19910: loss = 2016.69 (0.279 sec)\n",
            "train - step 19911: loss = 2051.59 (0.278 sec)\n",
            "train - step 19912: loss = 1452.48 (0.280 sec)\n",
            "train - step 19913: loss = 1914.24 (0.277 sec)\n",
            "train - step 19914: loss = 1230.35 (0.272 sec)\n",
            "train - step 19915: loss = 2013.53 (0.267 sec)\n",
            "train - step 19916: loss = 1355.64 (0.268 sec)\n",
            "train - step 19917: loss = 2059.37 (0.262 sec)\n",
            "train - step 19918: loss = 2018.63 (0.261 sec)\n",
            "train - step 19919: loss = 1783.40 (0.266 sec)\n",
            "train - step 19920: loss = 2104.93 (0.255 sec)\n",
            "train - step 19921: loss = 1812.21 (0.266 sec)\n",
            "train - step 19922: loss = 1831.22 (0.264 sec)\n",
            "train - step 19923: loss = 1776.81 (0.258 sec)\n",
            "train - step 19924: loss = 1816.40 (0.267 sec)\n",
            "train - step 19925: loss = 1817.88 (0.261 sec)\n",
            "train - step 19926: loss = 1966.33 (0.263 sec)\n",
            "train - step 19927: loss = 1257.59 (0.271 sec)\n",
            "train - step 19928: loss = 1616.79 (0.271 sec)\n",
            "train - step 19929: loss = 1581.08 (0.270 sec)\n",
            "train - step 19930: loss = 1645.05 (0.271 sec)\n",
            "train - step 19931: loss = 1529.04 (0.281 sec)\n",
            "train - step 19932: loss = 2101.28 (0.275 sec)\n",
            "train - step 19933: loss = 1622.75 (0.260 sec)\n",
            "train - step 19934: loss = 1624.43 (0.263 sec)\n",
            "train - step 19935: loss = 1732.61 (0.260 sec)\n",
            "train - step 19936: loss = 1892.08 (0.269 sec)\n",
            "train - step 19937: loss = 1705.78 (0.263 sec)\n",
            "train - step 19938: loss = 1661.22 (0.260 sec)\n",
            "train - step 19939: loss = 1610.81 (0.264 sec)\n",
            "train - step 19940: loss = 1956.19 (0.272 sec)\n",
            "train - step 19941: loss = 1820.93 (0.266 sec)\n",
            "train - step 19942: loss = 1758.71 (0.265 sec)\n",
            "train - step 19943: loss = 2077.75 (0.257 sec)\n",
            "train - step 19944: loss = 1656.07 (0.274 sec)\n",
            "train - step 19945: loss = 1705.81 (0.269 sec)\n",
            "train - step 19946: loss = 1677.71 (0.269 sec)\n",
            "train - step 19947: loss = 2021.69 (0.263 sec)\n",
            "train - step 19948: loss = 1681.91 (0.267 sec)\n",
            "train - step 19949: loss = 1740.65 (0.268 sec)\n",
            "train - step 19950: loss = 1193.43 (0.272 sec)\n",
            "train - step 19951: loss = 1046.32 (0.267 sec)\n",
            "train - step 19952: loss = 1605.88 (0.266 sec)\n",
            "train - step 19953: loss = 1259.04 (0.263 sec)\n",
            "train - step 19954: loss = 1677.35 (0.263 sec)\n",
            "train - step 19955: loss = 1859.98 (0.270 sec)\n",
            "train - step 19956: loss = 1890.43 (0.266 sec)\n",
            "train - step 19957: loss = 1643.50 (0.267 sec)\n",
            "train - step 19958: loss = 1980.68 (0.267 sec)\n",
            "train - step 19959: loss = 1498.55 (0.272 sec)\n",
            "train - step 19960: loss = 1801.32 (0.266 sec)\n",
            "train - step 19961: loss = 1554.59 (0.259 sec)\n",
            "train - step 19962: loss = 1515.90 (0.263 sec)\n",
            "train - step 19963: loss = 1849.07 (0.270 sec)\n",
            "train - step 19964: loss = 1907.19 (0.266 sec)\n",
            "train - step 19965: loss = 2081.80 (0.260 sec)\n",
            "train - step 19966: loss = 2039.75 (0.263 sec)\n",
            "train - step 19967: loss = 1728.83 (0.280 sec)\n",
            "train - step 19968: loss = 2092.09 (0.281 sec)\n",
            "train - step 19969: loss = 1505.68 (0.286 sec)\n",
            "train - step 19970: loss = 1889.72 (0.278 sec)\n",
            "train - step 19971: loss = 1870.76 (0.277 sec)\n",
            "train - step 19972: loss = 1327.59 (0.281 sec)\n",
            "train - step 19973: loss = 2020.96 (0.279 sec)\n",
            "train - step 19974: loss = 2026.63 (0.287 sec)\n",
            "train - step 19975: loss = 1572.64 (0.278 sec)\n",
            "train - step 19976: loss = 1290.22 (0.277 sec)\n",
            "train - step 19977: loss = 1755.06 (0.280 sec)\n",
            "train - step 19978: loss = 1520.08 (0.287 sec)\n",
            "train - step 19979: loss = 1648.31 (0.282 sec)\n",
            "train - step 19980: loss = 1714.88 (0.284 sec)\n",
            "train - step 19981: loss = 1816.45 (0.285 sec)\n",
            "train - step 19982: loss = 1665.79 (0.278 sec)\n",
            "train - step 19983: loss = 1537.03 (0.274 sec)\n",
            "train - step 19984: loss = 1778.92 (0.282 sec)\n",
            "train - step 19985: loss = 2050.77 (0.280 sec)\n",
            "train - step 19986: loss = 1853.50 (0.280 sec)\n",
            "train - step 19987: loss = 1753.91 (0.279 sec)\n",
            "train - step 19988: loss = 1377.32 (0.279 sec)\n",
            "train - step 19989: loss = 1833.57 (0.278 sec)\n",
            "train - step 19990: loss = 1903.05 (0.271 sec)\n",
            "train - step 19991: loss = 1305.32 (0.288 sec)\n",
            "train - step 19992: loss = 1671.49 (0.281 sec)\n",
            "train - step 19993: loss = 2005.47 (0.276 sec)\n",
            "train - step 19994: loss = 1361.52 (0.279 sec)\n",
            "train - step 19995: loss = 1474.84 (0.281 sec)\n",
            "train - step 19996: loss = 1548.45 (0.287 sec)\n",
            "train - step 19997: loss = 1158.54 (0.272 sec)\n",
            "train - step 19998: loss = 2089.42 (0.276 sec)\n",
            "train - step 19999: loss = 1676.10 (0.279 sec)\n",
            "train - step 20000: loss = 1257.87 (0.270 sec)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\n",
            "  warnings.warn(\"Attempting to use a closed FileWriter. \"\n",
            "--- 1837.4986996650696 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Ae2btDOO9U",
        "colab_type": "text"
      },
      "source": [
        "batch_Size为1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCya6w-5WbPL",
        "colab_type": "code",
        "outputId": "c4fc720e-5dd9-494d-bdef-9aa1354475d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1297
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/test_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 14:58:45.676671: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-24 14:58:45.676907: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1d15340 executing computations on platform Host. Devices:\n",
            "2019-05-24 14:58:45.676942: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 14:58:45.846221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 14:58:45.846764: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1d14dc0 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 14:58:45.846804: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 14:58:45.847193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 14:58:45.847223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 14:58:46.330470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 14:58:46.330544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 14:58:46.330556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 14:58:46.330842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "restoring graph...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-24 14:58:55.618926: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2019-05-24 14:58:56.017006: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x16901a38\n",
            "Computed correspondences for pair: 80, 81. Took 6.016101 seconds\n",
            "Computed correspondences for pair: 80, 82. Took 2.890532 seconds\n",
            "Computed correspondences for pair: 80, 83. Took 3.311220 seconds\n",
            "Computed correspondences for pair: 80, 84. Took 2.922148 seconds\n",
            "Computed correspondences for pair: 80, 85. Took 3.986931 seconds\n",
            "Computed correspondences for pair: 80, 86. Took 3.761114 seconds\n",
            "Computed correspondences for pair: 80, 87. Took 6.035250 seconds\n",
            "Computed correspondences for pair: 80, 88. Took 3.293047 seconds\n",
            "Computed correspondences for pair: 80, 89. Took 5.107372 seconds\n",
            "Computed correspondences for pair: 80, 90. Took 6.089169 seconds\n",
            "Computed correspondences for pair: 80, 91. Took 6.346677 seconds\n",
            "Computed correspondences for pair: 80, 92. Took 6.040219 seconds\n",
            "Computed correspondences for pair: 80, 93. Took 6.960735 seconds\n",
            "Computed correspondences for pair: 80, 94. Took 4.605958 seconds\n",
            "Computed correspondences for pair: 80, 95. Took 4.432664 seconds\n",
            "Computed correspondences for pair: 80, 96. Took 4.660488 seconds\n",
            "Computed correspondences for pair: 80, 97. Took 7.622775 seconds\n",
            "Computed correspondences for pair: 80, 98. Took 5.290095 seconds\n",
            "Computed correspondences for pair: 80, 99. Took 5.170465 seconds\n",
            "Computed correspondences for pair: 81, 82. Took 3.583857 seconds\n",
            "Computed correspondences for pair: 81, 83. Took 3.332448 seconds\n",
            "Computed correspondences for pair: 81, 84. Took 3.541415 seconds\n",
            "Computed correspondences for pair: 81, 85. Took 4.793189 seconds\n",
            "Computed correspondences for pair: 81, 86. Took 3.594090 seconds\n",
            "Computed correspondences for pair: 81, 87. Took 5.066342 seconds\n",
            "Computed correspondences for pair: 81, 88. Took 3.606508 seconds\n",
            "Computed correspondences for pair: 81, 89. Took 3.558881 seconds\n",
            "Computed correspondences for pair: 81, 90. Took 6.023358 seconds\n",
            "Computed correspondences for pair: 81, 91. Took 6.000006 seconds\n",
            "Computed correspondences for pair: 81, 92. Took 4.814340 seconds\n",
            "Computed correspondences for pair: 81, 93. Took 6.963375 seconds\n",
            "Computed correspondences for pair: 81, 94. Took 4.645931 seconds\n",
            "Computed correspondences for pair: 81, 95. Took 4.723440 seconds\n",
            "Computed correspondences for pair: 81, 96. Took 4.725892 seconds\n",
            "Computed correspondences for pair: 81, 97. Took 6.277716 seconds\n",
            "Computed correspondences for pair: 81, 98. Took 4.776244 seconds\n",
            "Computed correspondences for pair: 81, 99. Took 4.689020 seconds\n",
            "Computed correspondences for pair: 82, 83. Took 3.506805 seconds\n",
            "Computed correspondences for pair: 82, 84. Took 3.017817 seconds\n",
            "Computed correspondences for pair: 82, 85. Took 3.963661 seconds\n",
            "Computed correspondences for pair: 82, 86. Took 3.629520 seconds\n",
            "Computed correspondences for pair: 82, 87. Took 5.816173 seconds\n",
            "Computed correspondences for pair: 82, 88. Took 3.848329 seconds\n",
            "Computed correspondences for pair: 82, 89. Took 3.767460 seconds\n",
            "Computed correspondences for pair: 82, 90. Took 6.400216 seconds\n",
            "Computed correspondences for pair: 82, 91. Took 8.163707 seconds\n",
            "              \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Computed correspondences for pair: 82, 92. Took 6.724294 seconds\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNhnnMBHXw9Z",
        "colab_type": "code",
        "outputId": "cc025049-56f0-48e9-851e-4820c2359b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "!python drive/unsupervisedfmnet/test_DFMnet.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 15:17:21.932314: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-24 15:17:21.932553: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ae91e0 executing computations on platform Host. Devices:\n",
            "2019-05-24 15:17:21.932589: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 15:17:22.099555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 15:17:22.100064: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ae8c60 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 15:17:22.100099: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 15:17:22.100496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 15:17:22.100524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 15:17:22.581245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 15:17:22.581310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 15:17:22.581324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 15:17:22.581616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "restoring graph...\n",
            "Traceback (most recent call last):\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 123, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 119, in main\n",
            "    run_test()\n",
            "  File \"drive/unsupervisedfmnet/test_DFMnet.py\", line 53, in run_test\n",
            "    saver = tf.train.import_meta_graph('%smodel.ckpt-%s.meta'% (FLAGS.log_dir, FLAGS.num_model))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\n",
            "    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1447, in _import_meta_graph_with_return_elements\n",
            "    meta_graph_def = meta_graph.read_meta_graph_file(meta_graph_or_file)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py\", line 633, in read_meta_graph_file\n",
            "    raise IOError(\"File %s does not exist.\" % filename)\n",
            "OSError: File drive/unsupervisedfmnet/Training/model.ckpt-5000.meta does not exist.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD3UFuKWYARj",
        "colab_type": "code",
        "outputId": "a6a6765c-4f04-428b-9ea2-0e36754ee3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 24 15:16:52 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfEARJ9xbjZ3",
        "colab_type": "code",
        "outputId": "2043a58f-7479-4956-dd1c-7125b92e4a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# 你可以更改目录名\n",
        "LOG_DIR = 'drive/unsupervisedfmnet/Training/'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-24 15:22:22--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.45.248.161, 54.174.228.92, 34.199.255.1, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.45.248.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  14.3MB/s    in 1.1s    \n",
            "\n",
            "2019-05-24 15:22:23 (14.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://891648ac.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8BIngHbcP3S",
        "colab_type": "code",
        "outputId": "4fd45b02-2cdf-46cb-bc99-978c83d9f52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# 输入图像维度\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# 将类别向量转换成二分类矩阵\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "tbCallBack = TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "                         write_images=True)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tbCallBack])\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.2549 - acc: 0.9217 - val_loss: 0.0573 - val_acc: 0.9818\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0882 - acc: 0.9734 - val_loss: 0.0503 - val_acc: 0.9839\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0656 - acc: 0.9806 - val_loss: 0.0323 - val_acc: 0.9893\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0542 - acc: 0.9835 - val_loss: 0.0347 - val_acc: 0.9886\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0463 - acc: 0.9862 - val_loss: 0.0290 - val_acc: 0.9905\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0415 - acc: 0.9873 - val_loss: 0.0299 - val_acc: 0.9898\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0378 - acc: 0.9882 - val_loss: 0.0276 - val_acc: 0.9911\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0336 - acc: 0.9893 - val_loss: 0.0287 - val_acc: 0.9907\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0295 - val_acc: 0.9907\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0289 - acc: 0.9910 - val_loss: 0.0292 - val_acc: 0.9914\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0296 - acc: 0.9913 - val_loss: 0.0279 - val_acc: 0.9906\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0286 - acc: 0.9913 - val_loss: 0.0265 - val_acc: 0.9916\n",
            "Test loss: 0.026451352184264396\n",
            "Test accuracy: 0.9916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELuPuY7fZPsO",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}